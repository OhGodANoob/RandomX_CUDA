//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-25769353
// Cuda compilation tools, release 10.1, V10.1.105
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_35
.address_size 64

	// .globl	_Z7init_vmILi2EEvPvS0_S0_
.const .align 1 .b8 blake2b_sigma[192] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 10, 4, 8, 9, 15, 13, 6, 1, 12, 0, 2, 11, 7, 5, 3, 11, 8, 12, 0, 5, 2, 15, 13, 10, 14, 3, 6, 7, 1, 9, 4, 7, 9, 3, 1, 13, 12, 11, 14, 2, 6, 5, 10, 4, 0, 15, 8, 9, 0, 5, 7, 2, 4, 10, 15, 14, 1, 11, 12, 6, 8, 3, 13, 2, 12, 6, 10, 0, 11, 8, 3, 4, 13, 7, 5, 15, 14, 1, 9, 12, 5, 1, 15, 14, 13, 4, 10, 0, 7, 6, 3, 9, 2, 8, 11, 13, 11, 7, 14, 12, 1, 3, 9, 5, 0, 15, 4, 8, 6, 2, 10, 6, 15, 14, 9, 11, 3, 0, 8, 12, 2, 13, 7, 1, 4, 10, 5, 10, 2, 8, 4, 7, 6, 1, 5, 15, 11, 9, 14, 3, 12, 13, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 10, 4, 8, 9, 15, 13, 6, 1, 12, 0, 2, 11, 7, 5, 3};
.const .align 4 .b8 AES_TABLE[8192] = {198, 99, 99, 165, 248, 124, 124, 132, 238, 119, 119, 153, 246, 123, 123, 141, 255, 242, 242, 13, 214, 107, 107, 189, 222, 111, 111, 177, 145, 197, 197, 84, 96, 48, 48, 80, 2, 1, 1, 3, 206, 103, 103, 169, 86, 43, 43, 125, 231, 254, 254, 25, 181, 215, 215, 98, 77, 171, 171, 230, 236, 118, 118, 154, 143, 202, 202, 69, 31, 130, 130, 157, 137, 201, 201, 64, 250, 125, 125, 135, 239, 250, 250, 21, 178, 89, 89, 235, 142, 71, 71, 201, 251, 240, 240, 11, 65, 173, 173, 236, 179, 212, 212, 103, 95, 162, 162, 253, 69, 175, 175, 234, 35, 156, 156, 191, 83, 164, 164, 247, 228, 114, 114, 150, 155, 192, 192, 91, 117, 183, 183, 194, 225, 253, 253, 28, 61, 147, 147, 174, 76, 38, 38, 106, 108, 54, 54, 90, 126, 63, 63, 65, 245, 247, 247, 2, 131, 204, 204, 79, 104, 52, 52, 92, 81, 165, 165, 244, 209, 229, 229, 52, 249, 241, 241, 8, 226, 113, 113, 147, 171, 216, 216, 115, 98, 49, 49, 83, 42, 21, 21, 63, 8, 4, 4, 12, 149, 199, 199, 82, 70, 35, 35, 101, 157, 195, 195, 94, 48, 24, 24, 40, 55, 150, 150, 161, 10, 5, 5, 15, 47, 154, 154, 181, 14, 7, 7, 9, 36, 18, 18, 54, 27, 128, 128, 155, 223, 226, 226, 61, 205, 235, 235, 38, 78, 39, 39, 105, 127, 178, 178, 205, 234, 117, 117, 159, 18, 9, 9, 27, 29, 131, 131, 158, 88, 44, 44, 116, 52, 26, 26, 46, 54, 27, 27, 45, 220, 110, 110, 178, 180, 90, 90, 238, 91, 160, 160, 251, 164, 82, 82, 246, 118, 59, 59, 77, 183, 214, 214, 97, 125, 179, 179, 206, 82, 41, 41, 123, 221, 227, 227, 62, 94, 47, 47, 113, 19, 132, 132, 151, 166, 83, 83, 245, 185, 209, 209, 104, 0, 0, 0, 0, 193, 237, 237, 44, 64, 32, 32, 96, 227, 252, 252, 31, 121, 177, 177, 200, 182, 91, 91, 237, 212, 106, 106, 190, 141, 203, 203, 70, 103, 190, 190, 217, 114, 57, 57, 75, 148, 74, 74, 222, 152, 76, 76, 212, 176, 88, 88, 232, 133, 207, 207, 74, 187, 208, 208, 107, 197, 239, 239, 42, 79, 170, 170, 229, 237, 251, 251, 22, 134, 67, 67, 197, 154, 77, 77, 215, 102, 51, 51, 85, 17, 133, 133, 148, 138, 69, 69, 207, 233, 249, 249, 16, 4, 2, 2, 6, 254, 127, 127, 129, 160, 80, 80, 240, 120, 60, 60, 68, 37, 159, 159, 186, 75, 168, 168, 227, 162, 81, 81, 243, 93, 163, 163, 254, 128, 64, 64, 192, 5, 143, 143, 138, 63, 146, 146, 173, 33, 157, 157, 188, 112, 56, 56, 72, 241, 245, 245, 4, 99, 188, 188, 223, 119, 182, 182, 193, 175, 218, 218, 117, 66, 33, 33, 99, 32, 16, 16, 48, 229, 255, 255, 26, 253, 243, 243, 14, 191, 210, 210, 109, 129, 205, 205, 76, 24, 12, 12, 20, 38, 19, 19, 53, 195, 236, 236, 47, 190, 95, 95, 225, 53, 151, 151, 162, 136, 68, 68, 204, 46, 23, 23, 57, 147, 196, 196, 87, 85, 167, 167, 242, 252, 126, 126, 130, 122, 61, 61, 71, 200, 100, 100, 172, 186, 93, 93, 231, 50, 25, 25, 43, 230, 115, 115, 149, 192, 96, 96, 160, 25, 129, 129, 152, 158, 79, 79, 209, 163, 220, 220, 127, 68, 34, 34, 102, 84, 42, 42, 126, 59, 144, 144, 171, 11, 136, 136, 131, 140, 70, 70, 202, 199, 238, 238, 41, 107, 184, 184, 211, 40, 20, 20, 60, 167, 222, 222, 121, 188, 94, 94, 226, 22, 11, 11, 29, 173, 219, 219, 118, 219, 224, 224, 59, 100, 50, 50, 86, 116, 58, 58, 78, 20, 10, 10, 30, 146, 73, 73, 219, 12, 6, 6, 10, 72, 36, 36, 108, 184, 92, 92, 228, 159, 194, 194, 93, 189, 211, 211, 110, 67, 172, 172, 239, 196, 98, 98, 166, 57, 145, 145, 168, 49, 149, 149, 164, 211, 228, 228, 55, 242, 121, 121, 139, 213, 231, 231, 50, 139, 200, 200, 67, 110, 55, 55, 89, 218, 109, 109, 183, 1, 141, 141, 140, 177, 213, 213, 100, 156, 78, 78, 210, 73, 169, 169, 224, 216, 108, 108, 180, 172, 86, 86, 250, 243, 244, 244, 7, 207, 234, 234, 37, 202, 101, 101, 175, 244, 122, 122, 142, 71, 174, 174, 233, 16, 8, 8, 24, 111, 186, 186, 213, 240, 120, 120, 136, 74, 37, 37, 111, 92, 46, 46, 114, 56, 28, 28, 36, 87, 166, 166, 241, 115, 180, 180, 199, 151, 198, 198, 81, 203, 232, 232, 35, 161, 221, 221, 124, 232, 116, 116, 156, 62, 31, 31, 33, 150, 75, 75, 221, 97, 189, 189, 220, 13, 139, 139, 134, 15, 138, 138, 133, 224, 112, 112, 144, 124, 62, 62, 66, 113, 181, 181, 196, 204, 102, 102, 170, 144, 72, 72, 216, 6, 3, 3, 5, 247, 246, 246, 1, 28, 14, 14, 18, 194, 97, 97, 163, 106, 53, 53, 95, 174, 87, 87, 249, 105, 185, 185, 208, 23, 134, 134, 145, 153, 193, 193, 88, 58, 29, 29, 39, 39, 158, 158, 185, 217, 225, 225, 56, 235, 248, 248, 19, 43, 152, 152, 179, 34, 17, 17, 51, 210, 105, 105, 187, 169, 217, 217, 112, 7, 142, 142, 137, 51, 148, 148, 167, 45, 155, 155, 182, 60, 30, 30, 34, 21, 135, 135, 146, 201, 233, 233, 32, 135, 206, 206, 73, 170, 85, 85, 255, 80, 40, 40, 120, 165, 223, 223, 122, 3, 140, 140, 143, 89, 161, 161, 248, 9, 137, 137, 128, 26, 13, 13, 23, 101, 191, 191, 218, 215, 230, 230, 49, 132, 66, 66, 198, 208, 104, 104, 184, 130, 65, 65, 195, 41, 153, 153, 176, 90, 45, 45, 119, 30, 15, 15, 17, 123, 176, 176, 203, 168, 84, 84, 252, 109, 187, 187, 214, 44, 22, 22, 58, 165, 198, 99, 99, 132, 248, 124, 124, 153, 238, 119, 119, 141, 246, 123, 123, 13, 255, 242, 242, 189, 214, 107, 107, 177, 222, 111, 111, 84, 145, 197, 197, 80, 96, 48, 48, 3, 2, 1, 1, 169, 206, 103, 103, 125, 86, 43, 43, 25, 231, 254, 254, 98, 181, 215, 215, 230, 77, 171, 171, 154, 236, 118, 118, 69, 143, 202, 202, 157, 31, 130, 130, 64, 137, 201, 201, 135, 250, 125, 125, 21, 239, 250, 250, 235, 178, 89, 89, 201, 142, 71, 71, 11, 251, 240, 240, 236, 65, 173, 173, 103, 179, 212, 212, 253, 95, 162, 162, 234, 69, 175, 175, 191, 35, 156, 156, 247, 83, 164, 164, 150, 228, 114, 114, 91, 155, 192, 192, 194, 117, 183, 183, 28, 225, 253, 253, 174, 61, 147, 147, 106, 76, 38, 38, 90, 108, 54, 54, 65, 126, 63, 63, 2, 245, 247, 247, 79, 131, 204, 204, 92, 104, 52, 52, 244, 81, 165, 165, 52, 209, 229, 229, 8, 249, 241, 241, 147, 226, 113, 113, 115, 171, 216, 216, 83, 98, 49, 49, 63, 42, 21, 21, 12, 8, 4, 4, 82, 149, 199, 199, 101, 70, 35, 35, 94, 157, 195, 195, 40, 48, 24, 24, 161, 55, 150, 150, 15, 10, 5, 5, 181, 47, 154, 154, 9, 14, 7, 7, 54, 36, 18, 18, 155, 27, 128, 128, 61, 223, 226, 226, 38, 205, 235, 235, 105, 78, 39, 39, 205, 127, 178, 178, 159, 234, 117, 117, 27, 18, 9, 9, 158, 29, 131, 131, 116, 88, 44, 44, 46, 52, 26, 26, 45, 54, 27, 27, 178, 220, 110, 110, 238, 180, 90, 90, 251, 91, 160, 160, 246, 164, 82, 82, 77, 118, 59, 59, 97, 183, 214, 214, 206, 125, 179, 179, 123, 82, 41, 41, 62, 221, 227, 227, 113, 94, 47, 47, 151, 19, 132, 132, 245, 166, 83, 83, 104, 185, 209, 209, 0, 0, 0, 0, 44, 193, 237, 237, 96, 64, 32, 32, 31, 227, 252, 252, 200, 121, 177, 177, 237, 182, 91, 91, 190, 212, 106, 106, 70, 141, 203, 203, 217, 103, 190, 190, 75, 114, 57, 57, 222, 148, 74, 74, 212, 152, 76, 76, 232, 176, 88, 88, 74, 133, 207, 207, 107, 187, 208, 208, 42, 197, 239, 239, 229, 79, 170, 170, 22, 237, 251, 251, 197, 134, 67, 67, 215, 154, 77, 77, 85, 102, 51, 51, 148, 17, 133, 133, 207, 138, 69, 69, 16, 233, 249, 249, 6, 4, 2, 2, 129, 254, 127, 127, 240, 160, 80, 80, 68, 120, 60, 60, 186, 37, 159, 159, 227, 75, 168, 168, 243, 162, 81, 81, 254, 93, 163, 163, 192, 128, 64, 64, 138, 5, 143, 143, 173, 63, 146, 146, 188, 33, 157, 157, 72, 112, 56, 56, 4, 241, 245, 245, 223, 99, 188, 188, 193, 119, 182, 182, 117, 175, 218, 218, 99, 66, 33, 33, 48, 32, 16, 16, 26, 229, 255, 255, 14, 253, 243, 243, 109, 191, 210, 210, 76, 129, 205, 205, 20, 24, 12, 12, 53, 38, 19, 19, 47, 195, 236, 236, 225, 190, 95, 95, 162, 53, 151, 151, 204, 136, 68, 68, 57, 46, 23, 23, 87, 147, 196, 196, 242, 85, 167, 167, 130, 252, 126, 126, 71, 122, 61, 61, 172, 200, 100, 100, 231, 186, 93, 93, 43, 50, 25, 25, 149, 230, 115, 115, 160, 192, 96, 96, 152, 25, 129, 129, 209, 158, 79, 79, 127, 163, 220, 220, 102, 68, 34, 34, 126, 84, 42, 42, 171, 59, 144, 144, 131, 11, 136, 136, 202, 140, 70, 70, 41, 199, 238, 238, 211, 107, 184, 184, 60, 40, 20, 20, 121, 167, 222, 222, 226, 188, 94, 94, 29, 22, 11, 11, 118, 173, 219, 219, 59, 219, 224, 224, 86, 100, 50, 50, 78, 116, 58, 58, 30, 20, 10, 10, 219, 146, 73, 73, 10, 12, 6, 6, 108, 72, 36, 36, 228, 184, 92, 92, 93, 159, 194, 194, 110, 189, 211, 211, 239, 67, 172, 172, 166, 196, 98, 98, 168, 57, 145, 145, 164, 49, 149, 149, 55, 211, 228, 228, 139, 242, 121, 121, 50, 213, 231, 231, 67, 139, 200, 200, 89, 110, 55, 55, 183, 218, 109, 109, 140, 1, 141, 141, 100, 177, 213, 213, 210, 156, 78, 78, 224, 73, 169, 169, 180, 216, 108, 108, 250, 172, 86, 86, 7, 243, 244, 244, 37, 207, 234, 234, 175, 202, 101, 101, 142, 244, 122, 122, 233, 71, 174, 174, 24, 16, 8, 8, 213, 111, 186, 186, 136, 240, 120, 120, 111, 74, 37, 37, 114, 92, 46, 46, 36, 56, 28, 28, 241, 87, 166, 166, 199, 115, 180, 180, 81, 151, 198, 198, 35, 203, 232, 232, 124, 161, 221, 221, 156, 232, 116, 116, 33, 62, 31, 31, 221, 150, 75, 75, 220, 97, 189, 189, 134, 13, 139, 139, 133, 15, 138, 138, 144, 224, 112, 112, 66, 124, 62, 62, 196, 113, 181, 181, 170, 204, 102, 102, 216, 144, 72, 72, 5, 6, 3, 3, 1, 247, 246, 246, 18, 28, 14, 14, 163, 194, 97, 97, 95, 106, 53, 53, 249, 174, 87, 87, 208, 105, 185, 185, 145, 23, 134, 134, 88, 153, 193, 193, 39, 58, 29, 29, 185, 39, 158, 158, 56, 217, 225, 225, 19, 235, 248, 248, 179, 43, 152, 152, 51, 34, 17, 17, 187, 210, 105, 105, 112, 169, 217, 217, 137, 7, 142, 142, 167, 51, 148, 148, 182, 45, 155, 155, 34, 60, 30, 30, 146, 21, 135, 135, 32, 201, 233, 233, 73, 135, 206, 206, 255, 170, 85, 85, 120, 80, 40, 40, 122, 165, 223, 223, 143, 3, 140, 140, 248, 89, 161, 161, 128, 9, 137, 137, 23, 26, 13, 13, 218, 101, 191, 191, 49, 215, 230, 230, 198, 132, 66, 66, 184, 208, 104, 104, 195, 130, 65, 65, 176, 41, 153, 153, 119, 90, 45, 45, 17, 30, 15, 15, 203, 123, 176, 176, 252, 168, 84, 84, 214, 109, 187, 187, 58, 44, 22, 22, 99, 165, 198, 99, 124, 132, 248, 124, 119, 153, 238, 119, 123, 141, 246, 123, 242, 13, 255, 242, 107, 189, 214, 107, 111, 177, 222, 111, 197, 84, 145, 197, 48, 80, 96, 48, 1, 3, 2, 1, 103, 169, 206, 103, 43, 125, 86, 43, 254, 25, 231, 254, 215, 98, 181, 215, 171, 230, 77, 171, 118, 154, 236, 118, 202, 69, 143, 202, 130, 157, 31, 130, 201, 64, 137, 201, 125, 135, 250, 125, 250, 21, 239, 250, 89, 235, 178, 89, 71, 201, 142, 71, 240, 11, 251, 240, 173, 236, 65, 173, 212, 103, 179, 212, 162, 253, 95, 162, 175, 234, 69, 175, 156, 191, 35, 156, 164, 247, 83, 164, 114, 150, 228, 114, 192, 91, 155, 192, 183, 194, 117, 183, 253, 28, 225, 253, 147, 174, 61, 147, 38, 106, 76, 38, 54, 90, 108, 54, 63, 65, 126, 63, 247, 2, 245, 247, 204, 79, 131, 204, 52, 92, 104, 52, 165, 244, 81, 165, 229, 52, 209, 229, 241, 8, 249, 241, 113, 147, 226, 113, 216, 115, 171, 216, 49, 83, 98, 49, 21, 63, 42, 21, 4, 12, 8, 4, 199, 82, 149, 199, 35, 101, 70, 35, 195, 94, 157, 195, 24, 40, 48, 24, 150, 161, 55, 150, 5, 15, 10, 5, 154, 181, 47, 154, 7, 9, 14, 7, 18, 54, 36, 18, 128, 155, 27, 128, 226, 61, 223, 226, 235, 38, 205, 235, 39, 105, 78, 39, 178, 205, 127, 178, 117, 159, 234, 117, 9, 27, 18, 9, 131, 158, 29, 131, 44, 116, 88, 44, 26, 46, 52, 26, 27, 45, 54, 27, 110, 178, 220, 110, 90, 238, 180, 90, 160, 251, 91, 160, 82, 246, 164, 82, 59, 77, 118, 59, 214, 97, 183, 214, 179, 206, 125, 179, 41, 123, 82, 41, 227, 62, 221, 227, 47, 113, 94, 47, 132, 151, 19, 132, 83, 245, 166, 83, 209, 104, 185, 209, 0, 0, 0, 0, 237, 44, 193, 237, 32, 96, 64, 32, 252, 31, 227, 252, 177, 200, 121, 177, 91, 237, 182, 91, 106, 190, 212, 106, 203, 70, 141, 203, 190, 217, 103, 190, 57, 75, 114, 57, 74, 222, 148, 74, 76, 212, 152, 76, 88, 232, 176, 88, 207, 74, 133, 207, 208, 107, 187, 208, 239, 42, 197, 239, 170, 229, 79, 170, 251, 22, 237, 251, 67, 197, 134, 67, 77, 215, 154, 77, 51, 85, 102, 51, 133, 148, 17, 133, 69, 207, 138, 69, 249, 16, 233, 249, 2, 6, 4, 2, 127, 129, 254, 127, 80, 240, 160, 80, 60, 68, 120, 60, 159, 186, 37, 159, 168, 227, 75, 168, 81, 243, 162, 81, 163, 254, 93, 163, 64, 192, 128, 64, 143, 138, 5, 143, 146, 173, 63, 146, 157, 188, 33, 157, 56, 72, 112, 56, 245, 4, 241, 245, 188, 223, 99, 188, 182, 193, 119, 182, 218, 117, 175, 218, 33, 99, 66, 33, 16, 48, 32, 16, 255, 26, 229, 255, 243, 14, 253, 243, 210, 109, 191, 210, 205, 76, 129, 205, 12, 20, 24, 12, 19, 53, 38, 19, 236, 47, 195, 236, 95, 225, 190, 95, 151, 162, 53, 151, 68, 204, 136, 68, 23, 57, 46, 23, 196, 87, 147, 196, 167, 242, 85, 167, 126, 130, 252, 126, 61, 71, 122, 61, 100, 172, 200, 100, 93, 231, 186, 93, 25, 43, 50, 25, 115, 149, 230, 115, 96, 160, 192, 96, 129, 152, 25, 129, 79, 209, 158, 79, 220, 127, 163, 220, 34, 102, 68, 34, 42, 126, 84, 42, 144, 171, 59, 144, 136, 131, 11, 136, 70, 202, 140, 70, 238, 41, 199, 238, 184, 211, 107, 184, 20, 60, 40, 20, 222, 121, 167, 222, 94, 226, 188, 94, 11, 29, 22, 11, 219, 118, 173, 219, 224, 59, 219, 224, 50, 86, 100, 50, 58, 78, 116, 58, 10, 30, 20, 10, 73, 219, 146, 73, 6, 10, 12, 6, 36, 108, 72, 36, 92, 228, 184, 92, 194, 93, 159, 194, 211, 110, 189, 211, 172, 239, 67, 172, 98, 166, 196, 98, 145, 168, 57, 145, 149, 164, 49, 149, 228, 55, 211, 228, 121, 139, 242, 121, 231, 50, 213, 231, 200, 67, 139, 200, 55, 89, 110, 55, 109, 183, 218, 109, 141, 140, 1, 141, 213, 100, 177, 213, 78, 210, 156, 78, 169, 224, 73, 169, 108, 180, 216, 108, 86, 250, 172, 86, 244, 7, 243, 244, 234, 37, 207, 234, 101, 175, 202, 101, 122, 142, 244, 122, 174, 233, 71, 174, 8, 24, 16, 8, 186, 213, 111, 186, 120, 136, 240, 120, 37, 111, 74, 37, 46, 114, 92, 46, 28, 36, 56, 28, 166, 241, 87, 166, 180, 199, 115, 180, 198, 81, 151, 198, 232, 35, 203, 232, 221, 124, 161, 221, 116, 156, 232, 116, 31, 33, 62, 31, 75, 221, 150, 75, 189, 220, 97, 189, 139, 134, 13, 139, 138, 133, 15, 138, 112, 144, 224, 112, 62, 66, 124, 62, 181, 196, 113, 181, 102, 170, 204, 102, 72, 216, 144, 72, 3, 5, 6, 3, 246, 1, 247, 246, 14, 18, 28, 14, 97, 163, 194, 97, 53, 95, 106, 53, 87, 249, 174, 87, 185, 208, 105, 185, 134, 145, 23, 134, 193, 88, 153, 193, 29, 39, 58, 29, 158, 185, 39, 158, 225, 56, 217, 225, 248, 19, 235, 248, 152, 179, 43, 152, 17, 51, 34, 17, 105, 187, 210, 105, 217, 112, 169, 217, 142, 137, 7, 142, 148, 167, 51, 148, 155, 182, 45, 155, 30, 34, 60, 30, 135, 146, 21, 135, 233, 32, 201, 233, 206, 73, 135, 206, 85, 255, 170, 85, 40, 120, 80, 40, 223, 122, 165, 223, 140, 143, 3, 140, 161, 248, 89, 161, 137, 128, 9, 137, 13, 23, 26, 13, 191, 218, 101, 191, 230, 49, 215, 230, 66, 198, 132, 66, 104, 184, 208, 104, 65, 195, 130, 65, 153, 176, 41, 153, 45, 119, 90, 45, 15, 17, 30, 15, 176, 203, 123, 176, 84, 252, 168, 84, 187, 214, 109, 187, 22, 58, 44, 22, 99, 99, 165, 198, 124, 124, 132, 248, 119, 119, 153, 238, 123, 123, 141, 246, 242, 242, 13, 255, 107, 107, 189, 214, 111, 111, 177, 222, 197, 197, 84, 145, 48, 48, 80, 96, 1, 1, 3, 2, 103, 103, 169, 206, 43, 43, 125, 86, 254, 254, 25, 231, 215, 215, 98, 181, 171, 171, 230, 77, 118, 118, 154, 236, 202, 202, 69, 143, 130, 130, 157, 31, 201, 201, 64, 137, 125, 125, 135, 250, 250, 250, 21, 239, 89, 89, 235, 178, 71, 71, 201, 142, 240, 240, 11, 251, 173, 173, 236, 65, 212, 212, 103, 179, 162, 162, 253, 95, 175, 175, 234, 69, 156, 156, 191, 35, 164, 164, 247, 83, 114, 114, 150, 228, 192, 192, 91, 155, 183, 183, 194, 117, 253, 253, 28, 225, 147, 147, 174, 61, 38, 38, 106, 76, 54, 54, 90, 108, 63, 63, 65, 126, 247, 247, 2, 245, 204, 204, 79, 131, 52, 52, 92, 104, 165, 165, 244, 81, 229, 229, 52, 209, 241, 241, 8, 249, 113, 113, 147, 226, 216, 216, 115, 171, 49, 49, 83, 98, 21, 21, 63, 42, 4, 4, 12, 8, 199, 199, 82, 149, 35, 35, 101, 70, 195, 195, 94, 157, 24, 24, 40, 48, 150, 150, 161, 55, 5, 5, 15, 10, 154, 154, 181, 47, 7, 7, 9, 14, 18, 18, 54, 36, 128, 128, 155, 27, 226, 226, 61, 223, 235, 235, 38, 205, 39, 39, 105, 78, 178, 178, 205, 127, 117, 117, 159, 234, 9, 9, 27, 18, 131, 131, 158, 29, 44, 44, 116, 88, 26, 26, 46, 52, 27, 27, 45, 54, 110, 110, 178, 220, 90, 90, 238, 180, 160, 160, 251, 91, 82, 82, 246, 164, 59, 59, 77, 118, 214, 214, 97, 183, 179, 179, 206, 125, 41, 41, 123, 82, 227, 227, 62, 221, 47, 47, 113, 94, 132, 132, 151, 19, 83, 83, 245, 166, 209, 209, 104, 185, 0, 0, 0, 0, 237, 237, 44, 193, 32, 32, 96, 64, 252, 252, 31, 227, 177, 177, 200, 121, 91, 91, 237, 182, 106, 106, 190, 212, 203, 203, 70, 141, 190, 190, 217, 103, 57, 57, 75, 114, 74, 74, 222, 148, 76, 76, 212, 152, 88, 88, 232, 176, 207, 207, 74, 133, 208, 208, 107, 187, 239, 239, 42, 197, 170, 170, 229, 79, 251, 251, 22, 237, 67, 67, 197, 134, 77, 77, 215, 154, 51, 51, 85, 102, 133, 133, 148, 17, 69, 69, 207, 138, 249, 249, 16, 233, 2, 2, 6, 4, 127, 127, 129, 254, 80, 80, 240, 160, 60, 60, 68, 120, 159, 159, 186, 37, 168, 168, 227, 75, 81, 81, 243, 162, 163, 163, 254, 93, 64, 64, 192, 128, 143, 143, 138, 5, 146, 146, 173, 63, 157, 157, 188, 33, 56, 56, 72, 112, 245, 245, 4, 241, 188, 188, 223, 99, 182, 182, 193, 119, 218, 218, 117, 175, 33, 33, 99, 66, 16, 16, 48, 32, 255, 255, 26, 229, 243, 243, 14, 253, 210, 210, 109, 191, 205, 205, 76, 129, 12, 12, 20, 24, 19, 19, 53, 38, 236, 236, 47, 195, 95, 95, 225, 190, 151, 151, 162, 53, 68, 68, 204, 136, 23, 23, 57, 46, 196, 196, 87, 147, 167, 167, 242, 85, 126, 126, 130, 252, 61, 61, 71, 122, 100, 100, 172, 200, 93, 93, 231, 186, 25, 25, 43, 50, 115, 115, 149, 230, 96, 96, 160, 192, 129, 129, 152, 25, 79, 79, 209, 158, 220, 220, 127, 163, 34, 34, 102, 68, 42, 42, 126, 84, 144, 144, 171, 59, 136, 136, 131, 11, 70, 70, 202, 140, 238, 238, 41, 199, 184, 184, 211, 107, 20, 20, 60, 40, 222, 222, 121, 167, 94, 94, 226, 188, 11, 11, 29, 22, 219, 219, 118, 173, 224, 224, 59, 219, 50, 50, 86, 100, 58, 58, 78, 116, 10, 10, 30, 20, 73, 73, 219, 146, 6, 6, 10, 12, 36, 36, 108, 72, 92, 92, 228, 184, 194, 194, 93, 159, 211, 211, 110, 189, 172, 172, 239, 67, 98, 98, 166, 196, 145, 145, 168, 57, 149, 149, 164, 49, 228, 228, 55, 211, 121, 121, 139, 242, 231, 231, 50, 213, 200, 200, 67, 139, 55, 55, 89, 110, 109, 109, 183, 218, 141, 141, 140, 1, 213, 213, 100, 177, 78, 78, 210, 156, 169, 169, 224, 73, 108, 108, 180, 216, 86, 86, 250, 172, 244, 244, 7, 243, 234, 234, 37, 207, 101, 101, 175, 202, 122, 122, 142, 244, 174, 174, 233, 71, 8, 8, 24, 16, 186, 186, 213, 111, 120, 120, 136, 240, 37, 37, 111, 74, 46, 46, 114, 92, 28, 28, 36, 56, 166, 166, 241, 87, 180, 180, 199, 115, 198, 198, 81, 151, 232, 232, 35, 203, 221, 221, 124, 161, 116, 116, 156, 232, 31, 31, 33, 62, 75, 75, 221, 150, 189, 189, 220, 97, 139, 139, 134, 13, 138, 138, 133, 15, 112, 112, 144, 224, 62, 62, 66, 124, 181, 181, 196, 113, 102, 102, 170, 204, 72, 72, 216, 144, 3, 3, 5, 6, 246, 246, 1, 247, 14, 14, 18, 28, 97, 97, 163, 194, 53, 53, 95, 106, 87, 87, 249, 174, 185, 185, 208, 105, 134, 134, 145, 23, 193, 193, 88, 153, 29, 29, 39, 58, 158, 158, 185, 39, 225, 225, 56, 217, 248, 248, 19, 235, 152, 152, 179, 43, 17, 17, 51, 34, 105, 105, 187, 210, 217, 217, 112, 169, 142, 142, 137, 7, 148, 148, 167, 51, 155, 155, 182, 45, 30, 30, 34, 60, 135, 135, 146, 21, 233, 233, 32, 201, 206, 206, 73, 135, 85, 85, 255, 170, 40, 40, 120, 80, 223, 223, 122, 165, 140, 140, 143, 3, 161, 161, 248, 89, 137, 137, 128, 9, 13, 13, 23, 26, 191, 191, 218, 101, 230, 230, 49, 215, 66, 66, 198, 132, 104, 104, 184, 208, 65, 65, 195, 130, 153, 153, 176, 41, 45, 45, 119, 90, 15, 15, 17, 30, 176, 176, 203, 123, 84, 84, 252, 168, 187, 187, 214, 109, 22, 22, 58, 44, 81, 244, 167, 80, 126, 65, 101, 83, 26, 23, 164, 195, 58, 39, 94, 150, 59, 171, 107, 203, 31, 157, 69, 241, 172, 250, 88, 171, 75, 227, 3, 147, 32, 48, 250, 85, 173, 118, 109, 246, 136, 204, 118, 145, 245, 2, 76, 37, 79, 229, 215, 252, 197, 42, 203, 215, 38, 53, 68, 128, 181, 98, 163, 143, 222, 177, 90, 73, 37, 186, 27, 103, 69, 234, 14, 152, 93, 254, 192, 225, 195, 47, 117, 2, 129, 76, 240, 18, 141, 70, 151, 163, 107, 211, 249, 198, 3, 143, 95, 231, 21, 146, 156, 149, 191, 109, 122, 235, 149, 82, 89, 218, 212, 190, 131, 45, 88, 116, 33, 211, 73, 224, 105, 41, 142, 201, 200, 68, 117, 194, 137, 106, 244, 142, 121, 120, 153, 88, 62, 107, 39, 185, 113, 221, 190, 225, 79, 182, 240, 136, 173, 23, 201, 32, 172, 102, 125, 206, 58, 180, 99, 223, 74, 24, 229, 26, 49, 130, 151, 81, 51, 96, 98, 83, 127, 69, 177, 100, 119, 224, 187, 107, 174, 132, 254, 129, 160, 28, 249, 8, 43, 148, 112, 72, 104, 88, 143, 69, 253, 25, 148, 222, 108, 135, 82, 123, 248, 183, 171, 115, 211, 35, 114, 75, 2, 226, 227, 31, 143, 87, 102, 85, 171, 42, 178, 235, 40, 7, 47, 181, 194, 3, 134, 197, 123, 154, 211, 55, 8, 165, 48, 40, 135, 242, 35, 191, 165, 178, 2, 3, 106, 186, 237, 22, 130, 92, 138, 207, 28, 43, 167, 121, 180, 146, 243, 7, 242, 240, 78, 105, 226, 161, 101, 218, 244, 205, 6, 5, 190, 213, 209, 52, 98, 31, 196, 166, 254, 138, 52, 46, 83, 157, 162, 243, 85, 160, 5, 138, 225, 50, 164, 246, 235, 117, 11, 131, 236, 57, 64, 96, 239, 170, 94, 113, 159, 6, 189, 110, 16, 81, 62, 33, 138, 249, 150, 221, 6, 61, 221, 62, 5, 174, 77, 230, 189, 70, 145, 84, 141, 181, 113, 196, 93, 5, 4, 6, 212, 111, 96, 80, 21, 255, 25, 152, 251, 36, 214, 189, 233, 151, 137, 64, 67, 204, 103, 217, 158, 119, 176, 232, 66, 189, 7, 137, 139, 136, 231, 25, 91, 56, 121, 200, 238, 219, 161, 124, 10, 71, 124, 66, 15, 233, 248, 132, 30, 201, 0, 0, 0, 0, 9, 128, 134, 131, 50, 43, 237, 72, 30, 17, 112, 172, 108, 90, 114, 78, 253, 14, 255, 251, 15, 133, 56, 86, 61, 174, 213, 30, 54, 45, 57, 39, 10, 15, 217, 100, 104, 92, 166, 33, 155, 91, 84, 209, 36, 54, 46, 58, 12, 10, 103, 177, 147, 87, 231, 15, 180, 238, 150, 210, 27, 155, 145, 158, 128, 192, 197, 79, 97, 220, 32, 162, 90, 119, 75, 105, 28, 18, 26, 22, 226, 147, 186, 10, 192, 160, 42, 229, 60, 34, 224, 67, 18, 27, 23, 29, 14, 9, 13, 11, 242, 139, 199, 173, 45, 182, 168, 185, 20, 30, 169, 200, 87, 241, 25, 133, 175, 117, 7, 76, 238, 153, 221, 187, 163, 127, 96, 253, 247, 1, 38, 159, 92, 114, 245, 188, 68, 102, 59, 197, 91, 251, 126, 52, 139, 67, 41, 118, 203, 35, 198, 220, 182, 237, 252, 104, 184, 228, 241, 99, 215, 49, 220, 202, 66, 99, 133, 16, 19, 151, 34, 64, 132, 198, 17, 32, 133, 74, 36, 125, 210, 187, 61, 248, 174, 249, 50, 17, 199, 41, 161, 109, 29, 158, 47, 75, 220, 178, 48, 243, 13, 134, 82, 236, 119, 193, 227, 208, 43, 179, 22, 108, 169, 112, 185, 153, 17, 148, 72, 250, 71, 233, 100, 34, 168, 252, 140, 196, 160, 240, 63, 26, 86, 125, 44, 216, 34, 51, 144, 239, 135, 73, 78, 199, 217, 56, 209, 193, 140, 202, 162, 254, 152, 212, 11, 54, 166, 245, 129, 207, 165, 122, 222, 40, 218, 183, 142, 38, 63, 173, 191, 164, 44, 58, 157, 228, 80, 120, 146, 13, 106, 95, 204, 155, 84, 126, 70, 98, 246, 141, 19, 194, 144, 216, 184, 232, 46, 57, 247, 94, 130, 195, 175, 245, 159, 93, 128, 190, 105, 208, 147, 124, 111, 213, 45, 169, 207, 37, 18, 179, 200, 172, 153, 59, 16, 24, 125, 167, 232, 156, 99, 110, 219, 59, 187, 123, 205, 38, 120, 9, 110, 89, 24, 244, 236, 154, 183, 1, 131, 79, 154, 168, 230, 149, 110, 101, 170, 255, 230, 126, 33, 188, 207, 8, 239, 21, 232, 230, 186, 231, 155, 217, 74, 111, 54, 206, 234, 159, 9, 212, 41, 176, 124, 214, 49, 164, 178, 175, 42, 63, 35, 49, 198, 165, 148, 48, 53, 162, 102, 192, 116, 78, 188, 55, 252, 130, 202, 166, 224, 144, 208, 176, 51, 167, 216, 21, 241, 4, 152, 74, 65, 236, 218, 247, 127, 205, 80, 14, 23, 145, 246, 47, 118, 77, 214, 141, 67, 239, 176, 77, 204, 170, 77, 84, 228, 150, 4, 223, 158, 209, 181, 227, 76, 106, 136, 27, 193, 44, 31, 184, 70, 101, 81, 127, 157, 94, 234, 4, 1, 140, 53, 93, 250, 135, 116, 115, 251, 11, 65, 46, 179, 103, 29, 90, 146, 219, 210, 82, 233, 16, 86, 51, 109, 214, 71, 19, 154, 215, 97, 140, 55, 161, 12, 122, 89, 248, 20, 142, 235, 19, 60, 137, 206, 169, 39, 238, 183, 97, 201, 53, 225, 28, 229, 237, 122, 71, 177, 60, 156, 210, 223, 89, 85, 242, 115, 63, 24, 20, 206, 121, 115, 199, 55, 191, 83, 247, 205, 234, 95, 253, 170, 91, 223, 61, 111, 20, 120, 68, 219, 134, 202, 175, 243, 129, 185, 104, 196, 62, 56, 36, 52, 44, 194, 163, 64, 95, 22, 29, 195, 114, 188, 226, 37, 12, 40, 60, 73, 139, 255, 13, 149, 65, 57, 168, 1, 113, 8, 12, 179, 222, 216, 180, 228, 156, 100, 86, 193, 144, 123, 203, 132, 97, 213, 50, 182, 112, 72, 108, 92, 116, 208, 184, 87, 66, 80, 81, 244, 167, 83, 126, 65, 101, 195, 26, 23, 164, 150, 58, 39, 94, 203, 59, 171, 107, 241, 31, 157, 69, 171, 172, 250, 88, 147, 75, 227, 3, 85, 32, 48, 250, 246, 173, 118, 109, 145, 136, 204, 118, 37, 245, 2, 76, 252, 79, 229, 215, 215, 197, 42, 203, 128, 38, 53, 68, 143, 181, 98, 163, 73, 222, 177, 90, 103, 37, 186, 27, 152, 69, 234, 14, 225, 93, 254, 192, 2, 195, 47, 117, 18, 129, 76, 240, 163, 141, 70, 151, 198, 107, 211, 249, 231, 3, 143, 95, 149, 21, 146, 156, 235, 191, 109, 122, 218, 149, 82, 89, 45, 212, 190, 131, 211, 88, 116, 33, 41, 73, 224, 105, 68, 142, 201, 200, 106, 117, 194, 137, 120, 244, 142, 121, 107, 153, 88, 62, 221, 39, 185, 113, 182, 190, 225, 79, 23, 240, 136, 173, 102, 201, 32, 172, 180, 125, 206, 58, 24, 99, 223, 74, 130, 229, 26, 49, 96, 151, 81, 51, 69, 98, 83, 127, 224, 177, 100, 119, 132, 187, 107, 174, 28, 254, 129, 160, 148, 249, 8, 43, 88, 112, 72, 104, 25, 143, 69, 253, 135, 148, 222, 108, 183, 82, 123, 248, 35, 171, 115, 211, 226, 114, 75, 2, 87, 227, 31, 143, 42, 102, 85, 171, 7, 178, 235, 40, 3, 47, 181, 194, 154, 134, 197, 123, 165, 211, 55, 8, 242, 48, 40, 135, 178, 35, 191, 165, 186, 2, 3, 106, 92, 237, 22, 130, 43, 138, 207, 28, 146, 167, 121, 180, 240, 243, 7, 242, 161, 78, 105, 226, 205, 101, 218, 244, 213, 6, 5, 190, 31, 209, 52, 98, 138, 196, 166, 254, 157, 52, 46, 83, 160, 162, 243, 85, 50, 5, 138, 225, 117, 164, 246, 235, 57, 11, 131, 236, 170, 64, 96, 239, 6, 94, 113, 159, 81, 189, 110, 16, 249, 62, 33, 138, 61, 150, 221, 6, 174, 221, 62, 5, 70, 77, 230, 189, 181, 145, 84, 141, 5, 113, 196, 93, 111, 4, 6, 212, 255, 96, 80, 21, 36, 25, 152, 251, 151, 214, 189, 233, 204, 137, 64, 67, 119, 103, 217, 158, 189, 176, 232, 66, 136, 7, 137, 139, 56, 231, 25, 91, 219, 121, 200, 238, 71, 161, 124, 10, 233, 124, 66, 15, 201, 248, 132, 30, 0, 0, 0, 0, 131, 9, 128, 134, 72, 50, 43, 237, 172, 30, 17, 112, 78, 108, 90, 114, 251, 253, 14, 255, 86, 15, 133, 56, 30, 61, 174, 213, 39, 54, 45, 57, 100, 10, 15, 217, 33, 104, 92, 166, 209, 155, 91, 84, 58, 36, 54, 46, 177, 12, 10, 103, 15, 147, 87, 231, 210, 180, 238, 150, 158, 27, 155, 145, 79, 128, 192, 197, 162, 97, 220, 32, 105, 90, 119, 75, 22, 28, 18, 26, 10, 226, 147, 186, 229, 192, 160, 42, 67, 60, 34, 224, 29, 18, 27, 23, 11, 14, 9, 13, 173, 242, 139, 199, 185, 45, 182, 168, 200, 20, 30, 169, 133, 87, 241, 25, 76, 175, 117, 7, 187, 238, 153, 221, 253, 163, 127, 96, 159, 247, 1, 38, 188, 92, 114, 245, 197, 68, 102, 59, 52, 91, 251, 126, 118, 139, 67, 41, 220, 203, 35, 198, 104, 182, 237, 252, 99, 184, 228, 241, 202, 215, 49, 220, 16, 66, 99, 133, 64, 19, 151, 34, 32, 132, 198, 17, 125, 133, 74, 36, 248, 210, 187, 61, 17, 174, 249, 50, 109, 199, 41, 161, 75, 29, 158, 47, 243, 220, 178, 48, 236, 13, 134, 82, 208, 119, 193, 227, 108, 43, 179, 22, 153, 169, 112, 185, 250, 17, 148, 72, 34, 71, 233, 100, 196, 168, 252, 140, 26, 160, 240, 63, 216, 86, 125, 44, 239, 34, 51, 144, 199, 135, 73, 78, 193, 217, 56, 209, 254, 140, 202, 162, 54, 152, 212, 11, 207, 166, 245, 129, 40, 165, 122, 222, 38, 218, 183, 142, 164, 63, 173, 191, 228, 44, 58, 157, 13, 80, 120, 146, 155, 106, 95, 204, 98, 84, 126, 70, 194, 246, 141, 19, 232, 144, 216, 184, 94, 46, 57, 247, 245, 130, 195, 175, 190, 159, 93, 128, 124, 105, 208, 147, 169, 111, 213, 45, 179, 207, 37, 18, 59, 200, 172, 153, 167, 16, 24, 125, 110, 232, 156, 99, 123, 219, 59, 187, 9, 205, 38, 120, 244, 110, 89, 24, 1, 236, 154, 183, 168, 131, 79, 154, 101, 230, 149, 110, 126, 170, 255, 230, 8, 33, 188, 207, 230, 239, 21, 232, 217, 186, 231, 155, 206, 74, 111, 54, 212, 234, 159, 9, 214, 41, 176, 124, 175, 49, 164, 178, 49, 42, 63, 35, 48, 198, 165, 148, 192, 53, 162, 102, 55, 116, 78, 188, 166, 252, 130, 202, 176, 224, 144, 208, 21, 51, 167, 216, 74, 241, 4, 152, 247, 65, 236, 218, 14, 127, 205, 80, 47, 23, 145, 246, 141, 118, 77, 214, 77, 67, 239, 176, 84, 204, 170, 77, 223, 228, 150, 4, 227, 158, 209, 181, 27, 76, 106, 136, 184, 193, 44, 31, 127, 70, 101, 81, 4, 157, 94, 234, 93, 1, 140, 53, 115, 250, 135, 116, 46, 251, 11, 65, 90, 179, 103, 29, 82, 146, 219, 210, 51, 233, 16, 86, 19, 109, 214, 71, 140, 154, 215, 97, 122, 55, 161, 12, 142, 89, 248, 20, 137, 235, 19, 60, 238, 206, 169, 39, 53, 183, 97, 201, 237, 225, 28, 229, 60, 122, 71, 177, 89, 156, 210, 223, 63, 85, 242, 115, 121, 24, 20, 206, 191, 115, 199, 55, 234, 83, 247, 205, 91, 95, 253, 170, 20, 223, 61, 111, 134, 120, 68, 219, 129, 202, 175, 243, 62, 185, 104, 196, 44, 56, 36, 52, 95, 194, 163, 64, 114, 22, 29, 195, 12, 188, 226, 37, 139, 40, 60, 73, 65, 255, 13, 149, 113, 57, 168, 1, 222, 8, 12, 179, 156, 216, 180, 228, 144, 100, 86, 193, 97, 123, 203, 132, 112, 213, 50, 182, 116, 72, 108, 92, 66, 208, 184, 87, 167, 80, 81, 244, 101, 83, 126, 65, 164, 195, 26, 23, 94, 150, 58, 39, 107, 203, 59, 171, 69, 241, 31, 157, 88, 171, 172, 250, 3, 147, 75, 227, 250, 85, 32, 48, 109, 246, 173, 118, 118, 145, 136, 204, 76, 37, 245, 2, 215, 252, 79, 229, 203, 215, 197, 42, 68, 128, 38, 53, 163, 143, 181, 98, 90, 73, 222, 177, 27, 103, 37, 186, 14, 152, 69, 234, 192, 225, 93, 254, 117, 2, 195, 47, 240, 18, 129, 76, 151, 163, 141, 70, 249, 198, 107, 211, 95, 231, 3, 143, 156, 149, 21, 146, 122, 235, 191, 109, 89, 218, 149, 82, 131, 45, 212, 190, 33, 211, 88, 116, 105, 41, 73, 224, 200, 68, 142, 201, 137, 106, 117, 194, 121, 120, 244, 142, 62, 107, 153, 88, 113, 221, 39, 185, 79, 182, 190, 225, 173, 23, 240, 136, 172, 102, 201, 32, 58, 180, 125, 206, 74, 24, 99, 223, 49, 130, 229, 26, 51, 96, 151, 81, 127, 69, 98, 83, 119, 224, 177, 100, 174, 132, 187, 107, 160, 28, 254, 129, 43, 148, 249, 8, 104, 88, 112, 72, 253, 25, 143, 69, 108, 135, 148, 222, 248, 183, 82, 123, 211, 35, 171, 115, 2, 226, 114, 75, 143, 87, 227, 31, 171, 42, 102, 85, 40, 7, 178, 235, 194, 3, 47, 181, 123, 154, 134, 197, 8, 165, 211, 55, 135, 242, 48, 40, 165, 178, 35, 191, 106, 186, 2, 3, 130, 92, 237, 22, 28, 43, 138, 207, 180, 146, 167, 121, 242, 240, 243, 7, 226, 161, 78, 105, 244, 205, 101, 218, 190, 213, 6, 5, 98, 31, 209, 52, 254, 138, 196, 166, 83, 157, 52, 46, 85, 160, 162, 243, 225, 50, 5, 138, 235, 117, 164, 246, 236, 57, 11, 131, 239, 170, 64, 96, 159, 6, 94, 113, 16, 81, 189, 110, 138, 249, 62, 33, 6, 61, 150, 221, 5, 174, 221, 62, 189, 70, 77, 230, 141, 181, 145, 84, 93, 5, 113, 196, 212, 111, 4, 6, 21, 255, 96, 80, 251, 36, 25, 152, 233, 151, 214, 189, 67, 204, 137, 64, 158, 119, 103, 217, 66, 189, 176, 232, 139, 136, 7, 137, 91, 56, 231, 25, 238, 219, 121, 200, 10, 71, 161, 124, 15, 233, 124, 66, 30, 201, 248, 132, 0, 0, 0, 0, 134, 131, 9, 128, 237, 72, 50, 43, 112, 172, 30, 17, 114, 78, 108, 90, 255, 251, 253, 14, 56, 86, 15, 133, 213, 30, 61, 174, 57, 39, 54, 45, 217, 100, 10, 15, 166, 33, 104, 92, 84, 209, 155, 91, 46, 58, 36, 54, 103, 177, 12, 10, 231, 15, 147, 87, 150, 210, 180, 238, 145, 158, 27, 155, 197, 79, 128, 192, 32, 162, 97, 220, 75, 105, 90, 119, 26, 22, 28, 18, 186, 10, 226, 147, 42, 229, 192, 160, 224, 67, 60, 34, 23, 29, 18, 27, 13, 11, 14, 9, 199, 173, 242, 139, 168, 185, 45, 182, 169, 200, 20, 30, 25, 133, 87, 241, 7, 76, 175, 117, 221, 187, 238, 153, 96, 253, 163, 127, 38, 159, 247, 1, 245, 188, 92, 114, 59, 197, 68, 102, 126, 52, 91, 251, 41, 118, 139, 67, 198, 220, 203, 35, 252, 104, 182, 237, 241, 99, 184, 228, 220, 202, 215, 49, 133, 16, 66, 99, 34, 64, 19, 151, 17, 32, 132, 198, 36, 125, 133, 74, 61, 248, 210, 187, 50, 17, 174, 249, 161, 109, 199, 41, 47, 75, 29, 158, 48, 243, 220, 178, 82, 236, 13, 134, 227, 208, 119, 193, 22, 108, 43, 179, 185, 153, 169, 112, 72, 250, 17, 148, 100, 34, 71, 233, 140, 196, 168, 252, 63, 26, 160, 240, 44, 216, 86, 125, 144, 239, 34, 51, 78, 199, 135, 73, 209, 193, 217, 56, 162, 254, 140, 202, 11, 54, 152, 212, 129, 207, 166, 245, 222, 40, 165, 122, 142, 38, 218, 183, 191, 164, 63, 173, 157, 228, 44, 58, 146, 13, 80, 120, 204, 155, 106, 95, 70, 98, 84, 126, 19, 194, 246, 141, 184, 232, 144, 216, 247, 94, 46, 57, 175, 245, 130, 195, 128, 190, 159, 93, 147, 124, 105, 208, 45, 169, 111, 213, 18, 179, 207, 37, 153, 59, 200, 172, 125, 167, 16, 24, 99, 110, 232, 156, 187, 123, 219, 59, 120, 9, 205, 38, 24, 244, 110, 89, 183, 1, 236, 154, 154, 168, 131, 79, 110, 101, 230, 149, 230, 126, 170, 255, 207, 8, 33, 188, 232, 230, 239, 21, 155, 217, 186, 231, 54, 206, 74, 111, 9, 212, 234, 159, 124, 214, 41, 176, 178, 175, 49, 164, 35, 49, 42, 63, 148, 48, 198, 165, 102, 192, 53, 162, 188, 55, 116, 78, 202, 166, 252, 130, 208, 176, 224, 144, 216, 21, 51, 167, 152, 74, 241, 4, 218, 247, 65, 236, 80, 14, 127, 205, 246, 47, 23, 145, 214, 141, 118, 77, 176, 77, 67, 239, 77, 84, 204, 170, 4, 223, 228, 150, 181, 227, 158, 209, 136, 27, 76, 106, 31, 184, 193, 44, 81, 127, 70, 101, 234, 4, 157, 94, 53, 93, 1, 140, 116, 115, 250, 135, 65, 46, 251, 11, 29, 90, 179, 103, 210, 82, 146, 219, 86, 51, 233, 16, 71, 19, 109, 214, 97, 140, 154, 215, 12, 122, 55, 161, 20, 142, 89, 248, 60, 137, 235, 19, 39, 238, 206, 169, 201, 53, 183, 97, 229, 237, 225, 28, 177, 60, 122, 71, 223, 89, 156, 210, 115, 63, 85, 242, 206, 121, 24, 20, 55, 191, 115, 199, 205, 234, 83, 247, 170, 91, 95, 253, 111, 20, 223, 61, 219, 134, 120, 68, 243, 129, 202, 175, 196, 62, 185, 104, 52, 44, 56, 36, 64, 95, 194, 163, 195, 114, 22, 29, 37, 12, 188, 226, 73, 139, 40, 60, 149, 65, 255, 13, 1, 113, 57, 168, 179, 222, 8, 12, 228, 156, 216, 180, 193, 144, 100, 86, 132, 97, 123, 203, 182, 112, 213, 50, 92, 116, 72, 108, 87, 66, 208, 184, 244, 167, 80, 81, 65, 101, 83, 126, 23, 164, 195, 26, 39, 94, 150, 58, 171, 107, 203, 59, 157, 69, 241, 31, 250, 88, 171, 172, 227, 3, 147, 75, 48, 250, 85, 32, 118, 109, 246, 173, 204, 118, 145, 136, 2, 76, 37, 245, 229, 215, 252, 79, 42, 203, 215, 197, 53, 68, 128, 38, 98, 163, 143, 181, 177, 90, 73, 222, 186, 27, 103, 37, 234, 14, 152, 69, 254, 192, 225, 93, 47, 117, 2, 195, 76, 240, 18, 129, 70, 151, 163, 141, 211, 249, 198, 107, 143, 95, 231, 3, 146, 156, 149, 21, 109, 122, 235, 191, 82, 89, 218, 149, 190, 131, 45, 212, 116, 33, 211, 88, 224, 105, 41, 73, 201, 200, 68, 142, 194, 137, 106, 117, 142, 121, 120, 244, 88, 62, 107, 153, 185, 113, 221, 39, 225, 79, 182, 190, 136, 173, 23, 240, 32, 172, 102, 201, 206, 58, 180, 125, 223, 74, 24, 99, 26, 49, 130, 229, 81, 51, 96, 151, 83, 127, 69, 98, 100, 119, 224, 177, 107, 174, 132, 187, 129, 160, 28, 254, 8, 43, 148, 249, 72, 104, 88, 112, 69, 253, 25, 143, 222, 108, 135, 148, 123, 248, 183, 82, 115, 211, 35, 171, 75, 2, 226, 114, 31, 143, 87, 227, 85, 171, 42, 102, 235, 40, 7, 178, 181, 194, 3, 47, 197, 123, 154, 134, 55, 8, 165, 211, 40, 135, 242, 48, 191, 165, 178, 35, 3, 106, 186, 2, 22, 130, 92, 237, 207, 28, 43, 138, 121, 180, 146, 167, 7, 242, 240, 243, 105, 226, 161, 78, 218, 244, 205, 101, 5, 190, 213, 6, 52, 98, 31, 209, 166, 254, 138, 196, 46, 83, 157, 52, 243, 85, 160, 162, 138, 225, 50, 5, 246, 235, 117, 164, 131, 236, 57, 11, 96, 239, 170, 64, 113, 159, 6, 94, 110, 16, 81, 189, 33, 138, 249, 62, 221, 6, 61, 150, 62, 5, 174, 221, 230, 189, 70, 77, 84, 141, 181, 145, 196, 93, 5, 113, 6, 212, 111, 4, 80, 21, 255, 96, 152, 251, 36, 25, 189, 233, 151, 214, 64, 67, 204, 137, 217, 158, 119, 103, 232, 66, 189, 176, 137, 139, 136, 7, 25, 91, 56, 231, 200, 238, 219, 121, 124, 10, 71, 161, 66, 15, 233, 124, 132, 30, 201, 248, 0, 0, 0, 0, 128, 134, 131, 9, 43, 237, 72, 50, 17, 112, 172, 30, 90, 114, 78, 108, 14, 255, 251, 253, 133, 56, 86, 15, 174, 213, 30, 61, 45, 57, 39, 54, 15, 217, 100, 10, 92, 166, 33, 104, 91, 84, 209, 155, 54, 46, 58, 36, 10, 103, 177, 12, 87, 231, 15, 147, 238, 150, 210, 180, 155, 145, 158, 27, 192, 197, 79, 128, 220, 32, 162, 97, 119, 75, 105, 90, 18, 26, 22, 28, 147, 186, 10, 226, 160, 42, 229, 192, 34, 224, 67, 60, 27, 23, 29, 18, 9, 13, 11, 14, 139, 199, 173, 242, 182, 168, 185, 45, 30, 169, 200, 20, 241, 25, 133, 87, 117, 7, 76, 175, 153, 221, 187, 238, 127, 96, 253, 163, 1, 38, 159, 247, 114, 245, 188, 92, 102, 59, 197, 68, 251, 126, 52, 91, 67, 41, 118, 139, 35, 198, 220, 203, 237, 252, 104, 182, 228, 241, 99, 184, 49, 220, 202, 215, 99, 133, 16, 66, 151, 34, 64, 19, 198, 17, 32, 132, 74, 36, 125, 133, 187, 61, 248, 210, 249, 50, 17, 174, 41, 161, 109, 199, 158, 47, 75, 29, 178, 48, 243, 220, 134, 82, 236, 13, 193, 227, 208, 119, 179, 22, 108, 43, 112, 185, 153, 169, 148, 72, 250, 17, 233, 100, 34, 71, 252, 140, 196, 168, 240, 63, 26, 160, 125, 44, 216, 86, 51, 144, 239, 34, 73, 78, 199, 135, 56, 209, 193, 217, 202, 162, 254, 140, 212, 11, 54, 152, 245, 129, 207, 166, 122, 222, 40, 165, 183, 142, 38, 218, 173, 191, 164, 63, 58, 157, 228, 44, 120, 146, 13, 80, 95, 204, 155, 106, 126, 70, 98, 84, 141, 19, 194, 246, 216, 184, 232, 144, 57, 247, 94, 46, 195, 175, 245, 130, 93, 128, 190, 159, 208, 147, 124, 105, 213, 45, 169, 111, 37, 18, 179, 207, 172, 153, 59, 200, 24, 125, 167, 16, 156, 99, 110, 232, 59, 187, 123, 219, 38, 120, 9, 205, 89, 24, 244, 110, 154, 183, 1, 236, 79, 154, 168, 131, 149, 110, 101, 230, 255, 230, 126, 170, 188, 207, 8, 33, 21, 232, 230, 239, 231, 155, 217, 186, 111, 54, 206, 74, 159, 9, 212, 234, 176, 124, 214, 41, 164, 178, 175, 49, 63, 35, 49, 42, 165, 148, 48, 198, 162, 102, 192, 53, 78, 188, 55, 116, 130, 202, 166, 252, 144, 208, 176, 224, 167, 216, 21, 51, 4, 152, 74, 241, 236, 218, 247, 65, 205, 80, 14, 127, 145, 246, 47, 23, 77, 214, 141, 118, 239, 176, 77, 67, 170, 77, 84, 204, 150, 4, 223, 228, 209, 181, 227, 158, 106, 136, 27, 76, 44, 31, 184, 193, 101, 81, 127, 70, 94, 234, 4, 157, 140, 53, 93, 1, 135, 116, 115, 250, 11, 65, 46, 251, 103, 29, 90, 179, 219, 210, 82, 146, 16, 86, 51, 233, 214, 71, 19, 109, 215, 97, 140, 154, 161, 12, 122, 55, 248, 20, 142, 89, 19, 60, 137, 235, 169, 39, 238, 206, 97, 201, 53, 183, 28, 229, 237, 225, 71, 177, 60, 122, 210, 223, 89, 156, 242, 115, 63, 85, 20, 206, 121, 24, 199, 55, 191, 115, 247, 205, 234, 83, 253, 170, 91, 95, 61, 111, 20, 223, 68, 219, 134, 120, 175, 243, 129, 202, 104, 196, 62, 185, 36, 52, 44, 56, 163, 64, 95, 194, 29, 195, 114, 22, 226, 37, 12, 188, 60, 73, 139, 40, 13, 149, 65, 255, 168, 1, 113, 57, 12, 179, 222, 8, 180, 228, 156, 216, 86, 193, 144, 100, 203, 132, 97, 123, 50, 182, 112, 213, 108, 92, 116, 72, 184, 87, 66, 208};
.const .align 4 .b8 AES_KEY_FILL[64] = {83, 165, 172, 109, 9, 102, 113, 98, 43, 85, 181, 219, 23, 73, 244, 180, 7, 175, 124, 109, 13, 113, 106, 132, 120, 211, 37, 23, 78, 220, 161, 13, 241, 98, 18, 63, 198, 126, 148, 159, 79, 121, 192, 244, 69, 227, 32, 62, 53, 129, 239, 106, 124, 49, 186, 177, 136, 76, 49, 22, 84, 145, 22, 73};
.const .align 4 .b8 AES_STATE_HASH[64] = {13, 44, 181, 146, 222, 86, 168, 159, 71, 219, 130, 204, 173, 58, 152, 215, 110, 153, 141, 51, 152, 183, 199, 21, 90, 18, 158, 245, 87, 128, 231, 172, 23, 0, 119, 106, 208, 199, 98, 174, 107, 80, 121, 80, 228, 124, 160, 232, 12, 36, 10, 99, 141, 130, 173, 7, 5, 0, 161, 121, 72, 73, 153, 126};
// _ZZ7init_vmILi2EEvPvS0_S0_E18execution_plan_buf has been demoted
// _ZZ7init_vmILi4EEvPvS0_S0_E18execution_plan_buf has been demoted
// _ZZ7init_vmILi8EEvPvS0_S0_E18execution_plan_buf has been demoted
// _ZZ7init_vmILi16EEvPvS0_S0_E18execution_plan_buf has been demoted
// _ZZ10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local has been demoted
// _ZZ11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_jE1T has been demoted
// _ZZ11fillAes4Rx4ILy2176ELb0EEvPvS0_jE1T has been demoted
// _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvjE1T has been demoted
// _ZZ11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvjE1T has been demoted
// _ZZ11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvjE1T has been demoted

.visible .entry _Z7init_vmILi2EEvPvS0_S0_(
	.param .u64 _Z7init_vmILi2EEvPvS0_S0__param_0,
	.param .u64 _Z7init_vmILi2EEvPvS0_S0__param_1,
	.param .u64 _Z7init_vmILi2EEvPvS0_S0__param_2
)
.maxntid 32, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<294>;
	.reg .b16 	%rs<351>;
	.reg .b32 	%r<870>;
	.reg .b64 	%rd<386>;
	// demoted variable
	.shared .align 4 .b8 _ZZ7init_vmILi2EEvPvS0_S0_E18execution_plan_buf[2048];

	ld.param.u64 	%rd129, [_Z7init_vmILi2EEvPvS0_S0__param_0];
	ld.param.u64 	%rd130, [_Z7init_vmILi2EEvPvS0_S0__param_1];
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r790, %r1, 2;
	mov.u32 	%r3, %ntid.x;
	setp.gt.u32	%p4, %r790, 2047;
	@%p4 bra 	BB0_3;

	shl.b32 	%r4, %r3, 2;
	shl.b32 	%r216, %r1, 2;
	mov.u32 	%r217, _ZZ7init_vmILi2EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r789, %r217, %r216;

BB0_2:
	mov.u32 	%r218, 0;
	st.shared.u32 	[%r789], %r218;
	add.s32 	%r789, %r789, %r4;
	add.s32 	%r790, %r790, %r4;
	setp.lt.u32	%p5, %r790, 2048;
	@%p5 bra 	BB0_2;

BB0_3:
	bar.warp.sync 	-1;
	mov.u32 	%r219, %ctaid.x;
	mad.lo.s32 	%r220, %r219, %r3, %r1;
	shr.u32 	%r221, %r220, 3;
	and.b32  	%r222, %r220, 7;
	mul.wide.u32 	%rd132, %r221, 256;
	cvt.u64.u32	%rd133, %r222;
	or.b64  	%rd134, %rd132, %rd133;
	cvta.to.global.u64 	%rd135, %rd130;
	shl.b64 	%rd136, %rd134, 3;
	add.s64 	%rd137, %rd135, %rd136;
	mov.u64 	%rd360, 0;
	st.global.u64 	[%rd137], %rd360;
	mul.wide.u32 	%rd139, %r221, 2176;
	shr.u64 	%rd1, %rd139, 3;
	or.b64  	%rd140, %rd1, %rd133;
	cvta.to.global.u64 	%rd2, %rd129;
	shl.b64 	%rd141, %rd140, 3;
	add.s64 	%rd142, %rd2, %rd141;
	ld.global.u64 	%rd143, [%rd142];
	shr.u64 	%rd144, %rd143, 7;
	and.b64  	%rd145, %rd144, 139611588448485376;
	and.b64  	%rd146, %rd143, 4503599627370495;
	add.s64 	%rd147, %rd145, 4607182418800017408;
	or.b64  	%rd148, %rd147, %rd146;
	st.global.u64 	[%rd137+192], %rd148;
	setp.ne.s32	%p6, %r222, 0;
	@%p6 bra 	BB0_275;

	shl.b32 	%r224, %r1, 6;
	and.b32  	%r225, %r224, -512;
	mov.u32 	%r226, _ZZ7init_vmILi2EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r10, %r226, %r225;
	add.s64 	%rd3, %rd1, 16;
	mov.u32 	%r791, 0;
	mov.u64 	%rd359, %rd360;

BB0_5:
	cvt.u64.u32	%rd6, %r791;
	add.s64 	%rd151, %rd6, %rd1;
	shl.b64 	%rd152, %rd151, 3;
	add.s64 	%rd7, %rd2, %rd152;
	ld.global.u32 	%r12, [%rd7+128];
	and.b32  	%r227, %r12, -63489;
	st.global.u32 	[%rd7+128], %r227;
	and.b32  	%r13, %r12, 255;
	setp.lt.u32	%p7, %r13, 85;
	@%p7 bra 	BB0_18;
	bra.uni 	BB0_6;

BB0_18:
	bfe.u32 	%r788, %r12, 8, 3;
	shl.b32 	%r266, %r788, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r266,8;
	// inline asm
	mov.u64 	%rd192, 1;
	// inline asm
	bfi.b64 %rd359,%rd192,%rd359,%r266,8;
	// inline asm
	bra.uni 	BB0_19;

BB0_6:
	and.b32  	%r776, %r12, 255;
	add.s32 	%r228, %r776, -85;
	setp.lt.u32	%p8, %r228, 8;
	@%p8 bra 	BB0_16;
	bra.uni 	BB0_7;

BB0_16:
	ld.global.u32 	%r260, [%rd7+132];
	add.s32 	%r261, %r260, -1;
	and.b32  	%r262, %r261, %r260;
	setp.eq.s32	%p16, %r262, 0;
	@%p16 bra 	BB0_19;

	bfe.u32 	%r787, %r12, 8, 3;
	shl.b32 	%r264, %r787, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r264,8;
	// inline asm
	mov.u64 	%rd186, 1;
	// inline asm
	bfi.b64 %rd359,%rd186,%rd359,%r264,8;
	// inline asm
	bra.uni 	BB0_19;

BB0_7:
	and.b32  	%r777, %r12, 255;
	add.s32 	%r229, %r777, -93;
	setp.lt.u32	%p9, %r229, 32;
	@%p9 bra 	BB0_15;
	bra.uni 	BB0_8;

BB0_15:
	bfe.u32 	%r786, %r12, 8, 3;
	shl.b32 	%r259, %r786, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r259,8;
	// inline asm
	mov.u64 	%rd180, 1;
	// inline asm
	bfi.b64 %rd359,%rd180,%rd359,%r259,8;
	// inline asm
	bra.uni 	BB0_19;

BB0_8:
	and.b32  	%r778, %r12, 255;
	add.s32 	%r230, %r778, -125;
	setp.lt.u32	%p10, %r230, 4;
	@%p10 bra 	BB0_13;
	bra.uni 	BB0_9;

BB0_13:
	bfe.u32 	%r783, %r12, 16, 3;
	bfe.u32 	%r782, %r12, 8, 3;
	setp.eq.s32	%p15, %r783, %r782;
	@%p15 bra 	BB0_19;

	bfe.u32 	%r785, %r12, 16, 3;
	bfe.u32 	%r784, %r12, 8, 3;
	shl.b32 	%r255, %r784, 3;
	// inline asm
	bfi.b64 %rd164,%rd6,%rd360,%r255,8;
	// inline asm
	mov.u64 	%rd174, 1;
	// inline asm
	bfi.b64 %rd167,%rd174,%rd359,%r255,8;
	// inline asm
	shl.b32 	%r257, %r785, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd164,%r257,8;
	// inline asm
	// inline asm
	bfi.b64 %rd359,%rd174,%rd167,%r257,8;
	// inline asm
	bra.uni 	BB0_19;

BB0_9:
	and.b32  	%r779, %r12, 255;
	add.s32 	%r231, %r779, -129;
	setp.lt.u32	%p11, %r231, 94;
	@%p11 bra 	BB0_12;
	bra.uni 	BB0_10;

BB0_12:
	ld.global.u32 	%r252, [%rd7+128];
	or.b32  	%r253, %r252, 8192;
	st.global.u32 	[%rd7+128], %r253;
	bra.uni 	BB0_19;

BB0_10:
	and.b32  	%r780, %r12, 255;
	add.s32 	%r232, %r780, -223;
	setp.gt.u32	%p12, %r232, 15;
	@%p12 bra 	BB0_19;

	bfe.u32 	%r781, %r12, 8, 3;
	shl.b32 	%r234, %r781, 3;
	// inline asm
	bfe.u64 %rd153,%rd360,%r234,8;
	// inline asm
	cvt.u32.u64	%r235, %rd153;
	// inline asm
	bfe.u64 %rd155,%rd359,%r234,8;
	// inline asm
	cvt.u32.u64	%r236, %rd155;
	setp.eq.s32	%p13, %r236, 0;
	selp.b32	%r237, -1, %r235, %p13;
	setp.eq.s32	%p14, %r237, -1;
	selp.b32	%r238, 144, 16, %p14;
	or.b32  	%r239, %r238, %r781;
	shl.b32 	%r240, %r239, 8;
	shl.b32 	%r241, %r237, 16;
	and.b32  	%r242, %r241, 16711680;
	and.b32  	%r243, %r12, -16776961;
	or.b32  	%r244, %r242, %r243;
	or.b32  	%r245, %r244, %r240;
	st.global.u32 	[%rd7+128], %r245;
	cvt.s64.s32	%rd158, %r237;
	add.s64 	%rd159, %rd158, %rd3;
	shl.b64 	%rd160, %rd159, 3;
	add.s64 	%rd161, %rd160, %rd2;
	ld.global.u32 	%r246, [%rd161+8];
	or.b32  	%r247, %r246, 16384;
	st.global.u32 	[%rd161+8], %r247;
	shl.b32 	%r248, %r791, 8;
	or.b32  	%r249, %r248, %r791;
	shl.b32 	%r250, %r249, 16;
	or.b32  	%r251, %r250, %r249;
	cvt.u64.u32	%rd162, %r251;
	shl.b64 	%rd163, %rd162, 32;
	or.b64  	%rd360, %rd163, %rd162;
	mov.u64 	%rd359, 72340172838076673;

BB0_19:
	cvt.u32.u64	%r276, %rd6;
	add.s32 	%r791, %r276, 1;
	mov.u32 	%r21, -1;
	mov.u32 	%r17, 0;
	mov.u64 	%rd369, 0;
	mov.u16 	%rs323, 0;
	setp.ne.s32	%p17, %r791, 256;
	@%p17 bra 	BB0_5;

	mov.u32 	%r18, %r17;
	mov.u32 	%r817, %r17;
	mov.u32 	%r795, %r17;
	mov.u16 	%rs324, %rs323;
	mov.u16 	%rs325, %rs323;
	mov.u32 	%r797, %r17;
	mov.u64 	%rd25, %rd369;
	mov.u64 	%rd21, %rd369;
	mov.u64 	%rd22, %rd369;
	mov.u32 	%r801, %r17;
	mov.u32 	%r799, %r21;
	mov.u32 	%r800, %r21;
	bra.uni 	BB0_21;

BB0_33:
	add.s32 	%r298, %r28, -85;
	setp.lt.u32	%p31, %r298, 8;
	@%p31 bra 	BB0_72;
	bra.uni 	BB0_34;

BB0_72:
	add.s32 	%r337, %r285, -1;
	and.b32  	%r338, %r337, %r285;
	setp.eq.s32	%p50, %r338, 0;
	selp.b32	%r802, 0, %r35, %p50;
	selp.u16	%rs328, 1, 0, %p50;
	mov.u16 	%rs326, 0;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;
	mov.u16 	%rs331, %rs326;
	bra.uni 	BB0_73;

BB0_34:
	add.s32 	%r299, %r28, -93;
	setp.lt.u32	%p32, %r299, 2;
	mov.u16 	%rs333, 0;
	@%p32 bra 	BB0_35;
	bra.uni 	BB0_37;

BB0_35:
	mov.u32 	%r802, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;

BB0_36:
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB0_74;

BB0_37:
	add.s32 	%r300, %r28, -95;
	setp.lt.u32	%p33, %r300, 15;
	@%p33 bra 	BB0_38;

	add.s32 	%r301, %r28, -110;
	setp.lt.u32	%p34, %r301, 5;
	@%p34 bra 	BB0_41;
	bra.uni 	BB0_42;

BB0_41:
	mov.u32 	%r802, %r39;
	mov.u16 	%rs326, %rs331;
	bra.uni 	BB0_39;

BB0_42:
	add.s32 	%r302, %r28, -115;
	setp.lt.u32	%p35, %r302, 10;
	@%p35 bra 	BB0_38;
	bra.uni 	BB0_43;

BB0_38:
	mov.u16 	%rs326, %rs333;

BB0_39:
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB0_74;

BB0_43:
	add.s32 	%r303, %r28, -125;
	setp.lt.u32	%p36, %r303, 4;
	@%p36 bra 	BB0_71;
	bra.uni 	BB0_44;

BB0_71:
	bfe.u32 	%r764, %r284, 16, 3;
	setp.eq.s32	%p291, %r803, %r764;
	selp.b32	%r802, 0, %r802, %p291;
	selp.u16	%rs328, 1, 0, %p291;
	mov.u16 	%rs330, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs331, %rs330;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB0_74;

BB0_44:
	add.s32 	%r304, %r28, -129;
	setp.lt.u32	%p37, %r304, 8;
	@%p37 bra 	BB0_69;
	bra.uni 	BB0_45;

BB0_69:
	shl.b32 	%r749, %r803, 3;
	// inline asm
	bfe.u64 %rd223,%rd21,%r749,8;
	// inline asm
	cvt.u32.u64	%r802, %rd223;
	bra.uni 	BB0_70;

BB0_45:
	add.s32 	%r305, %r28, -137;
	setp.lt.u32	%p38, %r305, 20;
	@%p38 bra 	BB0_68;
	bra.uni 	BB0_46;

BB0_68:
	shr.u32 	%r748, %r284, 8;
	and.b32  	%r803, %r748, 3;
	shl.b32 	%r335, %r803, 3;
	// inline asm
	bfe.u64 %rd221,%rd21,%r335,8;
	// inline asm
	cvt.u32.u64	%r802, %rd221;
	bra.uni 	BB0_70;

BB0_46:
	add.s32 	%r306, %r28, -157;
	setp.lt.u32	%p39, %r306, 5;
	@%p39 bra 	BB0_67;
	bra.uni 	BB0_47;

BB0_67:
	shr.u32 	%r747, %r284, 8;
	and.b32  	%r803, %r747, 3;
	shl.b32 	%r332, %r803, 3;
	// inline asm
	bfe.u64 %rd219,%rd21,%r332,8;
	// inline asm
	cvt.u32.u64	%r333, %rd219;
	max.u32 	%r334, %r37, %r333;
	max.u32 	%r802, %r18, %r334;
	bra.uni 	BB0_65;

BB0_47:
	add.s32 	%r307, %r28, -162;
	setp.lt.u32	%p40, %r307, 20;
	@%p40 bra 	BB0_66;
	bra.uni 	BB0_48;

BB0_66:
	shr.u32 	%r746, %r284, 8;
	and.b32  	%r803, %r746, 3;
	shl.b32 	%r331, %r803, 3;
	// inline asm
	bfe.u64 %rd217,%rd21,%r331,8;
	// inline asm
	cvt.u32.u64	%r802, %rd217;
	bra.uni 	BB0_70;

BB0_48:
	add.s32 	%r308, %r28, -182;
	setp.lt.u32	%p41, %r308, 5;
	@%p41 bra 	BB0_64;
	bra.uni 	BB0_49;

BB0_64:
	shr.u32 	%r745, %r284, 8;
	and.b32  	%r803, %r745, 3;
	shl.b32 	%r328, %r803, 3;
	// inline asm
	bfe.u64 %rd215,%rd21,%r328,8;
	// inline asm
	cvt.u32.u64	%r329, %rd215;
	max.u32 	%r330, %r37, %r329;
	max.u32 	%r802, %r18, %r330;

BB0_65:
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs326;
	bra.uni 	BB0_74;

BB0_49:
	add.s32 	%r309, %r28, -187;
	setp.lt.u32	%p42, %r309, 6;
	@%p42 bra 	BB0_63;
	bra.uni 	BB0_50;

BB0_63:
	shr.u32 	%r744, %r284, 8;
	and.b32  	%r803, %r744, 3;
	shl.b32 	%r327, %r803, 3;
	// inline asm
	bfe.u64 %rd213,%rd21,%r327,8;
	// inline asm
	cvt.u32.u64	%r802, %rd213;
	bra.uni 	BB0_70;

BB0_50:
	add.s32 	%r310, %r28, -193;
	setp.lt.u32	%p43, %r310, 20;
	@%p43 bra 	BB0_62;
	bra.uni 	BB0_51;

BB0_62:
	shr.u32 	%r743, %r284, 8;
	and.b32  	%r326, %r743, 3;
	add.s32 	%r803, %r326, 4;
	shl.b32 	%r325, %r803, 3;
	// inline asm
	bfe.u64 %rd211,%rd21,%r325,8;
	// inline asm
	cvt.u32.u64	%r802, %rd211;
	bra.uni 	BB0_70;

BB0_51:
	add.s32 	%r311, %r28, -213;
	setp.lt.u32	%p44, %r311, 4;
	@%p44 bra 	BB0_61;
	bra.uni 	BB0_52;

BB0_61:
	shr.u32 	%r742, %r284, 8;
	and.b32  	%r322, %r742, 3;
	add.s32 	%r803, %r322, 4;
	shl.b32 	%r321, %r803, 3;
	// inline asm
	bfe.u64 %rd209,%rd21,%r321,8;
	// inline asm
	cvt.u32.u64	%r323, %rd209;
	max.u32 	%r324, %r37, %r323;
	max.u32 	%r802, %r18, %r324;
	bra.uni 	BB0_65;

BB0_52:
	add.s32 	%r312, %r28, -217;
	setp.lt.u32	%p45, %r312, 6;
	@%p45 bra 	BB0_60;
	bra.uni 	BB0_53;

BB0_60:
	shr.u32 	%r741, %r284, 8;
	and.b32  	%r320, %r741, 3;
	add.s32 	%r803, %r320, 4;
	shl.b32 	%r319, %r803, 3;
	// inline asm
	bfe.u64 %rd207,%rd21,%r319,8;
	// inline asm
	cvt.u32.u64	%r802, %rd207;

BB0_70:
	mov.u16 	%rs332, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	bra.uni 	BB0_74;

BB0_53:
	add.s32 	%r740, %r799, 1;
	add.s32 	%r313, %r28, -223;
	setp.lt.u32	%p46, %r313, 16;
	mov.u16 	%rs329, 1;
	@%p46 bra 	BB0_54;
	bra.uni 	BB0_55;

BB0_54:
	mov.u32 	%r801, %r740;
	mov.u32 	%r802, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	bra.uni 	BB0_36;

BB0_55:
	setp.eq.s32	%p47, %r28, 239;
	mov.u16 	%rs331, 1;
	@%p47 bra 	BB0_56;
	bra.uni 	BB0_57;

BB0_56:
	mov.u32 	%r802, %r37;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	mov.u16 	%rs333, %rs331;
	bra.uni 	BB0_74;

BB0_57:
	and.b32  	%r765, %r284, 240;
	mov.u16 	%rs328, 1;
	mov.u32 	%r314, 0;
	setp.ne.s32	%p48, %r765, 240;
	@%p48 bra 	BB0_58;

	add.s32 	%r315, %r800, 2;
	shr.u32 	%r316, %r315, 31;
	add.s32 	%r317, %r315, %r316;
	shr.s32 	%r318, %r317, 1;
	max.u32 	%r802, %r318, %r802;
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB0_74;

BB0_58:
	mov.u32 	%r802, %r314;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs328;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB0_74;

BB0_21:
	mov.u16 	%rs326, 0;
	cvt.u64.u32	%rd204, %r797;
	add.s64 	%rd205, %rd204, %rd1;
	shl.b64 	%rd206, %rd205, 3;
	add.s64 	%rd23, %rd2, %rd206;
	ld.global.v2.u32 	{%r284, %r285}, [%rd23+128];
	and.b32  	%r28, %r284, 255;
	bfe.u32 	%r803, %r284, 8, 3;
	bfe.u32 	%r286, %r284, 14, 1;
	and.b32  	%r287, %r286, 1;
	setp.eq.b32	%p1, %r287, 1;
	add.s32 	%r288, %r799, 1;
	selp.b32	%r801, %r288, %r801, %p1;
	selp.b16	%rs325, 1, %rs325, %p1;
	mov.u16 	%rs331, 1;
	shl.b32 	%r277, %r803, 3;
	// inline asm
	bfe.u64 %rd198,%rd369,%r277,8;
	// inline asm
	cvt.u32.u64	%r35, %rd198;
	shr.u32 	%r289, %r284, 13;
	and.b32  	%r36, %r289, 56;
	// inline asm
	bfe.u64 %rd200,%rd369,%r36,8;
	// inline asm
	cvt.u32.u64	%r37, %rd200;
	max.u32 	%r802, %r35, %r37;
	setp.lt.u32	%p18, %r28, 25;
	@%p18 bra 	BB0_22;

	mov.u16 	%rs327, 0;
	bfe.u32 	%r739, %r284, 16, 3;
	and.b32  	%r290, %r285, 1835008;
	setp.ne.s32	%p19, %r290, 0;
	setp.eq.s32	%p20, %r803, %r739;
	and.pred  	%p21, %p20, %p19;
	selp.b32	%r291, %r17, %r18, %p21;
	max.u32 	%r39, %r802, %r291;
	add.s32 	%r292, %r28, -25;
	setp.lt.u32	%p22, %r292, 7;
	mov.u16 	%rs331, 1;
	@%p22 bra 	BB0_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r40, %r284, 240;
	setp.eq.s32	%p23, %r40, 32;
	@%p23 bra 	BB0_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r293, %r28, -48;
	setp.lt.u32	%p24, %r293, 7;
	@%p24 bra 	BB0_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r294, %r28, -55;
	setp.lt.u32	%p25, %r294, 16;
	@%p25 bra 	BB0_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r295, %r28, -71;
	setp.lt.u32	%p26, %r295, 4;
	@%p26 bra 	BB0_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r296, %r28, -75;
	setp.lt.u32	%p27, %r296, 4;
	@%p27 bra 	BB0_22;

	mov.u16 	%rs327, 0;
	setp.eq.s32	%p28, %r28, 79;
	@%p28 bra 	BB0_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r297, %r284, 252;
	setp.eq.s32	%p29, %r297, 80;
	@%p29 bra 	BB0_22;
	bra.uni 	BB0_32;

BB0_22:
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;

BB0_73:
	mov.u16 	%rs332, %rs326;
	mov.u16 	%rs333, %rs326;
	bra.uni 	BB0_74;

BB0_32:
	mov.u16 	%rs327, 0;
	setp.eq.s32	%p30, %r28, 84;
	@%p30 bra 	BB0_24;
	bra.uni 	BB0_33;

BB0_24:
	mov.u32 	%r802, %r39;
	mov.u16 	%rs326, %rs331;
	mov.u16 	%rs328, %rs327;
	mov.u16 	%rs329, %rs327;
	mov.u16 	%rs330, %rs327;
	mov.u16 	%rs332, %rs327;
	mov.u16 	%rs333, %rs327;

BB0_74:
	setp.eq.s16	%p51, %rs328, 0;
	@%p51 bra 	BB0_76;
	bra.uni 	BB0_75;

BB0_76:
	bfe.u32 	%r757, %r284, 14, 1;
	and.b32  	%r756, %r757, 1;
	setp.eq.b32	%p290, %r756, 1;
	and.b16  	%rs279, %rs324, 255;
	setp.eq.s16	%p53, %rs279, 0;
	selp.u16	%rs334, 1, 0, %p290;
	@%p53 bra 	BB0_78;

	ld.global.u32 	%r339, [%rd23+128];
	or.b32  	%r340, %r339, 16384;
	st.global.u32 	[%rd23+128], %r340;
	mov.u16 	%rs334, 1;

BB0_78:
	shl.b32 	%r341, %r802, 1;
	max.u32 	%r65, %r341, %r801;
	setp.eq.s16	%p54, %rs333, 0;
	@%p54 bra 	BB0_80;
	bra.uni 	BB0_79;

BB0_80:
	setp.eq.s16	%p55, %rs332, 0;
	selp.b64	%rd226, %rd25, %rd22, %p55;
	shl.b32 	%r342, %r803, 3;
	// inline asm
	bfe.u64 %rd225,%rd226,%r342,8;
	// inline asm
	cvt.u32.u64	%r343, %rd225;
	shl.b32 	%r344, %r343, 1;
	max.u32 	%r814, %r344, %r65;
	bra.uni 	BB0_81;

BB0_75:
	and.b32  	%r750, %r284, 16384;
	setp.ne.s32	%p52, %r750, 0;
	selp.b16	%rs324, 1, %rs324, %p52;
	bra.uni 	BB0_159;

BB0_79:
	max.s32 	%r814, %r817, %r65;

BB0_81:
	setp.eq.s16	%p56, %rs330, 0;
	@%p56 bra 	BB0_83;

	shr.u32 	%r759, %r284, 13;
	and.b32  	%r758, %r759, 56;
	// inline asm
	bfe.u64 %rd227,%rd25,%r758,8;
	// inline asm
	cvt.u32.u64	%r346, %rd227;
	shl.b32 	%r347, %r346, 1;
	max.u32 	%r814, %r347, %r814;

BB0_83:
	setp.eq.s16	%p57, %rs332, 0;
	@%p57 bra 	BB0_126;
	bra.uni 	BB0_84;

BB0_126:
	add.s32 	%r383, %r799, 1;
	max.s32 	%r815, %r814, %r383;
	setp.gt.s32	%p134, %r814, %r799;
	@%p134 bra 	BB0_130;

BB0_127:
	add.s32 	%r384, %r10, %r814;
	ld.shared.u8 	%rs299, [%r384];
	setp.eq.s16	%p135, %rs299, 0;
	@%p135 bra 	BB0_128;

	add.s32 	%r95, %r814, 1;
	setp.lt.s32	%p136, %r814, %r799;
	mov.u32 	%r814, %r95;
	@%p136 bra 	BB0_127;
	bra.uni 	BB0_130;

BB0_117:
	add.s32 	%r814, %r814, 1;

BB0_84:
	add.s32 	%r72, %r10, %r814;
	ld.shared.u8 	%rs281, [%r72];
	setp.ne.s16	%p58, %rs281, 0;
	@%p58 bra 	BB0_117;

	add.s32 	%r766, %r10, %r814;
	ld.shared.u8 	%rs282, [%r766+1];
	setp.ne.s16	%p59, %rs282, 0;
	and.b32  	%r348, %r814, 1;
	setp.eq.b32	%p60, %r348, 1;
	or.pred  	%p61, %p59, %p60;
	@%p61 bra 	BB0_117;

	shr.u32 	%r349, %r814, 31;
	add.s32 	%r350, %r814, %r349;
	and.b32  	%r73, %r350, -2;
	mov.u16 	%rs341, 0;
	setp.ge.s32	%p62, %r73, %r814;
	@%p62 bra 	BB0_116;

	sub.s32 	%r74, %r814, %r73;
	and.b32  	%r75, %r74, 3;
	setp.eq.s32	%p63, %r75, 0;
	mov.u16 	%rs341, 0;
	@%p63 bra 	BB0_102;

	setp.eq.s32	%p64, %r75, 1;
	mov.u16 	%rs338, 0;
	@%p64 bra 	BB0_98;

	setp.eq.s32	%p65, %r75, 2;
	mov.u16 	%rs336, 0;
	@%p65 bra 	BB0_94;

	add.s32 	%r351, %r10, %r73;
	ld.shared.u8 	%rs18, [%r351];
	setp.eq.s16	%p66, %rs18, 0;
	setp.ne.s32	%p67, %r73, %r21;
	and.pred  	%p68, %p67, %p66;
	@%p68 bra 	BB0_92;

	setp.ne.s16	%p69, %rs334, 0;
	cvt.u64.u16	%rd229, %rs18;
	add.s64 	%rd230, %rd229, %rd3;
	shl.b64 	%rd231, %rd230, 3;
	add.s64 	%rd232, %rd2, %rd231;
	ld.global.u32 	%r352, [%rd232];
	and.b32  	%r353, %r352, 8192;
	setp.eq.s32	%p70, %r353, 0;
	and.b32  	%r354, %r352, 20480;
	setp.ne.s32	%p71, %r354, 0;
	or.pred  	%p72, %p69, %p71;
	and.pred  	%p73, %p70, %p72;
	mov.u16 	%rs336, 1;
	@%p73 bra 	BB0_93;

BB0_92:
	mov.u16 	%rs336, 0;

BB0_93:
	add.s32 	%r73, %r73, 1;

BB0_94:
	add.s32 	%r355, %r10, %r73;
	ld.shared.u8 	%rs21, [%r355];
	setp.eq.s16	%p74, %rs21, 0;
	setp.ne.s32	%p75, %r73, %r21;
	and.pred  	%p76, %p75, %p74;
	@%p76 bra 	BB0_96;

	setp.ne.s16	%p77, %rs334, 0;
	cvt.u64.u16	%rd233, %rs21;
	add.s64 	%rd234, %rd233, %rd3;
	shl.b64 	%rd235, %rd234, 3;
	add.s64 	%rd236, %rd2, %rd235;
	ld.global.u32 	%r356, [%rd236];
	and.b32  	%r357, %r356, 8192;
	setp.eq.s32	%p78, %r357, 0;
	and.b32  	%r358, %r356, 20480;
	setp.ne.s32	%p79, %r358, 0;
	or.pred  	%p80, %p77, %p79;
	and.pred  	%p81, %p78, %p80;
	mov.u16 	%rs338, 1;
	@%p81 bra 	BB0_97;

BB0_96:
	mov.u16 	%rs338, %rs336;

BB0_97:
	add.s32 	%r73, %r73, 1;

BB0_98:
	add.s32 	%r359, %r10, %r73;
	ld.shared.u8 	%rs24, [%r359];
	setp.eq.s16	%p82, %rs24, 0;
	setp.ne.s32	%p83, %r73, %r21;
	and.pred  	%p84, %p83, %p82;
	@%p84 bra 	BB0_100;

	setp.ne.s16	%p85, %rs334, 0;
	cvt.u64.u16	%rd237, %rs24;
	add.s64 	%rd238, %rd237, %rd3;
	shl.b64 	%rd239, %rd238, 3;
	add.s64 	%rd240, %rd2, %rd239;
	ld.global.u32 	%r360, [%rd240];
	and.b32  	%r361, %r360, 8192;
	setp.eq.s32	%p86, %r361, 0;
	and.b32  	%r362, %r360, 20480;
	setp.ne.s32	%p87, %r362, 0;
	or.pred  	%p88, %p85, %p87;
	and.pred  	%p89, %p86, %p88;
	mov.u16 	%rs341, 1;
	@%p89 bra 	BB0_101;

BB0_100:
	mov.u16 	%rs341, %rs338;

BB0_101:
	add.s32 	%r73, %r73, 1;

BB0_102:
	setp.lt.u32	%p90, %r74, 4;
	@%p90 bra 	BB0_116;

BB0_103:
	add.s32 	%r83, %r10, %r73;
	ld.shared.u8 	%rs28, [%r83];
	setp.eq.s16	%p91, %rs28, 0;
	setp.ne.s32	%p92, %r73, %r21;
	and.pred  	%p93, %p92, %p91;
	@%p93 bra 	BB0_105;

	setp.ne.s16	%p94, %rs334, 0;
	cvt.u64.u16	%rd241, %rs28;
	add.s64 	%rd242, %rd241, %rd3;
	shl.b64 	%rd243, %rd242, 3;
	add.s64 	%rd244, %rd2, %rd243;
	ld.global.u32 	%r363, [%rd244];
	and.b32  	%r364, %r363, 8192;
	setp.eq.s32	%p95, %r364, 0;
	and.b32  	%r365, %r363, 20480;
	setp.ne.s32	%p96, %r365, 0;
	or.pred  	%p97, %p94, %p96;
	and.pred  	%p98, %p95, %p97;
	mov.u16 	%rs342, 1;
	@%p98 bra 	BB0_106;

BB0_105:
	mov.u16 	%rs342, %rs341;

BB0_106:
	add.s32 	%r771, %r10, %r73;
	ld.shared.u8 	%rs30, [%r771+1];
	add.s32 	%r366, %r73, 1;
	setp.ne.s32	%p99, %r366, %r21;
	setp.eq.s16	%p100, %rs30, 0;
	and.pred  	%p101, %p99, %p100;
	@%p101 bra 	BB0_108;

	setp.ne.s16	%p102, %rs334, 0;
	cvt.u64.u16	%rd245, %rs30;
	add.s64 	%rd246, %rd245, %rd3;
	shl.b64 	%rd247, %rd246, 3;
	add.s64 	%rd248, %rd2, %rd247;
	ld.global.u32 	%r367, [%rd248];
	and.b32  	%r368, %r367, 8192;
	setp.eq.s32	%p103, %r368, 0;
	and.b32  	%r369, %r367, 20480;
	setp.ne.s32	%p104, %r369, 0;
	or.pred  	%p105, %p102, %p104;
	and.pred  	%p106, %p103, %p105;
	mov.u16 	%rs343, 1;
	@%p106 bra 	BB0_109;

BB0_108:
	mov.u16 	%rs343, %rs342;

BB0_109:
	add.s32 	%r772, %r10, %r73;
	ld.shared.u8 	%rs32, [%r772+2];
	add.s32 	%r370, %r73, 2;
	setp.ne.s32	%p107, %r370, %r21;
	setp.eq.s16	%p108, %rs32, 0;
	and.pred  	%p109, %p107, %p108;
	@%p109 bra 	BB0_111;

	setp.ne.s16	%p110, %rs334, 0;
	cvt.u64.u16	%rd249, %rs32;
	add.s64 	%rd250, %rd249, %rd3;
	shl.b64 	%rd251, %rd250, 3;
	add.s64 	%rd252, %rd2, %rd251;
	ld.global.u32 	%r371, [%rd252];
	and.b32  	%r372, %r371, 8192;
	setp.eq.s32	%p111, %r372, 0;
	and.b32  	%r373, %r371, 20480;
	setp.ne.s32	%p112, %r373, 0;
	or.pred  	%p113, %p110, %p112;
	and.pred  	%p114, %p111, %p113;
	mov.u16 	%rs344, 1;
	@%p114 bra 	BB0_112;

BB0_111:
	mov.u16 	%rs344, %rs343;

BB0_112:
	add.s32 	%r773, %r10, %r73;
	ld.shared.u8 	%rs34, [%r773+3];
	add.s32 	%r374, %r73, 3;
	setp.ne.s32	%p115, %r374, %r21;
	setp.eq.s16	%p116, %rs34, 0;
	and.pred  	%p117, %p115, %p116;
	@%p117 bra 	BB0_114;

	setp.ne.s16	%p118, %rs334, 0;
	cvt.u64.u16	%rd253, %rs34;
	add.s64 	%rd254, %rd253, %rd3;
	shl.b64 	%rd255, %rd254, 3;
	add.s64 	%rd256, %rd2, %rd255;
	ld.global.u32 	%r375, [%rd256];
	and.b32  	%r376, %r375, 8192;
	setp.eq.s32	%p119, %r376, 0;
	and.b32  	%r377, %r375, 20480;
	setp.ne.s32	%p120, %r377, 0;
	or.pred  	%p121, %p118, %p120;
	and.pred  	%p122, %p119, %p121;
	mov.u16 	%rs341, 1;
	@%p122 bra 	BB0_115;

BB0_114:
	mov.u16 	%rs341, %rs344;

BB0_115:
	add.s32 	%r73, %r73, 4;
	setp.lt.s32	%p123, %r73, %r814;
	@%p123 bra 	BB0_103;

BB0_116:
	and.b16  	%rs295, %rs341, 255;
	setp.eq.s16	%p124, %rs295, 0;
	@%p124 bra 	BB0_118;
	bra.uni 	BB0_117;

BB0_118:
	shr.u32 	%r770, %r814, 31;
	add.s32 	%r769, %r814, %r770;
	and.b32  	%r812, %r769, -2;
	setp.ge.s32	%p292, %r812, %r814;
	mov.u32 	%r378, -1;
	@%p292 bra 	BB0_119;

BB0_120:
	add.s32 	%r87, %r10, %r812;
	ld.shared.u8 	%rs37, [%r87];
	setp.eq.s16	%p126, %rs37, 0;
	setp.ne.s32	%p127, %r812, %r21;
	and.pred  	%p128, %p127, %p126;
	@%p128 bra 	BB0_122;

	cvt.u64.u16	%rd257, %rs37;
	add.s64 	%rd258, %rd257, %rd3;
	shl.b64 	%rd259, %rd258, 3;
	add.s64 	%rd260, %rd2, %rd259;
	ld.global.u8 	%rs296, [%rd260+1];
	and.b16  	%rs297, %rs296, 32;
	setp.eq.s16	%p129, %rs297, 0;
	@%p129 bra 	BB0_124;

BB0_122:
	mov.u32 	%r774, -1;
	add.s32 	%r812, %r812, 1;
	setp.lt.s32	%p130, %r812, %r814;
	@%p130 bra 	BB0_120;

	mov.u32 	%r812, %r774;
	bra.uni 	BB0_125;

BB0_119:
	mov.u32 	%r812, %r378;
	bra.uni 	BB0_125;

BB0_128:
	mov.u32 	%r815, %r814;
	bra.uni 	BB0_130;

BB0_124:
	add.s32 	%r775, %r10, %r812;
	add.s32 	%r767, %r10, %r814;
	setp.eq.s32	%p131, %r812, %r21;
	st.shared.u8 	[%r767], %rs37;
	ld.shared.u8 	%rs298, [%r775+1];
	st.shared.u8 	[%r767+1], %rs298;
	selp.b32	%r380, %r814, %r21, %p131;
	add.s32 	%r381, %r812, 1;
	setp.eq.s32	%p132, %r380, %r381;
	add.s32 	%r382, %r814, 1;
	selp.b32	%r21, %r382, %r380, %p132;

BB0_125:
	setp.lt.s32	%p133, %r812, 0;
	selp.b32	%r815, %r814, %r812, %p133;

BB0_130:
	setp.eq.s32	%p137, %r797, 0;
	selp.b16	%rs323, %rs332, %rs323, %p137;
	selp.b32	%r21, %r815, %r21, %p137;
	@%p54 bra 	BB0_132;

	shr.u32 	%r385, %r815, 31;
	add.s32 	%r386, %r815, %r385;
	and.b32  	%r387, %r386, -2;
	sub.s32 	%r388, %r815, %r387;
	add.s32 	%r389, %r815, 2;
	sub.s32 	%r817, %r389, %r388;

BB0_132:
	add.s32 	%r101, %r10, %r815;
	cvt.u16.u32	%rs39, %r797;
	st.shared.u8 	[%r101], %rs39;
	add.s32 	%r818, %r795, 1;
	@%p57 bra 	BB0_134;

	st.shared.u8 	[%r101+1], %rs39;
	add.s32 	%r818, %r795, 2;

BB0_134:
	shr.u32 	%r390, %r815, 31;
	add.s32 	%r391, %r815, %r390;
	shr.s32 	%r105, %r391, 1;
	setp.eq.s16	%p140, %rs331, 0;
	@%p140 bra 	BB0_136;

	shr.u32 	%r761, %r284, 13;
	and.b32  	%r760, %r761, 56;
	// inline asm
	bfe.u64 %rd261,%rd25,%r760,8;
	// inline asm
	cvt.u32.u64	%r394, %rd261;
	max.s32 	%r395, %r105, %r394;
	cvt.s64.s32	%rd264, %r395;
	// inline asm
	bfi.b64 %rd25,%rd264,%rd25,%r760,8;
	// inline asm

BB0_136:
	setp.ne.s16	%p141, %rs326, 0;
	setp.lt.s32	%p142, %r800, %r815;
	and.pred  	%p143, %p142, %p141;
	selp.b32	%r800, %r815, %r800, %p143;
	@%p54 bra 	BB0_138;
	bra.uni 	BB0_137;

BB0_138:
	@%p57 bra 	BB0_140;
	bra.uni 	BB0_139;

BB0_140:
	setp.ne.s16	%p146, %rs327, 0;
	@%p146 bra 	BB0_144;

	add.s32 	%r408, %r105, 1;
	cvt.u64.u32	%rd277, %r408;
	shl.b32 	%r407, %r803, 3;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r407,8;
	// inline asm
	@%p56 bra 	BB0_143;

	shr.u32 	%r763, %r284, 13;
	and.b32  	%r762, %r763, 56;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r762,8;
	// inline asm

BB0_143:
	// inline asm
	bfe.u64 %rd282,%rd25,%r407,8;
	// inline asm
	cvt.u32.u64	%r412, %rd282;
	max.s32 	%r413, %r105, %r412;
	cvt.s64.s32	%rd285, %r413;
	// inline asm
	bfi.b64 %rd25,%rd285,%rd25,%r407,8;
	// inline asm

BB0_144:
	setp.eq.s16	%p148, %rs329, 0;
	@%p148 bra 	BB0_146;

	add.s32 	%r414, %r105, 1;
	shl.b32 	%r415, %r414, 8;
	or.b32  	%r416, %r415, %r414;
	shl.b32 	%r417, %r416, 16;
	or.b32  	%r418, %r417, %r416;
	cvt.u64.u32	%rd287, %r418;
	shl.b64 	%rd288, %rd287, 32;
	or.b64  	%rd369, %rd288, %rd287;

BB0_146:
	setp.eq.s16	%p149, %rs327, 0;
	@%p149 bra 	BB0_148;

	shl.b32 	%r420, %r803, 3;
	// inline asm
	bfe.u64 %rd289,%rd25,%r420,8;
	// inline asm
	cvt.u32.u64	%r421, %rd289;
	max.s32 	%r422, %r105, %r421;
	cvt.s64.s32	%rd292, %r422;
	// inline asm
	bfi.b64 %rd25,%rd292,%rd25,%r420,8;
	// inline asm
	setp.gt.u32	%p150, %r284, -536870913;
	selp.b32	%r17, %r105, %r17, %p150;
	mov.u32 	%r18, %r105;
	bra.uni 	BB0_148;

BB0_137:
	add.s32 	%r396, %r105, 1;
	shl.b32 	%r397, %r396, 8;
	or.b32  	%r398, %r397, %r396;
	shl.b32 	%r399, %r398, 16;
	or.b32  	%r400, %r399, %r398;
	cvt.u64.u32	%rd266, %r400;
	shl.b64 	%rd267, %rd266, 32;
	or.b64  	%rd21, %rd267, %rd266;
	bra.uni 	BB0_148;

BB0_139:
	add.s32 	%r404, %r105, 1;
	cvt.u64.u32	%rd269, %r404;
	shl.b32 	%r403, %r803, 3;
	// inline asm
	bfi.b64 %rd21,%rd269,%rd21,%r403,8;
	// inline asm
	// inline asm
	bfe.u64 %rd271,%rd22,%r403,8;
	// inline asm
	cvt.u32.u64	%r405, %rd271;
	max.s32 	%r406, %r105, %r405;
	cvt.s64.s32	%rd274, %r406;
	// inline asm
	bfi.b64 %rd22,%rd274,%rd22,%r403,8;
	// inline asm

BB0_148:
	add.s32 	%r423, %r10, %r801;
	ld.shared.u8 	%rs300, [%r423];
	setp.eq.s16	%p151, %rs300, 0;
	setp.ne.s32	%p152, %r801, %r21;
	and.pred  	%p153, %p152, %p151;
	@%p153 bra 	BB0_154;

	and.b16  	%rs301, %rs325, 255;
	setp.eq.s16	%p154, %rs301, 0;
	@%p154 bra 	BB0_151;

	ld.global.u32 	%r424, [%rd23+128];
	or.b32  	%r425, %r424, 16384;
	st.global.u32 	[%rd23+128], %r425;

BB0_151:
	cvt.u32.u16	%r426, %rs332;
	add.s32 	%r801, %r426, %r801;

BB0_152:
	add.s32 	%r801, %r801, 1;
	mov.u16 	%rs325, 0;
	setp.gt.s32	%p155, %r801, 511;
	@%p155 bra 	BB0_154;

	add.s32 	%r427, %r10, %r801;
	ld.shared.u8 	%rs304, [%r427];
	setp.ne.s16	%p156, %rs304, 0;
	@%p156 bra 	BB0_152;

BB0_154:
	setp.eq.s16	%p157, %rs334, 0;
	@%p157 bra 	BB0_156;

	selp.b32	%r428, 1, 2, %p57;
	add.s32 	%r429, %r815, %r428;
	max.s32 	%r801, %r429, %r801;

BB0_156:
	cvt.u32.u16	%r430, %rs332;
	add.s32 	%r431, %r815, %r430;
	max.s32 	%r824, %r431, %r799;
	add.s32 	%r118, %r21, 1;

BB0_157:
	mov.u32 	%r119, %r824;
	add.s32 	%r432, %r10, %r119;
	ld.shared.u8 	%rs305, [%r432];
	setp.ne.s16	%p159, %rs305, 0;
	setp.eq.s32	%p160, %r119, %r21;
	or.pred  	%p161, %p160, %p159;
	setp.eq.s32	%p162, %r119, %r118;
	and.b16  	%rs306, %rs323, 255;
	setp.ne.s16	%p163, %rs306, 0;
	and.pred  	%p164, %p162, %p163;
	or.pred  	%p165, %p161, %p164;
	add.s32 	%r824, %r119, 1;
	@%p165 bra 	BB0_157;

	setp.ne.s16	%p166, %rs332, 0;
	mov.u16 	%rs324, 0;
	add.s32 	%r799, %r119, -1;
	setp.gt.s32	%p167, %r119, %r817;
	and.pred  	%p168, %p167, %p166;
	selp.b32	%r817, %r119, %r817, %p168;
	mov.u32 	%r795, %r818;

BB0_159:
	add.s32 	%r797, %r797, 1;
	setp.lt.u32	%p169, %r797, 256;
	@%p169 bra 	BB0_21;

	ld.param.u64 	%rd356, [_Z7init_vmILi2EEvPvS0_S0__param_1];
	cvta.to.global.u64 	%rd355, %rd356;
	mov.u32 	%r755, %tid.x;
	mov.u32 	%r754, %ntid.x;
	mov.u32 	%r753, %ctaid.x;
	mad.lo.s32 	%r752, %r753, %r754, %r755;
	shr.u32 	%r751, %r752, 3;
	ld.param.u64 	%rd354, [_Z7init_vmILi2EEvPvS0_S0__param_2];
	shr.u32 	%r433, %r799, 31;
	add.s32 	%r434, %r799, %r433;
	shr.s32 	%r435, %r434, 1;
	add.s32 	%r436, %r435, 1;
	cvta.to.global.u64 	%rd294, %rd354;
	atom.global.add.u32 	%r437, [%rd294], %r436;
	add.s64 	%rd295, %rd294, 4;
	atom.global.add.u32 	%r438, [%rd295], %r795;
	add.s64 	%rd297, %rd2, %rd139;
	ld.global.u32 	%r444, [%rd297+64];
	ld.global.u32 	%r445, [%rd297+80];
	ld.global.v2.u64 	{%rd298, %rd299}, [%rd297+96];
	and.b64  	%rd301, %rd298, 1;
	and.b64  	%rd302, %rd298, 2;
	shl.b64 	%rd303, %rd302, 7;
	or.b64  	%rd304, %rd303, %rd301;
	and.b64  	%rd305, %rd298, 4;
	shl.b64 	%rd306, %rd305, 14;
	or.b64  	%rd307, %rd304, %rd306;
	and.b64  	%rd308, %rd298, 8;
	shl.b64 	%rd309, %rd308, 21;
	or.b64  	%rd310, %rd307, %rd309;
	or.b64  	%rd311, %rd310, 100925952;
	shl.b64 	%rd312, %rd311, 3;
	and.b64  	%rd314, %rd299, 524287;
	shl.b64 	%rd315, %rd314, 6;
	ld.global.v2.u64 	{%rd316, %rd317}, [%rd297+112];
	and.b64  	%rd320, %rd316, 4194303;
	shr.u64 	%rd321, %rd316, 4;
	and.b64  	%rd322, %rd321, 1080863910568919040;
	or.b64  	%rd323, %rd320, %rd322;
	and.b64  	%rd324, %rd317, 4194303;
	shr.u64 	%rd325, %rd317, 4;
	and.b64  	%rd326, %rd325, 1080863910568919040;
	or.b64  	%rd327, %rd324, %rd326;
	mul.wide.u32 	%rd329, %r751, 2048;
	add.s64 	%rd47, %rd355, %rd329;
	cvt.u32.u64	%r446, %rd312;
	and.b32  	%r447, %r445, 2147483584;
	and.b32  	%r448, %r444, 2147483584;
	cvt.u32.u64	%r449, %rd315;
	st.global.v4.u32 	[%rd47+128], {%r448, %r447, %r446, %r449};
	or.b64  	%rd330, %rd327, 3458764513820540928;
	or.b64  	%rd331, %rd323, 3458764513820540928;
	st.global.v2.u64 	[%rd47+144], {%rd331, %rd330};
	add.s64 	%rd332, %rd329, %rd356;
	add.s64 	%rd48, %rd332, 1024;
	setp.lt.s32	%p170, %r799, 0;
	mov.u64 	%rd384, %rd48;
	@%p170 bra 	BB0_274;

	add.s64 	%rd383, %rd47, 1024;
	add.s32 	%r132, %r21, 1;
	mov.u32 	%r867, -1;
	mov.u32 	%r833, 0;
	mov.u32 	%r134, %r833;
	mov.u64 	%rd384, %rd48;
	mov.u32 	%r866, %r867;

BB0_162:
	add.s32 	%r454, %r10, %r833;
	ld.shared.u8 	%rs44, [%r454];
	setp.ne.s16	%p171, %rs44, 0;
	setp.eq.s32	%p172, %r833, %r21;
	or.pred  	%p173, %p172, %p171;
	setp.eq.s32	%p174, %r833, %r132;
	and.b16  	%rs308, %rs323, 255;
	setp.ne.s16	%p175, %rs308, 0;
	and.pred  	%p176, %p174, %p175;
	or.pred  	%p177, %p173, %p176;
	mov.u32 	%r869, %r833;
	@!%p177 bra 	BB0_273;
	bra.uni 	BB0_163;

BB0_163:
	add.s32 	%r837, %r833, 1;
	mov.u32 	%r841, 1;
	and.b32  	%r459, %r837, 1;
	setp.eq.b32	%p178, %r459, 1;
	not.pred 	%p179, %p178;
	mov.u32 	%r840, 0;
	setp.gt.u32	%p180, %r837, %r799;
	or.pred  	%p181, %p179, %p180;
	@%p181 bra 	BB0_168;

BB0_164:
	add.s32 	%r460, %r10, %r837;
	ld.shared.u8 	%rs45, [%r460];
	setp.ne.s16	%p182, %rs45, 0;
	setp.eq.s32	%p183, %r837, %r21;
	or.pred  	%p184, %p182, %p183;
	setp.eq.s32	%p185, %r837, %r132;
	and.pred  	%p187, %p185, %p175;
	or.pred  	%p188, %p184, %p187;
	@!%p188 bra 	BB0_168;
	bra.uni 	BB0_165;

BB0_165:
	and.b32  	%r461, %r841, 1;
	setp.eq.b32	%p190, %r461, 1;
	mov.pred 	%p293, 0;
	@!%p190 bra 	BB0_167;
	bra.uni 	BB0_166;

BB0_166:
	cvt.u64.u16	%rd333, %rs45;
	add.s64 	%rd334, %rd333, %rd3;
	shl.b64 	%rd335, %rd334, 3;
	add.s64 	%rd336, %rd2, %rd335;
	ld.global.u8 	%rs310, [%rd336+1];
	shr.u16 	%rs311, %rs310, 5;
	and.b16  	%rs312, %rs311, 1;
	setp.eq.b16	%p293, %rs312, 1;

BB0_167:
	selp.u32	%r462, 1, 0, %p293;
	add.s32 	%r840, %r462, %r840;
	add.s32 	%r841, %r841, 1;
	add.s32 	%r837, %r841, %r833;
	and.b32  	%r463, %r837, 1;
	setp.eq.b32	%p191, %r463, 1;
	setp.le.u32	%p192, %r837, %r799;
	and.pred  	%p193, %p191, %p192;
	@%p193 bra 	BB0_164;

BB0_168:
	add.s32 	%r464, %r841, 255;
	shl.b32 	%r465, %r464, 24;
	shl.b32 	%r466, %r840, 28;
	or.b32  	%r146, %r465, %r466;
	cvt.u64.u16	%rd337, %rs44;
	add.s64 	%rd338, %rd337, %rd3;
	shl.b64 	%rd339, %rd338, 3;
	add.s64 	%rd340, %rd2, %rd339;
	ld.global.v2.u32 	{%r467, %r468}, [%rd340];
	and.b32  	%r149, %r467, 255;
	shr.u32 	%r150, %r467, 8;
	bfe.u32 	%r151, %r467, 8, 3;
	shr.u32 	%r152, %r467, 16;
	bfe.u32 	%r153, %r467, 16, 3;
	and.b32  	%r469, %r833, 1;
	setp.eq.b32	%p194, %r469, 1;
	not.pred 	%p195, %p194;
	shr.u32 	%r470, %r467, 13;
	and.b32  	%r471, %r470, 1;
	setp.eq.b32	%p196, %r471, 1;
	and.pred  	%p197, %p195, %p196;
	selp.u32	%r472, 1, 0, %p197;
	add.s32 	%r869, %r472, %r833;
	shr.u32 	%r473, %r467, 14;
	and.b32  	%r474, %r473, 1;
	setp.eq.b32	%p198, %r474, 1;
	setp.lt.s32	%p199, %r867, 0;
	and.pred  	%p200, %p198, %p199;
	selp.b32	%r867, %r866, %r867, %p200;
	add.s32 	%r866, %r866, 1;
	setp.lt.u32	%p201, %r149, 25;
	mul.wide.u32 	%rd341, %r134, 4;
	add.s64 	%rd342, %rd47, %rd341;
	add.s64 	%rd54, %rd342, 256;
	@%p201 bra 	BB0_269;
	bra.uni 	BB0_169;

BB0_269:
	mov.u32 	%r864, 1048576;
	setp.ne.s32	%p287, %r151, 5;
	@%p287 bra 	BB0_272;

	shl.b32 	%r864, %r134, 6;
	setp.gt.u32	%p288, %r134, 189;
	@%p288 bra 	BB0_272;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r468;

BB0_272:
	or.b32  	%r732, %r151, %r146;
	shl.b32 	%r733, %r153, 3;
	or.b32  	%r734, %r732, %r733;
	shr.u32 	%r735, %r467, 11;
	and.b32  	%r736, %r735, 98304;
	or.b32  	%r737, %r734, %r736;
	or.b32  	%r738, %r737, %r864;
	add.s64 	%rd124, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r738;
	mov.u64 	%rd383, %rd124;
	bra.uni 	BB0_273;

BB0_169:
	add.s32 	%r475, %r149, -25;
	setp.lt.u32	%p202, %r475, 7;
	@%p202 bra 	BB0_266;
	bra.uni 	BB0_170;

BB0_266:
	setp.gt.u32	%p282, %r134, 189;
	mov.u32 	%r863, %r134;
	@%p282 bra 	BB0_268;

	and.b32  	%r718, %r467, 50331648;
	setp.eq.s32	%p283, %r718, 0;
	selp.b32	%r719, 2, 1, %p283;
	setp.eq.s32	%p284, %r153, %r151;
	selp.b32	%r720, 3, %r719, %p284;
	and.b32  	%r721, %r468, -65011713;
	setp.eq.s32	%p285, %r720, 2;
	selp.b32	%r722, 29360128, 23068672, %p285;
	setp.eq.s32	%p286, %r720, 1;
	selp.b32	%r723, 37748736, %r722, %p286;
	or.b32  	%r724, %r723, %r721;
	add.s32 	%r863, %r134, 1;
	st.global.u32 	[%rd54], %r724;

BB0_268:
	shl.b32 	%r725, %r134, 6;
	or.b32  	%r726, %r725, %r146;
	or.b32  	%r727, %r726, %r151;
	shl.b32 	%r728, %r153, 3;
	or.b32  	%r729, %r727, %r728;
	or.b32  	%r730, %r729, 1064960;
	add.s64 	%rd122, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r730;
	mov.u64 	%rd383, %rd122;
	mov.u32 	%r134, %r863;
	bra.uni 	BB0_273;

BB0_170:
	and.b32  	%r157, %r467, 240;
	setp.eq.s32	%p203, %r157, 32;
	@%p203 bra 	BB0_262;
	bra.uni 	BB0_171;

BB0_262:
	shl.b32 	%r713, %r153, 3;
	or.b32  	%r714, %r151, %r713;
	or.b32  	%r861, %r714, 1572864;
	setp.ne.s32	%p280, %r153, %r151;
	@%p280 bra 	BB0_265;

	shl.b32 	%r715, %r134, 6;
	or.b32  	%r716, %r715, %r861;
	or.b32  	%r861, %r716, 131072;
	setp.gt.u32	%p281, %r134, 189;
	@%p281 bra 	BB0_265;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r468;

BB0_265:
	add.s64 	%rd120, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r717, %r861, %r146;
	st.global.u32 	[%rd383], %r717;
	mov.u64 	%rd383, %rd120;
	bra.uni 	BB0_273;

BB0_171:
	add.s32 	%r476, %r149, -48;
	setp.lt.u32	%p204, %r476, 7;
	@%p204 bra 	BB0_259;
	bra.uni 	BB0_172;

BB0_259:
	setp.gt.u32	%p275, %r134, 189;
	mov.u32 	%r860, %r134;
	@%p275 bra 	BB0_261;

	and.b32  	%r700, %r467, 50331648;
	setp.eq.s32	%p276, %r700, 0;
	selp.b32	%r701, 2, 1, %p276;
	setp.eq.s32	%p277, %r153, %r151;
	selp.b32	%r702, 3, %r701, %p277;
	and.b32  	%r703, %r468, -65011713;
	setp.eq.s32	%p278, %r702, 2;
	selp.b32	%r704, 29360128, 23068672, %p278;
	setp.eq.s32	%p279, %r702, 1;
	selp.b32	%r705, 37748736, %r704, %p279;
	or.b32  	%r706, %r705, %r703;
	add.s32 	%r860, %r134, 1;
	st.global.u32 	[%rd54], %r706;

BB0_261:
	shl.b32 	%r707, %r134, 6;
	or.b32  	%r708, %r707, %r146;
	or.b32  	%r709, %r708, %r151;
	shl.b32 	%r710, %r153, 3;
	or.b32  	%r711, %r709, %r710;
	or.b32  	%r712, %r711, 1589248;
	add.s64 	%rd118, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r712;
	mov.u64 	%rd383, %rd118;
	mov.u32 	%r134, %r860;
	bra.uni 	BB0_273;

BB0_172:
	add.s32 	%r477, %r149, -55;
	setp.lt.u32	%p205, %r477, 16;
	@%p205 bra 	BB0_255;
	bra.uni 	BB0_173;

BB0_255:
	shl.b32 	%r695, %r153, 3;
	or.b32  	%r696, %r151, %r695;
	or.b32  	%r858, %r696, 2097152;
	setp.ne.s32	%p273, %r153, %r151;
	@%p273 bra 	BB0_258;

	shl.b32 	%r697, %r134, 6;
	or.b32  	%r698, %r697, %r858;
	or.b32  	%r858, %r698, 131072;
	setp.gt.u32	%p274, %r134, 189;
	@%p274 bra 	BB0_258;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r468;

BB0_258:
	add.s64 	%rd116, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r699, %r858, %r146;
	st.global.u32 	[%rd383], %r699;
	mov.u64 	%rd383, %rd116;
	bra.uni 	BB0_273;

BB0_173:
	add.s32 	%r478, %r149, -71;
	setp.lt.u32	%p206, %r478, 4;
	@%p206 bra 	BB0_252;
	bra.uni 	BB0_174;

BB0_252:
	setp.gt.u32	%p268, %r134, 189;
	mov.u32 	%r857, %r134;
	@%p268 bra 	BB0_254;

	and.b32  	%r682, %r467, 50331648;
	setp.eq.s32	%p269, %r682, 0;
	selp.b32	%r683, 2, 1, %p269;
	setp.eq.s32	%p270, %r153, %r151;
	selp.b32	%r684, 3, %r683, %p270;
	and.b32  	%r685, %r468, -65011713;
	setp.eq.s32	%p271, %r684, 2;
	selp.b32	%r686, 29360128, 23068672, %p271;
	setp.eq.s32	%p272, %r684, 1;
	selp.b32	%r687, 37748736, %r686, %p272;
	or.b32  	%r688, %r687, %r685;
	add.s32 	%r857, %r134, 1;
	st.global.u32 	[%rd54], %r688;

BB0_254:
	shl.b32 	%r689, %r134, 6;
	or.b32  	%r690, %r689, %r146;
	or.b32  	%r691, %r690, %r151;
	shl.b32 	%r692, %r153, 3;
	or.b32  	%r693, %r691, %r692;
	or.b32  	%r694, %r693, 2113536;
	add.s64 	%rd114, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r694;
	mov.u64 	%rd383, %rd114;
	mov.u32 	%r134, %r857;
	bra.uni 	BB0_273;

BB0_174:
	add.s32 	%r479, %r149, -75;
	setp.lt.u32	%p207, %r479, 4;
	@%p207 bra 	BB0_251;
	bra.uni 	BB0_175;

BB0_251:
	shl.b32 	%r678, %r153, 3;
	or.b32  	%r679, %r146, %r151;
	or.b32  	%r680, %r679, %r678;
	or.b32  	%r681, %r680, 6291456;
	add.s64 	%rd112, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r681;
	mov.u64 	%rd383, %rd112;
	bra.uni 	BB0_273;

BB0_175:
	setp.eq.s32	%p208, %r149, 79;
	@%p208 bra 	BB0_248;
	bra.uni 	BB0_176;

BB0_248:
	setp.gt.u32	%p263, %r134, 189;
	mov.u32 	%r856, %r134;
	@%p263 bra 	BB0_250;

	and.b32  	%r665, %r467, 50331648;
	setp.eq.s32	%p264, %r665, 0;
	selp.b32	%r666, 2, 1, %p264;
	setp.eq.s32	%p265, %r153, %r151;
	selp.b32	%r667, 3, %r666, %p265;
	and.b32  	%r668, %r468, -65011713;
	setp.eq.s32	%p266, %r667, 2;
	selp.b32	%r669, 29360128, 23068672, %p266;
	setp.eq.s32	%p267, %r667, 1;
	selp.b32	%r670, 37748736, %r669, %p267;
	or.b32  	%r671, %r670, %r668;
	add.s32 	%r856, %r134, 1;
	st.global.u32 	[%rd54], %r671;

BB0_250:
	shl.b32 	%r672, %r134, 6;
	or.b32  	%r673, %r672, %r146;
	or.b32  	%r674, %r673, %r151;
	shl.b32 	%r675, %r153, 3;
	or.b32  	%r676, %r674, %r675;
	or.b32  	%r677, %r676, 6307840;
	add.s64 	%rd110, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r677;
	mov.u64 	%rd383, %rd110;
	mov.u32 	%r134, %r856;
	bra.uni 	BB0_273;

BB0_176:
	and.b32  	%r480, %r467, 252;
	setp.eq.s32	%p209, %r480, 80;
	@%p209 bra 	BB0_247;
	bra.uni 	BB0_177;

BB0_247:
	shl.b32 	%r661, %r153, 3;
	or.b32  	%r662, %r146, %r151;
	or.b32  	%r663, %r662, %r661;
	or.b32  	%r664, %r663, 4194304;
	add.s64 	%rd108, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r664;
	mov.u64 	%rd383, %rd108;
	bra.uni 	BB0_273;

BB0_177:
	setp.eq.s32	%p210, %r149, 84;
	@%p210 bra 	BB0_244;
	bra.uni 	BB0_178;

BB0_244:
	setp.gt.u32	%p258, %r134, 189;
	mov.u32 	%r855, %r134;
	@%p258 bra 	BB0_246;

	and.b32  	%r648, %r467, 50331648;
	setp.eq.s32	%p259, %r648, 0;
	selp.b32	%r649, 2, 1, %p259;
	setp.eq.s32	%p260, %r153, %r151;
	selp.b32	%r650, 3, %r649, %p260;
	and.b32  	%r651, %r468, -65011713;
	setp.eq.s32	%p261, %r650, 2;
	selp.b32	%r652, 29360128, 23068672, %p261;
	setp.eq.s32	%p262, %r650, 1;
	selp.b32	%r653, 37748736, %r652, %p262;
	or.b32  	%r654, %r653, %r651;
	add.s32 	%r855, %r134, 1;
	st.global.u32 	[%rd54], %r654;

BB0_246:
	shl.b32 	%r655, %r134, 6;
	or.b32  	%r656, %r655, %r146;
	or.b32  	%r657, %r656, %r151;
	shl.b32 	%r658, %r153, 3;
	or.b32  	%r659, %r657, %r658;
	or.b32  	%r660, %r659, 4210688;
	add.s64 	%rd106, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r660;
	mov.u64 	%rd383, %rd106;
	mov.u32 	%r134, %r855;
	bra.uni 	BB0_273;

BB0_178:
	add.s32 	%r481, %r149, -85;
	setp.lt.u32	%p211, %r481, 8;
	add.s32 	%r482, %r134, 1;
	mul.wide.u32 	%rd343, %r482, 4;
	add.s64 	%rd344, %rd47, %rd343;
	add.s64 	%rd55, %rd344, 256;
	@%p211 bra 	BB0_234;
	bra.uni 	BB0_179;

BB0_234:
	add.s32 	%r630, %r468, -1;
	and.b32  	%r631, %r630, %r468;
	setp.eq.s32	%p252, %r631, 0;
	mov.u64 	%rd381, 1;
	@%p252 bra 	BB0_239;

	cvt.u64.u32	%rd92, %r468;
	// inline asm
	bfind.u32 %r632,%r468;
	// inline asm
	mov.pred 	%p253, 0;
	@%p253 bra 	BB0_237;
	bra.uni 	BB0_236;

BB0_237:
	cvt.u32.u64	%r636, %rd92;
	mov.u32 	%r853, 0;
	div.u32 	%r637, %r853, %r636;
	rem.u32 	%r638, %r853, %r636;
	cvt.u64.u32	%rd381, %r637;
	cvt.u64.u32	%rd380, %r638;
	bra.uni 	BB0_238;

BB0_179:
	add.s32 	%r483, %r149, -93;
	setp.lt.u32	%p212, %r483, 2;
	@%p212 bra 	BB0_233;
	bra.uni 	BB0_180;

BB0_233:
	or.b32  	%r628, %r146, %r151;
	or.b32  	%r629, %r628, 5242880;
	add.s64 	%rd90, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r629;
	mov.u64 	%rd383, %rd90;
	bra.uni 	BB0_273;

BB0_180:
	add.s32 	%r484, %r149, -95;
	setp.lt.u32	%p213, %r484, 15;
	@%p213 bra 	BB0_229;
	bra.uni 	BB0_181;

BB0_229:
	shl.b32 	%r623, %r153, 3;
	or.b32  	%r624, %r151, %r623;
	or.b32  	%r851, %r624, 3145728;
	setp.ne.s32	%p250, %r153, %r151;
	@%p250 bra 	BB0_232;

	shl.b32 	%r625, %r134, 6;
	or.b32  	%r626, %r625, %r851;
	or.b32  	%r851, %r626, 131072;
	setp.gt.u32	%p251, %r134, 189;
	@%p251 bra 	BB0_232;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r468;

BB0_232:
	add.s64 	%rd88, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r627, %r851, %r146;
	st.global.u32 	[%rd383], %r627;
	mov.u64 	%rd383, %rd88;
	bra.uni 	BB0_273;

BB0_236:
	mov.u64 	%rd346, -9223372036854775808;
	div.u64 	%rd381, %rd346, %rd92;
	rem.u64 	%rd380, %rd346, %rd92;
	mov.u32 	%r853, 0;

BB0_238:
	sub.s64 	%rd347, %rd92, %rd380;
	setp.ge.u64	%p254, %rd380, %rd347;
	selp.u64	%rd348, 1, 0, %p254;
	shl.b64 	%rd349, %rd381, 1;
	or.b64  	%rd381, %rd348, %rd349;
	selp.b64	%rd350, %rd92, 0, %p254;
	shl.b64 	%rd351, %rd380, 1;
	sub.s64 	%rd380, %rd351, %rd350;
	add.s32 	%r853, %r853, 1;
	setp.le.u32	%p255, %r853, %r632;
	@%p255 bra 	BB0_238;

BB0_239:
	setp.eq.s64	%p256, %rd381, 1;
	@%p256 bra 	BB0_243;
	bra.uni 	BB0_240;

BB0_243:
	or.b32  	%r647, %r146, 8388608;
	add.s64 	%rd104, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r647;
	mov.u64 	%rd383, %rd104;
	bra.uni 	BB0_273;

BB0_240:
	setp.gt.u32	%p257, %r134, 188;
	mov.u32 	%r854, %r134;
	@%p257 bra 	BB0_242;

	mov.b64	{%r639, %r640}, %rd381;
	st.global.u32 	[%rd54], %r639;
	st.global.u32 	[%rd55], %r640;
	add.s32 	%r854, %r134, 2;

BB0_242:
	shl.b32 	%r641, %r134, 6;
	or.b32  	%r642, %r641, %r146;
	or.b32  	%r643, %r642, %r151;
	shl.b32 	%r644, %r153, 3;
	or.b32  	%r645, %r643, %r644;
	or.b32  	%r646, %r645, 2359296;
	add.s64 	%rd102, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r646;
	mov.u64 	%rd383, %rd102;
	mov.u32 	%r134, %r854;
	bra.uni 	BB0_273;

BB0_181:
	add.s32 	%r485, %r149, -110;
	setp.lt.u32	%p214, %r485, 5;
	@%p214 bra 	BB0_226;
	bra.uni 	BB0_182;

BB0_226:
	setp.gt.u32	%p245, %r134, 189;
	mov.u32 	%r850, %r134;
	@%p245 bra 	BB0_228;

	and.b32  	%r610, %r467, 50331648;
	setp.eq.s32	%p246, %r610, 0;
	selp.b32	%r611, 2, 1, %p246;
	setp.eq.s32	%p247, %r153, %r151;
	selp.b32	%r612, 3, %r611, %p247;
	and.b32  	%r613, %r468, -65011713;
	setp.eq.s32	%p248, %r612, 2;
	selp.b32	%r614, 29360128, 23068672, %p248;
	setp.eq.s32	%p249, %r612, 1;
	selp.b32	%r615, 37748736, %r614, %p249;
	or.b32  	%r616, %r615, %r613;
	add.s32 	%r850, %r134, 1;
	st.global.u32 	[%rd54], %r616;

BB0_228:
	shl.b32 	%r617, %r134, 6;
	or.b32  	%r618, %r617, %r146;
	or.b32  	%r619, %r618, %r151;
	shl.b32 	%r620, %r153, 3;
	or.b32  	%r621, %r619, %r620;
	or.b32  	%r622, %r621, 3162112;
	add.s64 	%rd86, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r622;
	mov.u64 	%rd383, %rd86;
	mov.u32 	%r134, %r850;
	bra.uni 	BB0_273;

BB0_182:
	add.s32 	%r486, %r149, -115;
	setp.lt.u32	%p215, %r486, 10;
	@%p215 bra 	BB0_222;
	bra.uni 	BB0_183;

BB0_222:
	shl.b32 	%r605, %r153, 3;
	or.b32  	%r606, %r151, %r605;
	or.b32  	%r848, %r606, 7340032;
	setp.ne.s32	%p243, %r153, %r151;
	@%p243 bra 	BB0_225;

	shl.b32 	%r607, %r134, 6;
	or.b32  	%r608, %r607, %r848;
	or.b32  	%r848, %r608, 131072;
	setp.gt.u32	%p244, %r134, 189;
	@%p244 bra 	BB0_225;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r468;

BB0_225:
	add.s64 	%rd84, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r609, %r848, %r146;
	st.global.u32 	[%rd383], %r609;
	mov.u64 	%rd383, %rd84;
	bra.uni 	BB0_273;

BB0_183:
	add.s32 	%r487, %r149, -125;
	setp.lt.u32	%p216, %r487, 4;
	@%p216 bra 	BB0_221;
	bra.uni 	BB0_184;

BB0_221:
	shl.b32 	%r600, %r153, 3;
	or.b32  	%r601, %r151, %r600;
	or.b32  	%r602, %r601, 8388608;
	setp.eq.s32	%p242, %r153, %r151;
	selp.b32	%r603, 8388608, %r602, %p242;
	or.b32  	%r604, %r603, %r146;
	add.s64 	%rd82, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r604;
	mov.u64 	%rd383, %rd82;
	bra.uni 	BB0_273;

BB0_184:
	add.s32 	%r488, %r149, -129;
	setp.lt.u32	%p217, %r488, 8;
	@%p217 bra 	BB0_220;
	bra.uni 	BB0_185;

BB0_220:
	or.b32  	%r598, %r146, %r151;
	or.b32  	%r599, %r598, 11534336;
	add.s64 	%rd80, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r599;
	mov.u64 	%rd383, %rd80;
	bra.uni 	BB0_273;

BB0_185:
	add.s32 	%r489, %r149, -137;
	setp.lt.u32	%p218, %r489, 20;
	@%p218 bra 	BB0_219;
	bra.uni 	BB0_186;

BB0_219:
	and.b32  	%r592, %r150, 3;
	and.b32  	%r593, %r152, 3;
	shl.b32 	%r594, %r593, 4;
	or.b32  	%r595, %r146, %r592;
	or.b32  	%r596, %r595, %r594;
	or.b32  	%r597, %r596, 12582912;
	add.s64 	%rd78, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r597;
	mov.u64 	%rd383, %rd78;
	bra.uni 	BB0_273;

BB0_186:
	add.s32 	%r490, %r149, -157;
	setp.lt.u32	%p219, %r490, 5;
	@%p219 bra 	BB0_216;
	bra.uni 	BB0_187;

BB0_216:
	setp.gt.u32	%p240, %r134, 189;
	mov.u32 	%r847, %r134;
	@%p240 bra 	BB0_218;

	and.b32  	%r581, %r467, 50331648;
	setp.eq.s32	%p241, %r581, 0;
	selp.b32	%r582, 29360128, 37748736, %p241;
	and.b32  	%r583, %r468, -65011713;
	or.b32  	%r584, %r582, %r583;
	add.s32 	%r847, %r134, 1;
	st.global.u32 	[%rd54], %r584;

BB0_218:
	shl.b32 	%r585, %r134, 6;
	or.b32  	%r586, %r585, %r146;
	and.b32  	%r587, %r150, 3;
	or.b32  	%r588, %r586, %r587;
	shl.b32 	%r589, %r153, 3;
	or.b32  	%r590, %r588, %r589;
	or.b32  	%r591, %r590, 12599296;
	add.s64 	%rd76, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r591;
	mov.u64 	%rd383, %rd76;
	mov.u32 	%r134, %r847;
	bra.uni 	BB0_273;

BB0_187:
	add.s32 	%r491, %r149, -162;
	setp.lt.u32	%p220, %r491, 20;
	@%p220 bra 	BB0_215;
	bra.uni 	BB0_188;

BB0_215:
	and.b32  	%r575, %r150, 3;
	and.b32  	%r576, %r152, 3;
	shl.b32 	%r577, %r576, 4;
	or.b32  	%r578, %r146, %r575;
	or.b32  	%r579, %r578, %r577;
	or.b32  	%r580, %r579, 13107200;
	add.s64 	%rd74, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r580;
	mov.u64 	%rd383, %rd74;
	bra.uni 	BB0_273;

BB0_188:
	add.s32 	%r492, %r149, -182;
	setp.lt.u32	%p221, %r492, 5;
	@%p221 bra 	BB0_212;
	bra.uni 	BB0_189;

BB0_212:
	setp.gt.u32	%p238, %r134, 189;
	mov.u32 	%r846, %r134;
	@%p238 bra 	BB0_214;

	and.b32  	%r564, %r467, 50331648;
	setp.eq.s32	%p239, %r564, 0;
	selp.b32	%r565, 29360128, 37748736, %p239;
	and.b32  	%r566, %r468, -65011713;
	or.b32  	%r567, %r565, %r566;
	add.s32 	%r846, %r134, 1;
	st.global.u32 	[%rd54], %r567;

BB0_214:
	shl.b32 	%r568, %r134, 6;
	or.b32  	%r569, %r568, %r146;
	and.b32  	%r570, %r150, 3;
	or.b32  	%r571, %r569, %r570;
	shl.b32 	%r572, %r153, 3;
	or.b32  	%r573, %r571, %r572;
	or.b32  	%r574, %r573, 13123584;
	add.s64 	%rd72, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r574;
	mov.u64 	%rd383, %rd72;
	mov.u32 	%r134, %r846;
	bra.uni 	BB0_273;

BB0_189:
	add.s32 	%r493, %r149, -187;
	setp.lt.u32	%p222, %r493, 6;
	@%p222 bra 	BB0_209;
	bra.uni 	BB0_190;

BB0_209:
	setp.gt.u32	%p237, %r134, 188;
	mov.u32 	%r845, %r134;
	@%p237 bra 	BB0_211;

	mov.u32 	%r557, 0;
	st.global.u32 	[%rd54], %r557;
	mov.u32 	%r558, -2131755008;
	st.global.u32 	[%rd55], %r558;
	add.s32 	%r845, %r134, 2;

BB0_211:
	shl.b32 	%r559, %r134, 6;
	or.b32  	%r560, %r559, %r146;
	and.b32  	%r561, %r150, 3;
	or.b32  	%r562, %r560, %r561;
	or.b32  	%r563, %r562, 3407872;
	add.s64 	%rd70, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r563;
	mov.u64 	%rd383, %rd70;
	mov.u32 	%r134, %r845;
	bra.uni 	BB0_273;

BB0_190:
	add.s32 	%r494, %r149, -193;
	setp.lt.u32	%p223, %r494, 20;
	@%p223 bra 	BB0_208;
	bra.uni 	BB0_191;

BB0_208:
	and.b32  	%r550, %r150, 3;
	add.s32 	%r551, %r550, 4;
	and.b32  	%r552, %r152, 3;
	shl.b32 	%r553, %r552, 4;
	or.b32  	%r554, %r146, %r553;
	or.b32  	%r555, %r554, %r551;
	or.b32  	%r556, %r555, 12615680;
	add.s64 	%rd68, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r556;
	mov.u64 	%rd383, %rd68;
	bra.uni 	BB0_273;

BB0_191:
	add.s32 	%r495, %r149, -213;
	setp.lt.u32	%p224, %r495, 4;
	@%p224 bra 	BB0_205;
	bra.uni 	BB0_192;

BB0_205:
	setp.gt.u32	%p235, %r134, 189;
	mov.u32 	%r844, %r134;
	@%p235 bra 	BB0_207;

	and.b32  	%r538, %r467, 50331648;
	setp.eq.s32	%p236, %r538, 0;
	selp.b32	%r539, 29360128, 37748736, %p236;
	and.b32  	%r540, %r468, -65011713;
	or.b32  	%r541, %r539, %r540;
	add.s32 	%r844, %r134, 1;
	st.global.u32 	[%rd54], %r541;

BB0_207:
	shl.b32 	%r542, %r134, 6;
	or.b32  	%r543, %r542, %r146;
	shl.b32 	%r544, %r153, 3;
	or.b32  	%r545, %r543, %r544;
	and.b32  	%r546, %r150, 3;
	add.s32 	%r547, %r546, 4;
	or.b32  	%r548, %r545, %r547;
	or.b32  	%r549, %r548, 15745024;
	add.s64 	%rd66, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r549;
	mov.u64 	%rd383, %rd66;
	mov.u32 	%r134, %r844;
	bra.uni 	BB0_273;

BB0_192:
	add.s32 	%r496, %r149, -217;
	setp.lt.u32	%p225, %r496, 6;
	@%p225 bra 	BB0_204;
	bra.uni 	BB0_193;

BB0_204:
	and.b32  	%r534, %r150, 3;
	add.s32 	%r535, %r534, 4;
	or.b32  	%r536, %r146, %r535;
	or.b32  	%r537, %r536, 14680064;
	add.s64 	%rd64, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r537;
	mov.u64 	%rd383, %rd64;
	bra.uni 	BB0_273;

BB0_193:
	add.s32 	%r497, %r149, -223;
	setp.lt.u32	%p226, %r497, 16;
	@%p226 bra 	BB0_201;
	bra.uni 	BB0_194;

BB0_201:
	setp.gt.u32	%p234, %r134, 188;
	mov.u32 	%r843, %r134;
	@%p234 bra 	BB0_203;

	shr.u32 	%r518, %r467, 28;
	add.s32 	%r519, %r518, 8;
	add.s32 	%r520, %r518, 7;
	mov.u32 	%r521, 1;
	shl.b32 	%r522, %r521, %r520;
	not.b32 	%r523, %r522;
	shl.b32 	%r524, %r521, %r519;
	or.b32  	%r525, %r524, %r468;
	and.b32  	%r526, %r525, %r523;
	st.global.u32 	[%rd54], %r526;
	shl.b32 	%r527, %r867, 5;
	or.b32  	%r528, %r527, %r519;
	st.global.u32 	[%rd55], %r528;
	add.s32 	%r843, %r134, 2;

BB0_203:
	shl.b32 	%r530, %r134, 6;
	or.b32  	%r531, %r530, %r146;
	or.b32  	%r532, %r531, %r151;
	or.b32  	%r533, %r532, 9437184;
	add.s64 	%rd62, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r533;
	mov.u32 	%r867, -1;
	mov.u64 	%rd383, %rd62;
	mov.u32 	%r134, %r843;
	bra.uni 	BB0_273;

BB0_194:
	setp.eq.s32	%p227, %r149, 239;
	@%p227 bra 	BB0_200;
	bra.uni 	BB0_195;

BB0_200:
	shl.b32 	%r512, %r153, 3;
	and.b32  	%r513, %r468, 63;
	shl.b32 	%r514, %r513, 6;
	or.b32  	%r515, %r146, %r514;
	or.b32  	%r516, %r515, %r512;
	or.b32  	%r517, %r516, 13631488;
	add.s64 	%rd60, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r517;
	mov.u64 	%rd383, %rd60;
	bra.uni 	BB0_273;

BB0_195:
	setp.eq.s32	%p228, %r157, 240;
	@%p228 bra 	BB0_197;
	bra.uni 	BB0_196;

BB0_197:
	setp.gt.u32	%p229, %r134, 189;
	mov.u32 	%r842, %r134;
	@%p229 bra 	BB0_199;

	and.b32  	%r499, %r467, 50331648;
	setp.eq.s32	%p230, %r499, 0;
	selp.b32	%r500, 2, 1, %p230;
	setp.gt.u32	%p231, %r467, -536870913;
	selp.b32	%r501, 3, %r500, %p231;
	and.b32  	%r502, %r468, -65011713;
	setp.eq.s32	%p232, %r501, 2;
	selp.b32	%r503, 29360128, 23068672, %p232;
	setp.eq.s32	%p233, %r501, 1;
	selp.b32	%r504, 37748736, %r503, %p233;
	or.b32  	%r505, %r504, %r502;
	st.global.u32 	[%rd54], %r505;
	mov.u32 	%r842, %r482;

BB0_199:
	shl.b32 	%r506, %r134, 6;
	or.b32  	%r507, %r506, %r146;
	or.b32  	%r508, %r507, %r151;
	shl.b32 	%r509, %r153, 3;
	or.b32  	%r510, %r508, %r509;
	or.b32  	%r511, %r510, 10502144;
	add.s64 	%rd58, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r511;
	mov.u64 	%rd383, %rd58;
	mov.u32 	%r134, %r842;
	bra.uni 	BB0_273;

BB0_196:
	or.b32  	%r498, %r146, 8388608;
	add.s64 	%rd56, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r498;
	mov.u64 	%rd383, %rd56;

BB0_273:
	add.s32 	%r833, %r869, 1;
	setp.lt.s32	%p289, %r869, %r799;
	@%p289 bra 	BB0_162;

BB0_274:
	sub.s64 	%rd352, %rd384, %rd48;
	shr.u64 	%rd353, %rd352, 2;
	st.global.u32 	[%rd47+160], %rd353;

BB0_275:
	ret;
}

	// .globl	_Z7init_vmILi4EEvPvS0_S0_
.visible .entry _Z7init_vmILi4EEvPvS0_S0_(
	.param .u64 _Z7init_vmILi4EEvPvS0_S0__param_0,
	.param .u64 _Z7init_vmILi4EEvPvS0_S0__param_1,
	.param .u64 _Z7init_vmILi4EEvPvS0_S0__param_2
)
.maxntid 32, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<293>;
	.reg .b16 	%rs<351>;
	.reg .b32 	%r<888>;
	.reg .b64 	%rd<386>;
	// demoted variable
	.shared .align 4 .b8 _ZZ7init_vmILi4EEvPvS0_S0_E18execution_plan_buf[4096];

	ld.param.u64 	%rd129, [_Z7init_vmILi4EEvPvS0_S0__param_0];
	ld.param.u64 	%rd130, [_Z7init_vmILi4EEvPvS0_S0__param_1];
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r808, %r1, 2;
	mov.u32 	%r3, %ntid.x;
	setp.gt.u32	%p4, %r808, 4095;
	@%p4 bra 	BB1_3;

	shl.b32 	%r4, %r3, 2;
	shl.b32 	%r217, %r1, 2;
	mov.u32 	%r218, _ZZ7init_vmILi4EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r807, %r218, %r217;

BB1_2:
	mov.u32 	%r219, 0;
	st.shared.u32 	[%r807], %r219;
	add.s32 	%r807, %r807, %r4;
	add.s32 	%r808, %r808, %r4;
	setp.lt.u32	%p5, %r808, 4096;
	@%p5 bra 	BB1_2;

BB1_3:
	bar.warp.sync 	-1;
	mov.u32 	%r220, %ctaid.x;
	mad.lo.s32 	%r221, %r220, %r3, %r1;
	shr.u32 	%r222, %r221, 3;
	and.b32  	%r223, %r221, 7;
	mul.wide.u32 	%rd132, %r222, 256;
	cvt.u64.u32	%rd133, %r223;
	or.b64  	%rd134, %rd132, %rd133;
	cvta.to.global.u64 	%rd135, %rd130;
	shl.b64 	%rd136, %rd134, 3;
	add.s64 	%rd137, %rd135, %rd136;
	mov.u64 	%rd360, 0;
	st.global.u64 	[%rd137], %rd360;
	mul.wide.u32 	%rd139, %r222, 2176;
	shr.u64 	%rd1, %rd139, 3;
	or.b64  	%rd140, %rd1, %rd133;
	cvta.to.global.u64 	%rd2, %rd129;
	shl.b64 	%rd141, %rd140, 3;
	add.s64 	%rd142, %rd2, %rd141;
	ld.global.u64 	%rd143, [%rd142];
	shr.u64 	%rd144, %rd143, 7;
	and.b64  	%rd145, %rd144, 139611588448485376;
	and.b64  	%rd146, %rd143, 4503599627370495;
	add.s64 	%rd147, %rd145, 4607182418800017408;
	or.b64  	%rd148, %rd147, %rd146;
	st.global.u64 	[%rd137+192], %rd148;
	setp.ne.s32	%p6, %r223, 0;
	@%p6 bra 	BB1_275;

	shl.b32 	%r225, %r1, 7;
	and.b32  	%r226, %r225, -1024;
	mov.u32 	%r227, _ZZ7init_vmILi4EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r10, %r227, %r226;
	add.s64 	%rd3, %rd1, 16;
	mov.u32 	%r809, 0;
	mov.u64 	%rd359, %rd360;

BB1_5:
	cvt.u64.u32	%rd6, %r809;
	add.s64 	%rd151, %rd6, %rd1;
	shl.b64 	%rd152, %rd151, 3;
	add.s64 	%rd7, %rd2, %rd152;
	ld.global.u32 	%r12, [%rd7+128];
	and.b32  	%r228, %r12, -63489;
	st.global.u32 	[%rd7+128], %r228;
	and.b32  	%r13, %r12, 255;
	setp.lt.u32	%p7, %r13, 85;
	@%p7 bra 	BB1_18;
	bra.uni 	BB1_6;

BB1_18:
	bfe.u32 	%r806, %r12, 8, 3;
	shl.b32 	%r267, %r806, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r267,8;
	// inline asm
	mov.u64 	%rd192, 1;
	// inline asm
	bfi.b64 %rd359,%rd192,%rd359,%r267,8;
	// inline asm
	bra.uni 	BB1_19;

BB1_6:
	and.b32  	%r794, %r12, 255;
	add.s32 	%r229, %r794, -85;
	setp.lt.u32	%p8, %r229, 8;
	@%p8 bra 	BB1_16;
	bra.uni 	BB1_7;

BB1_16:
	ld.global.u32 	%r261, [%rd7+132];
	add.s32 	%r262, %r261, -1;
	and.b32  	%r263, %r262, %r261;
	setp.eq.s32	%p16, %r263, 0;
	@%p16 bra 	BB1_19;

	bfe.u32 	%r805, %r12, 8, 3;
	shl.b32 	%r265, %r805, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r265,8;
	// inline asm
	mov.u64 	%rd186, 1;
	// inline asm
	bfi.b64 %rd359,%rd186,%rd359,%r265,8;
	// inline asm
	bra.uni 	BB1_19;

BB1_7:
	and.b32  	%r795, %r12, 255;
	add.s32 	%r230, %r795, -93;
	setp.lt.u32	%p9, %r230, 32;
	@%p9 bra 	BB1_15;
	bra.uni 	BB1_8;

BB1_15:
	bfe.u32 	%r804, %r12, 8, 3;
	shl.b32 	%r260, %r804, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r260,8;
	// inline asm
	mov.u64 	%rd180, 1;
	// inline asm
	bfi.b64 %rd359,%rd180,%rd359,%r260,8;
	// inline asm
	bra.uni 	BB1_19;

BB1_8:
	and.b32  	%r796, %r12, 255;
	add.s32 	%r231, %r796, -125;
	setp.lt.u32	%p10, %r231, 4;
	@%p10 bra 	BB1_13;
	bra.uni 	BB1_9;

BB1_13:
	bfe.u32 	%r801, %r12, 16, 3;
	bfe.u32 	%r800, %r12, 8, 3;
	setp.eq.s32	%p15, %r801, %r800;
	@%p15 bra 	BB1_19;

	bfe.u32 	%r803, %r12, 16, 3;
	bfe.u32 	%r802, %r12, 8, 3;
	shl.b32 	%r256, %r802, 3;
	// inline asm
	bfi.b64 %rd164,%rd6,%rd360,%r256,8;
	// inline asm
	mov.u64 	%rd174, 1;
	// inline asm
	bfi.b64 %rd167,%rd174,%rd359,%r256,8;
	// inline asm
	shl.b32 	%r258, %r803, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd164,%r258,8;
	// inline asm
	// inline asm
	bfi.b64 %rd359,%rd174,%rd167,%r258,8;
	// inline asm
	bra.uni 	BB1_19;

BB1_9:
	and.b32  	%r797, %r12, 255;
	add.s32 	%r232, %r797, -129;
	setp.lt.u32	%p11, %r232, 94;
	@%p11 bra 	BB1_12;
	bra.uni 	BB1_10;

BB1_12:
	ld.global.u32 	%r253, [%rd7+128];
	or.b32  	%r254, %r253, 8192;
	st.global.u32 	[%rd7+128], %r254;
	bra.uni 	BB1_19;

BB1_10:
	and.b32  	%r798, %r12, 255;
	add.s32 	%r233, %r798, -223;
	setp.gt.u32	%p12, %r233, 15;
	@%p12 bra 	BB1_19;

	bfe.u32 	%r799, %r12, 8, 3;
	shl.b32 	%r235, %r799, 3;
	// inline asm
	bfe.u64 %rd153,%rd360,%r235,8;
	// inline asm
	cvt.u32.u64	%r236, %rd153;
	// inline asm
	bfe.u64 %rd155,%rd359,%r235,8;
	// inline asm
	cvt.u32.u64	%r237, %rd155;
	setp.eq.s32	%p13, %r237, 0;
	selp.b32	%r238, -1, %r236, %p13;
	setp.eq.s32	%p14, %r238, -1;
	selp.b32	%r239, 144, 16, %p14;
	or.b32  	%r240, %r239, %r799;
	shl.b32 	%r241, %r240, 8;
	shl.b32 	%r242, %r238, 16;
	and.b32  	%r243, %r242, 16711680;
	and.b32  	%r244, %r12, -16776961;
	or.b32  	%r245, %r243, %r244;
	or.b32  	%r246, %r245, %r241;
	st.global.u32 	[%rd7+128], %r246;
	cvt.s64.s32	%rd158, %r238;
	add.s64 	%rd159, %rd158, %rd3;
	shl.b64 	%rd160, %rd159, 3;
	add.s64 	%rd161, %rd160, %rd2;
	ld.global.u32 	%r247, [%rd161+8];
	or.b32  	%r248, %r247, 16384;
	st.global.u32 	[%rd161+8], %r248;
	shl.b32 	%r249, %r809, 8;
	or.b32  	%r250, %r249, %r809;
	shl.b32 	%r251, %r250, 16;
	or.b32  	%r252, %r251, %r250;
	cvt.u64.u32	%rd162, %r252;
	shl.b64 	%rd163, %rd162, 32;
	or.b64  	%rd360, %rd163, %rd162;
	mov.u64 	%rd359, 72340172838076673;

BB1_19:
	cvt.u32.u64	%r277, %rd6;
	add.s32 	%r809, %r277, 1;
	mov.u32 	%r21, -1;
	mov.u32 	%r17, 0;
	mov.u64 	%rd369, 0;
	mov.u16 	%rs323, 0;
	setp.ne.s32	%p17, %r809, 256;
	@%p17 bra 	BB1_5;

	mov.u32 	%r18, %r17;
	mov.u32 	%r835, %r17;
	mov.u32 	%r813, %r17;
	mov.u16 	%rs324, %rs323;
	mov.u16 	%rs325, %rs323;
	mov.u32 	%r815, %r17;
	mov.u64 	%rd25, %rd369;
	mov.u64 	%rd21, %rd369;
	mov.u64 	%rd22, %rd369;
	mov.u32 	%r819, %r17;
	mov.u32 	%r817, %r21;
	mov.u32 	%r818, %r21;
	bra.uni 	BB1_21;

BB1_33:
	add.s32 	%r299, %r28, -85;
	setp.lt.u32	%p31, %r299, 8;
	@%p31 bra 	BB1_72;
	bra.uni 	BB1_34;

BB1_72:
	add.s32 	%r339, %r286, -1;
	and.b32  	%r340, %r339, %r286;
	setp.eq.s32	%p50, %r340, 0;
	selp.b32	%r820, 0, %r35, %p50;
	selp.u16	%rs328, 1, 0, %p50;
	mov.u16 	%rs326, 0;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;
	mov.u16 	%rs331, %rs326;
	bra.uni 	BB1_73;

BB1_34:
	add.s32 	%r300, %r28, -93;
	setp.lt.u32	%p32, %r300, 2;
	mov.u16 	%rs333, 0;
	@%p32 bra 	BB1_35;
	bra.uni 	BB1_37;

BB1_35:
	mov.u32 	%r820, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;

BB1_36:
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB1_74;

BB1_37:
	add.s32 	%r301, %r28, -95;
	setp.lt.u32	%p33, %r301, 15;
	@%p33 bra 	BB1_38;

	add.s32 	%r302, %r28, -110;
	setp.lt.u32	%p34, %r302, 5;
	@%p34 bra 	BB1_41;
	bra.uni 	BB1_42;

BB1_41:
	mov.u32 	%r820, %r39;
	mov.u16 	%rs326, %rs331;
	bra.uni 	BB1_39;

BB1_42:
	add.s32 	%r303, %r28, -115;
	setp.lt.u32	%p35, %r303, 10;
	@%p35 bra 	BB1_38;
	bra.uni 	BB1_43;

BB1_38:
	mov.u16 	%rs326, %rs333;

BB1_39:
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB1_74;

BB1_43:
	add.s32 	%r304, %r28, -125;
	setp.lt.u32	%p36, %r304, 4;
	@%p36 bra 	BB1_71;
	bra.uni 	BB1_44;

BB1_71:
	bfe.u32 	%r776, %r285, 16, 3;
	setp.eq.s32	%p290, %r821, %r776;
	selp.b32	%r820, 0, %r820, %p290;
	selp.u16	%rs328, 1, 0, %p290;
	mov.u16 	%rs330, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs331, %rs330;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB1_74;

BB1_44:
	add.s32 	%r305, %r28, -129;
	setp.lt.u32	%p37, %r305, 8;
	@%p37 bra 	BB1_69;
	bra.uni 	BB1_45;

BB1_69:
	shl.b32 	%r762, %r821, 3;
	// inline asm
	bfe.u64 %rd223,%rd21,%r762,8;
	// inline asm
	cvt.u32.u64	%r820, %rd223;
	bra.uni 	BB1_70;

BB1_45:
	add.s32 	%r306, %r28, -137;
	setp.lt.u32	%p38, %r306, 20;
	@%p38 bra 	BB1_68;
	bra.uni 	BB1_46;

BB1_68:
	shr.u32 	%r761, %r285, 8;
	and.b32  	%r821, %r761, 3;
	shl.b32 	%r337, %r821, 3;
	// inline asm
	bfe.u64 %rd221,%rd21,%r337,8;
	// inline asm
	cvt.u32.u64	%r820, %rd221;
	bra.uni 	BB1_70;

BB1_46:
	add.s32 	%r307, %r28, -157;
	setp.lt.u32	%p39, %r307, 5;
	@%p39 bra 	BB1_67;
	bra.uni 	BB1_47;

BB1_67:
	shr.u32 	%r760, %r285, 8;
	and.b32  	%r821, %r760, 3;
	shl.b32 	%r334, %r821, 3;
	// inline asm
	bfe.u64 %rd219,%rd21,%r334,8;
	// inline asm
	cvt.u32.u64	%r335, %rd219;
	max.u32 	%r336, %r37, %r335;
	max.u32 	%r820, %r18, %r336;
	bra.uni 	BB1_65;

BB1_47:
	add.s32 	%r308, %r28, -162;
	setp.lt.u32	%p40, %r308, 20;
	@%p40 bra 	BB1_66;
	bra.uni 	BB1_48;

BB1_66:
	shr.u32 	%r759, %r285, 8;
	and.b32  	%r821, %r759, 3;
	shl.b32 	%r333, %r821, 3;
	// inline asm
	bfe.u64 %rd217,%rd21,%r333,8;
	// inline asm
	cvt.u32.u64	%r820, %rd217;
	bra.uni 	BB1_70;

BB1_48:
	add.s32 	%r309, %r28, -182;
	setp.lt.u32	%p41, %r309, 5;
	@%p41 bra 	BB1_64;
	bra.uni 	BB1_49;

BB1_64:
	shr.u32 	%r758, %r285, 8;
	and.b32  	%r821, %r758, 3;
	shl.b32 	%r330, %r821, 3;
	// inline asm
	bfe.u64 %rd215,%rd21,%r330,8;
	// inline asm
	cvt.u32.u64	%r331, %rd215;
	max.u32 	%r332, %r37, %r331;
	max.u32 	%r820, %r18, %r332;

BB1_65:
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs326;
	bra.uni 	BB1_74;

BB1_49:
	add.s32 	%r310, %r28, -187;
	setp.lt.u32	%p42, %r310, 6;
	@%p42 bra 	BB1_63;
	bra.uni 	BB1_50;

BB1_63:
	shr.u32 	%r757, %r285, 8;
	and.b32  	%r821, %r757, 3;
	shl.b32 	%r329, %r821, 3;
	// inline asm
	bfe.u64 %rd213,%rd21,%r329,8;
	// inline asm
	cvt.u32.u64	%r820, %rd213;
	bra.uni 	BB1_70;

BB1_50:
	add.s32 	%r311, %r28, -193;
	setp.lt.u32	%p43, %r311, 20;
	@%p43 bra 	BB1_62;
	bra.uni 	BB1_51;

BB1_62:
	shr.u32 	%r756, %r285, 8;
	and.b32  	%r328, %r756, 3;
	add.s32 	%r821, %r328, 4;
	shl.b32 	%r327, %r821, 3;
	// inline asm
	bfe.u64 %rd211,%rd21,%r327,8;
	// inline asm
	cvt.u32.u64	%r820, %rd211;
	bra.uni 	BB1_70;

BB1_51:
	add.s32 	%r312, %r28, -213;
	setp.lt.u32	%p44, %r312, 4;
	@%p44 bra 	BB1_61;
	bra.uni 	BB1_52;

BB1_61:
	shr.u32 	%r755, %r285, 8;
	and.b32  	%r324, %r755, 3;
	add.s32 	%r821, %r324, 4;
	shl.b32 	%r323, %r821, 3;
	// inline asm
	bfe.u64 %rd209,%rd21,%r323,8;
	// inline asm
	cvt.u32.u64	%r325, %rd209;
	max.u32 	%r326, %r37, %r325;
	max.u32 	%r820, %r18, %r326;
	bra.uni 	BB1_65;

BB1_52:
	add.s32 	%r313, %r28, -217;
	setp.lt.u32	%p45, %r313, 6;
	@%p45 bra 	BB1_60;
	bra.uni 	BB1_53;

BB1_60:
	shr.u32 	%r754, %r285, 8;
	and.b32  	%r322, %r754, 3;
	add.s32 	%r821, %r322, 4;
	shl.b32 	%r321, %r821, 3;
	// inline asm
	bfe.u64 %rd207,%rd21,%r321,8;
	// inline asm
	cvt.u32.u64	%r820, %rd207;

BB1_70:
	mov.u16 	%rs332, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	bra.uni 	BB1_74;

BB1_53:
	add.s32 	%r753, %r817, 1;
	add.s32 	%r314, %r28, -223;
	setp.lt.u32	%p46, %r314, 16;
	mov.u16 	%rs329, 1;
	@%p46 bra 	BB1_54;
	bra.uni 	BB1_55;

BB1_54:
	mov.u32 	%r819, %r753;
	mov.u32 	%r820, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	bra.uni 	BB1_36;

BB1_55:
	setp.eq.s32	%p47, %r28, 239;
	mov.u16 	%rs331, 1;
	@%p47 bra 	BB1_56;
	bra.uni 	BB1_57;

BB1_56:
	mov.u32 	%r820, %r37;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	mov.u16 	%rs333, %rs331;
	bra.uni 	BB1_74;

BB1_57:
	and.b32  	%r777, %r285, 240;
	mov.u16 	%rs328, 1;
	mov.u32 	%r315, 0;
	setp.ne.s32	%p48, %r777, 240;
	@%p48 bra 	BB1_58;

	add.s32 	%r316, %r818, 4;
	shr.s32 	%r317, %r316, 31;
	shr.u32 	%r318, %r317, 30;
	add.s32 	%r319, %r316, %r318;
	shr.s32 	%r320, %r319, 2;
	max.u32 	%r820, %r320, %r820;
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB1_74;

BB1_58:
	mov.u32 	%r820, %r315;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs328;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB1_74;

BB1_21:
	mov.u16 	%rs326, 0;
	cvt.u64.u32	%rd204, %r815;
	add.s64 	%rd205, %rd204, %rd1;
	shl.b64 	%rd206, %rd205, 3;
	add.s64 	%rd23, %rd2, %rd206;
	ld.global.v2.u32 	{%r285, %r286}, [%rd23+128];
	and.b32  	%r28, %r285, 255;
	bfe.u32 	%r821, %r285, 8, 3;
	bfe.u32 	%r287, %r285, 14, 1;
	and.b32  	%r288, %r287, 1;
	setp.eq.b32	%p1, %r288, 1;
	add.s32 	%r289, %r817, 1;
	selp.b32	%r819, %r289, %r819, %p1;
	selp.b16	%rs325, 1, %rs325, %p1;
	mov.u16 	%rs331, 1;
	shl.b32 	%r278, %r821, 3;
	// inline asm
	bfe.u64 %rd198,%rd369,%r278,8;
	// inline asm
	cvt.u32.u64	%r35, %rd198;
	shr.u32 	%r290, %r285, 13;
	and.b32  	%r36, %r290, 56;
	// inline asm
	bfe.u64 %rd200,%rd369,%r36,8;
	// inline asm
	cvt.u32.u64	%r37, %rd200;
	max.u32 	%r820, %r35, %r37;
	setp.lt.u32	%p18, %r28, 25;
	@%p18 bra 	BB1_22;

	mov.u16 	%rs327, 0;
	bfe.u32 	%r752, %r285, 16, 3;
	and.b32  	%r291, %r286, 1835008;
	setp.ne.s32	%p19, %r291, 0;
	setp.eq.s32	%p20, %r821, %r752;
	and.pred  	%p21, %p20, %p19;
	selp.b32	%r292, %r17, %r18, %p21;
	max.u32 	%r39, %r820, %r292;
	add.s32 	%r293, %r28, -25;
	setp.lt.u32	%p22, %r293, 7;
	mov.u16 	%rs331, 1;
	@%p22 bra 	BB1_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r40, %r285, 240;
	setp.eq.s32	%p23, %r40, 32;
	@%p23 bra 	BB1_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r294, %r28, -48;
	setp.lt.u32	%p24, %r294, 7;
	@%p24 bra 	BB1_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r295, %r28, -55;
	setp.lt.u32	%p25, %r295, 16;
	@%p25 bra 	BB1_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r296, %r28, -71;
	setp.lt.u32	%p26, %r296, 4;
	@%p26 bra 	BB1_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r297, %r28, -75;
	setp.lt.u32	%p27, %r297, 4;
	@%p27 bra 	BB1_22;

	mov.u16 	%rs327, 0;
	setp.eq.s32	%p28, %r28, 79;
	@%p28 bra 	BB1_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r298, %r285, 252;
	setp.eq.s32	%p29, %r298, 80;
	@%p29 bra 	BB1_22;
	bra.uni 	BB1_32;

BB1_22:
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;

BB1_73:
	mov.u16 	%rs332, %rs326;
	mov.u16 	%rs333, %rs326;
	bra.uni 	BB1_74;

BB1_32:
	mov.u16 	%rs327, 0;
	setp.eq.s32	%p30, %r28, 84;
	@%p30 bra 	BB1_24;
	bra.uni 	BB1_33;

BB1_24:
	mov.u32 	%r820, %r39;
	mov.u16 	%rs326, %rs331;
	mov.u16 	%rs328, %rs327;
	mov.u16 	%rs329, %rs327;
	mov.u16 	%rs330, %rs327;
	mov.u16 	%rs332, %rs327;
	mov.u16 	%rs333, %rs327;

BB1_74:
	setp.eq.s16	%p51, %rs328, 0;
	@%p51 bra 	BB1_76;
	bra.uni 	BB1_75;

BB1_76:
	bfe.u32 	%r769, %r285, 14, 1;
	and.b32  	%r768, %r769, 1;
	setp.eq.b32	%p289, %r768, 1;
	and.b16  	%rs279, %rs324, 255;
	setp.eq.s16	%p53, %rs279, 0;
	selp.u16	%rs334, 1, 0, %p289;
	@%p53 bra 	BB1_78;

	ld.global.u32 	%r341, [%rd23+128];
	or.b32  	%r342, %r341, 16384;
	st.global.u32 	[%rd23+128], %r342;
	mov.u16 	%rs334, 1;

BB1_78:
	shl.b32 	%r343, %r820, 2;
	max.u32 	%r65, %r343, %r819;
	setp.eq.s16	%p54, %rs333, 0;
	@%p54 bra 	BB1_80;
	bra.uni 	BB1_79;

BB1_80:
	setp.eq.s16	%p55, %rs332, 0;
	selp.b64	%rd226, %rd25, %rd22, %p55;
	shl.b32 	%r344, %r821, 3;
	// inline asm
	bfe.u64 %rd225,%rd226,%r344,8;
	// inline asm
	cvt.u32.u64	%r345, %rd225;
	shl.b32 	%r346, %r345, 2;
	max.u32 	%r832, %r346, %r65;
	bra.uni 	BB1_81;

BB1_75:
	and.b32  	%r763, %r285, 16384;
	setp.ne.s32	%p52, %r763, 0;
	selp.b16	%rs324, 1, %rs324, %p52;
	bra.uni 	BB1_159;

BB1_79:
	max.s32 	%r832, %r835, %r65;

BB1_81:
	setp.eq.s16	%p56, %rs330, 0;
	@%p56 bra 	BB1_83;

	shr.u32 	%r771, %r285, 13;
	and.b32  	%r770, %r771, 56;
	// inline asm
	bfe.u64 %rd227,%rd25,%r770,8;
	// inline asm
	cvt.u32.u64	%r348, %rd227;
	shl.b32 	%r349, %r348, 2;
	max.u32 	%r832, %r349, %r832;

BB1_83:
	setp.eq.s16	%p57, %rs332, 0;
	@%p57 bra 	BB1_126;
	bra.uni 	BB1_84;

BB1_126:
	add.s32 	%r393, %r817, 1;
	max.s32 	%r833, %r832, %r393;
	setp.gt.s32	%p134, %r832, %r817;
	@%p134 bra 	BB1_130;

BB1_127:
	add.s32 	%r394, %r10, %r832;
	ld.shared.u8 	%rs299, [%r394];
	setp.eq.s16	%p135, %rs299, 0;
	@%p135 bra 	BB1_128;

	add.s32 	%r96, %r832, 1;
	setp.lt.s32	%p136, %r832, %r817;
	mov.u32 	%r832, %r96;
	@%p136 bra 	BB1_127;
	bra.uni 	BB1_130;

BB1_117:
	add.s32 	%r832, %r832, 1;

BB1_84:
	add.s32 	%r72, %r10, %r832;
	ld.shared.u8 	%rs281, [%r72];
	setp.ne.s16	%p58, %rs281, 0;
	@%p58 bra 	BB1_117;

	add.s32 	%r778, %r10, %r832;
	ld.shared.u8 	%rs282, [%r778+1];
	setp.ne.s16	%p59, %rs282, 0;
	add.s32 	%r350, %r832, 1;
	and.b32  	%r351, %r350, 3;
	setp.eq.s32	%p60, %r351, 0;
	or.pred  	%p61, %p59, %p60;
	@%p61 bra 	BB1_117;

	shr.s32 	%r352, %r832, 31;
	shr.u32 	%r353, %r352, 30;
	add.s32 	%r354, %r832, %r353;
	and.b32  	%r74, %r354, -4;
	mov.u16 	%rs341, 0;
	setp.ge.s32	%p62, %r74, %r832;
	@%p62 bra 	BB1_116;

	sub.s32 	%r75, %r832, %r74;
	and.b32  	%r76, %r75, 3;
	setp.eq.s32	%p63, %r76, 0;
	mov.u16 	%rs341, 0;
	@%p63 bra 	BB1_102;

	setp.eq.s32	%p64, %r76, 1;
	mov.u16 	%rs338, 0;
	@%p64 bra 	BB1_98;

	setp.eq.s32	%p65, %r76, 2;
	mov.u16 	%rs336, 0;
	@%p65 bra 	BB1_94;

	shr.s32 	%r788, %r832, 31;
	shr.u32 	%r787, %r788, 30;
	add.s32 	%r786, %r832, %r787;
	shr.s32 	%r785, %r786, 2;
	shl.b32 	%r356, %r1, 5;
	and.b32  	%r357, %r356, 1073741568;
	add.s32 	%r358, %r357, %r785;
	shl.b32 	%r359, %r358, 2;
	add.s32 	%r361, %r227, %r359;
	ld.shared.u8 	%rs18, [%r361];
	setp.eq.s16	%p66, %rs18, 0;
	setp.ne.s32	%p67, %r74, %r21;
	and.pred  	%p68, %p67, %p66;
	@%p68 bra 	BB1_92;

	setp.ne.s16	%p69, %rs334, 0;
	cvt.u64.u16	%rd229, %rs18;
	add.s64 	%rd230, %rd229, %rd3;
	shl.b64 	%rd231, %rd230, 3;
	add.s64 	%rd232, %rd2, %rd231;
	ld.global.u32 	%r362, [%rd232];
	and.b32  	%r363, %r362, 8192;
	setp.eq.s32	%p70, %r363, 0;
	and.b32  	%r364, %r362, 20480;
	setp.ne.s32	%p71, %r364, 0;
	or.pred  	%p72, %p69, %p71;
	and.pred  	%p73, %p70, %p72;
	mov.u16 	%rs336, 1;
	@%p73 bra 	BB1_93;

BB1_92:
	mov.u16 	%rs336, 0;

BB1_93:
	add.s32 	%r74, %r74, 1;

BB1_94:
	add.s32 	%r365, %r10, %r74;
	ld.shared.u8 	%rs21, [%r365];
	setp.eq.s16	%p74, %rs21, 0;
	setp.ne.s32	%p75, %r74, %r21;
	and.pred  	%p76, %p75, %p74;
	@%p76 bra 	BB1_96;

	setp.ne.s16	%p77, %rs334, 0;
	cvt.u64.u16	%rd233, %rs21;
	add.s64 	%rd234, %rd233, %rd3;
	shl.b64 	%rd235, %rd234, 3;
	add.s64 	%rd236, %rd2, %rd235;
	ld.global.u32 	%r366, [%rd236];
	and.b32  	%r367, %r366, 8192;
	setp.eq.s32	%p78, %r367, 0;
	and.b32  	%r368, %r366, 20480;
	setp.ne.s32	%p79, %r368, 0;
	or.pred  	%p80, %p77, %p79;
	and.pred  	%p81, %p78, %p80;
	mov.u16 	%rs338, 1;
	@%p81 bra 	BB1_97;

BB1_96:
	mov.u16 	%rs338, %rs336;

BB1_97:
	add.s32 	%r74, %r74, 1;

BB1_98:
	add.s32 	%r369, %r10, %r74;
	ld.shared.u8 	%rs24, [%r369];
	setp.eq.s16	%p82, %rs24, 0;
	setp.ne.s32	%p83, %r74, %r21;
	and.pred  	%p84, %p83, %p82;
	@%p84 bra 	BB1_100;

	setp.ne.s16	%p85, %rs334, 0;
	cvt.u64.u16	%rd237, %rs24;
	add.s64 	%rd238, %rd237, %rd3;
	shl.b64 	%rd239, %rd238, 3;
	add.s64 	%rd240, %rd2, %rd239;
	ld.global.u32 	%r370, [%rd240];
	and.b32  	%r371, %r370, 8192;
	setp.eq.s32	%p86, %r371, 0;
	and.b32  	%r372, %r370, 20480;
	setp.ne.s32	%p87, %r372, 0;
	or.pred  	%p88, %p85, %p87;
	and.pred  	%p89, %p86, %p88;
	mov.u16 	%rs341, 1;
	@%p89 bra 	BB1_101;

BB1_100:
	mov.u16 	%rs341, %rs338;

BB1_101:
	add.s32 	%r74, %r74, 1;

BB1_102:
	setp.lt.u32	%p90, %r75, 4;
	@%p90 bra 	BB1_116;

BB1_103:
	add.s32 	%r84, %r10, %r74;
	ld.shared.u8 	%rs28, [%r84];
	setp.eq.s16	%p91, %rs28, 0;
	setp.ne.s32	%p92, %r74, %r21;
	and.pred  	%p93, %p92, %p91;
	@%p93 bra 	BB1_105;

	setp.ne.s16	%p94, %rs334, 0;
	cvt.u64.u16	%rd241, %rs28;
	add.s64 	%rd242, %rd241, %rd3;
	shl.b64 	%rd243, %rd242, 3;
	add.s64 	%rd244, %rd2, %rd243;
	ld.global.u32 	%r373, [%rd244];
	and.b32  	%r374, %r373, 8192;
	setp.eq.s32	%p95, %r374, 0;
	and.b32  	%r375, %r373, 20480;
	setp.ne.s32	%p96, %r375, 0;
	or.pred  	%p97, %p94, %p96;
	and.pred  	%p98, %p95, %p97;
	mov.u16 	%rs342, 1;
	@%p98 bra 	BB1_106;

BB1_105:
	mov.u16 	%rs342, %rs341;

BB1_106:
	add.s32 	%r789, %r10, %r74;
	ld.shared.u8 	%rs30, [%r789+1];
	add.s32 	%r376, %r74, 1;
	setp.ne.s32	%p99, %r376, %r21;
	setp.eq.s16	%p100, %rs30, 0;
	and.pred  	%p101, %p99, %p100;
	@%p101 bra 	BB1_108;

	setp.ne.s16	%p102, %rs334, 0;
	cvt.u64.u16	%rd245, %rs30;
	add.s64 	%rd246, %rd245, %rd3;
	shl.b64 	%rd247, %rd246, 3;
	add.s64 	%rd248, %rd2, %rd247;
	ld.global.u32 	%r377, [%rd248];
	and.b32  	%r378, %r377, 8192;
	setp.eq.s32	%p103, %r378, 0;
	and.b32  	%r379, %r377, 20480;
	setp.ne.s32	%p104, %r379, 0;
	or.pred  	%p105, %p102, %p104;
	and.pred  	%p106, %p103, %p105;
	mov.u16 	%rs343, 1;
	@%p106 bra 	BB1_109;

BB1_108:
	mov.u16 	%rs343, %rs342;

BB1_109:
	add.s32 	%r790, %r10, %r74;
	ld.shared.u8 	%rs32, [%r790+2];
	add.s32 	%r380, %r74, 2;
	setp.ne.s32	%p107, %r380, %r21;
	setp.eq.s16	%p108, %rs32, 0;
	and.pred  	%p109, %p107, %p108;
	@%p109 bra 	BB1_111;

	setp.ne.s16	%p110, %rs334, 0;
	cvt.u64.u16	%rd249, %rs32;
	add.s64 	%rd250, %rd249, %rd3;
	shl.b64 	%rd251, %rd250, 3;
	add.s64 	%rd252, %rd2, %rd251;
	ld.global.u32 	%r381, [%rd252];
	and.b32  	%r382, %r381, 8192;
	setp.eq.s32	%p111, %r382, 0;
	and.b32  	%r383, %r381, 20480;
	setp.ne.s32	%p112, %r383, 0;
	or.pred  	%p113, %p110, %p112;
	and.pred  	%p114, %p111, %p113;
	mov.u16 	%rs344, 1;
	@%p114 bra 	BB1_112;

BB1_111:
	mov.u16 	%rs344, %rs343;

BB1_112:
	add.s32 	%r791, %r10, %r74;
	ld.shared.u8 	%rs34, [%r791+3];
	add.s32 	%r384, %r74, 3;
	setp.ne.s32	%p115, %r384, %r21;
	setp.eq.s16	%p116, %rs34, 0;
	and.pred  	%p117, %p115, %p116;
	@%p117 bra 	BB1_114;

	setp.ne.s16	%p118, %rs334, 0;
	cvt.u64.u16	%rd253, %rs34;
	add.s64 	%rd254, %rd253, %rd3;
	shl.b64 	%rd255, %rd254, 3;
	add.s64 	%rd256, %rd2, %rd255;
	ld.global.u32 	%r385, [%rd256];
	and.b32  	%r386, %r385, 8192;
	setp.eq.s32	%p119, %r386, 0;
	and.b32  	%r387, %r385, 20480;
	setp.ne.s32	%p120, %r387, 0;
	or.pred  	%p121, %p118, %p120;
	and.pred  	%p122, %p119, %p121;
	mov.u16 	%rs341, 1;
	@%p122 bra 	BB1_115;

BB1_114:
	mov.u16 	%rs341, %rs344;

BB1_115:
	add.s32 	%r74, %r74, 4;
	setp.lt.s32	%p123, %r74, %r832;
	@%p123 bra 	BB1_103;

BB1_116:
	and.b16  	%rs295, %rs341, 255;
	setp.eq.s16	%p124, %rs295, 0;
	@%p124 bra 	BB1_118;
	bra.uni 	BB1_117;

BB1_118:
	shr.s32 	%r784, %r832, 31;
	shr.u32 	%r783, %r784, 30;
	add.s32 	%r782, %r832, %r783;
	and.b32  	%r830, %r782, -4;
	setp.ge.s32	%p291, %r830, %r832;
	mov.u32 	%r388, -1;
	@%p291 bra 	BB1_119;

BB1_120:
	add.s32 	%r88, %r10, %r830;
	ld.shared.u8 	%rs37, [%r88];
	setp.eq.s16	%p126, %rs37, 0;
	setp.ne.s32	%p127, %r830, %r21;
	and.pred  	%p128, %p127, %p126;
	@%p128 bra 	BB1_122;

	cvt.u64.u16	%rd257, %rs37;
	add.s64 	%rd258, %rd257, %rd3;
	shl.b64 	%rd259, %rd258, 3;
	add.s64 	%rd260, %rd2, %rd259;
	ld.global.u8 	%rs296, [%rd260+1];
	and.b16  	%rs297, %rs296, 32;
	setp.eq.s16	%p129, %rs297, 0;
	@%p129 bra 	BB1_124;

BB1_122:
	mov.u32 	%r792, -1;
	add.s32 	%r830, %r830, 1;
	setp.lt.s32	%p130, %r830, %r832;
	@%p130 bra 	BB1_120;

	mov.u32 	%r830, %r792;
	bra.uni 	BB1_125;

BB1_119:
	mov.u32 	%r830, %r388;
	bra.uni 	BB1_125;

BB1_128:
	mov.u32 	%r833, %r832;
	bra.uni 	BB1_130;

BB1_124:
	add.s32 	%r793, %r10, %r830;
	add.s32 	%r780, %r832, 1;
	add.s32 	%r779, %r10, %r832;
	setp.eq.s32	%p131, %r830, %r21;
	st.shared.u8 	[%r779], %rs37;
	ld.shared.u8 	%rs298, [%r793+1];
	st.shared.u8 	[%r779+1], %rs298;
	selp.b32	%r390, %r832, %r21, %p131;
	add.s32 	%r391, %r830, 1;
	setp.eq.s32	%p132, %r390, %r391;
	selp.b32	%r21, %r780, %r390, %p132;

BB1_125:
	setp.lt.s32	%p133, %r830, 0;
	selp.b32	%r833, %r832, %r830, %p133;

BB1_130:
	setp.eq.s32	%p137, %r815, 0;
	selp.b16	%rs323, %rs332, %rs323, %p137;
	selp.b32	%r21, %r833, %r21, %p137;
	@%p54 bra 	BB1_132;

	shr.s32 	%r395, %r833, 31;
	shr.u32 	%r396, %r395, 30;
	add.s32 	%r397, %r833, %r396;
	and.b32  	%r398, %r397, -4;
	sub.s32 	%r399, %r833, %r398;
	add.s32 	%r400, %r833, 4;
	sub.s32 	%r835, %r400, %r399;

BB1_132:
	add.s32 	%r102, %r10, %r833;
	cvt.u16.u32	%rs39, %r815;
	st.shared.u8 	[%r102], %rs39;
	add.s32 	%r836, %r813, 1;
	@%p57 bra 	BB1_134;

	st.shared.u8 	[%r102+1], %rs39;
	add.s32 	%r836, %r813, 2;

BB1_134:
	shr.s32 	%r401, %r833, 31;
	shr.u32 	%r402, %r401, 30;
	add.s32 	%r403, %r833, %r402;
	shr.s32 	%r106, %r403, 2;
	setp.eq.s16	%p140, %rs331, 0;
	@%p140 bra 	BB1_136;

	shr.u32 	%r773, %r285, 13;
	and.b32  	%r772, %r773, 56;
	// inline asm
	bfe.u64 %rd261,%rd25,%r772,8;
	// inline asm
	cvt.u32.u64	%r406, %rd261;
	max.s32 	%r407, %r106, %r406;
	cvt.s64.s32	%rd264, %r407;
	// inline asm
	bfi.b64 %rd25,%rd264,%rd25,%r772,8;
	// inline asm

BB1_136:
	setp.ne.s16	%p141, %rs326, 0;
	setp.lt.s32	%p142, %r818, %r833;
	and.pred  	%p143, %p142, %p141;
	selp.b32	%r818, %r833, %r818, %p143;
	@%p54 bra 	BB1_138;
	bra.uni 	BB1_137;

BB1_138:
	@%p57 bra 	BB1_140;
	bra.uni 	BB1_139;

BB1_140:
	setp.ne.s16	%p146, %rs327, 0;
	@%p146 bra 	BB1_144;

	add.s32 	%r420, %r106, 1;
	cvt.u64.u32	%rd277, %r420;
	shl.b32 	%r419, %r821, 3;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r419,8;
	// inline asm
	@%p56 bra 	BB1_143;

	shr.u32 	%r775, %r285, 13;
	and.b32  	%r774, %r775, 56;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r774,8;
	// inline asm

BB1_143:
	// inline asm
	bfe.u64 %rd282,%rd25,%r419,8;
	// inline asm
	cvt.u32.u64	%r424, %rd282;
	max.s32 	%r425, %r106, %r424;
	cvt.s64.s32	%rd285, %r425;
	// inline asm
	bfi.b64 %rd25,%rd285,%rd25,%r419,8;
	// inline asm

BB1_144:
	setp.eq.s16	%p148, %rs329, 0;
	@%p148 bra 	BB1_146;

	add.s32 	%r426, %r106, 1;
	shl.b32 	%r427, %r426, 8;
	or.b32  	%r428, %r427, %r426;
	shl.b32 	%r429, %r428, 16;
	or.b32  	%r430, %r429, %r428;
	cvt.u64.u32	%rd287, %r430;
	shl.b64 	%rd288, %rd287, 32;
	or.b64  	%rd369, %rd288, %rd287;

BB1_146:
	setp.eq.s16	%p149, %rs327, 0;
	@%p149 bra 	BB1_148;

	shl.b32 	%r432, %r821, 3;
	// inline asm
	bfe.u64 %rd289,%rd25,%r432,8;
	// inline asm
	cvt.u32.u64	%r433, %rd289;
	max.s32 	%r434, %r106, %r433;
	cvt.s64.s32	%rd292, %r434;
	// inline asm
	bfi.b64 %rd25,%rd292,%rd25,%r432,8;
	// inline asm
	setp.gt.u32	%p150, %r285, -536870913;
	selp.b32	%r17, %r106, %r17, %p150;
	mov.u32 	%r18, %r106;
	bra.uni 	BB1_148;

BB1_137:
	add.s32 	%r408, %r106, 1;
	shl.b32 	%r409, %r408, 8;
	or.b32  	%r410, %r409, %r408;
	shl.b32 	%r411, %r410, 16;
	or.b32  	%r412, %r411, %r410;
	cvt.u64.u32	%rd266, %r412;
	shl.b64 	%rd267, %rd266, 32;
	or.b64  	%rd21, %rd267, %rd266;
	bra.uni 	BB1_148;

BB1_139:
	add.s32 	%r416, %r106, 1;
	cvt.u64.u32	%rd269, %r416;
	shl.b32 	%r415, %r821, 3;
	// inline asm
	bfi.b64 %rd21,%rd269,%rd21,%r415,8;
	// inline asm
	// inline asm
	bfe.u64 %rd271,%rd22,%r415,8;
	// inline asm
	cvt.u32.u64	%r417, %rd271;
	max.s32 	%r418, %r106, %r417;
	cvt.s64.s32	%rd274, %r418;
	// inline asm
	bfi.b64 %rd22,%rd274,%rd22,%r415,8;
	// inline asm

BB1_148:
	add.s32 	%r435, %r10, %r819;
	ld.shared.u8 	%rs300, [%r435];
	setp.eq.s16	%p151, %rs300, 0;
	setp.ne.s32	%p152, %r819, %r21;
	and.pred  	%p153, %p152, %p151;
	@%p153 bra 	BB1_154;

	and.b16  	%rs301, %rs325, 255;
	setp.eq.s16	%p154, %rs301, 0;
	@%p154 bra 	BB1_151;

	ld.global.u32 	%r436, [%rd23+128];
	or.b32  	%r437, %r436, 16384;
	st.global.u32 	[%rd23+128], %r437;

BB1_151:
	cvt.u32.u16	%r438, %rs332;
	add.s32 	%r819, %r438, %r819;

BB1_152:
	add.s32 	%r819, %r819, 1;
	mov.u16 	%rs325, 0;
	setp.gt.s32	%p155, %r819, 1023;
	@%p155 bra 	BB1_154;

	add.s32 	%r439, %r10, %r819;
	ld.shared.u8 	%rs304, [%r439];
	setp.ne.s16	%p156, %rs304, 0;
	@%p156 bra 	BB1_152;

BB1_154:
	setp.eq.s16	%p157, %rs334, 0;
	@%p157 bra 	BB1_156;

	selp.b32	%r440, 1, 2, %p57;
	add.s32 	%r441, %r833, %r440;
	max.s32 	%r819, %r441, %r819;

BB1_156:
	cvt.u32.u16	%r442, %rs332;
	add.s32 	%r443, %r833, %r442;
	max.s32 	%r842, %r443, %r817;
	add.s32 	%r119, %r21, 1;

BB1_157:
	mov.u32 	%r120, %r842;
	add.s32 	%r444, %r10, %r120;
	ld.shared.u8 	%rs305, [%r444];
	setp.ne.s16	%p159, %rs305, 0;
	setp.eq.s32	%p160, %r120, %r21;
	or.pred  	%p161, %p160, %p159;
	setp.eq.s32	%p162, %r120, %r119;
	and.b16  	%rs306, %rs323, 255;
	setp.ne.s16	%p163, %rs306, 0;
	and.pred  	%p164, %p162, %p163;
	or.pred  	%p165, %p161, %p164;
	add.s32 	%r842, %r120, 1;
	@%p165 bra 	BB1_157;

	setp.ne.s16	%p166, %rs332, 0;
	mov.u16 	%rs324, 0;
	add.s32 	%r817, %r120, -1;
	setp.gt.s32	%p167, %r120, %r835;
	and.pred  	%p168, %p167, %p166;
	selp.b32	%r835, %r120, %r835, %p168;
	mov.u32 	%r813, %r836;

BB1_159:
	add.s32 	%r815, %r815, 1;
	setp.lt.u32	%p169, %r815, 256;
	@%p169 bra 	BB1_21;

	ld.param.u64 	%rd356, [_Z7init_vmILi4EEvPvS0_S0__param_1];
	cvta.to.global.u64 	%rd355, %rd356;
	mov.u32 	%r767, %ntid.x;
	mov.u32 	%r766, %ctaid.x;
	mad.lo.s32 	%r765, %r766, %r767, %r1;
	shr.u32 	%r764, %r765, 3;
	ld.param.u64 	%rd354, [_Z7init_vmILi4EEvPvS0_S0__param_2];
	shr.s32 	%r445, %r817, 31;
	shr.u32 	%r446, %r445, 30;
	add.s32 	%r447, %r817, %r446;
	shr.s32 	%r448, %r447, 2;
	add.s32 	%r449, %r448, 1;
	cvta.to.global.u64 	%rd294, %rd354;
	atom.global.add.u32 	%r450, [%rd294], %r449;
	add.s64 	%rd295, %rd294, 4;
	atom.global.add.u32 	%r451, [%rd295], %r813;
	add.s64 	%rd297, %rd2, %rd139;
	ld.global.u32 	%r457, [%rd297+64];
	ld.global.u32 	%r458, [%rd297+80];
	ld.global.v2.u64 	{%rd298, %rd299}, [%rd297+96];
	and.b64  	%rd301, %rd298, 1;
	and.b64  	%rd302, %rd298, 2;
	shl.b64 	%rd303, %rd302, 7;
	or.b64  	%rd304, %rd303, %rd301;
	and.b64  	%rd305, %rd298, 4;
	shl.b64 	%rd306, %rd305, 14;
	or.b64  	%rd307, %rd304, %rd306;
	and.b64  	%rd308, %rd298, 8;
	shl.b64 	%rd309, %rd308, 21;
	or.b64  	%rd310, %rd307, %rd309;
	or.b64  	%rd311, %rd310, 100925952;
	shl.b64 	%rd312, %rd311, 3;
	and.b64  	%rd314, %rd299, 524287;
	shl.b64 	%rd315, %rd314, 6;
	ld.global.v2.u64 	{%rd316, %rd317}, [%rd297+112];
	and.b64  	%rd320, %rd316, 4194303;
	shr.u64 	%rd321, %rd316, 4;
	and.b64  	%rd322, %rd321, 1080863910568919040;
	or.b64  	%rd323, %rd320, %rd322;
	and.b64  	%rd324, %rd317, 4194303;
	shr.u64 	%rd325, %rd317, 4;
	and.b64  	%rd326, %rd325, 1080863910568919040;
	or.b64  	%rd327, %rd324, %rd326;
	mul.wide.u32 	%rd329, %r764, 2048;
	add.s64 	%rd47, %rd355, %rd329;
	cvt.u32.u64	%r459, %rd312;
	and.b32  	%r460, %r458, 2147483584;
	and.b32  	%r461, %r457, 2147483584;
	cvt.u32.u64	%r462, %rd315;
	st.global.v4.u32 	[%rd47+128], {%r461, %r460, %r459, %r462};
	or.b64  	%rd330, %rd327, 3458764513820540928;
	or.b64  	%rd331, %rd323, 3458764513820540928;
	st.global.v2.u64 	[%rd47+144], {%rd331, %rd330};
	add.s64 	%rd332, %rd329, %rd356;
	add.s64 	%rd48, %rd332, 1024;
	setp.lt.s32	%p170, %r817, 0;
	mov.u64 	%rd384, %rd48;
	@%p170 bra 	BB1_274;

	add.s64 	%rd383, %rd47, 1024;
	add.s32 	%r133, %r21, 1;
	mov.u32 	%r885, -1;
	mov.u32 	%r851, 0;
	mov.u32 	%r135, %r851;
	mov.u64 	%rd384, %rd48;
	mov.u32 	%r884, %r885;

BB1_162:
	add.s32 	%r467, %r10, %r851;
	ld.shared.u8 	%rs44, [%r467];
	setp.ne.s16	%p171, %rs44, 0;
	setp.eq.s32	%p172, %r851, %r21;
	or.pred  	%p173, %p172, %p171;
	setp.eq.s32	%p174, %r851, %r133;
	and.b16  	%rs308, %rs323, 255;
	setp.ne.s16	%p175, %rs308, 0;
	and.pred  	%p176, %p174, %p175;
	or.pred  	%p177, %p173, %p176;
	mov.u32 	%r887, %r851;
	@!%p177 bra 	BB1_273;
	bra.uni 	BB1_163;

BB1_163:
	add.s32 	%r855, %r851, 1;
	mov.u32 	%r859, 1;
	and.b32  	%r472, %r855, 3;
	setp.eq.s32	%p178, %r472, 0;
	mov.u32 	%r858, 0;
	setp.gt.u32	%p179, %r855, %r817;
	or.pred  	%p180, %p178, %p179;
	@%p180 bra 	BB1_168;

BB1_164:
	add.s32 	%r473, %r10, %r855;
	ld.shared.u8 	%rs45, [%r473];
	setp.ne.s16	%p181, %rs45, 0;
	setp.eq.s32	%p182, %r855, %r21;
	or.pred  	%p183, %p181, %p182;
	setp.eq.s32	%p184, %r855, %r133;
	and.pred  	%p186, %p184, %p175;
	or.pred  	%p187, %p183, %p186;
	@!%p187 bra 	BB1_168;
	bra.uni 	BB1_165;

BB1_165:
	and.b32  	%r474, %r859, 1;
	setp.eq.b32	%p189, %r474, 1;
	mov.pred 	%p292, 0;
	@!%p189 bra 	BB1_167;
	bra.uni 	BB1_166;

BB1_166:
	cvt.u64.u16	%rd333, %rs45;
	add.s64 	%rd334, %rd333, %rd3;
	shl.b64 	%rd335, %rd334, 3;
	add.s64 	%rd336, %rd2, %rd335;
	ld.global.u8 	%rs310, [%rd336+1];
	shr.u16 	%rs311, %rs310, 5;
	and.b16  	%rs312, %rs311, 1;
	setp.eq.b16	%p292, %rs312, 1;

BB1_167:
	selp.u32	%r475, 1, 0, %p292;
	add.s32 	%r858, %r475, %r858;
	add.s32 	%r859, %r859, 1;
	add.s32 	%r855, %r859, %r851;
	and.b32  	%r476, %r855, 3;
	setp.ne.s32	%p190, %r476, 0;
	setp.le.u32	%p191, %r855, %r817;
	and.pred  	%p192, %p190, %p191;
	@%p192 bra 	BB1_164;

BB1_168:
	add.s32 	%r477, %r859, 255;
	shl.b32 	%r478, %r477, 24;
	shl.b32 	%r479, %r858, 28;
	or.b32  	%r147, %r478, %r479;
	cvt.u64.u16	%rd337, %rs44;
	add.s64 	%rd338, %rd337, %rd3;
	shl.b64 	%rd339, %rd338, 3;
	add.s64 	%rd340, %rd2, %rd339;
	ld.global.v2.u32 	{%r480, %r481}, [%rd340];
	and.b32  	%r150, %r480, 255;
	shr.u32 	%r151, %r480, 8;
	bfe.u32 	%r152, %r480, 8, 3;
	shr.u32 	%r153, %r480, 16;
	bfe.u32 	%r154, %r480, 16, 3;
	and.b32  	%r482, %r851, 1;
	setp.eq.b32	%p193, %r482, 1;
	not.pred 	%p194, %p193;
	shr.u32 	%r483, %r480, 13;
	and.b32  	%r484, %r483, 1;
	setp.eq.b32	%p195, %r484, 1;
	and.pred  	%p196, %p194, %p195;
	selp.u32	%r485, 1, 0, %p196;
	add.s32 	%r887, %r485, %r851;
	shr.u32 	%r486, %r480, 14;
	and.b32  	%r487, %r486, 1;
	setp.eq.b32	%p197, %r487, 1;
	setp.lt.s32	%p198, %r885, 0;
	and.pred  	%p199, %p197, %p198;
	selp.b32	%r885, %r884, %r885, %p199;
	add.s32 	%r884, %r884, 1;
	setp.lt.u32	%p200, %r150, 25;
	mul.wide.u32 	%rd341, %r135, 4;
	add.s64 	%rd342, %rd47, %rd341;
	add.s64 	%rd54, %rd342, 256;
	@%p200 bra 	BB1_269;
	bra.uni 	BB1_169;

BB1_269:
	mov.u32 	%r882, 1048576;
	setp.ne.s32	%p286, %r152, 5;
	@%p286 bra 	BB1_272;

	shl.b32 	%r882, %r135, 6;
	setp.gt.u32	%p287, %r135, 189;
	@%p287 bra 	BB1_272;

	add.s32 	%r135, %r135, 1;
	st.global.u32 	[%rd54], %r481;

BB1_272:
	or.b32  	%r745, %r152, %r147;
	shl.b32 	%r746, %r154, 3;
	or.b32  	%r747, %r745, %r746;
	shr.u32 	%r748, %r480, 11;
	and.b32  	%r749, %r748, 98304;
	or.b32  	%r750, %r747, %r749;
	or.b32  	%r751, %r750, %r882;
	add.s64 	%rd124, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r751;
	mov.u64 	%rd383, %rd124;
	bra.uni 	BB1_273;

BB1_169:
	add.s32 	%r488, %r150, -25;
	setp.lt.u32	%p201, %r488, 7;
	@%p201 bra 	BB1_266;
	bra.uni 	BB1_170;

BB1_266:
	setp.gt.u32	%p281, %r135, 189;
	mov.u32 	%r881, %r135;
	@%p281 bra 	BB1_268;

	and.b32  	%r731, %r480, 50331648;
	setp.eq.s32	%p282, %r731, 0;
	selp.b32	%r732, 2, 1, %p282;
	setp.eq.s32	%p283, %r154, %r152;
	selp.b32	%r733, 3, %r732, %p283;
	and.b32  	%r734, %r481, -65011713;
	setp.eq.s32	%p284, %r733, 2;
	selp.b32	%r735, 29360128, 23068672, %p284;
	setp.eq.s32	%p285, %r733, 1;
	selp.b32	%r736, 37748736, %r735, %p285;
	or.b32  	%r737, %r736, %r734;
	add.s32 	%r881, %r135, 1;
	st.global.u32 	[%rd54], %r737;

BB1_268:
	shl.b32 	%r738, %r135, 6;
	or.b32  	%r739, %r738, %r147;
	or.b32  	%r740, %r739, %r152;
	shl.b32 	%r741, %r154, 3;
	or.b32  	%r742, %r740, %r741;
	or.b32  	%r743, %r742, 1064960;
	add.s64 	%rd122, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r743;
	mov.u64 	%rd383, %rd122;
	mov.u32 	%r135, %r881;
	bra.uni 	BB1_273;

BB1_170:
	and.b32  	%r158, %r480, 240;
	setp.eq.s32	%p202, %r158, 32;
	@%p202 bra 	BB1_262;
	bra.uni 	BB1_171;

BB1_262:
	shl.b32 	%r726, %r154, 3;
	or.b32  	%r727, %r152, %r726;
	or.b32  	%r879, %r727, 1572864;
	setp.ne.s32	%p279, %r154, %r152;
	@%p279 bra 	BB1_265;

	shl.b32 	%r728, %r135, 6;
	or.b32  	%r729, %r728, %r879;
	or.b32  	%r879, %r729, 131072;
	setp.gt.u32	%p280, %r135, 189;
	@%p280 bra 	BB1_265;

	add.s32 	%r135, %r135, 1;
	st.global.u32 	[%rd54], %r481;

BB1_265:
	add.s64 	%rd120, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r730, %r879, %r147;
	st.global.u32 	[%rd383], %r730;
	mov.u64 	%rd383, %rd120;
	bra.uni 	BB1_273;

BB1_171:
	add.s32 	%r489, %r150, -48;
	setp.lt.u32	%p203, %r489, 7;
	@%p203 bra 	BB1_259;
	bra.uni 	BB1_172;

BB1_259:
	setp.gt.u32	%p274, %r135, 189;
	mov.u32 	%r878, %r135;
	@%p274 bra 	BB1_261;

	and.b32  	%r713, %r480, 50331648;
	setp.eq.s32	%p275, %r713, 0;
	selp.b32	%r714, 2, 1, %p275;
	setp.eq.s32	%p276, %r154, %r152;
	selp.b32	%r715, 3, %r714, %p276;
	and.b32  	%r716, %r481, -65011713;
	setp.eq.s32	%p277, %r715, 2;
	selp.b32	%r717, 29360128, 23068672, %p277;
	setp.eq.s32	%p278, %r715, 1;
	selp.b32	%r718, 37748736, %r717, %p278;
	or.b32  	%r719, %r718, %r716;
	add.s32 	%r878, %r135, 1;
	st.global.u32 	[%rd54], %r719;

BB1_261:
	shl.b32 	%r720, %r135, 6;
	or.b32  	%r721, %r720, %r147;
	or.b32  	%r722, %r721, %r152;
	shl.b32 	%r723, %r154, 3;
	or.b32  	%r724, %r722, %r723;
	or.b32  	%r725, %r724, 1589248;
	add.s64 	%rd118, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r725;
	mov.u64 	%rd383, %rd118;
	mov.u32 	%r135, %r878;
	bra.uni 	BB1_273;

BB1_172:
	add.s32 	%r490, %r150, -55;
	setp.lt.u32	%p204, %r490, 16;
	@%p204 bra 	BB1_255;
	bra.uni 	BB1_173;

BB1_255:
	shl.b32 	%r708, %r154, 3;
	or.b32  	%r709, %r152, %r708;
	or.b32  	%r876, %r709, 2097152;
	setp.ne.s32	%p272, %r154, %r152;
	@%p272 bra 	BB1_258;

	shl.b32 	%r710, %r135, 6;
	or.b32  	%r711, %r710, %r876;
	or.b32  	%r876, %r711, 131072;
	setp.gt.u32	%p273, %r135, 189;
	@%p273 bra 	BB1_258;

	add.s32 	%r135, %r135, 1;
	st.global.u32 	[%rd54], %r481;

BB1_258:
	add.s64 	%rd116, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r712, %r876, %r147;
	st.global.u32 	[%rd383], %r712;
	mov.u64 	%rd383, %rd116;
	bra.uni 	BB1_273;

BB1_173:
	add.s32 	%r491, %r150, -71;
	setp.lt.u32	%p205, %r491, 4;
	@%p205 bra 	BB1_252;
	bra.uni 	BB1_174;

BB1_252:
	setp.gt.u32	%p267, %r135, 189;
	mov.u32 	%r875, %r135;
	@%p267 bra 	BB1_254;

	and.b32  	%r695, %r480, 50331648;
	setp.eq.s32	%p268, %r695, 0;
	selp.b32	%r696, 2, 1, %p268;
	setp.eq.s32	%p269, %r154, %r152;
	selp.b32	%r697, 3, %r696, %p269;
	and.b32  	%r698, %r481, -65011713;
	setp.eq.s32	%p270, %r697, 2;
	selp.b32	%r699, 29360128, 23068672, %p270;
	setp.eq.s32	%p271, %r697, 1;
	selp.b32	%r700, 37748736, %r699, %p271;
	or.b32  	%r701, %r700, %r698;
	add.s32 	%r875, %r135, 1;
	st.global.u32 	[%rd54], %r701;

BB1_254:
	shl.b32 	%r702, %r135, 6;
	or.b32  	%r703, %r702, %r147;
	or.b32  	%r704, %r703, %r152;
	shl.b32 	%r705, %r154, 3;
	or.b32  	%r706, %r704, %r705;
	or.b32  	%r707, %r706, 2113536;
	add.s64 	%rd114, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r707;
	mov.u64 	%rd383, %rd114;
	mov.u32 	%r135, %r875;
	bra.uni 	BB1_273;

BB1_174:
	add.s32 	%r492, %r150, -75;
	setp.lt.u32	%p206, %r492, 4;
	@%p206 bra 	BB1_251;
	bra.uni 	BB1_175;

BB1_251:
	shl.b32 	%r691, %r154, 3;
	or.b32  	%r692, %r147, %r152;
	or.b32  	%r693, %r692, %r691;
	or.b32  	%r694, %r693, 6291456;
	add.s64 	%rd112, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r694;
	mov.u64 	%rd383, %rd112;
	bra.uni 	BB1_273;

BB1_175:
	setp.eq.s32	%p207, %r150, 79;
	@%p207 bra 	BB1_248;
	bra.uni 	BB1_176;

BB1_248:
	setp.gt.u32	%p262, %r135, 189;
	mov.u32 	%r874, %r135;
	@%p262 bra 	BB1_250;

	and.b32  	%r678, %r480, 50331648;
	setp.eq.s32	%p263, %r678, 0;
	selp.b32	%r679, 2, 1, %p263;
	setp.eq.s32	%p264, %r154, %r152;
	selp.b32	%r680, 3, %r679, %p264;
	and.b32  	%r681, %r481, -65011713;
	setp.eq.s32	%p265, %r680, 2;
	selp.b32	%r682, 29360128, 23068672, %p265;
	setp.eq.s32	%p266, %r680, 1;
	selp.b32	%r683, 37748736, %r682, %p266;
	or.b32  	%r684, %r683, %r681;
	add.s32 	%r874, %r135, 1;
	st.global.u32 	[%rd54], %r684;

BB1_250:
	shl.b32 	%r685, %r135, 6;
	or.b32  	%r686, %r685, %r147;
	or.b32  	%r687, %r686, %r152;
	shl.b32 	%r688, %r154, 3;
	or.b32  	%r689, %r687, %r688;
	or.b32  	%r690, %r689, 6307840;
	add.s64 	%rd110, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r690;
	mov.u64 	%rd383, %rd110;
	mov.u32 	%r135, %r874;
	bra.uni 	BB1_273;

BB1_176:
	and.b32  	%r493, %r480, 252;
	setp.eq.s32	%p208, %r493, 80;
	@%p208 bra 	BB1_247;
	bra.uni 	BB1_177;

BB1_247:
	shl.b32 	%r674, %r154, 3;
	or.b32  	%r675, %r147, %r152;
	or.b32  	%r676, %r675, %r674;
	or.b32  	%r677, %r676, 4194304;
	add.s64 	%rd108, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r677;
	mov.u64 	%rd383, %rd108;
	bra.uni 	BB1_273;

BB1_177:
	setp.eq.s32	%p209, %r150, 84;
	@%p209 bra 	BB1_244;
	bra.uni 	BB1_178;

BB1_244:
	setp.gt.u32	%p257, %r135, 189;
	mov.u32 	%r873, %r135;
	@%p257 bra 	BB1_246;

	and.b32  	%r661, %r480, 50331648;
	setp.eq.s32	%p258, %r661, 0;
	selp.b32	%r662, 2, 1, %p258;
	setp.eq.s32	%p259, %r154, %r152;
	selp.b32	%r663, 3, %r662, %p259;
	and.b32  	%r664, %r481, -65011713;
	setp.eq.s32	%p260, %r663, 2;
	selp.b32	%r665, 29360128, 23068672, %p260;
	setp.eq.s32	%p261, %r663, 1;
	selp.b32	%r666, 37748736, %r665, %p261;
	or.b32  	%r667, %r666, %r664;
	add.s32 	%r873, %r135, 1;
	st.global.u32 	[%rd54], %r667;

BB1_246:
	shl.b32 	%r668, %r135, 6;
	or.b32  	%r669, %r668, %r147;
	or.b32  	%r670, %r669, %r152;
	shl.b32 	%r671, %r154, 3;
	or.b32  	%r672, %r670, %r671;
	or.b32  	%r673, %r672, 4210688;
	add.s64 	%rd106, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r673;
	mov.u64 	%rd383, %rd106;
	mov.u32 	%r135, %r873;
	bra.uni 	BB1_273;

BB1_178:
	add.s32 	%r494, %r150, -85;
	setp.lt.u32	%p210, %r494, 8;
	add.s32 	%r495, %r135, 1;
	mul.wide.u32 	%rd343, %r495, 4;
	add.s64 	%rd344, %rd47, %rd343;
	add.s64 	%rd55, %rd344, 256;
	@%p210 bra 	BB1_234;
	bra.uni 	BB1_179;

BB1_234:
	add.s32 	%r643, %r481, -1;
	and.b32  	%r644, %r643, %r481;
	setp.eq.s32	%p251, %r644, 0;
	mov.u64 	%rd381, 1;
	@%p251 bra 	BB1_239;

	cvt.u64.u32	%rd92, %r481;
	// inline asm
	bfind.u32 %r645,%r481;
	// inline asm
	mov.pred 	%p252, 0;
	@%p252 bra 	BB1_237;
	bra.uni 	BB1_236;

BB1_237:
	cvt.u32.u64	%r649, %rd92;
	mov.u32 	%r871, 0;
	div.u32 	%r650, %r871, %r649;
	rem.u32 	%r651, %r871, %r649;
	cvt.u64.u32	%rd381, %r650;
	cvt.u64.u32	%rd380, %r651;
	bra.uni 	BB1_238;

BB1_179:
	add.s32 	%r496, %r150, -93;
	setp.lt.u32	%p211, %r496, 2;
	@%p211 bra 	BB1_233;
	bra.uni 	BB1_180;

BB1_233:
	or.b32  	%r641, %r147, %r152;
	or.b32  	%r642, %r641, 5242880;
	add.s64 	%rd90, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r642;
	mov.u64 	%rd383, %rd90;
	bra.uni 	BB1_273;

BB1_180:
	add.s32 	%r497, %r150, -95;
	setp.lt.u32	%p212, %r497, 15;
	@%p212 bra 	BB1_229;
	bra.uni 	BB1_181;

BB1_229:
	shl.b32 	%r636, %r154, 3;
	or.b32  	%r637, %r152, %r636;
	or.b32  	%r869, %r637, 3145728;
	setp.ne.s32	%p249, %r154, %r152;
	@%p249 bra 	BB1_232;

	shl.b32 	%r638, %r135, 6;
	or.b32  	%r639, %r638, %r869;
	or.b32  	%r869, %r639, 131072;
	setp.gt.u32	%p250, %r135, 189;
	@%p250 bra 	BB1_232;

	add.s32 	%r135, %r135, 1;
	st.global.u32 	[%rd54], %r481;

BB1_232:
	add.s64 	%rd88, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r640, %r869, %r147;
	st.global.u32 	[%rd383], %r640;
	mov.u64 	%rd383, %rd88;
	bra.uni 	BB1_273;

BB1_236:
	mov.u64 	%rd346, -9223372036854775808;
	div.u64 	%rd381, %rd346, %rd92;
	rem.u64 	%rd380, %rd346, %rd92;
	mov.u32 	%r871, 0;

BB1_238:
	sub.s64 	%rd347, %rd92, %rd380;
	setp.ge.u64	%p253, %rd380, %rd347;
	selp.u64	%rd348, 1, 0, %p253;
	shl.b64 	%rd349, %rd381, 1;
	or.b64  	%rd381, %rd348, %rd349;
	selp.b64	%rd350, %rd92, 0, %p253;
	shl.b64 	%rd351, %rd380, 1;
	sub.s64 	%rd380, %rd351, %rd350;
	add.s32 	%r871, %r871, 1;
	setp.le.u32	%p254, %r871, %r645;
	@%p254 bra 	BB1_238;

BB1_239:
	setp.eq.s64	%p255, %rd381, 1;
	@%p255 bra 	BB1_243;
	bra.uni 	BB1_240;

BB1_243:
	or.b32  	%r660, %r147, 8388608;
	add.s64 	%rd104, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r660;
	mov.u64 	%rd383, %rd104;
	bra.uni 	BB1_273;

BB1_240:
	setp.gt.u32	%p256, %r135, 188;
	mov.u32 	%r872, %r135;
	@%p256 bra 	BB1_242;

	mov.b64	{%r652, %r653}, %rd381;
	st.global.u32 	[%rd54], %r652;
	st.global.u32 	[%rd55], %r653;
	add.s32 	%r872, %r135, 2;

BB1_242:
	shl.b32 	%r654, %r135, 6;
	or.b32  	%r655, %r654, %r147;
	or.b32  	%r656, %r655, %r152;
	shl.b32 	%r657, %r154, 3;
	or.b32  	%r658, %r656, %r657;
	or.b32  	%r659, %r658, 2359296;
	add.s64 	%rd102, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r659;
	mov.u64 	%rd383, %rd102;
	mov.u32 	%r135, %r872;
	bra.uni 	BB1_273;

BB1_181:
	add.s32 	%r498, %r150, -110;
	setp.lt.u32	%p213, %r498, 5;
	@%p213 bra 	BB1_226;
	bra.uni 	BB1_182;

BB1_226:
	setp.gt.u32	%p244, %r135, 189;
	mov.u32 	%r868, %r135;
	@%p244 bra 	BB1_228;

	and.b32  	%r623, %r480, 50331648;
	setp.eq.s32	%p245, %r623, 0;
	selp.b32	%r624, 2, 1, %p245;
	setp.eq.s32	%p246, %r154, %r152;
	selp.b32	%r625, 3, %r624, %p246;
	and.b32  	%r626, %r481, -65011713;
	setp.eq.s32	%p247, %r625, 2;
	selp.b32	%r627, 29360128, 23068672, %p247;
	setp.eq.s32	%p248, %r625, 1;
	selp.b32	%r628, 37748736, %r627, %p248;
	or.b32  	%r629, %r628, %r626;
	add.s32 	%r868, %r135, 1;
	st.global.u32 	[%rd54], %r629;

BB1_228:
	shl.b32 	%r630, %r135, 6;
	or.b32  	%r631, %r630, %r147;
	or.b32  	%r632, %r631, %r152;
	shl.b32 	%r633, %r154, 3;
	or.b32  	%r634, %r632, %r633;
	or.b32  	%r635, %r634, 3162112;
	add.s64 	%rd86, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r635;
	mov.u64 	%rd383, %rd86;
	mov.u32 	%r135, %r868;
	bra.uni 	BB1_273;

BB1_182:
	add.s32 	%r499, %r150, -115;
	setp.lt.u32	%p214, %r499, 10;
	@%p214 bra 	BB1_222;
	bra.uni 	BB1_183;

BB1_222:
	shl.b32 	%r618, %r154, 3;
	or.b32  	%r619, %r152, %r618;
	or.b32  	%r866, %r619, 7340032;
	setp.ne.s32	%p242, %r154, %r152;
	@%p242 bra 	BB1_225;

	shl.b32 	%r620, %r135, 6;
	or.b32  	%r621, %r620, %r866;
	or.b32  	%r866, %r621, 131072;
	setp.gt.u32	%p243, %r135, 189;
	@%p243 bra 	BB1_225;

	add.s32 	%r135, %r135, 1;
	st.global.u32 	[%rd54], %r481;

BB1_225:
	add.s64 	%rd84, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r622, %r866, %r147;
	st.global.u32 	[%rd383], %r622;
	mov.u64 	%rd383, %rd84;
	bra.uni 	BB1_273;

BB1_183:
	add.s32 	%r500, %r150, -125;
	setp.lt.u32	%p215, %r500, 4;
	@%p215 bra 	BB1_221;
	bra.uni 	BB1_184;

BB1_221:
	shl.b32 	%r613, %r154, 3;
	or.b32  	%r614, %r152, %r613;
	or.b32  	%r615, %r614, 8388608;
	setp.eq.s32	%p241, %r154, %r152;
	selp.b32	%r616, 8388608, %r615, %p241;
	or.b32  	%r617, %r616, %r147;
	add.s64 	%rd82, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r617;
	mov.u64 	%rd383, %rd82;
	bra.uni 	BB1_273;

BB1_184:
	add.s32 	%r501, %r150, -129;
	setp.lt.u32	%p216, %r501, 8;
	@%p216 bra 	BB1_220;
	bra.uni 	BB1_185;

BB1_220:
	or.b32  	%r611, %r147, %r152;
	or.b32  	%r612, %r611, 11534336;
	add.s64 	%rd80, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r612;
	mov.u64 	%rd383, %rd80;
	bra.uni 	BB1_273;

BB1_185:
	add.s32 	%r502, %r150, -137;
	setp.lt.u32	%p217, %r502, 20;
	@%p217 bra 	BB1_219;
	bra.uni 	BB1_186;

BB1_219:
	and.b32  	%r605, %r151, 3;
	and.b32  	%r606, %r153, 3;
	shl.b32 	%r607, %r606, 4;
	or.b32  	%r608, %r147, %r605;
	or.b32  	%r609, %r608, %r607;
	or.b32  	%r610, %r609, 12582912;
	add.s64 	%rd78, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r610;
	mov.u64 	%rd383, %rd78;
	bra.uni 	BB1_273;

BB1_186:
	add.s32 	%r503, %r150, -157;
	setp.lt.u32	%p218, %r503, 5;
	@%p218 bra 	BB1_216;
	bra.uni 	BB1_187;

BB1_216:
	setp.gt.u32	%p239, %r135, 189;
	mov.u32 	%r865, %r135;
	@%p239 bra 	BB1_218;

	and.b32  	%r594, %r480, 50331648;
	setp.eq.s32	%p240, %r594, 0;
	selp.b32	%r595, 29360128, 37748736, %p240;
	and.b32  	%r596, %r481, -65011713;
	or.b32  	%r597, %r595, %r596;
	add.s32 	%r865, %r135, 1;
	st.global.u32 	[%rd54], %r597;

BB1_218:
	shl.b32 	%r598, %r135, 6;
	or.b32  	%r599, %r598, %r147;
	and.b32  	%r600, %r151, 3;
	or.b32  	%r601, %r599, %r600;
	shl.b32 	%r602, %r154, 3;
	or.b32  	%r603, %r601, %r602;
	or.b32  	%r604, %r603, 12599296;
	add.s64 	%rd76, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r604;
	mov.u64 	%rd383, %rd76;
	mov.u32 	%r135, %r865;
	bra.uni 	BB1_273;

BB1_187:
	add.s32 	%r504, %r150, -162;
	setp.lt.u32	%p219, %r504, 20;
	@%p219 bra 	BB1_215;
	bra.uni 	BB1_188;

BB1_215:
	and.b32  	%r588, %r151, 3;
	and.b32  	%r589, %r153, 3;
	shl.b32 	%r590, %r589, 4;
	or.b32  	%r591, %r147, %r588;
	or.b32  	%r592, %r591, %r590;
	or.b32  	%r593, %r592, 13107200;
	add.s64 	%rd74, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r593;
	mov.u64 	%rd383, %rd74;
	bra.uni 	BB1_273;

BB1_188:
	add.s32 	%r505, %r150, -182;
	setp.lt.u32	%p220, %r505, 5;
	@%p220 bra 	BB1_212;
	bra.uni 	BB1_189;

BB1_212:
	setp.gt.u32	%p237, %r135, 189;
	mov.u32 	%r864, %r135;
	@%p237 bra 	BB1_214;

	and.b32  	%r577, %r480, 50331648;
	setp.eq.s32	%p238, %r577, 0;
	selp.b32	%r578, 29360128, 37748736, %p238;
	and.b32  	%r579, %r481, -65011713;
	or.b32  	%r580, %r578, %r579;
	add.s32 	%r864, %r135, 1;
	st.global.u32 	[%rd54], %r580;

BB1_214:
	shl.b32 	%r581, %r135, 6;
	or.b32  	%r582, %r581, %r147;
	and.b32  	%r583, %r151, 3;
	or.b32  	%r584, %r582, %r583;
	shl.b32 	%r585, %r154, 3;
	or.b32  	%r586, %r584, %r585;
	or.b32  	%r587, %r586, 13123584;
	add.s64 	%rd72, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r587;
	mov.u64 	%rd383, %rd72;
	mov.u32 	%r135, %r864;
	bra.uni 	BB1_273;

BB1_189:
	add.s32 	%r506, %r150, -187;
	setp.lt.u32	%p221, %r506, 6;
	@%p221 bra 	BB1_209;
	bra.uni 	BB1_190;

BB1_209:
	setp.gt.u32	%p236, %r135, 188;
	mov.u32 	%r863, %r135;
	@%p236 bra 	BB1_211;

	mov.u32 	%r570, 0;
	st.global.u32 	[%rd54], %r570;
	mov.u32 	%r571, -2131755008;
	st.global.u32 	[%rd55], %r571;
	add.s32 	%r863, %r135, 2;

BB1_211:
	shl.b32 	%r572, %r135, 6;
	or.b32  	%r573, %r572, %r147;
	and.b32  	%r574, %r151, 3;
	or.b32  	%r575, %r573, %r574;
	or.b32  	%r576, %r575, 3407872;
	add.s64 	%rd70, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r576;
	mov.u64 	%rd383, %rd70;
	mov.u32 	%r135, %r863;
	bra.uni 	BB1_273;

BB1_190:
	add.s32 	%r507, %r150, -193;
	setp.lt.u32	%p222, %r507, 20;
	@%p222 bra 	BB1_208;
	bra.uni 	BB1_191;

BB1_208:
	and.b32  	%r563, %r151, 3;
	add.s32 	%r564, %r563, 4;
	and.b32  	%r565, %r153, 3;
	shl.b32 	%r566, %r565, 4;
	or.b32  	%r567, %r147, %r566;
	or.b32  	%r568, %r567, %r564;
	or.b32  	%r569, %r568, 12615680;
	add.s64 	%rd68, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r569;
	mov.u64 	%rd383, %rd68;
	bra.uni 	BB1_273;

BB1_191:
	add.s32 	%r508, %r150, -213;
	setp.lt.u32	%p223, %r508, 4;
	@%p223 bra 	BB1_205;
	bra.uni 	BB1_192;

BB1_205:
	setp.gt.u32	%p234, %r135, 189;
	mov.u32 	%r862, %r135;
	@%p234 bra 	BB1_207;

	and.b32  	%r551, %r480, 50331648;
	setp.eq.s32	%p235, %r551, 0;
	selp.b32	%r552, 29360128, 37748736, %p235;
	and.b32  	%r553, %r481, -65011713;
	or.b32  	%r554, %r552, %r553;
	add.s32 	%r862, %r135, 1;
	st.global.u32 	[%rd54], %r554;

BB1_207:
	shl.b32 	%r555, %r135, 6;
	or.b32  	%r556, %r555, %r147;
	shl.b32 	%r557, %r154, 3;
	or.b32  	%r558, %r556, %r557;
	and.b32  	%r559, %r151, 3;
	add.s32 	%r560, %r559, 4;
	or.b32  	%r561, %r558, %r560;
	or.b32  	%r562, %r561, 15745024;
	add.s64 	%rd66, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r562;
	mov.u64 	%rd383, %rd66;
	mov.u32 	%r135, %r862;
	bra.uni 	BB1_273;

BB1_192:
	add.s32 	%r509, %r150, -217;
	setp.lt.u32	%p224, %r509, 6;
	@%p224 bra 	BB1_204;
	bra.uni 	BB1_193;

BB1_204:
	and.b32  	%r547, %r151, 3;
	add.s32 	%r548, %r547, 4;
	or.b32  	%r549, %r147, %r548;
	or.b32  	%r550, %r549, 14680064;
	add.s64 	%rd64, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r550;
	mov.u64 	%rd383, %rd64;
	bra.uni 	BB1_273;

BB1_193:
	add.s32 	%r510, %r150, -223;
	setp.lt.u32	%p225, %r510, 16;
	@%p225 bra 	BB1_201;
	bra.uni 	BB1_194;

BB1_201:
	setp.gt.u32	%p233, %r135, 188;
	mov.u32 	%r861, %r135;
	@%p233 bra 	BB1_203;

	shr.u32 	%r531, %r480, 28;
	add.s32 	%r532, %r531, 8;
	add.s32 	%r533, %r531, 7;
	mov.u32 	%r534, 1;
	shl.b32 	%r535, %r534, %r533;
	not.b32 	%r536, %r535;
	shl.b32 	%r537, %r534, %r532;
	or.b32  	%r538, %r537, %r481;
	and.b32  	%r539, %r538, %r536;
	st.global.u32 	[%rd54], %r539;
	shl.b32 	%r540, %r885, 5;
	or.b32  	%r541, %r540, %r532;
	st.global.u32 	[%rd55], %r541;
	add.s32 	%r861, %r135, 2;

BB1_203:
	shl.b32 	%r543, %r135, 6;
	or.b32  	%r544, %r543, %r147;
	or.b32  	%r545, %r544, %r152;
	or.b32  	%r546, %r545, 9437184;
	add.s64 	%rd62, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r546;
	mov.u32 	%r885, -1;
	mov.u64 	%rd383, %rd62;
	mov.u32 	%r135, %r861;
	bra.uni 	BB1_273;

BB1_194:
	setp.eq.s32	%p226, %r150, 239;
	@%p226 bra 	BB1_200;
	bra.uni 	BB1_195;

BB1_200:
	shl.b32 	%r525, %r154, 3;
	and.b32  	%r526, %r481, 63;
	shl.b32 	%r527, %r526, 6;
	or.b32  	%r528, %r147, %r527;
	or.b32  	%r529, %r528, %r525;
	or.b32  	%r530, %r529, 13631488;
	add.s64 	%rd60, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r530;
	mov.u64 	%rd383, %rd60;
	bra.uni 	BB1_273;

BB1_195:
	setp.eq.s32	%p227, %r158, 240;
	@%p227 bra 	BB1_197;
	bra.uni 	BB1_196;

BB1_197:
	setp.gt.u32	%p228, %r135, 189;
	mov.u32 	%r860, %r135;
	@%p228 bra 	BB1_199;

	and.b32  	%r512, %r480, 50331648;
	setp.eq.s32	%p229, %r512, 0;
	selp.b32	%r513, 2, 1, %p229;
	setp.gt.u32	%p230, %r480, -536870913;
	selp.b32	%r514, 3, %r513, %p230;
	and.b32  	%r515, %r481, -65011713;
	setp.eq.s32	%p231, %r514, 2;
	selp.b32	%r516, 29360128, 23068672, %p231;
	setp.eq.s32	%p232, %r514, 1;
	selp.b32	%r517, 37748736, %r516, %p232;
	or.b32  	%r518, %r517, %r515;
	st.global.u32 	[%rd54], %r518;
	mov.u32 	%r860, %r495;

BB1_199:
	shl.b32 	%r519, %r135, 6;
	or.b32  	%r520, %r519, %r147;
	or.b32  	%r521, %r520, %r152;
	shl.b32 	%r522, %r154, 3;
	or.b32  	%r523, %r521, %r522;
	or.b32  	%r524, %r523, 10502144;
	add.s64 	%rd58, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r524;
	mov.u64 	%rd383, %rd58;
	mov.u32 	%r135, %r860;
	bra.uni 	BB1_273;

BB1_196:
	or.b32  	%r511, %r147, 8388608;
	add.s64 	%rd56, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r511;
	mov.u64 	%rd383, %rd56;

BB1_273:
	add.s32 	%r851, %r887, 1;
	setp.lt.s32	%p288, %r887, %r817;
	@%p288 bra 	BB1_162;

BB1_274:
	sub.s64 	%rd352, %rd384, %rd48;
	shr.u64 	%rd353, %rd352, 2;
	st.global.u32 	[%rd47+160], %rd353;

BB1_275:
	ret;
}

	// .globl	_Z7init_vmILi8EEvPvS0_S0_
.visible .entry _Z7init_vmILi8EEvPvS0_S0_(
	.param .u64 _Z7init_vmILi8EEvPvS0_S0__param_0,
	.param .u64 _Z7init_vmILi8EEvPvS0_S0__param_1,
	.param .u64 _Z7init_vmILi8EEvPvS0_S0__param_2
)
.maxntid 32, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<293>;
	.reg .b16 	%rs<351>;
	.reg .b32 	%r<878>;
	.reg .b64 	%rd<386>;
	// demoted variable
	.shared .align 4 .b8 _ZZ7init_vmILi8EEvPvS0_S0_E18execution_plan_buf[8192];

	ld.param.u64 	%rd129, [_Z7init_vmILi8EEvPvS0_S0__param_0];
	ld.param.u64 	%rd130, [_Z7init_vmILi8EEvPvS0_S0__param_1];
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r798, %r1, 2;
	mov.u32 	%r3, %ntid.x;
	setp.gt.u32	%p4, %r798, 8191;
	@%p4 bra 	BB2_3;

	shl.b32 	%r4, %r3, 2;
	shl.b32 	%r216, %r1, 2;
	mov.u32 	%r217, _ZZ7init_vmILi8EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r797, %r217, %r216;

BB2_2:
	mov.u32 	%r218, 0;
	st.shared.u32 	[%r797], %r218;
	add.s32 	%r797, %r797, %r4;
	add.s32 	%r798, %r798, %r4;
	setp.lt.u32	%p5, %r798, 8192;
	@%p5 bra 	BB2_2;

BB2_3:
	bar.warp.sync 	-1;
	mov.u32 	%r219, %ctaid.x;
	mad.lo.s32 	%r220, %r219, %r3, %r1;
	shr.u32 	%r221, %r220, 3;
	and.b32  	%r222, %r220, 7;
	mul.wide.u32 	%rd132, %r221, 256;
	cvt.u64.u32	%rd133, %r222;
	or.b64  	%rd134, %rd132, %rd133;
	cvta.to.global.u64 	%rd135, %rd130;
	shl.b64 	%rd136, %rd134, 3;
	add.s64 	%rd137, %rd135, %rd136;
	mov.u64 	%rd360, 0;
	st.global.u64 	[%rd137], %rd360;
	mul.wide.u32 	%rd139, %r221, 2176;
	shr.u64 	%rd1, %rd139, 3;
	or.b64  	%rd140, %rd1, %rd133;
	cvta.to.global.u64 	%rd2, %rd129;
	shl.b64 	%rd141, %rd140, 3;
	add.s64 	%rd142, %rd2, %rd141;
	ld.global.u64 	%rd143, [%rd142];
	shr.u64 	%rd144, %rd143, 7;
	and.b64  	%rd145, %rd144, 139611588448485376;
	and.b64  	%rd146, %rd143, 4503599627370495;
	add.s64 	%rd147, %rd145, 4607182418800017408;
	or.b64  	%rd148, %rd147, %rd146;
	st.global.u64 	[%rd137+192], %rd148;
	setp.ne.s32	%p6, %r222, 0;
	@%p6 bra 	BB2_275;

	shl.b32 	%r224, %r1, 8;
	and.b32  	%r225, %r224, -2048;
	mov.u32 	%r226, _ZZ7init_vmILi8EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r10, %r226, %r225;
	add.s64 	%rd3, %rd1, 16;
	mov.u32 	%r799, 0;
	mov.u64 	%rd359, %rd360;

BB2_5:
	cvt.u64.u32	%rd6, %r799;
	add.s64 	%rd151, %rd6, %rd1;
	shl.b64 	%rd152, %rd151, 3;
	add.s64 	%rd7, %rd2, %rd152;
	ld.global.u32 	%r12, [%rd7+128];
	and.b32  	%r227, %r12, -63489;
	st.global.u32 	[%rd7+128], %r227;
	and.b32  	%r13, %r12, 255;
	setp.lt.u32	%p7, %r13, 85;
	@%p7 bra 	BB2_18;
	bra.uni 	BB2_6;

BB2_18:
	bfe.u32 	%r796, %r12, 8, 3;
	shl.b32 	%r266, %r796, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r266,8;
	// inline asm
	mov.u64 	%rd192, 1;
	// inline asm
	bfi.b64 %rd359,%rd192,%rd359,%r266,8;
	// inline asm
	bra.uni 	BB2_19;

BB2_6:
	and.b32  	%r784, %r12, 255;
	add.s32 	%r228, %r784, -85;
	setp.lt.u32	%p8, %r228, 8;
	@%p8 bra 	BB2_16;
	bra.uni 	BB2_7;

BB2_16:
	ld.global.u32 	%r260, [%rd7+132];
	add.s32 	%r261, %r260, -1;
	and.b32  	%r262, %r261, %r260;
	setp.eq.s32	%p16, %r262, 0;
	@%p16 bra 	BB2_19;

	bfe.u32 	%r795, %r12, 8, 3;
	shl.b32 	%r264, %r795, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r264,8;
	// inline asm
	mov.u64 	%rd186, 1;
	// inline asm
	bfi.b64 %rd359,%rd186,%rd359,%r264,8;
	// inline asm
	bra.uni 	BB2_19;

BB2_7:
	and.b32  	%r785, %r12, 255;
	add.s32 	%r229, %r785, -93;
	setp.lt.u32	%p9, %r229, 32;
	@%p9 bra 	BB2_15;
	bra.uni 	BB2_8;

BB2_15:
	bfe.u32 	%r794, %r12, 8, 3;
	shl.b32 	%r259, %r794, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r259,8;
	// inline asm
	mov.u64 	%rd180, 1;
	// inline asm
	bfi.b64 %rd359,%rd180,%rd359,%r259,8;
	// inline asm
	bra.uni 	BB2_19;

BB2_8:
	and.b32  	%r786, %r12, 255;
	add.s32 	%r230, %r786, -125;
	setp.lt.u32	%p10, %r230, 4;
	@%p10 bra 	BB2_13;
	bra.uni 	BB2_9;

BB2_13:
	bfe.u32 	%r791, %r12, 16, 3;
	bfe.u32 	%r790, %r12, 8, 3;
	setp.eq.s32	%p15, %r791, %r790;
	@%p15 bra 	BB2_19;

	bfe.u32 	%r793, %r12, 16, 3;
	bfe.u32 	%r792, %r12, 8, 3;
	shl.b32 	%r255, %r792, 3;
	// inline asm
	bfi.b64 %rd164,%rd6,%rd360,%r255,8;
	// inline asm
	mov.u64 	%rd174, 1;
	// inline asm
	bfi.b64 %rd167,%rd174,%rd359,%r255,8;
	// inline asm
	shl.b32 	%r257, %r793, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd164,%r257,8;
	// inline asm
	// inline asm
	bfi.b64 %rd359,%rd174,%rd167,%r257,8;
	// inline asm
	bra.uni 	BB2_19;

BB2_9:
	and.b32  	%r787, %r12, 255;
	add.s32 	%r231, %r787, -129;
	setp.lt.u32	%p11, %r231, 94;
	@%p11 bra 	BB2_12;
	bra.uni 	BB2_10;

BB2_12:
	ld.global.u32 	%r252, [%rd7+128];
	or.b32  	%r253, %r252, 8192;
	st.global.u32 	[%rd7+128], %r253;
	bra.uni 	BB2_19;

BB2_10:
	and.b32  	%r788, %r12, 255;
	add.s32 	%r232, %r788, -223;
	setp.gt.u32	%p12, %r232, 15;
	@%p12 bra 	BB2_19;

	bfe.u32 	%r789, %r12, 8, 3;
	shl.b32 	%r234, %r789, 3;
	// inline asm
	bfe.u64 %rd153,%rd360,%r234,8;
	// inline asm
	cvt.u32.u64	%r235, %rd153;
	// inline asm
	bfe.u64 %rd155,%rd359,%r234,8;
	// inline asm
	cvt.u32.u64	%r236, %rd155;
	setp.eq.s32	%p13, %r236, 0;
	selp.b32	%r237, -1, %r235, %p13;
	setp.eq.s32	%p14, %r237, -1;
	selp.b32	%r238, 144, 16, %p14;
	or.b32  	%r239, %r238, %r789;
	shl.b32 	%r240, %r239, 8;
	shl.b32 	%r241, %r237, 16;
	and.b32  	%r242, %r241, 16711680;
	and.b32  	%r243, %r12, -16776961;
	or.b32  	%r244, %r242, %r243;
	or.b32  	%r245, %r244, %r240;
	st.global.u32 	[%rd7+128], %r245;
	cvt.s64.s32	%rd158, %r237;
	add.s64 	%rd159, %rd158, %rd3;
	shl.b64 	%rd160, %rd159, 3;
	add.s64 	%rd161, %rd160, %rd2;
	ld.global.u32 	%r246, [%rd161+8];
	or.b32  	%r247, %r246, 16384;
	st.global.u32 	[%rd161+8], %r247;
	shl.b32 	%r248, %r799, 8;
	or.b32  	%r249, %r248, %r799;
	shl.b32 	%r250, %r249, 16;
	or.b32  	%r251, %r250, %r249;
	cvt.u64.u32	%rd162, %r251;
	shl.b64 	%rd163, %rd162, 32;
	or.b64  	%rd360, %rd163, %rd162;
	mov.u64 	%rd359, 72340172838076673;

BB2_19:
	cvt.u32.u64	%r276, %rd6;
	add.s32 	%r799, %r276, 1;
	mov.u32 	%r21, -1;
	mov.u32 	%r17, 0;
	mov.u64 	%rd369, 0;
	mov.u16 	%rs323, 0;
	setp.ne.s32	%p17, %r799, 256;
	@%p17 bra 	BB2_5;

	mov.u32 	%r18, %r17;
	mov.u32 	%r825, %r17;
	mov.u32 	%r803, %r17;
	mov.u16 	%rs324, %rs323;
	mov.u16 	%rs325, %rs323;
	mov.u32 	%r805, %r17;
	mov.u64 	%rd25, %rd369;
	mov.u64 	%rd21, %rd369;
	mov.u64 	%rd22, %rd369;
	mov.u32 	%r809, %r17;
	mov.u32 	%r807, %r21;
	mov.u32 	%r808, %r21;
	bra.uni 	BB2_21;

BB2_33:
	add.s32 	%r298, %r28, -85;
	setp.lt.u32	%p31, %r298, 8;
	@%p31 bra 	BB2_72;
	bra.uni 	BB2_34;

BB2_72:
	add.s32 	%r338, %r285, -1;
	and.b32  	%r339, %r338, %r285;
	setp.eq.s32	%p50, %r339, 0;
	selp.b32	%r810, 0, %r35, %p50;
	selp.u16	%rs328, 1, 0, %p50;
	mov.u16 	%rs326, 0;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;
	mov.u16 	%rs331, %rs326;
	bra.uni 	BB2_73;

BB2_34:
	add.s32 	%r299, %r28, -93;
	setp.lt.u32	%p32, %r299, 2;
	mov.u16 	%rs333, 0;
	@%p32 bra 	BB2_35;
	bra.uni 	BB2_37;

BB2_35:
	mov.u32 	%r810, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;

BB2_36:
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB2_74;

BB2_37:
	add.s32 	%r300, %r28, -95;
	setp.lt.u32	%p33, %r300, 15;
	@%p33 bra 	BB2_38;

	add.s32 	%r301, %r28, -110;
	setp.lt.u32	%p34, %r301, 5;
	@%p34 bra 	BB2_41;
	bra.uni 	BB2_42;

BB2_41:
	mov.u32 	%r810, %r39;
	mov.u16 	%rs326, %rs331;
	bra.uni 	BB2_39;

BB2_42:
	add.s32 	%r302, %r28, -115;
	setp.lt.u32	%p35, %r302, 10;
	@%p35 bra 	BB2_38;
	bra.uni 	BB2_43;

BB2_38:
	mov.u16 	%rs326, %rs333;

BB2_39:
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB2_74;

BB2_43:
	add.s32 	%r303, %r28, -125;
	setp.lt.u32	%p36, %r303, 4;
	@%p36 bra 	BB2_71;
	bra.uni 	BB2_44;

BB2_71:
	bfe.u32 	%r770, %r284, 16, 3;
	setp.eq.s32	%p290, %r811, %r770;
	selp.b32	%r810, 0, %r810, %p290;
	selp.u16	%rs328, 1, 0, %p290;
	mov.u16 	%rs330, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs331, %rs330;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB2_74;

BB2_44:
	add.s32 	%r304, %r28, -129;
	setp.lt.u32	%p37, %r304, 8;
	@%p37 bra 	BB2_69;
	bra.uni 	BB2_45;

BB2_69:
	shl.b32 	%r755, %r811, 3;
	// inline asm
	bfe.u64 %rd223,%rd21,%r755,8;
	// inline asm
	cvt.u32.u64	%r810, %rd223;
	bra.uni 	BB2_70;

BB2_45:
	add.s32 	%r305, %r28, -137;
	setp.lt.u32	%p38, %r305, 20;
	@%p38 bra 	BB2_68;
	bra.uni 	BB2_46;

BB2_68:
	shr.u32 	%r754, %r284, 8;
	and.b32  	%r811, %r754, 3;
	shl.b32 	%r336, %r811, 3;
	// inline asm
	bfe.u64 %rd221,%rd21,%r336,8;
	// inline asm
	cvt.u32.u64	%r810, %rd221;
	bra.uni 	BB2_70;

BB2_46:
	add.s32 	%r306, %r28, -157;
	setp.lt.u32	%p39, %r306, 5;
	@%p39 bra 	BB2_67;
	bra.uni 	BB2_47;

BB2_67:
	shr.u32 	%r753, %r284, 8;
	and.b32  	%r811, %r753, 3;
	shl.b32 	%r333, %r811, 3;
	// inline asm
	bfe.u64 %rd219,%rd21,%r333,8;
	// inline asm
	cvt.u32.u64	%r334, %rd219;
	max.u32 	%r335, %r37, %r334;
	max.u32 	%r810, %r18, %r335;
	bra.uni 	BB2_65;

BB2_47:
	add.s32 	%r307, %r28, -162;
	setp.lt.u32	%p40, %r307, 20;
	@%p40 bra 	BB2_66;
	bra.uni 	BB2_48;

BB2_66:
	shr.u32 	%r752, %r284, 8;
	and.b32  	%r811, %r752, 3;
	shl.b32 	%r332, %r811, 3;
	// inline asm
	bfe.u64 %rd217,%rd21,%r332,8;
	// inline asm
	cvt.u32.u64	%r810, %rd217;
	bra.uni 	BB2_70;

BB2_48:
	add.s32 	%r308, %r28, -182;
	setp.lt.u32	%p41, %r308, 5;
	@%p41 bra 	BB2_64;
	bra.uni 	BB2_49;

BB2_64:
	shr.u32 	%r751, %r284, 8;
	and.b32  	%r811, %r751, 3;
	shl.b32 	%r329, %r811, 3;
	// inline asm
	bfe.u64 %rd215,%rd21,%r329,8;
	// inline asm
	cvt.u32.u64	%r330, %rd215;
	max.u32 	%r331, %r37, %r330;
	max.u32 	%r810, %r18, %r331;

BB2_65:
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs326;
	bra.uni 	BB2_74;

BB2_49:
	add.s32 	%r309, %r28, -187;
	setp.lt.u32	%p42, %r309, 6;
	@%p42 bra 	BB2_63;
	bra.uni 	BB2_50;

BB2_63:
	shr.u32 	%r750, %r284, 8;
	and.b32  	%r811, %r750, 3;
	shl.b32 	%r328, %r811, 3;
	// inline asm
	bfe.u64 %rd213,%rd21,%r328,8;
	// inline asm
	cvt.u32.u64	%r810, %rd213;
	bra.uni 	BB2_70;

BB2_50:
	add.s32 	%r310, %r28, -193;
	setp.lt.u32	%p43, %r310, 20;
	@%p43 bra 	BB2_62;
	bra.uni 	BB2_51;

BB2_62:
	shr.u32 	%r749, %r284, 8;
	and.b32  	%r327, %r749, 3;
	add.s32 	%r811, %r327, 4;
	shl.b32 	%r326, %r811, 3;
	// inline asm
	bfe.u64 %rd211,%rd21,%r326,8;
	// inline asm
	cvt.u32.u64	%r810, %rd211;
	bra.uni 	BB2_70;

BB2_51:
	add.s32 	%r311, %r28, -213;
	setp.lt.u32	%p44, %r311, 4;
	@%p44 bra 	BB2_61;
	bra.uni 	BB2_52;

BB2_61:
	shr.u32 	%r748, %r284, 8;
	and.b32  	%r323, %r748, 3;
	add.s32 	%r811, %r323, 4;
	shl.b32 	%r322, %r811, 3;
	// inline asm
	bfe.u64 %rd209,%rd21,%r322,8;
	// inline asm
	cvt.u32.u64	%r324, %rd209;
	max.u32 	%r325, %r37, %r324;
	max.u32 	%r810, %r18, %r325;
	bra.uni 	BB2_65;

BB2_52:
	add.s32 	%r312, %r28, -217;
	setp.lt.u32	%p45, %r312, 6;
	@%p45 bra 	BB2_60;
	bra.uni 	BB2_53;

BB2_60:
	shr.u32 	%r747, %r284, 8;
	and.b32  	%r321, %r747, 3;
	add.s32 	%r811, %r321, 4;
	shl.b32 	%r320, %r811, 3;
	// inline asm
	bfe.u64 %rd207,%rd21,%r320,8;
	// inline asm
	cvt.u32.u64	%r810, %rd207;

BB2_70:
	mov.u16 	%rs332, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	bra.uni 	BB2_74;

BB2_53:
	add.s32 	%r746, %r807, 1;
	add.s32 	%r313, %r28, -223;
	setp.lt.u32	%p46, %r313, 16;
	mov.u16 	%rs329, 1;
	@%p46 bra 	BB2_54;
	bra.uni 	BB2_55;

BB2_54:
	mov.u32 	%r809, %r746;
	mov.u32 	%r810, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	bra.uni 	BB2_36;

BB2_55:
	setp.eq.s32	%p47, %r28, 239;
	mov.u16 	%rs331, 1;
	@%p47 bra 	BB2_56;
	bra.uni 	BB2_57;

BB2_56:
	mov.u32 	%r810, %r37;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	mov.u16 	%rs333, %rs331;
	bra.uni 	BB2_74;

BB2_57:
	and.b32  	%r771, %r284, 240;
	mov.u16 	%rs328, 1;
	mov.u32 	%r314, 0;
	setp.ne.s32	%p48, %r771, 240;
	@%p48 bra 	BB2_58;

	add.s32 	%r315, %r808, 8;
	shr.s32 	%r316, %r315, 31;
	shr.u32 	%r317, %r316, 29;
	add.s32 	%r318, %r315, %r317;
	shr.s32 	%r319, %r318, 3;
	max.u32 	%r810, %r319, %r810;
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB2_74;

BB2_58:
	mov.u32 	%r810, %r314;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs328;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB2_74;

BB2_21:
	mov.u16 	%rs326, 0;
	cvt.u64.u32	%rd204, %r805;
	add.s64 	%rd205, %rd204, %rd1;
	shl.b64 	%rd206, %rd205, 3;
	add.s64 	%rd23, %rd2, %rd206;
	ld.global.v2.u32 	{%r284, %r285}, [%rd23+128];
	and.b32  	%r28, %r284, 255;
	bfe.u32 	%r811, %r284, 8, 3;
	bfe.u32 	%r286, %r284, 14, 1;
	and.b32  	%r287, %r286, 1;
	setp.eq.b32	%p1, %r287, 1;
	add.s32 	%r288, %r807, 1;
	selp.b32	%r809, %r288, %r809, %p1;
	selp.b16	%rs325, 1, %rs325, %p1;
	mov.u16 	%rs331, 1;
	shl.b32 	%r277, %r811, 3;
	// inline asm
	bfe.u64 %rd198,%rd369,%r277,8;
	// inline asm
	cvt.u32.u64	%r35, %rd198;
	shr.u32 	%r289, %r284, 13;
	and.b32  	%r36, %r289, 56;
	// inline asm
	bfe.u64 %rd200,%rd369,%r36,8;
	// inline asm
	cvt.u32.u64	%r37, %rd200;
	max.u32 	%r810, %r35, %r37;
	setp.lt.u32	%p18, %r28, 25;
	@%p18 bra 	BB2_22;

	mov.u16 	%rs327, 0;
	bfe.u32 	%r745, %r284, 16, 3;
	and.b32  	%r290, %r285, 1835008;
	setp.ne.s32	%p19, %r290, 0;
	setp.eq.s32	%p20, %r811, %r745;
	and.pred  	%p21, %p20, %p19;
	selp.b32	%r291, %r17, %r18, %p21;
	max.u32 	%r39, %r810, %r291;
	add.s32 	%r292, %r28, -25;
	setp.lt.u32	%p22, %r292, 7;
	mov.u16 	%rs331, 1;
	@%p22 bra 	BB2_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r40, %r284, 240;
	setp.eq.s32	%p23, %r40, 32;
	@%p23 bra 	BB2_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r293, %r28, -48;
	setp.lt.u32	%p24, %r293, 7;
	@%p24 bra 	BB2_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r294, %r28, -55;
	setp.lt.u32	%p25, %r294, 16;
	@%p25 bra 	BB2_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r295, %r28, -71;
	setp.lt.u32	%p26, %r295, 4;
	@%p26 bra 	BB2_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r296, %r28, -75;
	setp.lt.u32	%p27, %r296, 4;
	@%p27 bra 	BB2_22;

	mov.u16 	%rs327, 0;
	setp.eq.s32	%p28, %r28, 79;
	@%p28 bra 	BB2_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r297, %r284, 252;
	setp.eq.s32	%p29, %r297, 80;
	@%p29 bra 	BB2_22;
	bra.uni 	BB2_32;

BB2_22:
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;

BB2_73:
	mov.u16 	%rs332, %rs326;
	mov.u16 	%rs333, %rs326;
	bra.uni 	BB2_74;

BB2_32:
	mov.u16 	%rs327, 0;
	setp.eq.s32	%p30, %r28, 84;
	@%p30 bra 	BB2_24;
	bra.uni 	BB2_33;

BB2_24:
	mov.u32 	%r810, %r39;
	mov.u16 	%rs326, %rs331;
	mov.u16 	%rs328, %rs327;
	mov.u16 	%rs329, %rs327;
	mov.u16 	%rs330, %rs327;
	mov.u16 	%rs332, %rs327;
	mov.u16 	%rs333, %rs327;

BB2_74:
	setp.eq.s16	%p51, %rs328, 0;
	@%p51 bra 	BB2_76;
	bra.uni 	BB2_75;

BB2_76:
	bfe.u32 	%r763, %r284, 14, 1;
	and.b32  	%r762, %r763, 1;
	setp.eq.b32	%p289, %r762, 1;
	and.b16  	%rs279, %rs324, 255;
	setp.eq.s16	%p53, %rs279, 0;
	selp.u16	%rs334, 1, 0, %p289;
	@%p53 bra 	BB2_78;

	ld.global.u32 	%r340, [%rd23+128];
	or.b32  	%r341, %r340, 16384;
	st.global.u32 	[%rd23+128], %r341;
	mov.u16 	%rs334, 1;

BB2_78:
	shl.b32 	%r342, %r810, 3;
	max.u32 	%r65, %r342, %r809;
	setp.eq.s16	%p54, %rs333, 0;
	@%p54 bra 	BB2_80;
	bra.uni 	BB2_79;

BB2_80:
	setp.eq.s16	%p55, %rs332, 0;
	selp.b64	%rd226, %rd25, %rd22, %p55;
	shl.b32 	%r343, %r811, 3;
	// inline asm
	bfe.u64 %rd225,%rd226,%r343,8;
	// inline asm
	cvt.u32.u64	%r344, %rd225;
	shl.b32 	%r345, %r344, 3;
	max.u32 	%r822, %r345, %r65;
	bra.uni 	BB2_81;

BB2_75:
	and.b32  	%r756, %r284, 16384;
	setp.ne.s32	%p52, %r756, 0;
	selp.b16	%rs324, 1, %rs324, %p52;
	bra.uni 	BB2_159;

BB2_79:
	max.s32 	%r822, %r825, %r65;

BB2_81:
	setp.eq.s16	%p56, %rs330, 0;
	@%p56 bra 	BB2_83;

	shr.u32 	%r765, %r284, 13;
	and.b32  	%r764, %r765, 56;
	// inline asm
	bfe.u64 %rd227,%rd25,%r764,8;
	// inline asm
	cvt.u32.u64	%r347, %rd227;
	shl.b32 	%r348, %r347, 3;
	max.u32 	%r822, %r348, %r822;

BB2_83:
	setp.eq.s16	%p57, %rs332, 0;
	@%p57 bra 	BB2_126;
	bra.uni 	BB2_84;

BB2_126:
	add.s32 	%r386, %r807, 1;
	max.s32 	%r823, %r822, %r386;
	setp.gt.s32	%p134, %r822, %r807;
	@%p134 bra 	BB2_130;

BB2_127:
	add.s32 	%r387, %r10, %r822;
	ld.shared.u8 	%rs299, [%r387];
	setp.eq.s16	%p135, %rs299, 0;
	@%p135 bra 	BB2_128;

	add.s32 	%r95, %r822, 1;
	setp.lt.s32	%p136, %r822, %r807;
	mov.u32 	%r822, %r95;
	@%p136 bra 	BB2_127;
	bra.uni 	BB2_130;

BB2_117:
	add.s32 	%r822, %r822, 1;

BB2_84:
	add.s32 	%r72, %r10, %r822;
	ld.shared.u8 	%rs281, [%r72];
	setp.ne.s16	%p58, %rs281, 0;
	@%p58 bra 	BB2_117;

	add.s32 	%r772, %r10, %r822;
	ld.shared.u8 	%rs282, [%r772+1];
	setp.ne.s16	%p59, %rs282, 0;
	add.s32 	%r349, %r822, 1;
	and.b32  	%r350, %r349, 7;
	setp.eq.s32	%p60, %r350, 0;
	or.pred  	%p61, %p59, %p60;
	@%p61 bra 	BB2_117;

	shr.s32 	%r351, %r822, 31;
	shr.u32 	%r352, %r351, 29;
	add.s32 	%r353, %r822, %r352;
	and.b32  	%r73, %r353, -8;
	mov.u16 	%rs341, 0;
	setp.ge.s32	%p62, %r73, %r822;
	@%p62 bra 	BB2_116;

	sub.s32 	%r74, %r822, %r73;
	and.b32  	%r75, %r74, 3;
	setp.eq.s32	%p63, %r75, 0;
	mov.u16 	%rs341, 0;
	@%p63 bra 	BB2_102;

	setp.eq.s32	%p64, %r75, 1;
	mov.u16 	%rs338, 0;
	@%p64 bra 	BB2_98;

	setp.eq.s32	%p65, %r75, 2;
	mov.u16 	%rs336, 0;
	@%p65 bra 	BB2_94;

	add.s32 	%r354, %r10, %r73;
	ld.shared.u8 	%rs18, [%r354];
	setp.eq.s16	%p66, %rs18, 0;
	setp.ne.s32	%p67, %r73, %r21;
	and.pred  	%p68, %p67, %p66;
	@%p68 bra 	BB2_92;

	setp.ne.s16	%p69, %rs334, 0;
	cvt.u64.u16	%rd229, %rs18;
	add.s64 	%rd230, %rd229, %rd3;
	shl.b64 	%rd231, %rd230, 3;
	add.s64 	%rd232, %rd2, %rd231;
	ld.global.u32 	%r355, [%rd232];
	and.b32  	%r356, %r355, 8192;
	setp.eq.s32	%p70, %r356, 0;
	and.b32  	%r357, %r355, 20480;
	setp.ne.s32	%p71, %r357, 0;
	or.pred  	%p72, %p69, %p71;
	and.pred  	%p73, %p70, %p72;
	mov.u16 	%rs336, 1;
	@%p73 bra 	BB2_93;

BB2_92:
	mov.u16 	%rs336, 0;

BB2_93:
	add.s32 	%r73, %r73, 1;

BB2_94:
	add.s32 	%r358, %r10, %r73;
	ld.shared.u8 	%rs21, [%r358];
	setp.eq.s16	%p74, %rs21, 0;
	setp.ne.s32	%p75, %r73, %r21;
	and.pred  	%p76, %p75, %p74;
	@%p76 bra 	BB2_96;

	setp.ne.s16	%p77, %rs334, 0;
	cvt.u64.u16	%rd233, %rs21;
	add.s64 	%rd234, %rd233, %rd3;
	shl.b64 	%rd235, %rd234, 3;
	add.s64 	%rd236, %rd2, %rd235;
	ld.global.u32 	%r359, [%rd236];
	and.b32  	%r360, %r359, 8192;
	setp.eq.s32	%p78, %r360, 0;
	and.b32  	%r361, %r359, 20480;
	setp.ne.s32	%p79, %r361, 0;
	or.pred  	%p80, %p77, %p79;
	and.pred  	%p81, %p78, %p80;
	mov.u16 	%rs338, 1;
	@%p81 bra 	BB2_97;

BB2_96:
	mov.u16 	%rs338, %rs336;

BB2_97:
	add.s32 	%r73, %r73, 1;

BB2_98:
	add.s32 	%r362, %r10, %r73;
	ld.shared.u8 	%rs24, [%r362];
	setp.eq.s16	%p82, %rs24, 0;
	setp.ne.s32	%p83, %r73, %r21;
	and.pred  	%p84, %p83, %p82;
	@%p84 bra 	BB2_100;

	setp.ne.s16	%p85, %rs334, 0;
	cvt.u64.u16	%rd237, %rs24;
	add.s64 	%rd238, %rd237, %rd3;
	shl.b64 	%rd239, %rd238, 3;
	add.s64 	%rd240, %rd2, %rd239;
	ld.global.u32 	%r363, [%rd240];
	and.b32  	%r364, %r363, 8192;
	setp.eq.s32	%p86, %r364, 0;
	and.b32  	%r365, %r363, 20480;
	setp.ne.s32	%p87, %r365, 0;
	or.pred  	%p88, %p85, %p87;
	and.pred  	%p89, %p86, %p88;
	mov.u16 	%rs341, 1;
	@%p89 bra 	BB2_101;

BB2_100:
	mov.u16 	%rs341, %rs338;

BB2_101:
	add.s32 	%r73, %r73, 1;

BB2_102:
	setp.lt.u32	%p90, %r74, 4;
	@%p90 bra 	BB2_116;

BB2_103:
	add.s32 	%r83, %r10, %r73;
	ld.shared.u8 	%rs28, [%r83];
	setp.eq.s16	%p91, %rs28, 0;
	setp.ne.s32	%p92, %r73, %r21;
	and.pred  	%p93, %p92, %p91;
	@%p93 bra 	BB2_105;

	setp.ne.s16	%p94, %rs334, 0;
	cvt.u64.u16	%rd241, %rs28;
	add.s64 	%rd242, %rd241, %rd3;
	shl.b64 	%rd243, %rd242, 3;
	add.s64 	%rd244, %rd2, %rd243;
	ld.global.u32 	%r366, [%rd244];
	and.b32  	%r367, %r366, 8192;
	setp.eq.s32	%p95, %r367, 0;
	and.b32  	%r368, %r366, 20480;
	setp.ne.s32	%p96, %r368, 0;
	or.pred  	%p97, %p94, %p96;
	and.pred  	%p98, %p95, %p97;
	mov.u16 	%rs342, 1;
	@%p98 bra 	BB2_106;

BB2_105:
	mov.u16 	%rs342, %rs341;

BB2_106:
	add.s32 	%r779, %r10, %r73;
	ld.shared.u8 	%rs30, [%r779+1];
	add.s32 	%r369, %r73, 1;
	setp.ne.s32	%p99, %r369, %r21;
	setp.eq.s16	%p100, %rs30, 0;
	and.pred  	%p101, %p99, %p100;
	@%p101 bra 	BB2_108;

	setp.ne.s16	%p102, %rs334, 0;
	cvt.u64.u16	%rd245, %rs30;
	add.s64 	%rd246, %rd245, %rd3;
	shl.b64 	%rd247, %rd246, 3;
	add.s64 	%rd248, %rd2, %rd247;
	ld.global.u32 	%r370, [%rd248];
	and.b32  	%r371, %r370, 8192;
	setp.eq.s32	%p103, %r371, 0;
	and.b32  	%r372, %r370, 20480;
	setp.ne.s32	%p104, %r372, 0;
	or.pred  	%p105, %p102, %p104;
	and.pred  	%p106, %p103, %p105;
	mov.u16 	%rs343, 1;
	@%p106 bra 	BB2_109;

BB2_108:
	mov.u16 	%rs343, %rs342;

BB2_109:
	add.s32 	%r780, %r10, %r73;
	ld.shared.u8 	%rs32, [%r780+2];
	add.s32 	%r373, %r73, 2;
	setp.ne.s32	%p107, %r373, %r21;
	setp.eq.s16	%p108, %rs32, 0;
	and.pred  	%p109, %p107, %p108;
	@%p109 bra 	BB2_111;

	setp.ne.s16	%p110, %rs334, 0;
	cvt.u64.u16	%rd249, %rs32;
	add.s64 	%rd250, %rd249, %rd3;
	shl.b64 	%rd251, %rd250, 3;
	add.s64 	%rd252, %rd2, %rd251;
	ld.global.u32 	%r374, [%rd252];
	and.b32  	%r375, %r374, 8192;
	setp.eq.s32	%p111, %r375, 0;
	and.b32  	%r376, %r374, 20480;
	setp.ne.s32	%p112, %r376, 0;
	or.pred  	%p113, %p110, %p112;
	and.pred  	%p114, %p111, %p113;
	mov.u16 	%rs344, 1;
	@%p114 bra 	BB2_112;

BB2_111:
	mov.u16 	%rs344, %rs343;

BB2_112:
	add.s32 	%r781, %r10, %r73;
	ld.shared.u8 	%rs34, [%r781+3];
	add.s32 	%r377, %r73, 3;
	setp.ne.s32	%p115, %r377, %r21;
	setp.eq.s16	%p116, %rs34, 0;
	and.pred  	%p117, %p115, %p116;
	@%p117 bra 	BB2_114;

	setp.ne.s16	%p118, %rs334, 0;
	cvt.u64.u16	%rd253, %rs34;
	add.s64 	%rd254, %rd253, %rd3;
	shl.b64 	%rd255, %rd254, 3;
	add.s64 	%rd256, %rd2, %rd255;
	ld.global.u32 	%r378, [%rd256];
	and.b32  	%r379, %r378, 8192;
	setp.eq.s32	%p119, %r379, 0;
	and.b32  	%r380, %r378, 20480;
	setp.ne.s32	%p120, %r380, 0;
	or.pred  	%p121, %p118, %p120;
	and.pred  	%p122, %p119, %p121;
	mov.u16 	%rs341, 1;
	@%p122 bra 	BB2_115;

BB2_114:
	mov.u16 	%rs341, %rs344;

BB2_115:
	add.s32 	%r73, %r73, 4;
	setp.lt.s32	%p123, %r73, %r822;
	@%p123 bra 	BB2_103;

BB2_116:
	and.b16  	%rs295, %rs341, 255;
	setp.eq.s16	%p124, %rs295, 0;
	@%p124 bra 	BB2_118;
	bra.uni 	BB2_117;

BB2_118:
	shr.s32 	%r778, %r822, 31;
	shr.u32 	%r777, %r778, 29;
	add.s32 	%r776, %r822, %r777;
	and.b32  	%r820, %r776, -8;
	setp.ge.s32	%p291, %r820, %r822;
	mov.u32 	%r381, -1;
	@%p291 bra 	BB2_119;

BB2_120:
	add.s32 	%r87, %r10, %r820;
	ld.shared.u8 	%rs37, [%r87];
	setp.eq.s16	%p126, %rs37, 0;
	setp.ne.s32	%p127, %r820, %r21;
	and.pred  	%p128, %p127, %p126;
	@%p128 bra 	BB2_122;

	cvt.u64.u16	%rd257, %rs37;
	add.s64 	%rd258, %rd257, %rd3;
	shl.b64 	%rd259, %rd258, 3;
	add.s64 	%rd260, %rd2, %rd259;
	ld.global.u8 	%rs296, [%rd260+1];
	and.b16  	%rs297, %rs296, 32;
	setp.eq.s16	%p129, %rs297, 0;
	@%p129 bra 	BB2_124;

BB2_122:
	mov.u32 	%r782, -1;
	add.s32 	%r820, %r820, 1;
	setp.lt.s32	%p130, %r820, %r822;
	@%p130 bra 	BB2_120;

	mov.u32 	%r820, %r782;
	bra.uni 	BB2_125;

BB2_119:
	mov.u32 	%r820, %r381;
	bra.uni 	BB2_125;

BB2_128:
	mov.u32 	%r823, %r822;
	bra.uni 	BB2_130;

BB2_124:
	add.s32 	%r783, %r10, %r820;
	add.s32 	%r774, %r822, 1;
	add.s32 	%r773, %r10, %r822;
	setp.eq.s32	%p131, %r820, %r21;
	st.shared.u8 	[%r773], %rs37;
	ld.shared.u8 	%rs298, [%r783+1];
	st.shared.u8 	[%r773+1], %rs298;
	selp.b32	%r383, %r822, %r21, %p131;
	add.s32 	%r384, %r820, 1;
	setp.eq.s32	%p132, %r383, %r384;
	selp.b32	%r21, %r774, %r383, %p132;

BB2_125:
	setp.lt.s32	%p133, %r820, 0;
	selp.b32	%r823, %r822, %r820, %p133;

BB2_130:
	setp.eq.s32	%p137, %r805, 0;
	selp.b16	%rs323, %rs332, %rs323, %p137;
	selp.b32	%r21, %r823, %r21, %p137;
	@%p54 bra 	BB2_132;

	shr.s32 	%r388, %r823, 31;
	shr.u32 	%r389, %r388, 29;
	add.s32 	%r390, %r823, %r389;
	and.b32  	%r391, %r390, -8;
	sub.s32 	%r392, %r823, %r391;
	add.s32 	%r393, %r823, 8;
	sub.s32 	%r825, %r393, %r392;

BB2_132:
	add.s32 	%r101, %r10, %r823;
	cvt.u16.u32	%rs39, %r805;
	st.shared.u8 	[%r101], %rs39;
	add.s32 	%r826, %r803, 1;
	@%p57 bra 	BB2_134;

	st.shared.u8 	[%r101+1], %rs39;
	add.s32 	%r826, %r803, 2;

BB2_134:
	shr.s32 	%r394, %r823, 31;
	shr.u32 	%r395, %r394, 29;
	add.s32 	%r396, %r823, %r395;
	shr.s32 	%r105, %r396, 3;
	setp.eq.s16	%p140, %rs331, 0;
	@%p140 bra 	BB2_136;

	shr.u32 	%r767, %r284, 13;
	and.b32  	%r766, %r767, 56;
	// inline asm
	bfe.u64 %rd261,%rd25,%r766,8;
	// inline asm
	cvt.u32.u64	%r399, %rd261;
	max.s32 	%r400, %r105, %r399;
	cvt.s64.s32	%rd264, %r400;
	// inline asm
	bfi.b64 %rd25,%rd264,%rd25,%r766,8;
	// inline asm

BB2_136:
	setp.ne.s16	%p141, %rs326, 0;
	setp.lt.s32	%p142, %r808, %r823;
	and.pred  	%p143, %p142, %p141;
	selp.b32	%r808, %r823, %r808, %p143;
	@%p54 bra 	BB2_138;
	bra.uni 	BB2_137;

BB2_138:
	@%p57 bra 	BB2_140;
	bra.uni 	BB2_139;

BB2_140:
	setp.ne.s16	%p146, %rs327, 0;
	@%p146 bra 	BB2_144;

	add.s32 	%r413, %r105, 1;
	cvt.u64.u32	%rd277, %r413;
	shl.b32 	%r412, %r811, 3;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r412,8;
	// inline asm
	@%p56 bra 	BB2_143;

	shr.u32 	%r769, %r284, 13;
	and.b32  	%r768, %r769, 56;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r768,8;
	// inline asm

BB2_143:
	// inline asm
	bfe.u64 %rd282,%rd25,%r412,8;
	// inline asm
	cvt.u32.u64	%r417, %rd282;
	max.s32 	%r418, %r105, %r417;
	cvt.s64.s32	%rd285, %r418;
	// inline asm
	bfi.b64 %rd25,%rd285,%rd25,%r412,8;
	// inline asm

BB2_144:
	setp.eq.s16	%p148, %rs329, 0;
	@%p148 bra 	BB2_146;

	add.s32 	%r419, %r105, 1;
	shl.b32 	%r420, %r419, 8;
	or.b32  	%r421, %r420, %r419;
	shl.b32 	%r422, %r421, 16;
	or.b32  	%r423, %r422, %r421;
	cvt.u64.u32	%rd287, %r423;
	shl.b64 	%rd288, %rd287, 32;
	or.b64  	%rd369, %rd288, %rd287;

BB2_146:
	setp.eq.s16	%p149, %rs327, 0;
	@%p149 bra 	BB2_148;

	shl.b32 	%r425, %r811, 3;
	// inline asm
	bfe.u64 %rd289,%rd25,%r425,8;
	// inline asm
	cvt.u32.u64	%r426, %rd289;
	max.s32 	%r427, %r105, %r426;
	cvt.s64.s32	%rd292, %r427;
	// inline asm
	bfi.b64 %rd25,%rd292,%rd25,%r425,8;
	// inline asm
	setp.gt.u32	%p150, %r284, -536870913;
	selp.b32	%r17, %r105, %r17, %p150;
	mov.u32 	%r18, %r105;
	bra.uni 	BB2_148;

BB2_137:
	add.s32 	%r401, %r105, 1;
	shl.b32 	%r402, %r401, 8;
	or.b32  	%r403, %r402, %r401;
	shl.b32 	%r404, %r403, 16;
	or.b32  	%r405, %r404, %r403;
	cvt.u64.u32	%rd266, %r405;
	shl.b64 	%rd267, %rd266, 32;
	or.b64  	%rd21, %rd267, %rd266;
	bra.uni 	BB2_148;

BB2_139:
	add.s32 	%r409, %r105, 1;
	cvt.u64.u32	%rd269, %r409;
	shl.b32 	%r408, %r811, 3;
	// inline asm
	bfi.b64 %rd21,%rd269,%rd21,%r408,8;
	// inline asm
	// inline asm
	bfe.u64 %rd271,%rd22,%r408,8;
	// inline asm
	cvt.u32.u64	%r410, %rd271;
	max.s32 	%r411, %r105, %r410;
	cvt.s64.s32	%rd274, %r411;
	// inline asm
	bfi.b64 %rd22,%rd274,%rd22,%r408,8;
	// inline asm

BB2_148:
	add.s32 	%r428, %r10, %r809;
	ld.shared.u8 	%rs300, [%r428];
	setp.eq.s16	%p151, %rs300, 0;
	setp.ne.s32	%p152, %r809, %r21;
	and.pred  	%p153, %p152, %p151;
	@%p153 bra 	BB2_154;

	and.b16  	%rs301, %rs325, 255;
	setp.eq.s16	%p154, %rs301, 0;
	@%p154 bra 	BB2_151;

	ld.global.u32 	%r429, [%rd23+128];
	or.b32  	%r430, %r429, 16384;
	st.global.u32 	[%rd23+128], %r430;

BB2_151:
	cvt.u32.u16	%r431, %rs332;
	add.s32 	%r809, %r431, %r809;

BB2_152:
	add.s32 	%r809, %r809, 1;
	mov.u16 	%rs325, 0;
	setp.gt.s32	%p155, %r809, 2047;
	@%p155 bra 	BB2_154;

	add.s32 	%r432, %r10, %r809;
	ld.shared.u8 	%rs304, [%r432];
	setp.ne.s16	%p156, %rs304, 0;
	@%p156 bra 	BB2_152;

BB2_154:
	setp.eq.s16	%p157, %rs334, 0;
	@%p157 bra 	BB2_156;

	selp.b32	%r433, 1, 2, %p57;
	add.s32 	%r434, %r823, %r433;
	max.s32 	%r809, %r434, %r809;

BB2_156:
	cvt.u32.u16	%r435, %rs332;
	add.s32 	%r436, %r823, %r435;
	max.s32 	%r832, %r436, %r807;
	add.s32 	%r118, %r21, 1;

BB2_157:
	mov.u32 	%r119, %r832;
	add.s32 	%r437, %r10, %r119;
	ld.shared.u8 	%rs305, [%r437];
	setp.ne.s16	%p159, %rs305, 0;
	setp.eq.s32	%p160, %r119, %r21;
	or.pred  	%p161, %p160, %p159;
	setp.eq.s32	%p162, %r119, %r118;
	and.b16  	%rs306, %rs323, 255;
	setp.ne.s16	%p163, %rs306, 0;
	and.pred  	%p164, %p162, %p163;
	or.pred  	%p165, %p161, %p164;
	add.s32 	%r832, %r119, 1;
	@%p165 bra 	BB2_157;

	setp.ne.s16	%p166, %rs332, 0;
	mov.u16 	%rs324, 0;
	add.s32 	%r807, %r119, -1;
	setp.gt.s32	%p167, %r119, %r825;
	and.pred  	%p168, %p167, %p166;
	selp.b32	%r825, %r119, %r825, %p168;
	mov.u32 	%r803, %r826;

BB2_159:
	add.s32 	%r805, %r805, 1;
	setp.lt.u32	%p169, %r805, 256;
	@%p169 bra 	BB2_21;

	ld.param.u64 	%rd356, [_Z7init_vmILi8EEvPvS0_S0__param_1];
	cvta.to.global.u64 	%rd355, %rd356;
	mov.u32 	%r761, %tid.x;
	mov.u32 	%r760, %ntid.x;
	mov.u32 	%r759, %ctaid.x;
	mad.lo.s32 	%r758, %r759, %r760, %r761;
	shr.u32 	%r757, %r758, 3;
	ld.param.u64 	%rd354, [_Z7init_vmILi8EEvPvS0_S0__param_2];
	shr.s32 	%r438, %r807, 31;
	shr.u32 	%r439, %r438, 29;
	add.s32 	%r440, %r807, %r439;
	shr.s32 	%r441, %r440, 3;
	add.s32 	%r442, %r441, 1;
	cvta.to.global.u64 	%rd294, %rd354;
	atom.global.add.u32 	%r443, [%rd294], %r442;
	add.s64 	%rd295, %rd294, 4;
	atom.global.add.u32 	%r444, [%rd295], %r803;
	add.s64 	%rd297, %rd2, %rd139;
	ld.global.u32 	%r450, [%rd297+64];
	ld.global.u32 	%r451, [%rd297+80];
	ld.global.v2.u64 	{%rd298, %rd299}, [%rd297+96];
	and.b64  	%rd301, %rd298, 1;
	and.b64  	%rd302, %rd298, 2;
	shl.b64 	%rd303, %rd302, 7;
	or.b64  	%rd304, %rd303, %rd301;
	and.b64  	%rd305, %rd298, 4;
	shl.b64 	%rd306, %rd305, 14;
	or.b64  	%rd307, %rd304, %rd306;
	and.b64  	%rd308, %rd298, 8;
	shl.b64 	%rd309, %rd308, 21;
	or.b64  	%rd310, %rd307, %rd309;
	or.b64  	%rd311, %rd310, 100925952;
	shl.b64 	%rd312, %rd311, 3;
	and.b64  	%rd314, %rd299, 524287;
	shl.b64 	%rd315, %rd314, 6;
	ld.global.v2.u64 	{%rd316, %rd317}, [%rd297+112];
	and.b64  	%rd320, %rd316, 4194303;
	shr.u64 	%rd321, %rd316, 4;
	and.b64  	%rd322, %rd321, 1080863910568919040;
	or.b64  	%rd323, %rd320, %rd322;
	and.b64  	%rd324, %rd317, 4194303;
	shr.u64 	%rd325, %rd317, 4;
	and.b64  	%rd326, %rd325, 1080863910568919040;
	or.b64  	%rd327, %rd324, %rd326;
	mul.wide.u32 	%rd329, %r757, 2048;
	add.s64 	%rd47, %rd355, %rd329;
	cvt.u32.u64	%r452, %rd312;
	and.b32  	%r453, %r451, 2147483584;
	and.b32  	%r454, %r450, 2147483584;
	cvt.u32.u64	%r455, %rd315;
	st.global.v4.u32 	[%rd47+128], {%r454, %r453, %r452, %r455};
	or.b64  	%rd330, %rd327, 3458764513820540928;
	or.b64  	%rd331, %rd323, 3458764513820540928;
	st.global.v2.u64 	[%rd47+144], {%rd331, %rd330};
	add.s64 	%rd332, %rd329, %rd356;
	add.s64 	%rd48, %rd332, 1024;
	setp.lt.s32	%p170, %r807, 0;
	mov.u64 	%rd384, %rd48;
	@%p170 bra 	BB2_274;

	add.s64 	%rd383, %rd47, 1024;
	add.s32 	%r132, %r21, 1;
	mov.u32 	%r875, -1;
	mov.u32 	%r841, 0;
	mov.u32 	%r134, %r841;
	mov.u64 	%rd384, %rd48;
	mov.u32 	%r874, %r875;

BB2_162:
	add.s32 	%r460, %r10, %r841;
	ld.shared.u8 	%rs44, [%r460];
	setp.ne.s16	%p171, %rs44, 0;
	setp.eq.s32	%p172, %r841, %r21;
	or.pred  	%p173, %p172, %p171;
	setp.eq.s32	%p174, %r841, %r132;
	and.b16  	%rs308, %rs323, 255;
	setp.ne.s16	%p175, %rs308, 0;
	and.pred  	%p176, %p174, %p175;
	or.pred  	%p177, %p173, %p176;
	mov.u32 	%r877, %r841;
	@!%p177 bra 	BB2_273;
	bra.uni 	BB2_163;

BB2_163:
	add.s32 	%r845, %r841, 1;
	mov.u32 	%r849, 1;
	and.b32  	%r465, %r845, 7;
	setp.eq.s32	%p178, %r465, 0;
	mov.u32 	%r848, 0;
	setp.gt.u32	%p179, %r845, %r807;
	or.pred  	%p180, %p178, %p179;
	@%p180 bra 	BB2_168;

BB2_164:
	add.s32 	%r466, %r10, %r845;
	ld.shared.u8 	%rs45, [%r466];
	setp.ne.s16	%p181, %rs45, 0;
	setp.eq.s32	%p182, %r845, %r21;
	or.pred  	%p183, %p181, %p182;
	setp.eq.s32	%p184, %r845, %r132;
	and.pred  	%p186, %p184, %p175;
	or.pred  	%p187, %p183, %p186;
	@!%p187 bra 	BB2_168;
	bra.uni 	BB2_165;

BB2_165:
	and.b32  	%r467, %r849, 1;
	setp.eq.b32	%p189, %r467, 1;
	mov.pred 	%p292, 0;
	@!%p189 bra 	BB2_167;
	bra.uni 	BB2_166;

BB2_166:
	cvt.u64.u16	%rd333, %rs45;
	add.s64 	%rd334, %rd333, %rd3;
	shl.b64 	%rd335, %rd334, 3;
	add.s64 	%rd336, %rd2, %rd335;
	ld.global.u8 	%rs310, [%rd336+1];
	shr.u16 	%rs311, %rs310, 5;
	and.b16  	%rs312, %rs311, 1;
	setp.eq.b16	%p292, %rs312, 1;

BB2_167:
	selp.u32	%r468, 1, 0, %p292;
	add.s32 	%r848, %r468, %r848;
	add.s32 	%r849, %r849, 1;
	add.s32 	%r845, %r849, %r841;
	and.b32  	%r469, %r845, 7;
	setp.ne.s32	%p190, %r469, 0;
	setp.le.u32	%p191, %r845, %r807;
	and.pred  	%p192, %p190, %p191;
	@%p192 bra 	BB2_164;

BB2_168:
	add.s32 	%r470, %r849, 255;
	shl.b32 	%r471, %r470, 24;
	shl.b32 	%r472, %r848, 28;
	or.b32  	%r146, %r471, %r472;
	cvt.u64.u16	%rd337, %rs44;
	add.s64 	%rd338, %rd337, %rd3;
	shl.b64 	%rd339, %rd338, 3;
	add.s64 	%rd340, %rd2, %rd339;
	ld.global.v2.u32 	{%r473, %r474}, [%rd340];
	and.b32  	%r149, %r473, 255;
	shr.u32 	%r150, %r473, 8;
	bfe.u32 	%r151, %r473, 8, 3;
	shr.u32 	%r152, %r473, 16;
	bfe.u32 	%r153, %r473, 16, 3;
	and.b32  	%r475, %r841, 1;
	setp.eq.b32	%p193, %r475, 1;
	not.pred 	%p194, %p193;
	shr.u32 	%r476, %r473, 13;
	and.b32  	%r477, %r476, 1;
	setp.eq.b32	%p195, %r477, 1;
	and.pred  	%p196, %p194, %p195;
	selp.u32	%r478, 1, 0, %p196;
	add.s32 	%r877, %r478, %r841;
	shr.u32 	%r479, %r473, 14;
	and.b32  	%r480, %r479, 1;
	setp.eq.b32	%p197, %r480, 1;
	setp.lt.s32	%p198, %r875, 0;
	and.pred  	%p199, %p197, %p198;
	selp.b32	%r875, %r874, %r875, %p199;
	add.s32 	%r874, %r874, 1;
	setp.lt.u32	%p200, %r149, 25;
	mul.wide.u32 	%rd341, %r134, 4;
	add.s64 	%rd342, %rd47, %rd341;
	add.s64 	%rd54, %rd342, 256;
	@%p200 bra 	BB2_269;
	bra.uni 	BB2_169;

BB2_269:
	mov.u32 	%r872, 1048576;
	setp.ne.s32	%p286, %r151, 5;
	@%p286 bra 	BB2_272;

	shl.b32 	%r872, %r134, 6;
	setp.gt.u32	%p287, %r134, 189;
	@%p287 bra 	BB2_272;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB2_272:
	or.b32  	%r738, %r151, %r146;
	shl.b32 	%r739, %r153, 3;
	or.b32  	%r740, %r738, %r739;
	shr.u32 	%r741, %r473, 11;
	and.b32  	%r742, %r741, 98304;
	or.b32  	%r743, %r740, %r742;
	or.b32  	%r744, %r743, %r872;
	add.s64 	%rd124, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r744;
	mov.u64 	%rd383, %rd124;
	bra.uni 	BB2_273;

BB2_169:
	add.s32 	%r481, %r149, -25;
	setp.lt.u32	%p201, %r481, 7;
	@%p201 bra 	BB2_266;
	bra.uni 	BB2_170;

BB2_266:
	setp.gt.u32	%p281, %r134, 189;
	mov.u32 	%r871, %r134;
	@%p281 bra 	BB2_268;

	and.b32  	%r724, %r473, 50331648;
	setp.eq.s32	%p282, %r724, 0;
	selp.b32	%r725, 2, 1, %p282;
	setp.eq.s32	%p283, %r153, %r151;
	selp.b32	%r726, 3, %r725, %p283;
	and.b32  	%r727, %r474, -65011713;
	setp.eq.s32	%p284, %r726, 2;
	selp.b32	%r728, 29360128, 23068672, %p284;
	setp.eq.s32	%p285, %r726, 1;
	selp.b32	%r729, 37748736, %r728, %p285;
	or.b32  	%r730, %r729, %r727;
	add.s32 	%r871, %r134, 1;
	st.global.u32 	[%rd54], %r730;

BB2_268:
	shl.b32 	%r731, %r134, 6;
	or.b32  	%r732, %r731, %r146;
	or.b32  	%r733, %r732, %r151;
	shl.b32 	%r734, %r153, 3;
	or.b32  	%r735, %r733, %r734;
	or.b32  	%r736, %r735, 1064960;
	add.s64 	%rd122, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r736;
	mov.u64 	%rd383, %rd122;
	mov.u32 	%r134, %r871;
	bra.uni 	BB2_273;

BB2_170:
	and.b32  	%r157, %r473, 240;
	setp.eq.s32	%p202, %r157, 32;
	@%p202 bra 	BB2_262;
	bra.uni 	BB2_171;

BB2_262:
	shl.b32 	%r719, %r153, 3;
	or.b32  	%r720, %r151, %r719;
	or.b32  	%r869, %r720, 1572864;
	setp.ne.s32	%p279, %r153, %r151;
	@%p279 bra 	BB2_265;

	shl.b32 	%r721, %r134, 6;
	or.b32  	%r722, %r721, %r869;
	or.b32  	%r869, %r722, 131072;
	setp.gt.u32	%p280, %r134, 189;
	@%p280 bra 	BB2_265;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB2_265:
	add.s64 	%rd120, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r723, %r869, %r146;
	st.global.u32 	[%rd383], %r723;
	mov.u64 	%rd383, %rd120;
	bra.uni 	BB2_273;

BB2_171:
	add.s32 	%r482, %r149, -48;
	setp.lt.u32	%p203, %r482, 7;
	@%p203 bra 	BB2_259;
	bra.uni 	BB2_172;

BB2_259:
	setp.gt.u32	%p274, %r134, 189;
	mov.u32 	%r868, %r134;
	@%p274 bra 	BB2_261;

	and.b32  	%r706, %r473, 50331648;
	setp.eq.s32	%p275, %r706, 0;
	selp.b32	%r707, 2, 1, %p275;
	setp.eq.s32	%p276, %r153, %r151;
	selp.b32	%r708, 3, %r707, %p276;
	and.b32  	%r709, %r474, -65011713;
	setp.eq.s32	%p277, %r708, 2;
	selp.b32	%r710, 29360128, 23068672, %p277;
	setp.eq.s32	%p278, %r708, 1;
	selp.b32	%r711, 37748736, %r710, %p278;
	or.b32  	%r712, %r711, %r709;
	add.s32 	%r868, %r134, 1;
	st.global.u32 	[%rd54], %r712;

BB2_261:
	shl.b32 	%r713, %r134, 6;
	or.b32  	%r714, %r713, %r146;
	or.b32  	%r715, %r714, %r151;
	shl.b32 	%r716, %r153, 3;
	or.b32  	%r717, %r715, %r716;
	or.b32  	%r718, %r717, 1589248;
	add.s64 	%rd118, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r718;
	mov.u64 	%rd383, %rd118;
	mov.u32 	%r134, %r868;
	bra.uni 	BB2_273;

BB2_172:
	add.s32 	%r483, %r149, -55;
	setp.lt.u32	%p204, %r483, 16;
	@%p204 bra 	BB2_255;
	bra.uni 	BB2_173;

BB2_255:
	shl.b32 	%r701, %r153, 3;
	or.b32  	%r702, %r151, %r701;
	or.b32  	%r866, %r702, 2097152;
	setp.ne.s32	%p272, %r153, %r151;
	@%p272 bra 	BB2_258;

	shl.b32 	%r703, %r134, 6;
	or.b32  	%r704, %r703, %r866;
	or.b32  	%r866, %r704, 131072;
	setp.gt.u32	%p273, %r134, 189;
	@%p273 bra 	BB2_258;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB2_258:
	add.s64 	%rd116, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r705, %r866, %r146;
	st.global.u32 	[%rd383], %r705;
	mov.u64 	%rd383, %rd116;
	bra.uni 	BB2_273;

BB2_173:
	add.s32 	%r484, %r149, -71;
	setp.lt.u32	%p205, %r484, 4;
	@%p205 bra 	BB2_252;
	bra.uni 	BB2_174;

BB2_252:
	setp.gt.u32	%p267, %r134, 189;
	mov.u32 	%r865, %r134;
	@%p267 bra 	BB2_254;

	and.b32  	%r688, %r473, 50331648;
	setp.eq.s32	%p268, %r688, 0;
	selp.b32	%r689, 2, 1, %p268;
	setp.eq.s32	%p269, %r153, %r151;
	selp.b32	%r690, 3, %r689, %p269;
	and.b32  	%r691, %r474, -65011713;
	setp.eq.s32	%p270, %r690, 2;
	selp.b32	%r692, 29360128, 23068672, %p270;
	setp.eq.s32	%p271, %r690, 1;
	selp.b32	%r693, 37748736, %r692, %p271;
	or.b32  	%r694, %r693, %r691;
	add.s32 	%r865, %r134, 1;
	st.global.u32 	[%rd54], %r694;

BB2_254:
	shl.b32 	%r695, %r134, 6;
	or.b32  	%r696, %r695, %r146;
	or.b32  	%r697, %r696, %r151;
	shl.b32 	%r698, %r153, 3;
	or.b32  	%r699, %r697, %r698;
	or.b32  	%r700, %r699, 2113536;
	add.s64 	%rd114, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r700;
	mov.u64 	%rd383, %rd114;
	mov.u32 	%r134, %r865;
	bra.uni 	BB2_273;

BB2_174:
	add.s32 	%r485, %r149, -75;
	setp.lt.u32	%p206, %r485, 4;
	@%p206 bra 	BB2_251;
	bra.uni 	BB2_175;

BB2_251:
	shl.b32 	%r684, %r153, 3;
	or.b32  	%r685, %r146, %r151;
	or.b32  	%r686, %r685, %r684;
	or.b32  	%r687, %r686, 6291456;
	add.s64 	%rd112, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r687;
	mov.u64 	%rd383, %rd112;
	bra.uni 	BB2_273;

BB2_175:
	setp.eq.s32	%p207, %r149, 79;
	@%p207 bra 	BB2_248;
	bra.uni 	BB2_176;

BB2_248:
	setp.gt.u32	%p262, %r134, 189;
	mov.u32 	%r864, %r134;
	@%p262 bra 	BB2_250;

	and.b32  	%r671, %r473, 50331648;
	setp.eq.s32	%p263, %r671, 0;
	selp.b32	%r672, 2, 1, %p263;
	setp.eq.s32	%p264, %r153, %r151;
	selp.b32	%r673, 3, %r672, %p264;
	and.b32  	%r674, %r474, -65011713;
	setp.eq.s32	%p265, %r673, 2;
	selp.b32	%r675, 29360128, 23068672, %p265;
	setp.eq.s32	%p266, %r673, 1;
	selp.b32	%r676, 37748736, %r675, %p266;
	or.b32  	%r677, %r676, %r674;
	add.s32 	%r864, %r134, 1;
	st.global.u32 	[%rd54], %r677;

BB2_250:
	shl.b32 	%r678, %r134, 6;
	or.b32  	%r679, %r678, %r146;
	or.b32  	%r680, %r679, %r151;
	shl.b32 	%r681, %r153, 3;
	or.b32  	%r682, %r680, %r681;
	or.b32  	%r683, %r682, 6307840;
	add.s64 	%rd110, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r683;
	mov.u64 	%rd383, %rd110;
	mov.u32 	%r134, %r864;
	bra.uni 	BB2_273;

BB2_176:
	and.b32  	%r486, %r473, 252;
	setp.eq.s32	%p208, %r486, 80;
	@%p208 bra 	BB2_247;
	bra.uni 	BB2_177;

BB2_247:
	shl.b32 	%r667, %r153, 3;
	or.b32  	%r668, %r146, %r151;
	or.b32  	%r669, %r668, %r667;
	or.b32  	%r670, %r669, 4194304;
	add.s64 	%rd108, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r670;
	mov.u64 	%rd383, %rd108;
	bra.uni 	BB2_273;

BB2_177:
	setp.eq.s32	%p209, %r149, 84;
	@%p209 bra 	BB2_244;
	bra.uni 	BB2_178;

BB2_244:
	setp.gt.u32	%p257, %r134, 189;
	mov.u32 	%r863, %r134;
	@%p257 bra 	BB2_246;

	and.b32  	%r654, %r473, 50331648;
	setp.eq.s32	%p258, %r654, 0;
	selp.b32	%r655, 2, 1, %p258;
	setp.eq.s32	%p259, %r153, %r151;
	selp.b32	%r656, 3, %r655, %p259;
	and.b32  	%r657, %r474, -65011713;
	setp.eq.s32	%p260, %r656, 2;
	selp.b32	%r658, 29360128, 23068672, %p260;
	setp.eq.s32	%p261, %r656, 1;
	selp.b32	%r659, 37748736, %r658, %p261;
	or.b32  	%r660, %r659, %r657;
	add.s32 	%r863, %r134, 1;
	st.global.u32 	[%rd54], %r660;

BB2_246:
	shl.b32 	%r661, %r134, 6;
	or.b32  	%r662, %r661, %r146;
	or.b32  	%r663, %r662, %r151;
	shl.b32 	%r664, %r153, 3;
	or.b32  	%r665, %r663, %r664;
	or.b32  	%r666, %r665, 4210688;
	add.s64 	%rd106, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r666;
	mov.u64 	%rd383, %rd106;
	mov.u32 	%r134, %r863;
	bra.uni 	BB2_273;

BB2_178:
	add.s32 	%r487, %r149, -85;
	setp.lt.u32	%p210, %r487, 8;
	add.s32 	%r488, %r134, 1;
	mul.wide.u32 	%rd343, %r488, 4;
	add.s64 	%rd344, %rd47, %rd343;
	add.s64 	%rd55, %rd344, 256;
	@%p210 bra 	BB2_234;
	bra.uni 	BB2_179;

BB2_234:
	add.s32 	%r636, %r474, -1;
	and.b32  	%r637, %r636, %r474;
	setp.eq.s32	%p251, %r637, 0;
	mov.u64 	%rd381, 1;
	@%p251 bra 	BB2_239;

	cvt.u64.u32	%rd92, %r474;
	// inline asm
	bfind.u32 %r638,%r474;
	// inline asm
	mov.pred 	%p252, 0;
	@%p252 bra 	BB2_237;
	bra.uni 	BB2_236;

BB2_237:
	cvt.u32.u64	%r642, %rd92;
	mov.u32 	%r861, 0;
	div.u32 	%r643, %r861, %r642;
	rem.u32 	%r644, %r861, %r642;
	cvt.u64.u32	%rd381, %r643;
	cvt.u64.u32	%rd380, %r644;
	bra.uni 	BB2_238;

BB2_179:
	add.s32 	%r489, %r149, -93;
	setp.lt.u32	%p211, %r489, 2;
	@%p211 bra 	BB2_233;
	bra.uni 	BB2_180;

BB2_233:
	or.b32  	%r634, %r146, %r151;
	or.b32  	%r635, %r634, 5242880;
	add.s64 	%rd90, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r635;
	mov.u64 	%rd383, %rd90;
	bra.uni 	BB2_273;

BB2_180:
	add.s32 	%r490, %r149, -95;
	setp.lt.u32	%p212, %r490, 15;
	@%p212 bra 	BB2_229;
	bra.uni 	BB2_181;

BB2_229:
	shl.b32 	%r629, %r153, 3;
	or.b32  	%r630, %r151, %r629;
	or.b32  	%r859, %r630, 3145728;
	setp.ne.s32	%p249, %r153, %r151;
	@%p249 bra 	BB2_232;

	shl.b32 	%r631, %r134, 6;
	or.b32  	%r632, %r631, %r859;
	or.b32  	%r859, %r632, 131072;
	setp.gt.u32	%p250, %r134, 189;
	@%p250 bra 	BB2_232;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB2_232:
	add.s64 	%rd88, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r633, %r859, %r146;
	st.global.u32 	[%rd383], %r633;
	mov.u64 	%rd383, %rd88;
	bra.uni 	BB2_273;

BB2_236:
	mov.u64 	%rd346, -9223372036854775808;
	div.u64 	%rd381, %rd346, %rd92;
	rem.u64 	%rd380, %rd346, %rd92;
	mov.u32 	%r861, 0;

BB2_238:
	sub.s64 	%rd347, %rd92, %rd380;
	setp.ge.u64	%p253, %rd380, %rd347;
	selp.u64	%rd348, 1, 0, %p253;
	shl.b64 	%rd349, %rd381, 1;
	or.b64  	%rd381, %rd348, %rd349;
	selp.b64	%rd350, %rd92, 0, %p253;
	shl.b64 	%rd351, %rd380, 1;
	sub.s64 	%rd380, %rd351, %rd350;
	add.s32 	%r861, %r861, 1;
	setp.le.u32	%p254, %r861, %r638;
	@%p254 bra 	BB2_238;

BB2_239:
	setp.eq.s64	%p255, %rd381, 1;
	@%p255 bra 	BB2_243;
	bra.uni 	BB2_240;

BB2_243:
	or.b32  	%r653, %r146, 8388608;
	add.s64 	%rd104, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r653;
	mov.u64 	%rd383, %rd104;
	bra.uni 	BB2_273;

BB2_240:
	setp.gt.u32	%p256, %r134, 188;
	mov.u32 	%r862, %r134;
	@%p256 bra 	BB2_242;

	mov.b64	{%r645, %r646}, %rd381;
	st.global.u32 	[%rd54], %r645;
	st.global.u32 	[%rd55], %r646;
	add.s32 	%r862, %r134, 2;

BB2_242:
	shl.b32 	%r647, %r134, 6;
	or.b32  	%r648, %r647, %r146;
	or.b32  	%r649, %r648, %r151;
	shl.b32 	%r650, %r153, 3;
	or.b32  	%r651, %r649, %r650;
	or.b32  	%r652, %r651, 2359296;
	add.s64 	%rd102, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r652;
	mov.u64 	%rd383, %rd102;
	mov.u32 	%r134, %r862;
	bra.uni 	BB2_273;

BB2_181:
	add.s32 	%r491, %r149, -110;
	setp.lt.u32	%p213, %r491, 5;
	@%p213 bra 	BB2_226;
	bra.uni 	BB2_182;

BB2_226:
	setp.gt.u32	%p244, %r134, 189;
	mov.u32 	%r858, %r134;
	@%p244 bra 	BB2_228;

	and.b32  	%r616, %r473, 50331648;
	setp.eq.s32	%p245, %r616, 0;
	selp.b32	%r617, 2, 1, %p245;
	setp.eq.s32	%p246, %r153, %r151;
	selp.b32	%r618, 3, %r617, %p246;
	and.b32  	%r619, %r474, -65011713;
	setp.eq.s32	%p247, %r618, 2;
	selp.b32	%r620, 29360128, 23068672, %p247;
	setp.eq.s32	%p248, %r618, 1;
	selp.b32	%r621, 37748736, %r620, %p248;
	or.b32  	%r622, %r621, %r619;
	add.s32 	%r858, %r134, 1;
	st.global.u32 	[%rd54], %r622;

BB2_228:
	shl.b32 	%r623, %r134, 6;
	or.b32  	%r624, %r623, %r146;
	or.b32  	%r625, %r624, %r151;
	shl.b32 	%r626, %r153, 3;
	or.b32  	%r627, %r625, %r626;
	or.b32  	%r628, %r627, 3162112;
	add.s64 	%rd86, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r628;
	mov.u64 	%rd383, %rd86;
	mov.u32 	%r134, %r858;
	bra.uni 	BB2_273;

BB2_182:
	add.s32 	%r492, %r149, -115;
	setp.lt.u32	%p214, %r492, 10;
	@%p214 bra 	BB2_222;
	bra.uni 	BB2_183;

BB2_222:
	shl.b32 	%r611, %r153, 3;
	or.b32  	%r612, %r151, %r611;
	or.b32  	%r856, %r612, 7340032;
	setp.ne.s32	%p242, %r153, %r151;
	@%p242 bra 	BB2_225;

	shl.b32 	%r613, %r134, 6;
	or.b32  	%r614, %r613, %r856;
	or.b32  	%r856, %r614, 131072;
	setp.gt.u32	%p243, %r134, 189;
	@%p243 bra 	BB2_225;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB2_225:
	add.s64 	%rd84, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r615, %r856, %r146;
	st.global.u32 	[%rd383], %r615;
	mov.u64 	%rd383, %rd84;
	bra.uni 	BB2_273;

BB2_183:
	add.s32 	%r493, %r149, -125;
	setp.lt.u32	%p215, %r493, 4;
	@%p215 bra 	BB2_221;
	bra.uni 	BB2_184;

BB2_221:
	shl.b32 	%r606, %r153, 3;
	or.b32  	%r607, %r151, %r606;
	or.b32  	%r608, %r607, 8388608;
	setp.eq.s32	%p241, %r153, %r151;
	selp.b32	%r609, 8388608, %r608, %p241;
	or.b32  	%r610, %r609, %r146;
	add.s64 	%rd82, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r610;
	mov.u64 	%rd383, %rd82;
	bra.uni 	BB2_273;

BB2_184:
	add.s32 	%r494, %r149, -129;
	setp.lt.u32	%p216, %r494, 8;
	@%p216 bra 	BB2_220;
	bra.uni 	BB2_185;

BB2_220:
	or.b32  	%r604, %r146, %r151;
	or.b32  	%r605, %r604, 11534336;
	add.s64 	%rd80, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r605;
	mov.u64 	%rd383, %rd80;
	bra.uni 	BB2_273;

BB2_185:
	add.s32 	%r495, %r149, -137;
	setp.lt.u32	%p217, %r495, 20;
	@%p217 bra 	BB2_219;
	bra.uni 	BB2_186;

BB2_219:
	and.b32  	%r598, %r150, 3;
	and.b32  	%r599, %r152, 3;
	shl.b32 	%r600, %r599, 4;
	or.b32  	%r601, %r146, %r598;
	or.b32  	%r602, %r601, %r600;
	or.b32  	%r603, %r602, 12582912;
	add.s64 	%rd78, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r603;
	mov.u64 	%rd383, %rd78;
	bra.uni 	BB2_273;

BB2_186:
	add.s32 	%r496, %r149, -157;
	setp.lt.u32	%p218, %r496, 5;
	@%p218 bra 	BB2_216;
	bra.uni 	BB2_187;

BB2_216:
	setp.gt.u32	%p239, %r134, 189;
	mov.u32 	%r855, %r134;
	@%p239 bra 	BB2_218;

	and.b32  	%r587, %r473, 50331648;
	setp.eq.s32	%p240, %r587, 0;
	selp.b32	%r588, 29360128, 37748736, %p240;
	and.b32  	%r589, %r474, -65011713;
	or.b32  	%r590, %r588, %r589;
	add.s32 	%r855, %r134, 1;
	st.global.u32 	[%rd54], %r590;

BB2_218:
	shl.b32 	%r591, %r134, 6;
	or.b32  	%r592, %r591, %r146;
	and.b32  	%r593, %r150, 3;
	or.b32  	%r594, %r592, %r593;
	shl.b32 	%r595, %r153, 3;
	or.b32  	%r596, %r594, %r595;
	or.b32  	%r597, %r596, 12599296;
	add.s64 	%rd76, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r597;
	mov.u64 	%rd383, %rd76;
	mov.u32 	%r134, %r855;
	bra.uni 	BB2_273;

BB2_187:
	add.s32 	%r497, %r149, -162;
	setp.lt.u32	%p219, %r497, 20;
	@%p219 bra 	BB2_215;
	bra.uni 	BB2_188;

BB2_215:
	and.b32  	%r581, %r150, 3;
	and.b32  	%r582, %r152, 3;
	shl.b32 	%r583, %r582, 4;
	or.b32  	%r584, %r146, %r581;
	or.b32  	%r585, %r584, %r583;
	or.b32  	%r586, %r585, 13107200;
	add.s64 	%rd74, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r586;
	mov.u64 	%rd383, %rd74;
	bra.uni 	BB2_273;

BB2_188:
	add.s32 	%r498, %r149, -182;
	setp.lt.u32	%p220, %r498, 5;
	@%p220 bra 	BB2_212;
	bra.uni 	BB2_189;

BB2_212:
	setp.gt.u32	%p237, %r134, 189;
	mov.u32 	%r854, %r134;
	@%p237 bra 	BB2_214;

	and.b32  	%r570, %r473, 50331648;
	setp.eq.s32	%p238, %r570, 0;
	selp.b32	%r571, 29360128, 37748736, %p238;
	and.b32  	%r572, %r474, -65011713;
	or.b32  	%r573, %r571, %r572;
	add.s32 	%r854, %r134, 1;
	st.global.u32 	[%rd54], %r573;

BB2_214:
	shl.b32 	%r574, %r134, 6;
	or.b32  	%r575, %r574, %r146;
	and.b32  	%r576, %r150, 3;
	or.b32  	%r577, %r575, %r576;
	shl.b32 	%r578, %r153, 3;
	or.b32  	%r579, %r577, %r578;
	or.b32  	%r580, %r579, 13123584;
	add.s64 	%rd72, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r580;
	mov.u64 	%rd383, %rd72;
	mov.u32 	%r134, %r854;
	bra.uni 	BB2_273;

BB2_189:
	add.s32 	%r499, %r149, -187;
	setp.lt.u32	%p221, %r499, 6;
	@%p221 bra 	BB2_209;
	bra.uni 	BB2_190;

BB2_209:
	setp.gt.u32	%p236, %r134, 188;
	mov.u32 	%r853, %r134;
	@%p236 bra 	BB2_211;

	mov.u32 	%r563, 0;
	st.global.u32 	[%rd54], %r563;
	mov.u32 	%r564, -2131755008;
	st.global.u32 	[%rd55], %r564;
	add.s32 	%r853, %r134, 2;

BB2_211:
	shl.b32 	%r565, %r134, 6;
	or.b32  	%r566, %r565, %r146;
	and.b32  	%r567, %r150, 3;
	or.b32  	%r568, %r566, %r567;
	or.b32  	%r569, %r568, 3407872;
	add.s64 	%rd70, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r569;
	mov.u64 	%rd383, %rd70;
	mov.u32 	%r134, %r853;
	bra.uni 	BB2_273;

BB2_190:
	add.s32 	%r500, %r149, -193;
	setp.lt.u32	%p222, %r500, 20;
	@%p222 bra 	BB2_208;
	bra.uni 	BB2_191;

BB2_208:
	and.b32  	%r556, %r150, 3;
	add.s32 	%r557, %r556, 4;
	and.b32  	%r558, %r152, 3;
	shl.b32 	%r559, %r558, 4;
	or.b32  	%r560, %r146, %r559;
	or.b32  	%r561, %r560, %r557;
	or.b32  	%r562, %r561, 12615680;
	add.s64 	%rd68, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r562;
	mov.u64 	%rd383, %rd68;
	bra.uni 	BB2_273;

BB2_191:
	add.s32 	%r501, %r149, -213;
	setp.lt.u32	%p223, %r501, 4;
	@%p223 bra 	BB2_205;
	bra.uni 	BB2_192;

BB2_205:
	setp.gt.u32	%p234, %r134, 189;
	mov.u32 	%r852, %r134;
	@%p234 bra 	BB2_207;

	and.b32  	%r544, %r473, 50331648;
	setp.eq.s32	%p235, %r544, 0;
	selp.b32	%r545, 29360128, 37748736, %p235;
	and.b32  	%r546, %r474, -65011713;
	or.b32  	%r547, %r545, %r546;
	add.s32 	%r852, %r134, 1;
	st.global.u32 	[%rd54], %r547;

BB2_207:
	shl.b32 	%r548, %r134, 6;
	or.b32  	%r549, %r548, %r146;
	shl.b32 	%r550, %r153, 3;
	or.b32  	%r551, %r549, %r550;
	and.b32  	%r552, %r150, 3;
	add.s32 	%r553, %r552, 4;
	or.b32  	%r554, %r551, %r553;
	or.b32  	%r555, %r554, 15745024;
	add.s64 	%rd66, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r555;
	mov.u64 	%rd383, %rd66;
	mov.u32 	%r134, %r852;
	bra.uni 	BB2_273;

BB2_192:
	add.s32 	%r502, %r149, -217;
	setp.lt.u32	%p224, %r502, 6;
	@%p224 bra 	BB2_204;
	bra.uni 	BB2_193;

BB2_204:
	and.b32  	%r540, %r150, 3;
	add.s32 	%r541, %r540, 4;
	or.b32  	%r542, %r146, %r541;
	or.b32  	%r543, %r542, 14680064;
	add.s64 	%rd64, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r543;
	mov.u64 	%rd383, %rd64;
	bra.uni 	BB2_273;

BB2_193:
	add.s32 	%r503, %r149, -223;
	setp.lt.u32	%p225, %r503, 16;
	@%p225 bra 	BB2_201;
	bra.uni 	BB2_194;

BB2_201:
	setp.gt.u32	%p233, %r134, 188;
	mov.u32 	%r851, %r134;
	@%p233 bra 	BB2_203;

	shr.u32 	%r524, %r473, 28;
	add.s32 	%r525, %r524, 8;
	add.s32 	%r526, %r524, 7;
	mov.u32 	%r527, 1;
	shl.b32 	%r528, %r527, %r526;
	not.b32 	%r529, %r528;
	shl.b32 	%r530, %r527, %r525;
	or.b32  	%r531, %r530, %r474;
	and.b32  	%r532, %r531, %r529;
	st.global.u32 	[%rd54], %r532;
	shl.b32 	%r533, %r875, 5;
	or.b32  	%r534, %r533, %r525;
	st.global.u32 	[%rd55], %r534;
	add.s32 	%r851, %r134, 2;

BB2_203:
	shl.b32 	%r536, %r134, 6;
	or.b32  	%r537, %r536, %r146;
	or.b32  	%r538, %r537, %r151;
	or.b32  	%r539, %r538, 9437184;
	add.s64 	%rd62, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r539;
	mov.u32 	%r875, -1;
	mov.u64 	%rd383, %rd62;
	mov.u32 	%r134, %r851;
	bra.uni 	BB2_273;

BB2_194:
	setp.eq.s32	%p226, %r149, 239;
	@%p226 bra 	BB2_200;
	bra.uni 	BB2_195;

BB2_200:
	shl.b32 	%r518, %r153, 3;
	and.b32  	%r519, %r474, 63;
	shl.b32 	%r520, %r519, 6;
	or.b32  	%r521, %r146, %r520;
	or.b32  	%r522, %r521, %r518;
	or.b32  	%r523, %r522, 13631488;
	add.s64 	%rd60, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r523;
	mov.u64 	%rd383, %rd60;
	bra.uni 	BB2_273;

BB2_195:
	setp.eq.s32	%p227, %r157, 240;
	@%p227 bra 	BB2_197;
	bra.uni 	BB2_196;

BB2_197:
	setp.gt.u32	%p228, %r134, 189;
	mov.u32 	%r850, %r134;
	@%p228 bra 	BB2_199;

	and.b32  	%r505, %r473, 50331648;
	setp.eq.s32	%p229, %r505, 0;
	selp.b32	%r506, 2, 1, %p229;
	setp.gt.u32	%p230, %r473, -536870913;
	selp.b32	%r507, 3, %r506, %p230;
	and.b32  	%r508, %r474, -65011713;
	setp.eq.s32	%p231, %r507, 2;
	selp.b32	%r509, 29360128, 23068672, %p231;
	setp.eq.s32	%p232, %r507, 1;
	selp.b32	%r510, 37748736, %r509, %p232;
	or.b32  	%r511, %r510, %r508;
	st.global.u32 	[%rd54], %r511;
	mov.u32 	%r850, %r488;

BB2_199:
	shl.b32 	%r512, %r134, 6;
	or.b32  	%r513, %r512, %r146;
	or.b32  	%r514, %r513, %r151;
	shl.b32 	%r515, %r153, 3;
	or.b32  	%r516, %r514, %r515;
	or.b32  	%r517, %r516, 10502144;
	add.s64 	%rd58, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r517;
	mov.u64 	%rd383, %rd58;
	mov.u32 	%r134, %r850;
	bra.uni 	BB2_273;

BB2_196:
	or.b32  	%r504, %r146, 8388608;
	add.s64 	%rd56, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r504;
	mov.u64 	%rd383, %rd56;

BB2_273:
	add.s32 	%r841, %r877, 1;
	setp.lt.s32	%p288, %r877, %r807;
	@%p288 bra 	BB2_162;

BB2_274:
	sub.s64 	%rd352, %rd384, %rd48;
	shr.u64 	%rd353, %rd352, 2;
	st.global.u32 	[%rd47+160], %rd353;

BB2_275:
	ret;
}

	// .globl	_Z7init_vmILi16EEvPvS0_S0_
.visible .entry _Z7init_vmILi16EEvPvS0_S0_(
	.param .u64 _Z7init_vmILi16EEvPvS0_S0__param_0,
	.param .u64 _Z7init_vmILi16EEvPvS0_S0__param_1,
	.param .u64 _Z7init_vmILi16EEvPvS0_S0__param_2
)
.maxntid 32, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<293>;
	.reg .b16 	%rs<351>;
	.reg .b32 	%r<878>;
	.reg .b64 	%rd<386>;
	// demoted variable
	.shared .align 4 .b8 _ZZ7init_vmILi16EEvPvS0_S0_E18execution_plan_buf[16384];

	ld.param.u64 	%rd129, [_Z7init_vmILi16EEvPvS0_S0__param_0];
	ld.param.u64 	%rd130, [_Z7init_vmILi16EEvPvS0_S0__param_1];
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r798, %r1, 2;
	mov.u32 	%r3, %ntid.x;
	setp.gt.u32	%p4, %r798, 16383;
	@%p4 bra 	BB3_3;

	shl.b32 	%r4, %r3, 2;
	shl.b32 	%r216, %r1, 2;
	mov.u32 	%r217, _ZZ7init_vmILi16EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r797, %r217, %r216;

BB3_2:
	mov.u32 	%r218, 0;
	st.shared.u32 	[%r797], %r218;
	add.s32 	%r797, %r797, %r4;
	add.s32 	%r798, %r798, %r4;
	setp.lt.u32	%p5, %r798, 16384;
	@%p5 bra 	BB3_2;

BB3_3:
	bar.warp.sync 	-1;
	mov.u32 	%r219, %ctaid.x;
	mad.lo.s32 	%r220, %r219, %r3, %r1;
	shr.u32 	%r221, %r220, 3;
	and.b32  	%r222, %r220, 7;
	mul.wide.u32 	%rd132, %r221, 256;
	cvt.u64.u32	%rd133, %r222;
	or.b64  	%rd134, %rd132, %rd133;
	cvta.to.global.u64 	%rd135, %rd130;
	shl.b64 	%rd136, %rd134, 3;
	add.s64 	%rd137, %rd135, %rd136;
	mov.u64 	%rd360, 0;
	st.global.u64 	[%rd137], %rd360;
	mul.wide.u32 	%rd139, %r221, 2176;
	shr.u64 	%rd1, %rd139, 3;
	or.b64  	%rd140, %rd1, %rd133;
	cvta.to.global.u64 	%rd2, %rd129;
	shl.b64 	%rd141, %rd140, 3;
	add.s64 	%rd142, %rd2, %rd141;
	ld.global.u64 	%rd143, [%rd142];
	shr.u64 	%rd144, %rd143, 7;
	and.b64  	%rd145, %rd144, 139611588448485376;
	and.b64  	%rd146, %rd143, 4503599627370495;
	add.s64 	%rd147, %rd145, 4607182418800017408;
	or.b64  	%rd148, %rd147, %rd146;
	st.global.u64 	[%rd137+192], %rd148;
	setp.ne.s32	%p6, %r222, 0;
	@%p6 bra 	BB3_275;

	shl.b32 	%r224, %r1, 9;
	and.b32  	%r225, %r224, -4096;
	mov.u32 	%r226, _ZZ7init_vmILi16EEvPvS0_S0_E18execution_plan_buf;
	add.s32 	%r10, %r226, %r225;
	add.s64 	%rd3, %rd1, 16;
	mov.u32 	%r799, 0;
	mov.u64 	%rd359, %rd360;

BB3_5:
	cvt.u64.u32	%rd6, %r799;
	add.s64 	%rd151, %rd6, %rd1;
	shl.b64 	%rd152, %rd151, 3;
	add.s64 	%rd7, %rd2, %rd152;
	ld.global.u32 	%r12, [%rd7+128];
	and.b32  	%r227, %r12, -63489;
	st.global.u32 	[%rd7+128], %r227;
	and.b32  	%r13, %r12, 255;
	setp.lt.u32	%p7, %r13, 85;
	@%p7 bra 	BB3_18;
	bra.uni 	BB3_6;

BB3_18:
	bfe.u32 	%r796, %r12, 8, 3;
	shl.b32 	%r266, %r796, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r266,8;
	// inline asm
	mov.u64 	%rd192, 1;
	// inline asm
	bfi.b64 %rd359,%rd192,%rd359,%r266,8;
	// inline asm
	bra.uni 	BB3_19;

BB3_6:
	and.b32  	%r784, %r12, 255;
	add.s32 	%r228, %r784, -85;
	setp.lt.u32	%p8, %r228, 8;
	@%p8 bra 	BB3_16;
	bra.uni 	BB3_7;

BB3_16:
	ld.global.u32 	%r260, [%rd7+132];
	add.s32 	%r261, %r260, -1;
	and.b32  	%r262, %r261, %r260;
	setp.eq.s32	%p16, %r262, 0;
	@%p16 bra 	BB3_19;

	bfe.u32 	%r795, %r12, 8, 3;
	shl.b32 	%r264, %r795, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r264,8;
	// inline asm
	mov.u64 	%rd186, 1;
	// inline asm
	bfi.b64 %rd359,%rd186,%rd359,%r264,8;
	// inline asm
	bra.uni 	BB3_19;

BB3_7:
	and.b32  	%r785, %r12, 255;
	add.s32 	%r229, %r785, -93;
	setp.lt.u32	%p9, %r229, 32;
	@%p9 bra 	BB3_15;
	bra.uni 	BB3_8;

BB3_15:
	bfe.u32 	%r794, %r12, 8, 3;
	shl.b32 	%r259, %r794, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd360,%r259,8;
	// inline asm
	mov.u64 	%rd180, 1;
	// inline asm
	bfi.b64 %rd359,%rd180,%rd359,%r259,8;
	// inline asm
	bra.uni 	BB3_19;

BB3_8:
	and.b32  	%r786, %r12, 255;
	add.s32 	%r230, %r786, -125;
	setp.lt.u32	%p10, %r230, 4;
	@%p10 bra 	BB3_13;
	bra.uni 	BB3_9;

BB3_13:
	bfe.u32 	%r791, %r12, 16, 3;
	bfe.u32 	%r790, %r12, 8, 3;
	setp.eq.s32	%p15, %r791, %r790;
	@%p15 bra 	BB3_19;

	bfe.u32 	%r793, %r12, 16, 3;
	bfe.u32 	%r792, %r12, 8, 3;
	shl.b32 	%r255, %r792, 3;
	// inline asm
	bfi.b64 %rd164,%rd6,%rd360,%r255,8;
	// inline asm
	mov.u64 	%rd174, 1;
	// inline asm
	bfi.b64 %rd167,%rd174,%rd359,%r255,8;
	// inline asm
	shl.b32 	%r257, %r793, 3;
	// inline asm
	bfi.b64 %rd360,%rd6,%rd164,%r257,8;
	// inline asm
	// inline asm
	bfi.b64 %rd359,%rd174,%rd167,%r257,8;
	// inline asm
	bra.uni 	BB3_19;

BB3_9:
	and.b32  	%r787, %r12, 255;
	add.s32 	%r231, %r787, -129;
	setp.lt.u32	%p11, %r231, 94;
	@%p11 bra 	BB3_12;
	bra.uni 	BB3_10;

BB3_12:
	ld.global.u32 	%r252, [%rd7+128];
	or.b32  	%r253, %r252, 8192;
	st.global.u32 	[%rd7+128], %r253;
	bra.uni 	BB3_19;

BB3_10:
	and.b32  	%r788, %r12, 255;
	add.s32 	%r232, %r788, -223;
	setp.gt.u32	%p12, %r232, 15;
	@%p12 bra 	BB3_19;

	bfe.u32 	%r789, %r12, 8, 3;
	shl.b32 	%r234, %r789, 3;
	// inline asm
	bfe.u64 %rd153,%rd360,%r234,8;
	// inline asm
	cvt.u32.u64	%r235, %rd153;
	// inline asm
	bfe.u64 %rd155,%rd359,%r234,8;
	// inline asm
	cvt.u32.u64	%r236, %rd155;
	setp.eq.s32	%p13, %r236, 0;
	selp.b32	%r237, -1, %r235, %p13;
	setp.eq.s32	%p14, %r237, -1;
	selp.b32	%r238, 144, 16, %p14;
	or.b32  	%r239, %r238, %r789;
	shl.b32 	%r240, %r239, 8;
	shl.b32 	%r241, %r237, 16;
	and.b32  	%r242, %r241, 16711680;
	and.b32  	%r243, %r12, -16776961;
	or.b32  	%r244, %r242, %r243;
	or.b32  	%r245, %r244, %r240;
	st.global.u32 	[%rd7+128], %r245;
	cvt.s64.s32	%rd158, %r237;
	add.s64 	%rd159, %rd158, %rd3;
	shl.b64 	%rd160, %rd159, 3;
	add.s64 	%rd161, %rd160, %rd2;
	ld.global.u32 	%r246, [%rd161+8];
	or.b32  	%r247, %r246, 16384;
	st.global.u32 	[%rd161+8], %r247;
	shl.b32 	%r248, %r799, 8;
	or.b32  	%r249, %r248, %r799;
	shl.b32 	%r250, %r249, 16;
	or.b32  	%r251, %r250, %r249;
	cvt.u64.u32	%rd162, %r251;
	shl.b64 	%rd163, %rd162, 32;
	or.b64  	%rd360, %rd163, %rd162;
	mov.u64 	%rd359, 72340172838076673;

BB3_19:
	cvt.u32.u64	%r276, %rd6;
	add.s32 	%r799, %r276, 1;
	mov.u32 	%r21, -1;
	mov.u32 	%r17, 0;
	mov.u64 	%rd369, 0;
	mov.u16 	%rs323, 0;
	setp.ne.s32	%p17, %r799, 256;
	@%p17 bra 	BB3_5;

	mov.u32 	%r18, %r17;
	mov.u32 	%r825, %r17;
	mov.u32 	%r803, %r17;
	mov.u16 	%rs324, %rs323;
	mov.u16 	%rs325, %rs323;
	mov.u32 	%r805, %r17;
	mov.u64 	%rd25, %rd369;
	mov.u64 	%rd21, %rd369;
	mov.u64 	%rd22, %rd369;
	mov.u32 	%r809, %r17;
	mov.u32 	%r807, %r21;
	mov.u32 	%r808, %r21;
	bra.uni 	BB3_21;

BB3_33:
	add.s32 	%r298, %r28, -85;
	setp.lt.u32	%p31, %r298, 8;
	@%p31 bra 	BB3_72;
	bra.uni 	BB3_34;

BB3_72:
	add.s32 	%r338, %r285, -1;
	and.b32  	%r339, %r338, %r285;
	setp.eq.s32	%p50, %r339, 0;
	selp.b32	%r810, 0, %r35, %p50;
	selp.u16	%rs328, 1, 0, %p50;
	mov.u16 	%rs326, 0;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;
	mov.u16 	%rs331, %rs326;
	bra.uni 	BB3_73;

BB3_34:
	add.s32 	%r299, %r28, -93;
	setp.lt.u32	%p32, %r299, 2;
	mov.u16 	%rs333, 0;
	@%p32 bra 	BB3_35;
	bra.uni 	BB3_37;

BB3_35:
	mov.u32 	%r810, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;

BB3_36:
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB3_74;

BB3_37:
	add.s32 	%r300, %r28, -95;
	setp.lt.u32	%p33, %r300, 15;
	@%p33 bra 	BB3_38;

	add.s32 	%r301, %r28, -110;
	setp.lt.u32	%p34, %r301, 5;
	@%p34 bra 	BB3_41;
	bra.uni 	BB3_42;

BB3_41:
	mov.u32 	%r810, %r39;
	mov.u16 	%rs326, %rs331;
	bra.uni 	BB3_39;

BB3_42:
	add.s32 	%r302, %r28, -115;
	setp.lt.u32	%p35, %r302, 10;
	@%p35 bra 	BB3_38;
	bra.uni 	BB3_43;

BB3_38:
	mov.u16 	%rs326, %rs333;

BB3_39:
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB3_74;

BB3_43:
	add.s32 	%r303, %r28, -125;
	setp.lt.u32	%p36, %r303, 4;
	@%p36 bra 	BB3_71;
	bra.uni 	BB3_44;

BB3_71:
	bfe.u32 	%r770, %r284, 16, 3;
	setp.eq.s32	%p290, %r811, %r770;
	selp.b32	%r810, 0, %r810, %p290;
	selp.u16	%rs328, 1, 0, %p290;
	mov.u16 	%rs330, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs331, %rs330;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB3_74;

BB3_44:
	add.s32 	%r304, %r28, -129;
	setp.lt.u32	%p37, %r304, 8;
	@%p37 bra 	BB3_69;
	bra.uni 	BB3_45;

BB3_69:
	shl.b32 	%r755, %r811, 3;
	// inline asm
	bfe.u64 %rd223,%rd21,%r755,8;
	// inline asm
	cvt.u32.u64	%r810, %rd223;
	bra.uni 	BB3_70;

BB3_45:
	add.s32 	%r305, %r28, -137;
	setp.lt.u32	%p38, %r305, 20;
	@%p38 bra 	BB3_68;
	bra.uni 	BB3_46;

BB3_68:
	shr.u32 	%r754, %r284, 8;
	and.b32  	%r811, %r754, 3;
	shl.b32 	%r336, %r811, 3;
	// inline asm
	bfe.u64 %rd221,%rd21,%r336,8;
	// inline asm
	cvt.u32.u64	%r810, %rd221;
	bra.uni 	BB3_70;

BB3_46:
	add.s32 	%r306, %r28, -157;
	setp.lt.u32	%p39, %r306, 5;
	@%p39 bra 	BB3_67;
	bra.uni 	BB3_47;

BB3_67:
	shr.u32 	%r753, %r284, 8;
	and.b32  	%r811, %r753, 3;
	shl.b32 	%r333, %r811, 3;
	// inline asm
	bfe.u64 %rd219,%rd21,%r333,8;
	// inline asm
	cvt.u32.u64	%r334, %rd219;
	max.u32 	%r335, %r37, %r334;
	max.u32 	%r810, %r18, %r335;
	bra.uni 	BB3_65;

BB3_47:
	add.s32 	%r307, %r28, -162;
	setp.lt.u32	%p40, %r307, 20;
	@%p40 bra 	BB3_66;
	bra.uni 	BB3_48;

BB3_66:
	shr.u32 	%r752, %r284, 8;
	and.b32  	%r811, %r752, 3;
	shl.b32 	%r332, %r811, 3;
	// inline asm
	bfe.u64 %rd217,%rd21,%r332,8;
	// inline asm
	cvt.u32.u64	%r810, %rd217;
	bra.uni 	BB3_70;

BB3_48:
	add.s32 	%r308, %r28, -182;
	setp.lt.u32	%p41, %r308, 5;
	@%p41 bra 	BB3_64;
	bra.uni 	BB3_49;

BB3_64:
	shr.u32 	%r751, %r284, 8;
	and.b32  	%r811, %r751, 3;
	shl.b32 	%r329, %r811, 3;
	// inline asm
	bfe.u64 %rd215,%rd21,%r329,8;
	// inline asm
	cvt.u32.u64	%r330, %rd215;
	max.u32 	%r331, %r37, %r330;
	max.u32 	%r810, %r18, %r331;

BB3_65:
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs326;
	bra.uni 	BB3_74;

BB3_49:
	add.s32 	%r309, %r28, -187;
	setp.lt.u32	%p42, %r309, 6;
	@%p42 bra 	BB3_63;
	bra.uni 	BB3_50;

BB3_63:
	shr.u32 	%r750, %r284, 8;
	and.b32  	%r811, %r750, 3;
	shl.b32 	%r328, %r811, 3;
	// inline asm
	bfe.u64 %rd213,%rd21,%r328,8;
	// inline asm
	cvt.u32.u64	%r810, %rd213;
	bra.uni 	BB3_70;

BB3_50:
	add.s32 	%r310, %r28, -193;
	setp.lt.u32	%p43, %r310, 20;
	@%p43 bra 	BB3_62;
	bra.uni 	BB3_51;

BB3_62:
	shr.u32 	%r749, %r284, 8;
	and.b32  	%r327, %r749, 3;
	add.s32 	%r811, %r327, 4;
	shl.b32 	%r326, %r811, 3;
	// inline asm
	bfe.u64 %rd211,%rd21,%r326,8;
	// inline asm
	cvt.u32.u64	%r810, %rd211;
	bra.uni 	BB3_70;

BB3_51:
	add.s32 	%r311, %r28, -213;
	setp.lt.u32	%p44, %r311, 4;
	@%p44 bra 	BB3_61;
	bra.uni 	BB3_52;

BB3_61:
	shr.u32 	%r748, %r284, 8;
	and.b32  	%r323, %r748, 3;
	add.s32 	%r811, %r323, 4;
	shl.b32 	%r322, %r811, 3;
	// inline asm
	bfe.u64 %rd209,%rd21,%r322,8;
	// inline asm
	cvt.u32.u64	%r324, %rd209;
	max.u32 	%r325, %r37, %r324;
	max.u32 	%r810, %r18, %r325;
	bra.uni 	BB3_65;

BB3_52:
	add.s32 	%r312, %r28, -217;
	setp.lt.u32	%p45, %r312, 6;
	@%p45 bra 	BB3_60;
	bra.uni 	BB3_53;

BB3_60:
	shr.u32 	%r747, %r284, 8;
	and.b32  	%r321, %r747, 3;
	add.s32 	%r811, %r321, 4;
	shl.b32 	%r320, %r811, 3;
	// inline asm
	bfe.u64 %rd207,%rd21,%r320,8;
	// inline asm
	cvt.u32.u64	%r810, %rd207;

BB3_70:
	mov.u16 	%rs332, 1;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs333;
	bra.uni 	BB3_74;

BB3_53:
	add.s32 	%r746, %r807, 1;
	add.s32 	%r313, %r28, -223;
	setp.lt.u32	%p46, %r313, 16;
	mov.u16 	%rs329, 1;
	@%p46 bra 	BB3_54;
	bra.uni 	BB3_55;

BB3_54:
	mov.u32 	%r809, %r746;
	mov.u32 	%r810, %r35;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	bra.uni 	BB3_36;

BB3_55:
	setp.eq.s32	%p47, %r28, 239;
	mov.u16 	%rs331, 1;
	@%p47 bra 	BB3_56;
	bra.uni 	BB3_57;

BB3_56:
	mov.u32 	%r810, %r37;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs332, %rs333;
	mov.u16 	%rs333, %rs331;
	bra.uni 	BB3_74;

BB3_57:
	and.b32  	%r771, %r284, 240;
	mov.u16 	%rs328, 1;
	mov.u32 	%r314, 0;
	setp.ne.s32	%p48, %r771, 240;
	@%p48 bra 	BB3_58;

	add.s32 	%r315, %r808, 16;
	shr.s32 	%r316, %r315, 31;
	shr.u32 	%r317, %r316, 28;
	add.s32 	%r318, %r315, %r317;
	shr.s32 	%r319, %r318, 4;
	max.u32 	%r810, %r319, %r810;
	mov.u16 	%rs326, 1;
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs326;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB3_74;

BB3_58:
	mov.u32 	%r810, %r314;
	mov.u16 	%rs326, %rs333;
	mov.u16 	%rs327, %rs333;
	mov.u16 	%rs329, %rs333;
	mov.u16 	%rs330, %rs333;
	mov.u16 	%rs331, %rs328;
	mov.u16 	%rs332, %rs333;
	bra.uni 	BB3_74;

BB3_21:
	mov.u16 	%rs326, 0;
	cvt.u64.u32	%rd204, %r805;
	add.s64 	%rd205, %rd204, %rd1;
	shl.b64 	%rd206, %rd205, 3;
	add.s64 	%rd23, %rd2, %rd206;
	ld.global.v2.u32 	{%r284, %r285}, [%rd23+128];
	and.b32  	%r28, %r284, 255;
	bfe.u32 	%r811, %r284, 8, 3;
	bfe.u32 	%r286, %r284, 14, 1;
	and.b32  	%r287, %r286, 1;
	setp.eq.b32	%p1, %r287, 1;
	add.s32 	%r288, %r807, 1;
	selp.b32	%r809, %r288, %r809, %p1;
	selp.b16	%rs325, 1, %rs325, %p1;
	mov.u16 	%rs331, 1;
	shl.b32 	%r277, %r811, 3;
	// inline asm
	bfe.u64 %rd198,%rd369,%r277,8;
	// inline asm
	cvt.u32.u64	%r35, %rd198;
	shr.u32 	%r289, %r284, 13;
	and.b32  	%r36, %r289, 56;
	// inline asm
	bfe.u64 %rd200,%rd369,%r36,8;
	// inline asm
	cvt.u32.u64	%r37, %rd200;
	max.u32 	%r810, %r35, %r37;
	setp.lt.u32	%p18, %r28, 25;
	@%p18 bra 	BB3_22;

	mov.u16 	%rs327, 0;
	bfe.u32 	%r745, %r284, 16, 3;
	and.b32  	%r290, %r285, 1835008;
	setp.ne.s32	%p19, %r290, 0;
	setp.eq.s32	%p20, %r811, %r745;
	and.pred  	%p21, %p20, %p19;
	selp.b32	%r291, %r17, %r18, %p21;
	max.u32 	%r39, %r810, %r291;
	add.s32 	%r292, %r28, -25;
	setp.lt.u32	%p22, %r292, 7;
	mov.u16 	%rs331, 1;
	@%p22 bra 	BB3_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r40, %r284, 240;
	setp.eq.s32	%p23, %r40, 32;
	@%p23 bra 	BB3_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r293, %r28, -48;
	setp.lt.u32	%p24, %r293, 7;
	@%p24 bra 	BB3_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r294, %r28, -55;
	setp.lt.u32	%p25, %r294, 16;
	@%p25 bra 	BB3_22;

	mov.u16 	%rs327, 0;
	add.s32 	%r295, %r28, -71;
	setp.lt.u32	%p26, %r295, 4;
	@%p26 bra 	BB3_24;

	mov.u16 	%rs326, 0;
	add.s32 	%r296, %r28, -75;
	setp.lt.u32	%p27, %r296, 4;
	@%p27 bra 	BB3_22;

	mov.u16 	%rs327, 0;
	setp.eq.s32	%p28, %r28, 79;
	@%p28 bra 	BB3_24;

	mov.u16 	%rs326, 0;
	and.b32  	%r297, %r284, 252;
	setp.eq.s32	%p29, %r297, 80;
	@%p29 bra 	BB3_22;
	bra.uni 	BB3_32;

BB3_22:
	mov.u16 	%rs327, %rs326;
	mov.u16 	%rs328, %rs326;
	mov.u16 	%rs329, %rs326;
	mov.u16 	%rs330, %rs326;

BB3_73:
	mov.u16 	%rs332, %rs326;
	mov.u16 	%rs333, %rs326;
	bra.uni 	BB3_74;

BB3_32:
	mov.u16 	%rs327, 0;
	setp.eq.s32	%p30, %r28, 84;
	@%p30 bra 	BB3_24;
	bra.uni 	BB3_33;

BB3_24:
	mov.u32 	%r810, %r39;
	mov.u16 	%rs326, %rs331;
	mov.u16 	%rs328, %rs327;
	mov.u16 	%rs329, %rs327;
	mov.u16 	%rs330, %rs327;
	mov.u16 	%rs332, %rs327;
	mov.u16 	%rs333, %rs327;

BB3_74:
	setp.eq.s16	%p51, %rs328, 0;
	@%p51 bra 	BB3_76;
	bra.uni 	BB3_75;

BB3_76:
	bfe.u32 	%r763, %r284, 14, 1;
	and.b32  	%r762, %r763, 1;
	setp.eq.b32	%p289, %r762, 1;
	and.b16  	%rs279, %rs324, 255;
	setp.eq.s16	%p53, %rs279, 0;
	selp.u16	%rs334, 1, 0, %p289;
	@%p53 bra 	BB3_78;

	ld.global.u32 	%r340, [%rd23+128];
	or.b32  	%r341, %r340, 16384;
	st.global.u32 	[%rd23+128], %r341;
	mov.u16 	%rs334, 1;

BB3_78:
	shl.b32 	%r342, %r810, 4;
	max.u32 	%r65, %r342, %r809;
	setp.eq.s16	%p54, %rs333, 0;
	@%p54 bra 	BB3_80;
	bra.uni 	BB3_79;

BB3_80:
	setp.eq.s16	%p55, %rs332, 0;
	selp.b64	%rd226, %rd25, %rd22, %p55;
	shl.b32 	%r343, %r811, 3;
	// inline asm
	bfe.u64 %rd225,%rd226,%r343,8;
	// inline asm
	cvt.u32.u64	%r344, %rd225;
	shl.b32 	%r345, %r344, 4;
	max.u32 	%r822, %r345, %r65;
	bra.uni 	BB3_81;

BB3_75:
	and.b32  	%r756, %r284, 16384;
	setp.ne.s32	%p52, %r756, 0;
	selp.b16	%rs324, 1, %rs324, %p52;
	bra.uni 	BB3_159;

BB3_79:
	max.s32 	%r822, %r825, %r65;

BB3_81:
	setp.eq.s16	%p56, %rs330, 0;
	@%p56 bra 	BB3_83;

	shr.u32 	%r765, %r284, 13;
	and.b32  	%r764, %r765, 56;
	// inline asm
	bfe.u64 %rd227,%rd25,%r764,8;
	// inline asm
	cvt.u32.u64	%r347, %rd227;
	shl.b32 	%r348, %r347, 4;
	max.u32 	%r822, %r348, %r822;

BB3_83:
	setp.eq.s16	%p57, %rs332, 0;
	@%p57 bra 	BB3_126;
	bra.uni 	BB3_84;

BB3_126:
	add.s32 	%r386, %r807, 1;
	max.s32 	%r823, %r822, %r386;
	setp.gt.s32	%p134, %r822, %r807;
	@%p134 bra 	BB3_130;

BB3_127:
	add.s32 	%r387, %r10, %r822;
	ld.shared.u8 	%rs299, [%r387];
	setp.eq.s16	%p135, %rs299, 0;
	@%p135 bra 	BB3_128;

	add.s32 	%r95, %r822, 1;
	setp.lt.s32	%p136, %r822, %r807;
	mov.u32 	%r822, %r95;
	@%p136 bra 	BB3_127;
	bra.uni 	BB3_130;

BB3_117:
	add.s32 	%r822, %r822, 1;

BB3_84:
	add.s32 	%r72, %r10, %r822;
	ld.shared.u8 	%rs281, [%r72];
	setp.ne.s16	%p58, %rs281, 0;
	@%p58 bra 	BB3_117;

	add.s32 	%r772, %r10, %r822;
	ld.shared.u8 	%rs282, [%r772+1];
	setp.ne.s16	%p59, %rs282, 0;
	add.s32 	%r349, %r822, 1;
	and.b32  	%r350, %r349, 15;
	setp.eq.s32	%p60, %r350, 0;
	or.pred  	%p61, %p59, %p60;
	@%p61 bra 	BB3_117;

	shr.s32 	%r351, %r822, 31;
	shr.u32 	%r352, %r351, 28;
	add.s32 	%r353, %r822, %r352;
	and.b32  	%r73, %r353, -16;
	mov.u16 	%rs341, 0;
	setp.ge.s32	%p62, %r73, %r822;
	@%p62 bra 	BB3_116;

	sub.s32 	%r74, %r822, %r73;
	and.b32  	%r75, %r74, 3;
	setp.eq.s32	%p63, %r75, 0;
	mov.u16 	%rs341, 0;
	@%p63 bra 	BB3_102;

	setp.eq.s32	%p64, %r75, 1;
	mov.u16 	%rs338, 0;
	@%p64 bra 	BB3_98;

	setp.eq.s32	%p65, %r75, 2;
	mov.u16 	%rs336, 0;
	@%p65 bra 	BB3_94;

	add.s32 	%r354, %r10, %r73;
	ld.shared.u8 	%rs18, [%r354];
	setp.eq.s16	%p66, %rs18, 0;
	setp.ne.s32	%p67, %r73, %r21;
	and.pred  	%p68, %p67, %p66;
	@%p68 bra 	BB3_92;

	setp.ne.s16	%p69, %rs334, 0;
	cvt.u64.u16	%rd229, %rs18;
	add.s64 	%rd230, %rd229, %rd3;
	shl.b64 	%rd231, %rd230, 3;
	add.s64 	%rd232, %rd2, %rd231;
	ld.global.u32 	%r355, [%rd232];
	and.b32  	%r356, %r355, 8192;
	setp.eq.s32	%p70, %r356, 0;
	and.b32  	%r357, %r355, 20480;
	setp.ne.s32	%p71, %r357, 0;
	or.pred  	%p72, %p69, %p71;
	and.pred  	%p73, %p70, %p72;
	mov.u16 	%rs336, 1;
	@%p73 bra 	BB3_93;

BB3_92:
	mov.u16 	%rs336, 0;

BB3_93:
	add.s32 	%r73, %r73, 1;

BB3_94:
	add.s32 	%r358, %r10, %r73;
	ld.shared.u8 	%rs21, [%r358];
	setp.eq.s16	%p74, %rs21, 0;
	setp.ne.s32	%p75, %r73, %r21;
	and.pred  	%p76, %p75, %p74;
	@%p76 bra 	BB3_96;

	setp.ne.s16	%p77, %rs334, 0;
	cvt.u64.u16	%rd233, %rs21;
	add.s64 	%rd234, %rd233, %rd3;
	shl.b64 	%rd235, %rd234, 3;
	add.s64 	%rd236, %rd2, %rd235;
	ld.global.u32 	%r359, [%rd236];
	and.b32  	%r360, %r359, 8192;
	setp.eq.s32	%p78, %r360, 0;
	and.b32  	%r361, %r359, 20480;
	setp.ne.s32	%p79, %r361, 0;
	or.pred  	%p80, %p77, %p79;
	and.pred  	%p81, %p78, %p80;
	mov.u16 	%rs338, 1;
	@%p81 bra 	BB3_97;

BB3_96:
	mov.u16 	%rs338, %rs336;

BB3_97:
	add.s32 	%r73, %r73, 1;

BB3_98:
	add.s32 	%r362, %r10, %r73;
	ld.shared.u8 	%rs24, [%r362];
	setp.eq.s16	%p82, %rs24, 0;
	setp.ne.s32	%p83, %r73, %r21;
	and.pred  	%p84, %p83, %p82;
	@%p84 bra 	BB3_100;

	setp.ne.s16	%p85, %rs334, 0;
	cvt.u64.u16	%rd237, %rs24;
	add.s64 	%rd238, %rd237, %rd3;
	shl.b64 	%rd239, %rd238, 3;
	add.s64 	%rd240, %rd2, %rd239;
	ld.global.u32 	%r363, [%rd240];
	and.b32  	%r364, %r363, 8192;
	setp.eq.s32	%p86, %r364, 0;
	and.b32  	%r365, %r363, 20480;
	setp.ne.s32	%p87, %r365, 0;
	or.pred  	%p88, %p85, %p87;
	and.pred  	%p89, %p86, %p88;
	mov.u16 	%rs341, 1;
	@%p89 bra 	BB3_101;

BB3_100:
	mov.u16 	%rs341, %rs338;

BB3_101:
	add.s32 	%r73, %r73, 1;

BB3_102:
	setp.lt.u32	%p90, %r74, 4;
	@%p90 bra 	BB3_116;

BB3_103:
	add.s32 	%r83, %r10, %r73;
	ld.shared.u8 	%rs28, [%r83];
	setp.eq.s16	%p91, %rs28, 0;
	setp.ne.s32	%p92, %r73, %r21;
	and.pred  	%p93, %p92, %p91;
	@%p93 bra 	BB3_105;

	setp.ne.s16	%p94, %rs334, 0;
	cvt.u64.u16	%rd241, %rs28;
	add.s64 	%rd242, %rd241, %rd3;
	shl.b64 	%rd243, %rd242, 3;
	add.s64 	%rd244, %rd2, %rd243;
	ld.global.u32 	%r366, [%rd244];
	and.b32  	%r367, %r366, 8192;
	setp.eq.s32	%p95, %r367, 0;
	and.b32  	%r368, %r366, 20480;
	setp.ne.s32	%p96, %r368, 0;
	or.pred  	%p97, %p94, %p96;
	and.pred  	%p98, %p95, %p97;
	mov.u16 	%rs342, 1;
	@%p98 bra 	BB3_106;

BB3_105:
	mov.u16 	%rs342, %rs341;

BB3_106:
	add.s32 	%r779, %r10, %r73;
	ld.shared.u8 	%rs30, [%r779+1];
	add.s32 	%r369, %r73, 1;
	setp.ne.s32	%p99, %r369, %r21;
	setp.eq.s16	%p100, %rs30, 0;
	and.pred  	%p101, %p99, %p100;
	@%p101 bra 	BB3_108;

	setp.ne.s16	%p102, %rs334, 0;
	cvt.u64.u16	%rd245, %rs30;
	add.s64 	%rd246, %rd245, %rd3;
	shl.b64 	%rd247, %rd246, 3;
	add.s64 	%rd248, %rd2, %rd247;
	ld.global.u32 	%r370, [%rd248];
	and.b32  	%r371, %r370, 8192;
	setp.eq.s32	%p103, %r371, 0;
	and.b32  	%r372, %r370, 20480;
	setp.ne.s32	%p104, %r372, 0;
	or.pred  	%p105, %p102, %p104;
	and.pred  	%p106, %p103, %p105;
	mov.u16 	%rs343, 1;
	@%p106 bra 	BB3_109;

BB3_108:
	mov.u16 	%rs343, %rs342;

BB3_109:
	add.s32 	%r780, %r10, %r73;
	ld.shared.u8 	%rs32, [%r780+2];
	add.s32 	%r373, %r73, 2;
	setp.ne.s32	%p107, %r373, %r21;
	setp.eq.s16	%p108, %rs32, 0;
	and.pred  	%p109, %p107, %p108;
	@%p109 bra 	BB3_111;

	setp.ne.s16	%p110, %rs334, 0;
	cvt.u64.u16	%rd249, %rs32;
	add.s64 	%rd250, %rd249, %rd3;
	shl.b64 	%rd251, %rd250, 3;
	add.s64 	%rd252, %rd2, %rd251;
	ld.global.u32 	%r374, [%rd252];
	and.b32  	%r375, %r374, 8192;
	setp.eq.s32	%p111, %r375, 0;
	and.b32  	%r376, %r374, 20480;
	setp.ne.s32	%p112, %r376, 0;
	or.pred  	%p113, %p110, %p112;
	and.pred  	%p114, %p111, %p113;
	mov.u16 	%rs344, 1;
	@%p114 bra 	BB3_112;

BB3_111:
	mov.u16 	%rs344, %rs343;

BB3_112:
	add.s32 	%r781, %r10, %r73;
	ld.shared.u8 	%rs34, [%r781+3];
	add.s32 	%r377, %r73, 3;
	setp.ne.s32	%p115, %r377, %r21;
	setp.eq.s16	%p116, %rs34, 0;
	and.pred  	%p117, %p115, %p116;
	@%p117 bra 	BB3_114;

	setp.ne.s16	%p118, %rs334, 0;
	cvt.u64.u16	%rd253, %rs34;
	add.s64 	%rd254, %rd253, %rd3;
	shl.b64 	%rd255, %rd254, 3;
	add.s64 	%rd256, %rd2, %rd255;
	ld.global.u32 	%r378, [%rd256];
	and.b32  	%r379, %r378, 8192;
	setp.eq.s32	%p119, %r379, 0;
	and.b32  	%r380, %r378, 20480;
	setp.ne.s32	%p120, %r380, 0;
	or.pred  	%p121, %p118, %p120;
	and.pred  	%p122, %p119, %p121;
	mov.u16 	%rs341, 1;
	@%p122 bra 	BB3_115;

BB3_114:
	mov.u16 	%rs341, %rs344;

BB3_115:
	add.s32 	%r73, %r73, 4;
	setp.lt.s32	%p123, %r73, %r822;
	@%p123 bra 	BB3_103;

BB3_116:
	and.b16  	%rs295, %rs341, 255;
	setp.eq.s16	%p124, %rs295, 0;
	@%p124 bra 	BB3_118;
	bra.uni 	BB3_117;

BB3_118:
	shr.s32 	%r778, %r822, 31;
	shr.u32 	%r777, %r778, 28;
	add.s32 	%r776, %r822, %r777;
	and.b32  	%r820, %r776, -16;
	setp.ge.s32	%p291, %r820, %r822;
	mov.u32 	%r381, -1;
	@%p291 bra 	BB3_119;

BB3_120:
	add.s32 	%r87, %r10, %r820;
	ld.shared.u8 	%rs37, [%r87];
	setp.eq.s16	%p126, %rs37, 0;
	setp.ne.s32	%p127, %r820, %r21;
	and.pred  	%p128, %p127, %p126;
	@%p128 bra 	BB3_122;

	cvt.u64.u16	%rd257, %rs37;
	add.s64 	%rd258, %rd257, %rd3;
	shl.b64 	%rd259, %rd258, 3;
	add.s64 	%rd260, %rd2, %rd259;
	ld.global.u8 	%rs296, [%rd260+1];
	and.b16  	%rs297, %rs296, 32;
	setp.eq.s16	%p129, %rs297, 0;
	@%p129 bra 	BB3_124;

BB3_122:
	mov.u32 	%r782, -1;
	add.s32 	%r820, %r820, 1;
	setp.lt.s32	%p130, %r820, %r822;
	@%p130 bra 	BB3_120;

	mov.u32 	%r820, %r782;
	bra.uni 	BB3_125;

BB3_119:
	mov.u32 	%r820, %r381;
	bra.uni 	BB3_125;

BB3_128:
	mov.u32 	%r823, %r822;
	bra.uni 	BB3_130;

BB3_124:
	add.s32 	%r783, %r10, %r820;
	add.s32 	%r774, %r822, 1;
	add.s32 	%r773, %r10, %r822;
	setp.eq.s32	%p131, %r820, %r21;
	st.shared.u8 	[%r773], %rs37;
	ld.shared.u8 	%rs298, [%r783+1];
	st.shared.u8 	[%r773+1], %rs298;
	selp.b32	%r383, %r822, %r21, %p131;
	add.s32 	%r384, %r820, 1;
	setp.eq.s32	%p132, %r383, %r384;
	selp.b32	%r21, %r774, %r383, %p132;

BB3_125:
	setp.lt.s32	%p133, %r820, 0;
	selp.b32	%r823, %r822, %r820, %p133;

BB3_130:
	setp.eq.s32	%p137, %r805, 0;
	selp.b16	%rs323, %rs332, %rs323, %p137;
	selp.b32	%r21, %r823, %r21, %p137;
	@%p54 bra 	BB3_132;

	shr.s32 	%r388, %r823, 31;
	shr.u32 	%r389, %r388, 28;
	add.s32 	%r390, %r823, %r389;
	and.b32  	%r391, %r390, -16;
	sub.s32 	%r392, %r823, %r391;
	add.s32 	%r393, %r823, 16;
	sub.s32 	%r825, %r393, %r392;

BB3_132:
	add.s32 	%r101, %r10, %r823;
	cvt.u16.u32	%rs39, %r805;
	st.shared.u8 	[%r101], %rs39;
	add.s32 	%r826, %r803, 1;
	@%p57 bra 	BB3_134;

	st.shared.u8 	[%r101+1], %rs39;
	add.s32 	%r826, %r803, 2;

BB3_134:
	shr.s32 	%r394, %r823, 31;
	shr.u32 	%r395, %r394, 28;
	add.s32 	%r396, %r823, %r395;
	shr.s32 	%r105, %r396, 4;
	setp.eq.s16	%p140, %rs331, 0;
	@%p140 bra 	BB3_136;

	shr.u32 	%r767, %r284, 13;
	and.b32  	%r766, %r767, 56;
	// inline asm
	bfe.u64 %rd261,%rd25,%r766,8;
	// inline asm
	cvt.u32.u64	%r399, %rd261;
	max.s32 	%r400, %r105, %r399;
	cvt.s64.s32	%rd264, %r400;
	// inline asm
	bfi.b64 %rd25,%rd264,%rd25,%r766,8;
	// inline asm

BB3_136:
	setp.ne.s16	%p141, %rs326, 0;
	setp.lt.s32	%p142, %r808, %r823;
	and.pred  	%p143, %p142, %p141;
	selp.b32	%r808, %r823, %r808, %p143;
	@%p54 bra 	BB3_138;
	bra.uni 	BB3_137;

BB3_138:
	@%p57 bra 	BB3_140;
	bra.uni 	BB3_139;

BB3_140:
	setp.ne.s16	%p146, %rs327, 0;
	@%p146 bra 	BB3_144;

	add.s32 	%r413, %r105, 1;
	cvt.u64.u32	%rd277, %r413;
	shl.b32 	%r412, %r811, 3;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r412,8;
	// inline asm
	@%p56 bra 	BB3_143;

	shr.u32 	%r769, %r284, 13;
	and.b32  	%r768, %r769, 56;
	// inline asm
	bfi.b64 %rd369,%rd277,%rd369,%r768,8;
	// inline asm

BB3_143:
	// inline asm
	bfe.u64 %rd282,%rd25,%r412,8;
	// inline asm
	cvt.u32.u64	%r417, %rd282;
	max.s32 	%r418, %r105, %r417;
	cvt.s64.s32	%rd285, %r418;
	// inline asm
	bfi.b64 %rd25,%rd285,%rd25,%r412,8;
	// inline asm

BB3_144:
	setp.eq.s16	%p148, %rs329, 0;
	@%p148 bra 	BB3_146;

	add.s32 	%r419, %r105, 1;
	shl.b32 	%r420, %r419, 8;
	or.b32  	%r421, %r420, %r419;
	shl.b32 	%r422, %r421, 16;
	or.b32  	%r423, %r422, %r421;
	cvt.u64.u32	%rd287, %r423;
	shl.b64 	%rd288, %rd287, 32;
	or.b64  	%rd369, %rd288, %rd287;

BB3_146:
	setp.eq.s16	%p149, %rs327, 0;
	@%p149 bra 	BB3_148;

	shl.b32 	%r425, %r811, 3;
	// inline asm
	bfe.u64 %rd289,%rd25,%r425,8;
	// inline asm
	cvt.u32.u64	%r426, %rd289;
	max.s32 	%r427, %r105, %r426;
	cvt.s64.s32	%rd292, %r427;
	// inline asm
	bfi.b64 %rd25,%rd292,%rd25,%r425,8;
	// inline asm
	setp.gt.u32	%p150, %r284, -536870913;
	selp.b32	%r17, %r105, %r17, %p150;
	mov.u32 	%r18, %r105;
	bra.uni 	BB3_148;

BB3_137:
	add.s32 	%r401, %r105, 1;
	shl.b32 	%r402, %r401, 8;
	or.b32  	%r403, %r402, %r401;
	shl.b32 	%r404, %r403, 16;
	or.b32  	%r405, %r404, %r403;
	cvt.u64.u32	%rd266, %r405;
	shl.b64 	%rd267, %rd266, 32;
	or.b64  	%rd21, %rd267, %rd266;
	bra.uni 	BB3_148;

BB3_139:
	add.s32 	%r409, %r105, 1;
	cvt.u64.u32	%rd269, %r409;
	shl.b32 	%r408, %r811, 3;
	// inline asm
	bfi.b64 %rd21,%rd269,%rd21,%r408,8;
	// inline asm
	// inline asm
	bfe.u64 %rd271,%rd22,%r408,8;
	// inline asm
	cvt.u32.u64	%r410, %rd271;
	max.s32 	%r411, %r105, %r410;
	cvt.s64.s32	%rd274, %r411;
	// inline asm
	bfi.b64 %rd22,%rd274,%rd22,%r408,8;
	// inline asm

BB3_148:
	add.s32 	%r428, %r10, %r809;
	ld.shared.u8 	%rs300, [%r428];
	setp.eq.s16	%p151, %rs300, 0;
	setp.ne.s32	%p152, %r809, %r21;
	and.pred  	%p153, %p152, %p151;
	@%p153 bra 	BB3_154;

	and.b16  	%rs301, %rs325, 255;
	setp.eq.s16	%p154, %rs301, 0;
	@%p154 bra 	BB3_151;

	ld.global.u32 	%r429, [%rd23+128];
	or.b32  	%r430, %r429, 16384;
	st.global.u32 	[%rd23+128], %r430;

BB3_151:
	cvt.u32.u16	%r431, %rs332;
	add.s32 	%r809, %r431, %r809;

BB3_152:
	add.s32 	%r809, %r809, 1;
	mov.u16 	%rs325, 0;
	setp.gt.s32	%p155, %r809, 4095;
	@%p155 bra 	BB3_154;

	add.s32 	%r432, %r10, %r809;
	ld.shared.u8 	%rs304, [%r432];
	setp.ne.s16	%p156, %rs304, 0;
	@%p156 bra 	BB3_152;

BB3_154:
	setp.eq.s16	%p157, %rs334, 0;
	@%p157 bra 	BB3_156;

	selp.b32	%r433, 1, 2, %p57;
	add.s32 	%r434, %r823, %r433;
	max.s32 	%r809, %r434, %r809;

BB3_156:
	cvt.u32.u16	%r435, %rs332;
	add.s32 	%r436, %r823, %r435;
	max.s32 	%r832, %r436, %r807;
	add.s32 	%r118, %r21, 1;

BB3_157:
	mov.u32 	%r119, %r832;
	add.s32 	%r437, %r10, %r119;
	ld.shared.u8 	%rs305, [%r437];
	setp.ne.s16	%p159, %rs305, 0;
	setp.eq.s32	%p160, %r119, %r21;
	or.pred  	%p161, %p160, %p159;
	setp.eq.s32	%p162, %r119, %r118;
	and.b16  	%rs306, %rs323, 255;
	setp.ne.s16	%p163, %rs306, 0;
	and.pred  	%p164, %p162, %p163;
	or.pred  	%p165, %p161, %p164;
	add.s32 	%r832, %r119, 1;
	@%p165 bra 	BB3_157;

	setp.ne.s16	%p166, %rs332, 0;
	mov.u16 	%rs324, 0;
	add.s32 	%r807, %r119, -1;
	setp.gt.s32	%p167, %r119, %r825;
	and.pred  	%p168, %p167, %p166;
	selp.b32	%r825, %r119, %r825, %p168;
	mov.u32 	%r803, %r826;

BB3_159:
	add.s32 	%r805, %r805, 1;
	setp.lt.u32	%p169, %r805, 256;
	@%p169 bra 	BB3_21;

	ld.param.u64 	%rd356, [_Z7init_vmILi16EEvPvS0_S0__param_1];
	cvta.to.global.u64 	%rd355, %rd356;
	mov.u32 	%r761, %tid.x;
	mov.u32 	%r760, %ntid.x;
	mov.u32 	%r759, %ctaid.x;
	mad.lo.s32 	%r758, %r759, %r760, %r761;
	shr.u32 	%r757, %r758, 3;
	ld.param.u64 	%rd354, [_Z7init_vmILi16EEvPvS0_S0__param_2];
	shr.s32 	%r438, %r807, 31;
	shr.u32 	%r439, %r438, 28;
	add.s32 	%r440, %r807, %r439;
	shr.s32 	%r441, %r440, 4;
	add.s32 	%r442, %r441, 1;
	cvta.to.global.u64 	%rd294, %rd354;
	atom.global.add.u32 	%r443, [%rd294], %r442;
	add.s64 	%rd295, %rd294, 4;
	atom.global.add.u32 	%r444, [%rd295], %r803;
	add.s64 	%rd297, %rd2, %rd139;
	ld.global.u32 	%r450, [%rd297+64];
	ld.global.u32 	%r451, [%rd297+80];
	ld.global.v2.u64 	{%rd298, %rd299}, [%rd297+96];
	and.b64  	%rd301, %rd298, 1;
	and.b64  	%rd302, %rd298, 2;
	shl.b64 	%rd303, %rd302, 7;
	or.b64  	%rd304, %rd303, %rd301;
	and.b64  	%rd305, %rd298, 4;
	shl.b64 	%rd306, %rd305, 14;
	or.b64  	%rd307, %rd304, %rd306;
	and.b64  	%rd308, %rd298, 8;
	shl.b64 	%rd309, %rd308, 21;
	or.b64  	%rd310, %rd307, %rd309;
	or.b64  	%rd311, %rd310, 100925952;
	shl.b64 	%rd312, %rd311, 3;
	and.b64  	%rd314, %rd299, 524287;
	shl.b64 	%rd315, %rd314, 6;
	ld.global.v2.u64 	{%rd316, %rd317}, [%rd297+112];
	and.b64  	%rd320, %rd316, 4194303;
	shr.u64 	%rd321, %rd316, 4;
	and.b64  	%rd322, %rd321, 1080863910568919040;
	or.b64  	%rd323, %rd320, %rd322;
	and.b64  	%rd324, %rd317, 4194303;
	shr.u64 	%rd325, %rd317, 4;
	and.b64  	%rd326, %rd325, 1080863910568919040;
	or.b64  	%rd327, %rd324, %rd326;
	mul.wide.u32 	%rd329, %r757, 2048;
	add.s64 	%rd47, %rd355, %rd329;
	cvt.u32.u64	%r452, %rd312;
	and.b32  	%r453, %r451, 2147483584;
	and.b32  	%r454, %r450, 2147483584;
	cvt.u32.u64	%r455, %rd315;
	st.global.v4.u32 	[%rd47+128], {%r454, %r453, %r452, %r455};
	or.b64  	%rd330, %rd327, 3458764513820540928;
	or.b64  	%rd331, %rd323, 3458764513820540928;
	st.global.v2.u64 	[%rd47+144], {%rd331, %rd330};
	add.s64 	%rd332, %rd329, %rd356;
	add.s64 	%rd48, %rd332, 1024;
	setp.lt.s32	%p170, %r807, 0;
	mov.u64 	%rd384, %rd48;
	@%p170 bra 	BB3_274;

	add.s64 	%rd383, %rd47, 1024;
	add.s32 	%r132, %r21, 1;
	mov.u32 	%r875, -1;
	mov.u32 	%r841, 0;
	mov.u32 	%r134, %r841;
	mov.u64 	%rd384, %rd48;
	mov.u32 	%r874, %r875;

BB3_162:
	add.s32 	%r460, %r10, %r841;
	ld.shared.u8 	%rs44, [%r460];
	setp.ne.s16	%p171, %rs44, 0;
	setp.eq.s32	%p172, %r841, %r21;
	or.pred  	%p173, %p172, %p171;
	setp.eq.s32	%p174, %r841, %r132;
	and.b16  	%rs308, %rs323, 255;
	setp.ne.s16	%p175, %rs308, 0;
	and.pred  	%p176, %p174, %p175;
	or.pred  	%p177, %p173, %p176;
	mov.u32 	%r877, %r841;
	@!%p177 bra 	BB3_273;
	bra.uni 	BB3_163;

BB3_163:
	add.s32 	%r845, %r841, 1;
	mov.u32 	%r849, 1;
	and.b32  	%r465, %r845, 15;
	setp.eq.s32	%p178, %r465, 0;
	mov.u32 	%r848, 0;
	setp.gt.u32	%p179, %r845, %r807;
	or.pred  	%p180, %p178, %p179;
	@%p180 bra 	BB3_168;

BB3_164:
	add.s32 	%r466, %r10, %r845;
	ld.shared.u8 	%rs45, [%r466];
	setp.ne.s16	%p181, %rs45, 0;
	setp.eq.s32	%p182, %r845, %r21;
	or.pred  	%p183, %p181, %p182;
	setp.eq.s32	%p184, %r845, %r132;
	and.pred  	%p186, %p184, %p175;
	or.pred  	%p187, %p183, %p186;
	@!%p187 bra 	BB3_168;
	bra.uni 	BB3_165;

BB3_165:
	and.b32  	%r467, %r849, 1;
	setp.eq.b32	%p189, %r467, 1;
	mov.pred 	%p292, 0;
	@!%p189 bra 	BB3_167;
	bra.uni 	BB3_166;

BB3_166:
	cvt.u64.u16	%rd333, %rs45;
	add.s64 	%rd334, %rd333, %rd3;
	shl.b64 	%rd335, %rd334, 3;
	add.s64 	%rd336, %rd2, %rd335;
	ld.global.u8 	%rs310, [%rd336+1];
	shr.u16 	%rs311, %rs310, 5;
	and.b16  	%rs312, %rs311, 1;
	setp.eq.b16	%p292, %rs312, 1;

BB3_167:
	selp.u32	%r468, 1, 0, %p292;
	add.s32 	%r848, %r468, %r848;
	add.s32 	%r849, %r849, 1;
	add.s32 	%r845, %r849, %r841;
	and.b32  	%r469, %r845, 15;
	setp.ne.s32	%p190, %r469, 0;
	setp.le.u32	%p191, %r845, %r807;
	and.pred  	%p192, %p190, %p191;
	@%p192 bra 	BB3_164;

BB3_168:
	add.s32 	%r470, %r849, 255;
	shl.b32 	%r471, %r470, 24;
	shl.b32 	%r472, %r848, 28;
	or.b32  	%r146, %r471, %r472;
	cvt.u64.u16	%rd337, %rs44;
	add.s64 	%rd338, %rd337, %rd3;
	shl.b64 	%rd339, %rd338, 3;
	add.s64 	%rd340, %rd2, %rd339;
	ld.global.v2.u32 	{%r473, %r474}, [%rd340];
	and.b32  	%r149, %r473, 255;
	shr.u32 	%r150, %r473, 8;
	bfe.u32 	%r151, %r473, 8, 3;
	shr.u32 	%r152, %r473, 16;
	bfe.u32 	%r153, %r473, 16, 3;
	and.b32  	%r475, %r841, 1;
	setp.eq.b32	%p193, %r475, 1;
	not.pred 	%p194, %p193;
	shr.u32 	%r476, %r473, 13;
	and.b32  	%r477, %r476, 1;
	setp.eq.b32	%p195, %r477, 1;
	and.pred  	%p196, %p194, %p195;
	selp.u32	%r478, 1, 0, %p196;
	add.s32 	%r877, %r478, %r841;
	shr.u32 	%r479, %r473, 14;
	and.b32  	%r480, %r479, 1;
	setp.eq.b32	%p197, %r480, 1;
	setp.lt.s32	%p198, %r875, 0;
	and.pred  	%p199, %p197, %p198;
	selp.b32	%r875, %r874, %r875, %p199;
	add.s32 	%r874, %r874, 1;
	setp.lt.u32	%p200, %r149, 25;
	mul.wide.u32 	%rd341, %r134, 4;
	add.s64 	%rd342, %rd47, %rd341;
	add.s64 	%rd54, %rd342, 256;
	@%p200 bra 	BB3_269;
	bra.uni 	BB3_169;

BB3_269:
	mov.u32 	%r872, 1048576;
	setp.ne.s32	%p286, %r151, 5;
	@%p286 bra 	BB3_272;

	shl.b32 	%r872, %r134, 6;
	setp.gt.u32	%p287, %r134, 189;
	@%p287 bra 	BB3_272;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB3_272:
	or.b32  	%r738, %r151, %r146;
	shl.b32 	%r739, %r153, 3;
	or.b32  	%r740, %r738, %r739;
	shr.u32 	%r741, %r473, 11;
	and.b32  	%r742, %r741, 98304;
	or.b32  	%r743, %r740, %r742;
	or.b32  	%r744, %r743, %r872;
	add.s64 	%rd124, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r744;
	mov.u64 	%rd383, %rd124;
	bra.uni 	BB3_273;

BB3_169:
	add.s32 	%r481, %r149, -25;
	setp.lt.u32	%p201, %r481, 7;
	@%p201 bra 	BB3_266;
	bra.uni 	BB3_170;

BB3_266:
	setp.gt.u32	%p281, %r134, 189;
	mov.u32 	%r871, %r134;
	@%p281 bra 	BB3_268;

	and.b32  	%r724, %r473, 50331648;
	setp.eq.s32	%p282, %r724, 0;
	selp.b32	%r725, 2, 1, %p282;
	setp.eq.s32	%p283, %r153, %r151;
	selp.b32	%r726, 3, %r725, %p283;
	and.b32  	%r727, %r474, -65011713;
	setp.eq.s32	%p284, %r726, 2;
	selp.b32	%r728, 29360128, 23068672, %p284;
	setp.eq.s32	%p285, %r726, 1;
	selp.b32	%r729, 37748736, %r728, %p285;
	or.b32  	%r730, %r729, %r727;
	add.s32 	%r871, %r134, 1;
	st.global.u32 	[%rd54], %r730;

BB3_268:
	shl.b32 	%r731, %r134, 6;
	or.b32  	%r732, %r731, %r146;
	or.b32  	%r733, %r732, %r151;
	shl.b32 	%r734, %r153, 3;
	or.b32  	%r735, %r733, %r734;
	or.b32  	%r736, %r735, 1064960;
	add.s64 	%rd122, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r736;
	mov.u64 	%rd383, %rd122;
	mov.u32 	%r134, %r871;
	bra.uni 	BB3_273;

BB3_170:
	and.b32  	%r157, %r473, 240;
	setp.eq.s32	%p202, %r157, 32;
	@%p202 bra 	BB3_262;
	bra.uni 	BB3_171;

BB3_262:
	shl.b32 	%r719, %r153, 3;
	or.b32  	%r720, %r151, %r719;
	or.b32  	%r869, %r720, 1572864;
	setp.ne.s32	%p279, %r153, %r151;
	@%p279 bra 	BB3_265;

	shl.b32 	%r721, %r134, 6;
	or.b32  	%r722, %r721, %r869;
	or.b32  	%r869, %r722, 131072;
	setp.gt.u32	%p280, %r134, 189;
	@%p280 bra 	BB3_265;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB3_265:
	add.s64 	%rd120, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r723, %r869, %r146;
	st.global.u32 	[%rd383], %r723;
	mov.u64 	%rd383, %rd120;
	bra.uni 	BB3_273;

BB3_171:
	add.s32 	%r482, %r149, -48;
	setp.lt.u32	%p203, %r482, 7;
	@%p203 bra 	BB3_259;
	bra.uni 	BB3_172;

BB3_259:
	setp.gt.u32	%p274, %r134, 189;
	mov.u32 	%r868, %r134;
	@%p274 bra 	BB3_261;

	and.b32  	%r706, %r473, 50331648;
	setp.eq.s32	%p275, %r706, 0;
	selp.b32	%r707, 2, 1, %p275;
	setp.eq.s32	%p276, %r153, %r151;
	selp.b32	%r708, 3, %r707, %p276;
	and.b32  	%r709, %r474, -65011713;
	setp.eq.s32	%p277, %r708, 2;
	selp.b32	%r710, 29360128, 23068672, %p277;
	setp.eq.s32	%p278, %r708, 1;
	selp.b32	%r711, 37748736, %r710, %p278;
	or.b32  	%r712, %r711, %r709;
	add.s32 	%r868, %r134, 1;
	st.global.u32 	[%rd54], %r712;

BB3_261:
	shl.b32 	%r713, %r134, 6;
	or.b32  	%r714, %r713, %r146;
	or.b32  	%r715, %r714, %r151;
	shl.b32 	%r716, %r153, 3;
	or.b32  	%r717, %r715, %r716;
	or.b32  	%r718, %r717, 1589248;
	add.s64 	%rd118, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r718;
	mov.u64 	%rd383, %rd118;
	mov.u32 	%r134, %r868;
	bra.uni 	BB3_273;

BB3_172:
	add.s32 	%r483, %r149, -55;
	setp.lt.u32	%p204, %r483, 16;
	@%p204 bra 	BB3_255;
	bra.uni 	BB3_173;

BB3_255:
	shl.b32 	%r701, %r153, 3;
	or.b32  	%r702, %r151, %r701;
	or.b32  	%r866, %r702, 2097152;
	setp.ne.s32	%p272, %r153, %r151;
	@%p272 bra 	BB3_258;

	shl.b32 	%r703, %r134, 6;
	or.b32  	%r704, %r703, %r866;
	or.b32  	%r866, %r704, 131072;
	setp.gt.u32	%p273, %r134, 189;
	@%p273 bra 	BB3_258;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB3_258:
	add.s64 	%rd116, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r705, %r866, %r146;
	st.global.u32 	[%rd383], %r705;
	mov.u64 	%rd383, %rd116;
	bra.uni 	BB3_273;

BB3_173:
	add.s32 	%r484, %r149, -71;
	setp.lt.u32	%p205, %r484, 4;
	@%p205 bra 	BB3_252;
	bra.uni 	BB3_174;

BB3_252:
	setp.gt.u32	%p267, %r134, 189;
	mov.u32 	%r865, %r134;
	@%p267 bra 	BB3_254;

	and.b32  	%r688, %r473, 50331648;
	setp.eq.s32	%p268, %r688, 0;
	selp.b32	%r689, 2, 1, %p268;
	setp.eq.s32	%p269, %r153, %r151;
	selp.b32	%r690, 3, %r689, %p269;
	and.b32  	%r691, %r474, -65011713;
	setp.eq.s32	%p270, %r690, 2;
	selp.b32	%r692, 29360128, 23068672, %p270;
	setp.eq.s32	%p271, %r690, 1;
	selp.b32	%r693, 37748736, %r692, %p271;
	or.b32  	%r694, %r693, %r691;
	add.s32 	%r865, %r134, 1;
	st.global.u32 	[%rd54], %r694;

BB3_254:
	shl.b32 	%r695, %r134, 6;
	or.b32  	%r696, %r695, %r146;
	or.b32  	%r697, %r696, %r151;
	shl.b32 	%r698, %r153, 3;
	or.b32  	%r699, %r697, %r698;
	or.b32  	%r700, %r699, 2113536;
	add.s64 	%rd114, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r700;
	mov.u64 	%rd383, %rd114;
	mov.u32 	%r134, %r865;
	bra.uni 	BB3_273;

BB3_174:
	add.s32 	%r485, %r149, -75;
	setp.lt.u32	%p206, %r485, 4;
	@%p206 bra 	BB3_251;
	bra.uni 	BB3_175;

BB3_251:
	shl.b32 	%r684, %r153, 3;
	or.b32  	%r685, %r146, %r151;
	or.b32  	%r686, %r685, %r684;
	or.b32  	%r687, %r686, 6291456;
	add.s64 	%rd112, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r687;
	mov.u64 	%rd383, %rd112;
	bra.uni 	BB3_273;

BB3_175:
	setp.eq.s32	%p207, %r149, 79;
	@%p207 bra 	BB3_248;
	bra.uni 	BB3_176;

BB3_248:
	setp.gt.u32	%p262, %r134, 189;
	mov.u32 	%r864, %r134;
	@%p262 bra 	BB3_250;

	and.b32  	%r671, %r473, 50331648;
	setp.eq.s32	%p263, %r671, 0;
	selp.b32	%r672, 2, 1, %p263;
	setp.eq.s32	%p264, %r153, %r151;
	selp.b32	%r673, 3, %r672, %p264;
	and.b32  	%r674, %r474, -65011713;
	setp.eq.s32	%p265, %r673, 2;
	selp.b32	%r675, 29360128, 23068672, %p265;
	setp.eq.s32	%p266, %r673, 1;
	selp.b32	%r676, 37748736, %r675, %p266;
	or.b32  	%r677, %r676, %r674;
	add.s32 	%r864, %r134, 1;
	st.global.u32 	[%rd54], %r677;

BB3_250:
	shl.b32 	%r678, %r134, 6;
	or.b32  	%r679, %r678, %r146;
	or.b32  	%r680, %r679, %r151;
	shl.b32 	%r681, %r153, 3;
	or.b32  	%r682, %r680, %r681;
	or.b32  	%r683, %r682, 6307840;
	add.s64 	%rd110, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r683;
	mov.u64 	%rd383, %rd110;
	mov.u32 	%r134, %r864;
	bra.uni 	BB3_273;

BB3_176:
	and.b32  	%r486, %r473, 252;
	setp.eq.s32	%p208, %r486, 80;
	@%p208 bra 	BB3_247;
	bra.uni 	BB3_177;

BB3_247:
	shl.b32 	%r667, %r153, 3;
	or.b32  	%r668, %r146, %r151;
	or.b32  	%r669, %r668, %r667;
	or.b32  	%r670, %r669, 4194304;
	add.s64 	%rd108, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r670;
	mov.u64 	%rd383, %rd108;
	bra.uni 	BB3_273;

BB3_177:
	setp.eq.s32	%p209, %r149, 84;
	@%p209 bra 	BB3_244;
	bra.uni 	BB3_178;

BB3_244:
	setp.gt.u32	%p257, %r134, 189;
	mov.u32 	%r863, %r134;
	@%p257 bra 	BB3_246;

	and.b32  	%r654, %r473, 50331648;
	setp.eq.s32	%p258, %r654, 0;
	selp.b32	%r655, 2, 1, %p258;
	setp.eq.s32	%p259, %r153, %r151;
	selp.b32	%r656, 3, %r655, %p259;
	and.b32  	%r657, %r474, -65011713;
	setp.eq.s32	%p260, %r656, 2;
	selp.b32	%r658, 29360128, 23068672, %p260;
	setp.eq.s32	%p261, %r656, 1;
	selp.b32	%r659, 37748736, %r658, %p261;
	or.b32  	%r660, %r659, %r657;
	add.s32 	%r863, %r134, 1;
	st.global.u32 	[%rd54], %r660;

BB3_246:
	shl.b32 	%r661, %r134, 6;
	or.b32  	%r662, %r661, %r146;
	or.b32  	%r663, %r662, %r151;
	shl.b32 	%r664, %r153, 3;
	or.b32  	%r665, %r663, %r664;
	or.b32  	%r666, %r665, 4210688;
	add.s64 	%rd106, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r666;
	mov.u64 	%rd383, %rd106;
	mov.u32 	%r134, %r863;
	bra.uni 	BB3_273;

BB3_178:
	add.s32 	%r487, %r149, -85;
	setp.lt.u32	%p210, %r487, 8;
	add.s32 	%r488, %r134, 1;
	mul.wide.u32 	%rd343, %r488, 4;
	add.s64 	%rd344, %rd47, %rd343;
	add.s64 	%rd55, %rd344, 256;
	@%p210 bra 	BB3_234;
	bra.uni 	BB3_179;

BB3_234:
	add.s32 	%r636, %r474, -1;
	and.b32  	%r637, %r636, %r474;
	setp.eq.s32	%p251, %r637, 0;
	mov.u64 	%rd381, 1;
	@%p251 bra 	BB3_239;

	cvt.u64.u32	%rd92, %r474;
	// inline asm
	bfind.u32 %r638,%r474;
	// inline asm
	mov.pred 	%p252, 0;
	@%p252 bra 	BB3_237;
	bra.uni 	BB3_236;

BB3_237:
	cvt.u32.u64	%r642, %rd92;
	mov.u32 	%r861, 0;
	div.u32 	%r643, %r861, %r642;
	rem.u32 	%r644, %r861, %r642;
	cvt.u64.u32	%rd381, %r643;
	cvt.u64.u32	%rd380, %r644;
	bra.uni 	BB3_238;

BB3_179:
	add.s32 	%r489, %r149, -93;
	setp.lt.u32	%p211, %r489, 2;
	@%p211 bra 	BB3_233;
	bra.uni 	BB3_180;

BB3_233:
	or.b32  	%r634, %r146, %r151;
	or.b32  	%r635, %r634, 5242880;
	add.s64 	%rd90, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r635;
	mov.u64 	%rd383, %rd90;
	bra.uni 	BB3_273;

BB3_180:
	add.s32 	%r490, %r149, -95;
	setp.lt.u32	%p212, %r490, 15;
	@%p212 bra 	BB3_229;
	bra.uni 	BB3_181;

BB3_229:
	shl.b32 	%r629, %r153, 3;
	or.b32  	%r630, %r151, %r629;
	or.b32  	%r859, %r630, 3145728;
	setp.ne.s32	%p249, %r153, %r151;
	@%p249 bra 	BB3_232;

	shl.b32 	%r631, %r134, 6;
	or.b32  	%r632, %r631, %r859;
	or.b32  	%r859, %r632, 131072;
	setp.gt.u32	%p250, %r134, 189;
	@%p250 bra 	BB3_232;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB3_232:
	add.s64 	%rd88, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r633, %r859, %r146;
	st.global.u32 	[%rd383], %r633;
	mov.u64 	%rd383, %rd88;
	bra.uni 	BB3_273;

BB3_236:
	mov.u64 	%rd346, -9223372036854775808;
	div.u64 	%rd381, %rd346, %rd92;
	rem.u64 	%rd380, %rd346, %rd92;
	mov.u32 	%r861, 0;

BB3_238:
	sub.s64 	%rd347, %rd92, %rd380;
	setp.ge.u64	%p253, %rd380, %rd347;
	selp.u64	%rd348, 1, 0, %p253;
	shl.b64 	%rd349, %rd381, 1;
	or.b64  	%rd381, %rd348, %rd349;
	selp.b64	%rd350, %rd92, 0, %p253;
	shl.b64 	%rd351, %rd380, 1;
	sub.s64 	%rd380, %rd351, %rd350;
	add.s32 	%r861, %r861, 1;
	setp.le.u32	%p254, %r861, %r638;
	@%p254 bra 	BB3_238;

BB3_239:
	setp.eq.s64	%p255, %rd381, 1;
	@%p255 bra 	BB3_243;
	bra.uni 	BB3_240;

BB3_243:
	or.b32  	%r653, %r146, 8388608;
	add.s64 	%rd104, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r653;
	mov.u64 	%rd383, %rd104;
	bra.uni 	BB3_273;

BB3_240:
	setp.gt.u32	%p256, %r134, 188;
	mov.u32 	%r862, %r134;
	@%p256 bra 	BB3_242;

	mov.b64	{%r645, %r646}, %rd381;
	st.global.u32 	[%rd54], %r645;
	st.global.u32 	[%rd55], %r646;
	add.s32 	%r862, %r134, 2;

BB3_242:
	shl.b32 	%r647, %r134, 6;
	or.b32  	%r648, %r647, %r146;
	or.b32  	%r649, %r648, %r151;
	shl.b32 	%r650, %r153, 3;
	or.b32  	%r651, %r649, %r650;
	or.b32  	%r652, %r651, 2359296;
	add.s64 	%rd102, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r652;
	mov.u64 	%rd383, %rd102;
	mov.u32 	%r134, %r862;
	bra.uni 	BB3_273;

BB3_181:
	add.s32 	%r491, %r149, -110;
	setp.lt.u32	%p213, %r491, 5;
	@%p213 bra 	BB3_226;
	bra.uni 	BB3_182;

BB3_226:
	setp.gt.u32	%p244, %r134, 189;
	mov.u32 	%r858, %r134;
	@%p244 bra 	BB3_228;

	and.b32  	%r616, %r473, 50331648;
	setp.eq.s32	%p245, %r616, 0;
	selp.b32	%r617, 2, 1, %p245;
	setp.eq.s32	%p246, %r153, %r151;
	selp.b32	%r618, 3, %r617, %p246;
	and.b32  	%r619, %r474, -65011713;
	setp.eq.s32	%p247, %r618, 2;
	selp.b32	%r620, 29360128, 23068672, %p247;
	setp.eq.s32	%p248, %r618, 1;
	selp.b32	%r621, 37748736, %r620, %p248;
	or.b32  	%r622, %r621, %r619;
	add.s32 	%r858, %r134, 1;
	st.global.u32 	[%rd54], %r622;

BB3_228:
	shl.b32 	%r623, %r134, 6;
	or.b32  	%r624, %r623, %r146;
	or.b32  	%r625, %r624, %r151;
	shl.b32 	%r626, %r153, 3;
	or.b32  	%r627, %r625, %r626;
	or.b32  	%r628, %r627, 3162112;
	add.s64 	%rd86, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r628;
	mov.u64 	%rd383, %rd86;
	mov.u32 	%r134, %r858;
	bra.uni 	BB3_273;

BB3_182:
	add.s32 	%r492, %r149, -115;
	setp.lt.u32	%p214, %r492, 10;
	@%p214 bra 	BB3_222;
	bra.uni 	BB3_183;

BB3_222:
	shl.b32 	%r611, %r153, 3;
	or.b32  	%r612, %r151, %r611;
	or.b32  	%r856, %r612, 7340032;
	setp.ne.s32	%p242, %r153, %r151;
	@%p242 bra 	BB3_225;

	shl.b32 	%r613, %r134, 6;
	or.b32  	%r614, %r613, %r856;
	or.b32  	%r856, %r614, 131072;
	setp.gt.u32	%p243, %r134, 189;
	@%p243 bra 	BB3_225;

	add.s32 	%r134, %r134, 1;
	st.global.u32 	[%rd54], %r474;

BB3_225:
	add.s64 	%rd84, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	or.b32  	%r615, %r856, %r146;
	st.global.u32 	[%rd383], %r615;
	mov.u64 	%rd383, %rd84;
	bra.uni 	BB3_273;

BB3_183:
	add.s32 	%r493, %r149, -125;
	setp.lt.u32	%p215, %r493, 4;
	@%p215 bra 	BB3_221;
	bra.uni 	BB3_184;

BB3_221:
	shl.b32 	%r606, %r153, 3;
	or.b32  	%r607, %r151, %r606;
	or.b32  	%r608, %r607, 8388608;
	setp.eq.s32	%p241, %r153, %r151;
	selp.b32	%r609, 8388608, %r608, %p241;
	or.b32  	%r610, %r609, %r146;
	add.s64 	%rd82, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r610;
	mov.u64 	%rd383, %rd82;
	bra.uni 	BB3_273;

BB3_184:
	add.s32 	%r494, %r149, -129;
	setp.lt.u32	%p216, %r494, 8;
	@%p216 bra 	BB3_220;
	bra.uni 	BB3_185;

BB3_220:
	or.b32  	%r604, %r146, %r151;
	or.b32  	%r605, %r604, 11534336;
	add.s64 	%rd80, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r605;
	mov.u64 	%rd383, %rd80;
	bra.uni 	BB3_273;

BB3_185:
	add.s32 	%r495, %r149, -137;
	setp.lt.u32	%p217, %r495, 20;
	@%p217 bra 	BB3_219;
	bra.uni 	BB3_186;

BB3_219:
	and.b32  	%r598, %r150, 3;
	and.b32  	%r599, %r152, 3;
	shl.b32 	%r600, %r599, 4;
	or.b32  	%r601, %r146, %r598;
	or.b32  	%r602, %r601, %r600;
	or.b32  	%r603, %r602, 12582912;
	add.s64 	%rd78, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r603;
	mov.u64 	%rd383, %rd78;
	bra.uni 	BB3_273;

BB3_186:
	add.s32 	%r496, %r149, -157;
	setp.lt.u32	%p218, %r496, 5;
	@%p218 bra 	BB3_216;
	bra.uni 	BB3_187;

BB3_216:
	setp.gt.u32	%p239, %r134, 189;
	mov.u32 	%r855, %r134;
	@%p239 bra 	BB3_218;

	and.b32  	%r587, %r473, 50331648;
	setp.eq.s32	%p240, %r587, 0;
	selp.b32	%r588, 29360128, 37748736, %p240;
	and.b32  	%r589, %r474, -65011713;
	or.b32  	%r590, %r588, %r589;
	add.s32 	%r855, %r134, 1;
	st.global.u32 	[%rd54], %r590;

BB3_218:
	shl.b32 	%r591, %r134, 6;
	or.b32  	%r592, %r591, %r146;
	and.b32  	%r593, %r150, 3;
	or.b32  	%r594, %r592, %r593;
	shl.b32 	%r595, %r153, 3;
	or.b32  	%r596, %r594, %r595;
	or.b32  	%r597, %r596, 12599296;
	add.s64 	%rd76, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r597;
	mov.u64 	%rd383, %rd76;
	mov.u32 	%r134, %r855;
	bra.uni 	BB3_273;

BB3_187:
	add.s32 	%r497, %r149, -162;
	setp.lt.u32	%p219, %r497, 20;
	@%p219 bra 	BB3_215;
	bra.uni 	BB3_188;

BB3_215:
	and.b32  	%r581, %r150, 3;
	and.b32  	%r582, %r152, 3;
	shl.b32 	%r583, %r582, 4;
	or.b32  	%r584, %r146, %r581;
	or.b32  	%r585, %r584, %r583;
	or.b32  	%r586, %r585, 13107200;
	add.s64 	%rd74, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r586;
	mov.u64 	%rd383, %rd74;
	bra.uni 	BB3_273;

BB3_188:
	add.s32 	%r498, %r149, -182;
	setp.lt.u32	%p220, %r498, 5;
	@%p220 bra 	BB3_212;
	bra.uni 	BB3_189;

BB3_212:
	setp.gt.u32	%p237, %r134, 189;
	mov.u32 	%r854, %r134;
	@%p237 bra 	BB3_214;

	and.b32  	%r570, %r473, 50331648;
	setp.eq.s32	%p238, %r570, 0;
	selp.b32	%r571, 29360128, 37748736, %p238;
	and.b32  	%r572, %r474, -65011713;
	or.b32  	%r573, %r571, %r572;
	add.s32 	%r854, %r134, 1;
	st.global.u32 	[%rd54], %r573;

BB3_214:
	shl.b32 	%r574, %r134, 6;
	or.b32  	%r575, %r574, %r146;
	and.b32  	%r576, %r150, 3;
	or.b32  	%r577, %r575, %r576;
	shl.b32 	%r578, %r153, 3;
	or.b32  	%r579, %r577, %r578;
	or.b32  	%r580, %r579, 13123584;
	add.s64 	%rd72, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r580;
	mov.u64 	%rd383, %rd72;
	mov.u32 	%r134, %r854;
	bra.uni 	BB3_273;

BB3_189:
	add.s32 	%r499, %r149, -187;
	setp.lt.u32	%p221, %r499, 6;
	@%p221 bra 	BB3_209;
	bra.uni 	BB3_190;

BB3_209:
	setp.gt.u32	%p236, %r134, 188;
	mov.u32 	%r853, %r134;
	@%p236 bra 	BB3_211;

	mov.u32 	%r563, 0;
	st.global.u32 	[%rd54], %r563;
	mov.u32 	%r564, -2131755008;
	st.global.u32 	[%rd55], %r564;
	add.s32 	%r853, %r134, 2;

BB3_211:
	shl.b32 	%r565, %r134, 6;
	or.b32  	%r566, %r565, %r146;
	and.b32  	%r567, %r150, 3;
	or.b32  	%r568, %r566, %r567;
	or.b32  	%r569, %r568, 3407872;
	add.s64 	%rd70, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r569;
	mov.u64 	%rd383, %rd70;
	mov.u32 	%r134, %r853;
	bra.uni 	BB3_273;

BB3_190:
	add.s32 	%r500, %r149, -193;
	setp.lt.u32	%p222, %r500, 20;
	@%p222 bra 	BB3_208;
	bra.uni 	BB3_191;

BB3_208:
	and.b32  	%r556, %r150, 3;
	add.s32 	%r557, %r556, 4;
	and.b32  	%r558, %r152, 3;
	shl.b32 	%r559, %r558, 4;
	or.b32  	%r560, %r146, %r559;
	or.b32  	%r561, %r560, %r557;
	or.b32  	%r562, %r561, 12615680;
	add.s64 	%rd68, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r562;
	mov.u64 	%rd383, %rd68;
	bra.uni 	BB3_273;

BB3_191:
	add.s32 	%r501, %r149, -213;
	setp.lt.u32	%p223, %r501, 4;
	@%p223 bra 	BB3_205;
	bra.uni 	BB3_192;

BB3_205:
	setp.gt.u32	%p234, %r134, 189;
	mov.u32 	%r852, %r134;
	@%p234 bra 	BB3_207;

	and.b32  	%r544, %r473, 50331648;
	setp.eq.s32	%p235, %r544, 0;
	selp.b32	%r545, 29360128, 37748736, %p235;
	and.b32  	%r546, %r474, -65011713;
	or.b32  	%r547, %r545, %r546;
	add.s32 	%r852, %r134, 1;
	st.global.u32 	[%rd54], %r547;

BB3_207:
	shl.b32 	%r548, %r134, 6;
	or.b32  	%r549, %r548, %r146;
	shl.b32 	%r550, %r153, 3;
	or.b32  	%r551, %r549, %r550;
	and.b32  	%r552, %r150, 3;
	add.s32 	%r553, %r552, 4;
	or.b32  	%r554, %r551, %r553;
	or.b32  	%r555, %r554, 15745024;
	add.s64 	%rd66, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r555;
	mov.u64 	%rd383, %rd66;
	mov.u32 	%r134, %r852;
	bra.uni 	BB3_273;

BB3_192:
	add.s32 	%r502, %r149, -217;
	setp.lt.u32	%p224, %r502, 6;
	@%p224 bra 	BB3_204;
	bra.uni 	BB3_193;

BB3_204:
	and.b32  	%r540, %r150, 3;
	add.s32 	%r541, %r540, 4;
	or.b32  	%r542, %r146, %r541;
	or.b32  	%r543, %r542, 14680064;
	add.s64 	%rd64, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r543;
	mov.u64 	%rd383, %rd64;
	bra.uni 	BB3_273;

BB3_193:
	add.s32 	%r503, %r149, -223;
	setp.lt.u32	%p225, %r503, 16;
	@%p225 bra 	BB3_201;
	bra.uni 	BB3_194;

BB3_201:
	setp.gt.u32	%p233, %r134, 188;
	mov.u32 	%r851, %r134;
	@%p233 bra 	BB3_203;

	shr.u32 	%r524, %r473, 28;
	add.s32 	%r525, %r524, 8;
	add.s32 	%r526, %r524, 7;
	mov.u32 	%r527, 1;
	shl.b32 	%r528, %r527, %r526;
	not.b32 	%r529, %r528;
	shl.b32 	%r530, %r527, %r525;
	or.b32  	%r531, %r530, %r474;
	and.b32  	%r532, %r531, %r529;
	st.global.u32 	[%rd54], %r532;
	shl.b32 	%r533, %r875, 5;
	or.b32  	%r534, %r533, %r525;
	st.global.u32 	[%rd55], %r534;
	add.s32 	%r851, %r134, 2;

BB3_203:
	shl.b32 	%r536, %r134, 6;
	or.b32  	%r537, %r536, %r146;
	or.b32  	%r538, %r537, %r151;
	or.b32  	%r539, %r538, 9437184;
	add.s64 	%rd62, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r539;
	mov.u32 	%r875, -1;
	mov.u64 	%rd383, %rd62;
	mov.u32 	%r134, %r851;
	bra.uni 	BB3_273;

BB3_194:
	setp.eq.s32	%p226, %r149, 239;
	@%p226 bra 	BB3_200;
	bra.uni 	BB3_195;

BB3_200:
	shl.b32 	%r518, %r153, 3;
	and.b32  	%r519, %r474, 63;
	shl.b32 	%r520, %r519, 6;
	or.b32  	%r521, %r146, %r520;
	or.b32  	%r522, %r521, %r518;
	or.b32  	%r523, %r522, 13631488;
	add.s64 	%rd60, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r523;
	mov.u64 	%rd383, %rd60;
	bra.uni 	BB3_273;

BB3_195:
	setp.eq.s32	%p227, %r157, 240;
	@%p227 bra 	BB3_197;
	bra.uni 	BB3_196;

BB3_197:
	setp.gt.u32	%p228, %r134, 189;
	mov.u32 	%r850, %r134;
	@%p228 bra 	BB3_199;

	and.b32  	%r505, %r473, 50331648;
	setp.eq.s32	%p229, %r505, 0;
	selp.b32	%r506, 2, 1, %p229;
	setp.gt.u32	%p230, %r473, -536870913;
	selp.b32	%r507, 3, %r506, %p230;
	and.b32  	%r508, %r474, -65011713;
	setp.eq.s32	%p231, %r507, 2;
	selp.b32	%r509, 29360128, 23068672, %p231;
	setp.eq.s32	%p232, %r507, 1;
	selp.b32	%r510, 37748736, %r509, %p232;
	or.b32  	%r511, %r510, %r508;
	st.global.u32 	[%rd54], %r511;
	mov.u32 	%r850, %r488;

BB3_199:
	shl.b32 	%r512, %r134, 6;
	or.b32  	%r513, %r512, %r146;
	or.b32  	%r514, %r513, %r151;
	shl.b32 	%r515, %r153, 3;
	or.b32  	%r516, %r514, %r515;
	or.b32  	%r517, %r516, 10502144;
	add.s64 	%rd58, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r517;
	mov.u64 	%rd383, %rd58;
	mov.u32 	%r134, %r850;
	bra.uni 	BB3_273;

BB3_196:
	or.b32  	%r504, %r146, 8388608;
	add.s64 	%rd56, %rd383, 4;
	add.s64 	%rd384, %rd384, 4;
	st.global.u32 	[%rd383], %r504;
	mov.u64 	%rd383, %rd56;

BB3_273:
	add.s32 	%r841, %r877, 1;
	setp.lt.s32	%p288, %r877, %r807;
	@%p288 bra 	BB3_162;

BB3_274:
	sub.s64 	%rd352, %rd384, %rd48;
	shr.u64 	%rd353, %rd352, 2;
	st.global.u32 	[%rd47+160], %rd353;

BB3_275:
	ret;
}

	// .globl	_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 16, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<59>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<219>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<154>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd52, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd53, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd55, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd54, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r59, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_6];
	cvta.to.global.u64 	%rd1, %rd55;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r206, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r206, 4095;
	@%p1 bra 	BB4_3;

	mov.u32 	%r61, _ZZ10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r205, %r61, %r206;
	cvt.u64.u32	%rd2, %r4;
	cvta.to.global.u64 	%rd56, %rd52;
	cvt.u64.u32	%rd57, %r206;
	mul.wide.u32 	%rd58, %r5, 4096;
	add.s64 	%rd59, %rd58, %rd57;
	add.s64 	%rd147, %rd56, %rd59;

BB4_2:
	ld.global.u64 	%rd60, [%rd147];
	st.shared.u64 	[%r205], %rd60;
	add.s64 	%rd147, %rd147, %rd2;
	add.s32 	%r205, %r205, %r4;
	add.s32 	%r206, %r206, %r4;
	setp.lt.u32	%p2, %r206, 4096;
	@%p2 bra 	BB4_2;

BB4_3:
	bar.warp.sync 	-1;
	shr.u32 	%r62, %r1, 3;
	mul.wide.u32 	%rd6, %r62, 256;
	cvt.u32.u64	%r63, %rd6;
	shl.b32 	%r64, %r63, 3;
	mov.u32 	%r65, _ZZ10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r65, %r64;
	mad.lo.s32 	%r12, %r5, %r3, %r1;
	shr.u32 	%r66, %r12, 3;
	and.b32  	%r13, %r12, 7;
	ld.shared.v2.u32 	{%r217, %r216}, [%r11+128];
	ld.shared.u32 	%r17, [%r11+160];
	cvta.to.global.u64 	%rd61, %rd53;
	mul.wide.u32 	%rd62, %r66, 4;
	add.s64 	%rd8, %rd61, %rd62;
	ld.global.u32 	%r40, [%rd8];
	mul.lo.s32 	%r69, %r66, 2097216;
	cvt.u64.u32	%rd9, %r69;
	cvt.u64.u32	%rd10, %r13;
	or.b32  	%r70, %r13, %r63;
	shl.b32 	%r71, %r70, 3;
	add.s32 	%r19, %r65, %r71;
	setp.eq.s32	%p3, %r59, 0;
	@%p3 bra 	BB4_53;

	add.s32 	%r183, %r11, 128;
	ld.shared.u32 	%r73, [%r183+8];
	and.b32  	%r74, %r12, 1;
	shl.b32 	%r75, %r12, 3;
	and.b32  	%r76, %r75, 8;
	mov.u32 	%r77, 3;
	add.s32 	%r20, %r76, 64;
	add.s32 	%r184, %r11, 128;
	ld.shared.v2.u64 	{%rd63, %rd64}, [%r184+16];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p4, %rs3, 0;
	setp.lt.u32	%p5, %r13, 4;
	add.s32 	%r78, %r13, -4;
	shl.b32 	%r79, %r78, 1;
	cvt.s64.s32	%rd67, %r79;
	add.s64 	%rd68, %rd6, %rd67;
	add.s64 	%rd69, %rd68, 16;
	shl.b32 	%r80, %r13, 1;
	cvt.u64.u32	%rd70, %r80;
	add.s64 	%rd71, %rd6, %rd70;
	add.s64 	%rd72, %rd71, 8;
	selp.b64	%rd73, %rd72, %rd69, %p5;
	selp.b64	%rd11, -1, 72057594037927935, %p5;
	and.b32  	%r81, %r1, -8;
	and.b32  	%r82, %r12, 6;
	add.s32 	%r83, %r82, %r81;
	shl.b32 	%r21, %r77, %r83;
	ld.shared.u32 	%rd74, [%r11+140];
	shl.b32 	%r84, %r13, 3;
	cvt.u64.u32	%rd75, %r84;
	add.s64 	%rd12, %rd75, %rd9;
	add.s64 	%rd76, %rd73, 1;
	cvt.u32.u64	%r85, %rd76;
	shl.b32 	%r86, %r85, 3;
	add.s32 	%r22, %r65, %r86;
	shl.b32 	%r88, %r74, 5;
	cvt.u64.u32	%rd13, %r88;
	add.s64 	%rd14, %rd74, %rd75;
	ld.shared.u64 	%rd153, [%r19];
	cvta.to.global.u64 	%rd16, %rd54;
	shl.b32 	%r25, %r77, %r81;
	setp.eq.s32	%p6, %r74, 0;
	selp.b64	%rd17, %rd63, %rd64, %p6;
	selp.b64	%rd18, 0, %rd64, %p5;
	selp.b64	%rd19, 0, %rd63, %p5;
	cvt.u32.u64	%r89, %rd73;
	shl.b32 	%r90, %r89, 3;
	add.s32 	%r26, %r65, %r90;
	selp.b32	%r208, 0, %r217, %p4;
	selp.b32	%r209, 0, %r216, %p4;
	and.b32  	%r94, %r73, 255;
	add.s32 	%r32, %r11, %r94;
	mov.u32 	%r207, 0;

BB4_5:
	.pragma "nounroll";
	mov.u32 	%r36, %r216;
	mov.u32 	%r216, %r217;
	bfe.u32 	%r203, %r73, 8, 8;
	add.s32 	%r202, %r11, %r203;
	ld.shared.u64 	%rd77, [%r202];
	ld.shared.u64 	%rd78, [%r32];
	xor.b64  	%rd79, %rd77, %rd78;
	mov.b64	{%r95, %r96}, %rd79;
	xor.b32  	%r97, %r95, %r209;
	xor.b32  	%r98, %r96, %r208;
	and.b32  	%r99, %r97, 2097088;
	and.b32  	%r100, %r98, 2097088;
	cvt.u64.u32	%rd80, %r99;
	add.s64 	%rd81, %rd12, %rd80;
	add.s64 	%rd21, %rd1, %rd81;
	cvt.u64.u32	%rd82, %r100;
	add.s64 	%rd83, %rd12, %rd82;
	add.s64 	%rd22, %rd1, %rd83;
	ld.global.u64 	%rd84, [%rd21];
	xor.b64  	%rd85, %rd153, %rd84;
	st.shared.u64 	[%r19], %rd85;
	ld.global.v2.u32 	{%r101, %r102}, [%rd22];
	cvt.rn.f64.s32	%fd21, %r101;
	mov.b64 	 %rd86, %fd21;
	and.b64  	%rd87, %rd86, %rd11;
	or.b64  	%rd88, %rd87, %rd19;
	st.shared.u64 	[%r26], %rd88;
	cvt.rn.f64.s32	%fd22, %r102;
	mov.b64 	 %rd89, %fd22;
	and.b64  	%rd90, %rd89, %rd11;
	or.b64  	%rd91, %rd90, %rd18;
	st.shared.u64 	[%r22], %rd91;
	setp.gt.u32	%p7, %r13, 1;
	@%p7 bra 	BB4_52;

	setp.eq.s32	%p8, %r17, 0;
	mov.u32 	%r213, 0;
	add.s32 	%r185, %r11, 1016;
	st.shared.u32 	[%r185+4], %r40;
	@%p8 bra 	BB4_52;
	bra.uni 	BB4_7;

BB4_17:
	setp.eq.s32	%p21, %r44, 14;
	@%p21 bra 	BB4_31;
	bra.uni 	BB4_18;

BB4_31:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd33, %rd23;
	// inline asm
	rsqrt.approx.ftz.f64 %fd32, %fd33;
	// inline asm
	mul.f64 	%fd34, %fd33, %fd32;
	mul.f64 	%fd35, %fd32, 0dBFE0000000000000;
	mov.f64 	%fd36, 0d3FE0000000000000;
	fma.rn.f64 	%fd37, %fd35, %fd34, %fd36;
	fma.rn.f64 	%fd7, %fd34, %fd37, %fd34;
	mul.f64 	%fd38, %fd32, 0d3FE0000000000000;
	fma.rn.f64 	%fd8, %fd38, %fd37, %fd38;
	neg.f64 	%fd39, %fd7;
	fma.rn.f64 	%fd9, %fd39, %fd7, %fd33;
	fma.rn.f64 	%fd47, %fd9, %fd8, %fd7;
	setp.eq.s32	%p34, %r40, 0;
	@%p34 bra 	BB4_33;

	fma.rm.f64 	%fd40, %fd9, %fd8, %fd7;
	and.b32  	%r149, %r40, 1;
	setp.eq.b32	%p35, %r149, 1;
	not.pred 	%p36, %p35;
	selp.f64	%fd41, %fd47, %fd40, %p36;
	setp.eq.s32	%p37, %r40, 2;
	fma.rp.f64 	%fd42, %fd9, %fd8, %fd7;
	selp.f64	%fd47, %fd42, %fd41, %p37;

BB4_33:
	setp.eq.s64	%p38, %rd23, 9218868437227405312;
	selp.f64	%fd43, %fd33, %fd47, %p38;
	mov.b64 	 %rd152, %fd43;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_18:
	setp.eq.s32	%p22, %r44, 6;
	@%p22 bra 	BB4_30;
	bra.uni 	BB4_19;

BB4_30:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd152, %rd23, %rd152;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_19:
	setp.eq.s32	%p23, %r44, 4;
	@%p23 bra 	BB4_29;
	bra.uni 	BB4_20;

BB4_29:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd152, %rd23, %rd152;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_20:
	setp.eq.s32	%p24, %r44, 8;
	@%p24 bra 	BB4_28;
	bra.uni 	BB4_21;

BB4_28:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r47], %rd23;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_21:
	setp.eq.s32	%p25, %r44, 15;
	@%p25 bra 	BB4_25;
	bra.uni 	BB4_22;

BB4_25:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	cvt.u32.u64	%r145, %rd13;
	shr.u64 	%rd96, %rd152, %r145;
	cvt.u32.u64	%r146, %rd96;
	cvt.rn.f64.s32	%fd25, %r146;
	mov.b64 	 %rd97, %fd25;
	and.b64  	%rd98, %rd97, 72057594037927935;
	or.b64  	%rd99, %rd98, %rd17;
	mov.b64 	 %fd24, %rd99;
	// inline asm
	rcp.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	neg.f64 	%fd26, %fd24;
	mov.f64 	%fd27, 0d3FF0000000000000;
	fma.rn.f64 	%fd28, %fd26, %fd23, %fd27;
	fma.rn.f64 	%fd2, %fd23, %fd28, %fd23;
	mov.b64 	 %fd3, %rd23;
	mul.f64 	%fd4, %fd3, %fd2;
	fma.rn.f64 	%fd5, %fd26, %fd4, %fd3;
	fma.rn.f64 	%fd29, %fd2, %fd5, %fd4;
	mov.b64 	 %rd150, %fd29;
	setp.eq.s32	%p27, %r40, 0;
	@%p27 bra 	BB4_27;

	and.b32  	%r147, %r40, 1;
	setp.eq.b32	%p28, %r147, 1;
	not.pred 	%p29, %p28;
	fma.rm.f64 	%fd30, %fd2, %fd5, %fd4;
	mov.b64 	 %rd100, %fd30;
	selp.b64	%rd101, %rd150, %rd100, %p29;
	fma.rp.f64 	%fd31, %fd2, %fd5, %fd4;
	mov.b64 	 %rd102, %fd31;
	setp.eq.s32	%p30, %r40, 2;
	selp.b64	%rd150, %rd102, %rd101, %p30;

BB4_27:
	and.b64  	%rd103, %rd150, 9218868437227405312;
	mov.u64 	%rd104, 9218868437227405312;
	setp.eq.s64	%p31, %rd103, 9218868437227405312;
	and.b32  	%r148, %r40, 1;
	cvt.u64.u32	%rd105, %r148;
	sub.s64 	%rd106, %rd104, %rd105;
	selp.b64	%rd107, %rd106, %rd150, %p31;
	setp.eq.s64	%p32, %rd23, 9218868437227405312;
	selp.b64	%rd108, 9218868437227405312, %rd107, %p32;
	setp.eq.f64	%p33, %fd3, %fd24;
	selp.b64	%rd152, 4607182418800017408, %rd108, %p33;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_22:
	setp.eq.s32	%p26, %r44, 5;
	@%p26 bra 	BB4_24;
	bra.uni 	BB4_23;

BB4_24:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd152, %rd23;
	bra.uni 	BB4_50;

BB4_23:
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd94, %rd152, %r48;
	mov.u32 	%r139, 64;
	sub.s32 	%r140, %r139, %r48;
	shl.b64 	%rd95, %rd152, %r140;
	cvt.u32.u64	%r141, %rd94;
	cvt.u32.u64	%r142, %rd95;
	or.b32  	%r143, %r141, %r142;
	and.b32  	%r144, %r143, 3;
	add.s32 	%r186, %r11, 1016;
	st.shared.u32 	[%r186+4], %r144;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB4_51;

BB4_7:
	shl.b32 	%r106, %r213, 2;
	add.s32 	%r107, %r11, %r106;
	st.shared.u32 	[%r11+1016], %r213;
	ld.shared.u32 	%r108, [%r107+1024];
	bfe.u32 	%r109, %r108, 24, 1;
	bfe.u32 	%r41, %r108, 28, 1;
	sub.s32 	%r42, %r109, %r41;
	cvt.u32.u64	%r110, %rd10;
	setp.lt.u32	%p9, %r109, %r110;
	@%p9 bra 	BB4_51;

	shr.u32 	%r204, %r13, 1;
	sub.s32 	%r111, %r13, %r41;
	setp.lt.s32	%p10, %r111, %r41;
	selp.b32	%r112, %r204, %r111, %p10;
	add.s32 	%r113, %r112, %r213;
	shl.b32 	%r114, %r113, 2;
	add.s32 	%r115, %r11, %r114;
	ld.shared.u32 	%r43, [%r115+1024];
	bfe.u32 	%r48, %r43, 6, 8;
	shl.b32 	%r116, %r48, 2;
	add.s32 	%r117, %r11, %r116;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r44, %r43, 20, 4;
	and.b32  	%r45, %r43, 16384;
	selp.b32	%r118, 4, 3, %p10;
	selp.b32	%r119, %r20, 0, %p10;
	and.b32  	%r120, %r43, 7;
	shl.b32 	%r121, %r120, %r118;
	add.s32 	%r122, %r121, %r119;
	and.b32  	%r123, %r43, 56;
	setp.eq.s32	%p11, %r45, 0;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r124, %r20, 128;
	selp.b32	%r125, %r124, 0, %p12;
	add.s32 	%r126, %r125, %r123;
	add.s32 	%r46, %r11, %r122;
	add.s32 	%r47, %r11, %r126;
	ld.shared.u64 	%rd23, [%r46];
	ld.shared.u64 	%rd149, [%r47];
	ld.shared.u32 	%r49, [%r117+256];
	cvt.s64.s32	%rd25, %r49;
	ld.shared.u32 	%r50, [%r117+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p11 bra 	BB4_12;

	cvt.u32.u64	%r128, %rd25;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r127, %r128, 21, 5;
	// inline asm
	mov.u32 	%r129, -1;
	shr.u32 	%r130, %r129, %r127;
	add.s32 	%r131, %r130, -7;
	setp.eq.s32	%p13, %r127, 11;
	cvt.u32.u64	%r132, %rd149;
	selp.b32	%r133, 0, %r132, %p13;
	cvt.u32.u64	%r134, %rd23;
	setp.ne.s32	%p14, %r44, 10;
	selp.b32	%r135, %r133, %r134, %p14;
	add.s32 	%r136, %r135, %r49;
	and.b32  	%r137, %r136, %r131;
	cvt.u64.u32	%rd92, %r137;
	add.s64 	%rd93, %rd92, %rd9;
	add.s64 	%rd26, %rd1, %rd93;
	@%p14 bra 	BB4_11;
	bra.uni 	BB4_10;

BB4_11:
	ld.global.u64 	%rd149, [%rd26];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB4_12:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r138, %r43, 131072;
	setp.eq.s32	%p15, %r138, 0;
	selp.b64	%rd152, %rd149, %rd25, %p15;
	setp.lt.u32	%p16, %r44, 4;
	@%p16 bra 	BB4_49;
	bra.uni 	BB4_13;

BB4_49:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r170, %r43, 524288;
	setp.eq.s32	%p48, %r170, 0;
	neg.s64 	%rd114, %rd152;
	selp.b64	%rd115, %rd152, %rd114, %p48;
	setp.eq.s32	%p49, %r44, 0;
	selp.b64	%rd116, %rd25, 0, %p49;
	add.s64 	%rd117, %rd116, %rd23;
	bfe.u32 	%r171, %r43, 15, 2;
	shl.b64 	%rd118, %rd115, %r171;
	setp.lt.u32	%p50, %r44, 2;
	selp.b64	%rd119, %rd118, 0, %p50;
	add.s64 	%rd120, %rd117, %rd119;
	mov.b64	%rd121, {%r49, %r50};
	and.b32  	%r172, %r43, 262144;
	setp.eq.s32	%p51, %r172, 0;
	selp.b64	%rd122, %rd115, %rd121, %p51;
	setp.eq.s32	%p52, %r44, 2;
	selp.b64	%rd123, %rd122, 1, %p52;
	mul.lo.s64 	%rd124, %rd120, %rd123;
	setp.eq.s32	%p53, %r44, 3;
	selp.b64	%rd125, %rd122, 0, %p53;
	xor.b64  	%rd152, %rd124, %rd125;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_13:
	setp.eq.s32	%p17, %r44, 12;
	@%p17 bra 	BB4_39;
	bra.uni 	BB4_14;

BB4_39:
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p11 bra 	BB4_41;

	cvt.u32.u64	%r165, %rd13;
	shr.u64 	%rd111, %rd152, %r165;
	cvt.u32.u64	%r166, %rd111;
	cvt.rn.f64.s32	%fd44, %r166;
	mov.b64 	 %rd152, %fd44;

BB4_41:
	and.b32  	%r167, %r43, 524288;
	setp.eq.s32	%p43, %r167, 0;
	xor.b64  	%rd112, %rd152, -9223372036854775808;
	selp.b64	%rd113, %rd152, %rd112, %p43;
	shr.u32 	%r168, %r43, 15;
	and.b32  	%r169, %r168, 1;
	setp.eq.b32	%p44, %r169, 1;
	mov.b64 	 %fd13, %rd23;
	mov.b64 	 %fd45, %rd113;
	selp.f64	%fd14, %fd45, 0d3FF0000000000000, %p44;
	selp.f64	%fd15, 0d0000000000000000, %fd45, %p44;
	setp.eq.s32	%p45, %r40, 0;
	@%p45 bra 	BB4_47;
	bra.uni 	BB4_42;

BB4_47:
	fma.rn.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB4_48;

BB4_10:
	st.global.u64 	[%rd26], %rd149;
	bra.uni 	BB4_51;

BB4_14:
	setp.eq.s32	%p18, %r44, 9;
	@%p18 bra 	BB4_36;
	bra.uni 	BB4_15;

BB4_36:
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd152, %rd25, %rd23;
	cvt.u32.u64	%r158, %rd152;
	and.b32  	%r159, %r50, 31;
	mov.u32 	%r160, 255;
	shl.b32 	%r161, %r160, %r159;
	and.b32  	%r162, %r158, %r161;
	setp.ne.s32	%p41, %r162, 0;
	@%p41 bra 	BB4_38;

	shr.s32 	%r163, %r50, 5;
	sub.s32 	%r164, %r163, %r42;
	st.shared.u32 	[%r11+1016], %r164;

BB4_38:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_42:
	setp.eq.s32	%p46, %r40, 1;
	@%p46 bra 	BB4_46;
	bra.uni 	BB4_43;

BB4_46:
	fma.rm.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB4_48;

BB4_15:
	setp.eq.s32	%p19, %r44, 7;
	@%p19 bra 	BB4_35;
	bra.uni 	BB4_16;

BB4_35:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r156, %rd152;
	and.b32  	%r157, %r156, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd23, %r157;
	sub.u32 	%amt2, 64, %r157;
	shl.b64 	%rhs, %rd23, %amt2;
	add.u64 	%rd152, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_43:
	setp.eq.s32	%p47, %r40, 2;
	@%p47 bra 	BB4_45;
	bra.uni 	BB4_44;

BB4_45:
	fma.rp.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB4_48;

BB4_16:
	setp.eq.s32	%p20, %r44, 11;
	@%p20 bra 	BB4_34;
	bra.uni 	BB4_17;

BB4_34:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r150,%r151}, %rd23;
	// inline asm
	mov.u32 	%r154, 6175;
	mov.u32 	%r155, 1;
	shfl.sync.bfly.b32 	%r153|%p39, %r151, %r155, %r154, %r21;
	shfl.sync.bfly.b32 	%r152|%p40, %r150, %r155, %r154, %r21;
	// inline asm
	mov.b64 %rd152, {%r152,%r153};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB4_50;

BB4_44:
	fma.rz.f64 	%fd48, %fd13, %fd14, %fd15;

BB4_48:
	mov.b64 	 %rd152, %fd48;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB4_50:
	st.shared.u64 	[%r46], %rd152;
	// inline asm
	// EXECUTION END
	// inline asm

BB4_51:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	bar.warp.sync 	%r25;
	ld.shared.v2.u32 	{%r173, %r40}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r176, %r42, %r173;
	add.s32 	%r213, %r176, 1;
	setp.lt.u32	%p54, %r213, %r17;
	@%p54 bra 	BB4_7;

BB4_52:
	shr.u32 	%r191, %r73, 24;
	add.s32 	%r190, %r11, %r191;
	bfe.u32 	%r189, %r73, 16, 8;
	add.s32 	%r188, %r11, %r189;
	ld.param.u32 	%r187, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_5];
	ld.shared.u32 	%r179, [%r188];
	xor.b32  	%r180, %r179, %r36;
	ld.shared.u32 	%r181, [%r190];
	xor.b32  	%r182, %r180, %r181;
	and.b32  	%r217, %r182, 2147483584;
	cvt.u64.u32	%rd126, %r216;
	add.s64 	%rd127, %rd14, %rd126;
	add.s64 	%rd128, %rd16, %rd127;
	ld.global.u64 	%rd129, [%rd128];
	ld.shared.u64 	%rd130, [%r19];
	xor.b64  	%rd153, %rd129, %rd130;
	ld.shared.u64 	%rd131, [%r19+64];
	ld.shared.u64 	%rd132, [%r19+128];
	st.shared.u64 	[%r19], %rd153;
	xor.b64  	%rd133, %rd132, %rd131;
	st.global.u64 	[%rd22], %rd153;
	st.global.u64 	[%rd21], %rd133;
	add.s32 	%r207, %r207, 1;
	setp.lt.u32	%p55, %r207, %r187;
	mov.u32 	%r208, 0;
	mov.u32 	%r209, %r208;
	@%p55 bra 	BB4_5;
	bra.uni 	BB4_54;

BB4_53:
	ld.shared.u64 	%rd153, [%r19];

BB4_54:
	mov.u32 	%r196, %tid.x;
	mov.u32 	%r195, %ntid.x;
	mov.u32 	%r194, %ctaid.x;
	mad.lo.s32 	%r193, %r194, %r195, %r196;
	shr.u32 	%r192, %r193, 3;
	cvt.u64.u32	%rd142, %r192;
	ld.param.u64 	%rd141, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_0];
	shl.b64 	%rd49, %rd142, 8;
	add.s64 	%rd134, %rd49, %rd10;
	cvta.to.global.u64 	%rd50, %rd141;
	shl.b64 	%rd135, %rd134, 3;
	add.s64 	%rd51, %rd50, %rd135;
	st.global.u64 	[%rd51], %rd153;
	setp.ne.s32	%p56, %r13, 0;
	@%p56 bra 	BB4_56;

	ld.param.u64 	%rd146, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_1];
	mov.u32 	%r201, %tid.x;
	mov.u32 	%r200, %ntid.x;
	mov.u32 	%r199, %ctaid.x;
	mad.lo.s32 	%r198, %r199, %r200, %r201;
	shr.u32 	%r197, %r198, 3;
	mul.wide.u32 	%rd145, %r197, 4;
	cvta.to.global.u64 	%rd144, %rd146;
	add.s64 	%rd143, %rd144, %rd145;
	st.global.u32 	[%rd143], %r40;

BB4_56:
	ld.param.s8 	%rs5, [_Z10execute_vmILi2ELb0EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs4, %rs5, 255;
	setp.eq.s16	%p57, %rs4, 0;
	@%p57 bra 	BB4_58;

	ld.shared.u64 	%rd136, [%r19+64];
	ld.shared.f64 	%fd46, [%r19+128];
	mov.b64 	 %rd137, %fd46;
	xor.b64  	%rd138, %rd137, %rd136;
	st.global.u64 	[%rd51+64], %rd138;
	st.global.f64 	[%rd51+128], %fd46;
	bra.uni 	BB4_60;

BB4_58:
	@%p56 bra 	BB4_60;

	shl.b64 	%rd139, %rd49, 3;
	add.s64 	%rd140, %rd50, %rd139;
	st.global.u32 	[%rd140+128], %r217;
	st.global.u32 	[%rd140+132], %r216;

BB4_60:
	ret;
}

	// .globl	_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 16, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<59>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<220>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<154>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd52, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd53, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd55, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd54, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r59, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_6];
	cvta.to.global.u64 	%rd1, %rd55;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r207, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r207, 4095;
	@%p1 bra 	BB5_3;

	mov.u32 	%r61, _ZZ10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r206, %r61, %r207;
	cvt.u64.u32	%rd2, %r4;
	cvta.to.global.u64 	%rd56, %rd52;
	cvt.u64.u32	%rd57, %r207;
	mul.wide.u32 	%rd58, %r5, 4096;
	add.s64 	%rd59, %rd58, %rd57;
	add.s64 	%rd147, %rd56, %rd59;

BB5_2:
	ld.global.u64 	%rd60, [%rd147];
	st.shared.u64 	[%r206], %rd60;
	add.s64 	%rd147, %rd147, %rd2;
	add.s32 	%r206, %r206, %r4;
	add.s32 	%r207, %r207, %r4;
	setp.lt.u32	%p2, %r207, 4096;
	@%p2 bra 	BB5_2;

BB5_3:
	bar.warp.sync 	-1;
	shr.u32 	%r62, %r1, 3;
	mul.wide.u32 	%rd6, %r62, 256;
	cvt.u32.u64	%r63, %rd6;
	shl.b32 	%r64, %r63, 3;
	mov.u32 	%r65, _ZZ10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r65, %r64;
	mad.lo.s32 	%r12, %r5, %r3, %r1;
	shr.u32 	%r66, %r12, 3;
	and.b32  	%r13, %r12, 7;
	ld.shared.v2.u32 	{%r218, %r217}, [%r11+128];
	ld.shared.u32 	%r17, [%r11+160];
	cvta.to.global.u64 	%rd61, %rd53;
	mul.wide.u32 	%rd62, %r66, 4;
	add.s64 	%rd8, %rd61, %rd62;
	ld.global.u32 	%r40, [%rd8];
	mul.lo.s32 	%r69, %r66, 2097216;
	cvt.u64.u32	%rd9, %r69;
	cvt.u64.u32	%rd10, %r13;
	or.b32  	%r70, %r13, %r63;
	shl.b32 	%r71, %r70, 3;
	add.s32 	%r19, %r65, %r71;
	setp.eq.s32	%p3, %r59, 0;
	@%p3 bra 	BB5_53;

	add.s32 	%r184, %r11, 128;
	ld.shared.u32 	%r73, [%r184+8];
	and.b32  	%r74, %r12, 1;
	shl.b32 	%r75, %r12, 3;
	and.b32  	%r76, %r75, 8;
	mov.u32 	%r77, 3;
	add.s32 	%r20, %r76, 64;
	add.s32 	%r185, %r11, 128;
	ld.shared.v2.u64 	{%rd63, %rd64}, [%r185+16];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p4, %rs3, 0;
	setp.lt.u32	%p5, %r13, 4;
	add.s32 	%r78, %r13, -4;
	shl.b32 	%r79, %r78, 1;
	cvt.s64.s32	%rd67, %r79;
	add.s64 	%rd68, %rd6, %rd67;
	add.s64 	%rd69, %rd68, 16;
	shl.b32 	%r80, %r13, 1;
	cvt.u64.u32	%rd70, %r80;
	add.s64 	%rd71, %rd6, %rd70;
	add.s64 	%rd72, %rd71, 8;
	selp.b64	%rd73, %rd72, %rd69, %p5;
	selp.b64	%rd11, -1, 72057594037927935, %p5;
	and.b32  	%r81, %r1, -8;
	and.b32  	%r82, %r12, 6;
	add.s32 	%r83, %r82, %r81;
	shl.b32 	%r21, %r77, %r83;
	ld.shared.u32 	%rd74, [%r11+140];
	shl.b32 	%r84, %r13, 3;
	cvt.u64.u32	%rd75, %r84;
	add.s64 	%rd12, %rd75, %rd9;
	add.s64 	%rd76, %rd73, 1;
	cvt.u32.u64	%r85, %rd76;
	shl.b32 	%r86, %r85, 3;
	add.s32 	%r22, %r65, %r86;
	shl.b32 	%r88, %r74, 5;
	cvt.u64.u32	%rd13, %r88;
	add.s64 	%rd14, %rd74, %rd75;
	ld.shared.u64 	%rd153, [%r19];
	cvta.to.global.u64 	%rd16, %rd54;
	mov.u32 	%r89, 15;
	shl.b32 	%r25, %r89, %r81;
	setp.eq.s32	%p6, %r74, 0;
	selp.b64	%rd17, %rd63, %rd64, %p6;
	selp.b64	%rd18, 0, %rd64, %p5;
	selp.b64	%rd19, 0, %rd63, %p5;
	cvt.u32.u64	%r90, %rd73;
	shl.b32 	%r91, %r90, 3;
	add.s32 	%r26, %r65, %r91;
	selp.b32	%r209, 0, %r218, %p4;
	selp.b32	%r210, 0, %r217, %p4;
	and.b32  	%r95, %r73, 255;
	add.s32 	%r32, %r11, %r95;
	mov.u32 	%r208, 0;

BB5_5:
	.pragma "nounroll";
	mov.u32 	%r36, %r217;
	mov.u32 	%r217, %r218;
	bfe.u32 	%r204, %r73, 8, 8;
	add.s32 	%r203, %r11, %r204;
	cvt.u32.u64	%r96, %rd10;
	ld.shared.u64 	%rd77, [%r203];
	ld.shared.u64 	%rd78, [%r32];
	xor.b64  	%rd79, %rd77, %rd78;
	mov.b64	{%r97, %r98}, %rd79;
	xor.b32  	%r99, %r97, %r210;
	xor.b32  	%r100, %r98, %r209;
	and.b32  	%r101, %r99, 2097088;
	and.b32  	%r102, %r100, 2097088;
	cvt.u64.u32	%rd80, %r101;
	add.s64 	%rd81, %rd12, %rd80;
	add.s64 	%rd21, %rd1, %rd81;
	cvt.u64.u32	%rd82, %r102;
	add.s64 	%rd83, %rd12, %rd82;
	add.s64 	%rd22, %rd1, %rd83;
	ld.global.u64 	%rd84, [%rd21];
	xor.b64  	%rd85, %rd153, %rd84;
	st.shared.u64 	[%r19], %rd85;
	ld.global.v2.u32 	{%r103, %r104}, [%rd22];
	cvt.rn.f64.s32	%fd21, %r103;
	mov.b64 	 %rd86, %fd21;
	and.b64  	%rd87, %rd86, %rd11;
	or.b64  	%rd88, %rd87, %rd19;
	st.shared.u64 	[%r26], %rd88;
	cvt.rn.f64.s32	%fd22, %r104;
	mov.b64 	 %rd89, %fd22;
	and.b64  	%rd90, %rd89, %rd11;
	or.b64  	%rd91, %rd90, %rd18;
	st.shared.u64 	[%r22], %rd91;
	setp.gt.u32	%p7, %r96, 3;
	@%p7 bra 	BB5_52;

	setp.eq.s32	%p8, %r17, 0;
	mov.u32 	%r214, 0;
	add.s32 	%r186, %r11, 1016;
	st.shared.u32 	[%r186+4], %r40;
	@%p8 bra 	BB5_52;
	bra.uni 	BB5_7;

BB5_17:
	setp.eq.s32	%p21, %r44, 14;
	@%p21 bra 	BB5_31;
	bra.uni 	BB5_18;

BB5_31:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd33, %rd23;
	// inline asm
	rsqrt.approx.ftz.f64 %fd32, %fd33;
	// inline asm
	mul.f64 	%fd34, %fd33, %fd32;
	mul.f64 	%fd35, %fd32, 0dBFE0000000000000;
	mov.f64 	%fd36, 0d3FE0000000000000;
	fma.rn.f64 	%fd37, %fd35, %fd34, %fd36;
	fma.rn.f64 	%fd7, %fd34, %fd37, %fd34;
	mul.f64 	%fd38, %fd32, 0d3FE0000000000000;
	fma.rn.f64 	%fd8, %fd38, %fd37, %fd38;
	neg.f64 	%fd39, %fd7;
	fma.rn.f64 	%fd9, %fd39, %fd7, %fd33;
	fma.rn.f64 	%fd47, %fd9, %fd8, %fd7;
	setp.eq.s32	%p34, %r40, 0;
	@%p34 bra 	BB5_33;

	fma.rm.f64 	%fd40, %fd9, %fd8, %fd7;
	and.b32  	%r150, %r40, 1;
	setp.eq.b32	%p35, %r150, 1;
	not.pred 	%p36, %p35;
	selp.f64	%fd41, %fd47, %fd40, %p36;
	setp.eq.s32	%p37, %r40, 2;
	fma.rp.f64 	%fd42, %fd9, %fd8, %fd7;
	selp.f64	%fd47, %fd42, %fd41, %p37;

BB5_33:
	setp.eq.s64	%p38, %rd23, 9218868437227405312;
	selp.f64	%fd43, %fd33, %fd47, %p38;
	mov.b64 	 %rd152, %fd43;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_18:
	setp.eq.s32	%p22, %r44, 6;
	@%p22 bra 	BB5_30;
	bra.uni 	BB5_19;

BB5_30:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd152, %rd23, %rd152;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_19:
	setp.eq.s32	%p23, %r44, 4;
	@%p23 bra 	BB5_29;
	bra.uni 	BB5_20;

BB5_29:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd152, %rd23, %rd152;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_20:
	setp.eq.s32	%p24, %r44, 8;
	@%p24 bra 	BB5_28;
	bra.uni 	BB5_21;

BB5_28:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r47], %rd23;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_21:
	setp.eq.s32	%p25, %r44, 15;
	@%p25 bra 	BB5_25;
	bra.uni 	BB5_22;

BB5_25:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	cvt.u32.u64	%r146, %rd13;
	shr.u64 	%rd96, %rd152, %r146;
	cvt.u32.u64	%r147, %rd96;
	cvt.rn.f64.s32	%fd25, %r147;
	mov.b64 	 %rd97, %fd25;
	and.b64  	%rd98, %rd97, 72057594037927935;
	or.b64  	%rd99, %rd98, %rd17;
	mov.b64 	 %fd24, %rd99;
	// inline asm
	rcp.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	neg.f64 	%fd26, %fd24;
	mov.f64 	%fd27, 0d3FF0000000000000;
	fma.rn.f64 	%fd28, %fd26, %fd23, %fd27;
	fma.rn.f64 	%fd2, %fd23, %fd28, %fd23;
	mov.b64 	 %fd3, %rd23;
	mul.f64 	%fd4, %fd3, %fd2;
	fma.rn.f64 	%fd5, %fd26, %fd4, %fd3;
	fma.rn.f64 	%fd29, %fd2, %fd5, %fd4;
	mov.b64 	 %rd150, %fd29;
	setp.eq.s32	%p27, %r40, 0;
	@%p27 bra 	BB5_27;

	and.b32  	%r148, %r40, 1;
	setp.eq.b32	%p28, %r148, 1;
	not.pred 	%p29, %p28;
	fma.rm.f64 	%fd30, %fd2, %fd5, %fd4;
	mov.b64 	 %rd100, %fd30;
	selp.b64	%rd101, %rd150, %rd100, %p29;
	fma.rp.f64 	%fd31, %fd2, %fd5, %fd4;
	mov.b64 	 %rd102, %fd31;
	setp.eq.s32	%p30, %r40, 2;
	selp.b64	%rd150, %rd102, %rd101, %p30;

BB5_27:
	and.b64  	%rd103, %rd150, 9218868437227405312;
	mov.u64 	%rd104, 9218868437227405312;
	setp.eq.s64	%p31, %rd103, 9218868437227405312;
	and.b32  	%r149, %r40, 1;
	cvt.u64.u32	%rd105, %r149;
	sub.s64 	%rd106, %rd104, %rd105;
	selp.b64	%rd107, %rd106, %rd150, %p31;
	setp.eq.s64	%p32, %rd23, 9218868437227405312;
	selp.b64	%rd108, 9218868437227405312, %rd107, %p32;
	setp.eq.f64	%p33, %fd3, %fd24;
	selp.b64	%rd152, 4607182418800017408, %rd108, %p33;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_22:
	setp.eq.s32	%p26, %r44, 5;
	@%p26 bra 	BB5_24;
	bra.uni 	BB5_23;

BB5_24:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd152, %rd23;
	bra.uni 	BB5_50;

BB5_23:
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd94, %rd152, %r48;
	mov.u32 	%r140, 64;
	sub.s32 	%r141, %r140, %r48;
	shl.b64 	%rd95, %rd152, %r141;
	cvt.u32.u64	%r142, %rd94;
	cvt.u32.u64	%r143, %rd95;
	or.b32  	%r144, %r142, %r143;
	and.b32  	%r145, %r144, 3;
	add.s32 	%r187, %r11, 1016;
	st.shared.u32 	[%r187+4], %r145;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB5_51;

BB5_7:
	shl.b32 	%r108, %r214, 2;
	add.s32 	%r109, %r11, %r108;
	st.shared.u32 	[%r11+1016], %r214;
	ld.shared.u32 	%r110, [%r109+1024];
	bfe.u32 	%r111, %r110, 24, 2;
	bfe.u32 	%r41, %r110, 28, 2;
	sub.s32 	%r42, %r111, %r41;
	setp.lt.u32	%p9, %r111, %r13;
	@%p9 bra 	BB5_51;

	shr.u32 	%r205, %r13, 1;
	sub.s32 	%r112, %r13, %r41;
	setp.lt.s32	%p10, %r112, %r41;
	selp.b32	%r113, %r205, %r112, %p10;
	add.s32 	%r114, %r113, %r214;
	shl.b32 	%r115, %r114, 2;
	add.s32 	%r116, %r11, %r115;
	ld.shared.u32 	%r43, [%r116+1024];
	bfe.u32 	%r48, %r43, 6, 8;
	shl.b32 	%r117, %r48, 2;
	add.s32 	%r118, %r11, %r117;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r44, %r43, 20, 4;
	and.b32  	%r45, %r43, 16384;
	selp.b32	%r119, 4, 3, %p10;
	selp.b32	%r120, %r20, 0, %p10;
	and.b32  	%r121, %r43, 7;
	shl.b32 	%r122, %r121, %r119;
	add.s32 	%r123, %r122, %r120;
	and.b32  	%r124, %r43, 56;
	setp.eq.s32	%p11, %r45, 0;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r125, %r20, 128;
	selp.b32	%r126, %r125, 0, %p12;
	add.s32 	%r127, %r126, %r124;
	add.s32 	%r46, %r11, %r123;
	add.s32 	%r47, %r11, %r127;
	ld.shared.u64 	%rd23, [%r46];
	ld.shared.u64 	%rd149, [%r47];
	ld.shared.u32 	%r49, [%r118+256];
	cvt.s64.s32	%rd25, %r49;
	ld.shared.u32 	%r50, [%r118+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p11 bra 	BB5_12;

	cvt.u32.u64	%r129, %rd25;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r128, %r129, 21, 5;
	// inline asm
	mov.u32 	%r130, -1;
	shr.u32 	%r131, %r130, %r128;
	add.s32 	%r132, %r131, -7;
	setp.eq.s32	%p13, %r128, 11;
	cvt.u32.u64	%r133, %rd149;
	selp.b32	%r134, 0, %r133, %p13;
	cvt.u32.u64	%r135, %rd23;
	setp.ne.s32	%p14, %r44, 10;
	selp.b32	%r136, %r134, %r135, %p14;
	add.s32 	%r137, %r136, %r49;
	and.b32  	%r138, %r137, %r132;
	cvt.u64.u32	%rd92, %r138;
	add.s64 	%rd93, %rd92, %rd9;
	add.s64 	%rd26, %rd1, %rd93;
	@%p14 bra 	BB5_11;
	bra.uni 	BB5_10;

BB5_11:
	ld.global.u64 	%rd149, [%rd26];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB5_12:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r139, %r43, 131072;
	setp.eq.s32	%p15, %r139, 0;
	selp.b64	%rd152, %rd149, %rd25, %p15;
	setp.lt.u32	%p16, %r44, 4;
	@%p16 bra 	BB5_49;
	bra.uni 	BB5_13;

BB5_49:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r171, %r43, 524288;
	setp.eq.s32	%p48, %r171, 0;
	neg.s64 	%rd114, %rd152;
	selp.b64	%rd115, %rd152, %rd114, %p48;
	setp.eq.s32	%p49, %r44, 0;
	selp.b64	%rd116, %rd25, 0, %p49;
	add.s64 	%rd117, %rd116, %rd23;
	bfe.u32 	%r172, %r43, 15, 2;
	shl.b64 	%rd118, %rd115, %r172;
	setp.lt.u32	%p50, %r44, 2;
	selp.b64	%rd119, %rd118, 0, %p50;
	add.s64 	%rd120, %rd117, %rd119;
	mov.b64	%rd121, {%r49, %r50};
	and.b32  	%r173, %r43, 262144;
	setp.eq.s32	%p51, %r173, 0;
	selp.b64	%rd122, %rd115, %rd121, %p51;
	setp.eq.s32	%p52, %r44, 2;
	selp.b64	%rd123, %rd122, 1, %p52;
	mul.lo.s64 	%rd124, %rd120, %rd123;
	setp.eq.s32	%p53, %r44, 3;
	selp.b64	%rd125, %rd122, 0, %p53;
	xor.b64  	%rd152, %rd124, %rd125;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_13:
	setp.eq.s32	%p17, %r44, 12;
	@%p17 bra 	BB5_39;
	bra.uni 	BB5_14;

BB5_39:
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p11 bra 	BB5_41;

	cvt.u32.u64	%r166, %rd13;
	shr.u64 	%rd111, %rd152, %r166;
	cvt.u32.u64	%r167, %rd111;
	cvt.rn.f64.s32	%fd44, %r167;
	mov.b64 	 %rd152, %fd44;

BB5_41:
	and.b32  	%r168, %r43, 524288;
	setp.eq.s32	%p43, %r168, 0;
	xor.b64  	%rd112, %rd152, -9223372036854775808;
	selp.b64	%rd113, %rd152, %rd112, %p43;
	shr.u32 	%r169, %r43, 15;
	and.b32  	%r170, %r169, 1;
	setp.eq.b32	%p44, %r170, 1;
	mov.b64 	 %fd13, %rd23;
	mov.b64 	 %fd45, %rd113;
	selp.f64	%fd14, %fd45, 0d3FF0000000000000, %p44;
	selp.f64	%fd15, 0d0000000000000000, %fd45, %p44;
	setp.eq.s32	%p45, %r40, 0;
	@%p45 bra 	BB5_47;
	bra.uni 	BB5_42;

BB5_47:
	fma.rn.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB5_48;

BB5_10:
	st.global.u64 	[%rd26], %rd149;
	bra.uni 	BB5_51;

BB5_14:
	setp.eq.s32	%p18, %r44, 9;
	@%p18 bra 	BB5_36;
	bra.uni 	BB5_15;

BB5_36:
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd152, %rd25, %rd23;
	cvt.u32.u64	%r159, %rd152;
	and.b32  	%r160, %r50, 31;
	mov.u32 	%r161, 255;
	shl.b32 	%r162, %r161, %r160;
	and.b32  	%r163, %r159, %r162;
	setp.ne.s32	%p41, %r163, 0;
	@%p41 bra 	BB5_38;

	shr.s32 	%r164, %r50, 5;
	sub.s32 	%r165, %r164, %r42;
	st.shared.u32 	[%r11+1016], %r165;

BB5_38:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_42:
	setp.eq.s32	%p46, %r40, 1;
	@%p46 bra 	BB5_46;
	bra.uni 	BB5_43;

BB5_46:
	fma.rm.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB5_48;

BB5_15:
	setp.eq.s32	%p19, %r44, 7;
	@%p19 bra 	BB5_35;
	bra.uni 	BB5_16;

BB5_35:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r157, %rd152;
	and.b32  	%r158, %r157, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd23, %r158;
	sub.u32 	%amt2, 64, %r158;
	shl.b64 	%rhs, %rd23, %amt2;
	add.u64 	%rd152, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_43:
	setp.eq.s32	%p47, %r40, 2;
	@%p47 bra 	BB5_45;
	bra.uni 	BB5_44;

BB5_45:
	fma.rp.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB5_48;

BB5_16:
	setp.eq.s32	%p20, %r44, 11;
	@%p20 bra 	BB5_34;
	bra.uni 	BB5_17;

BB5_34:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r151,%r152}, %rd23;
	// inline asm
	mov.u32 	%r155, 6175;
	mov.u32 	%r156, 1;
	shfl.sync.bfly.b32 	%r154|%p39, %r152, %r156, %r155, %r21;
	shfl.sync.bfly.b32 	%r153|%p40, %r151, %r156, %r155, %r21;
	// inline asm
	mov.b64 %rd152, {%r153,%r154};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB5_50;

BB5_44:
	fma.rz.f64 	%fd48, %fd13, %fd14, %fd15;

BB5_48:
	mov.b64 	 %rd152, %fd48;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB5_50:
	st.shared.u64 	[%r46], %rd152;
	// inline asm
	// EXECUTION END
	// inline asm

BB5_51:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	bar.warp.sync 	%r25;
	ld.shared.v2.u32 	{%r174, %r40}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r177, %r42, %r174;
	add.s32 	%r214, %r177, 1;
	setp.lt.u32	%p54, %r214, %r17;
	@%p54 bra 	BB5_7;

BB5_52:
	shr.u32 	%r192, %r73, 24;
	add.s32 	%r191, %r11, %r192;
	bfe.u32 	%r190, %r73, 16, 8;
	add.s32 	%r189, %r11, %r190;
	ld.param.u32 	%r188, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_5];
	ld.shared.u32 	%r180, [%r189];
	xor.b32  	%r181, %r180, %r36;
	ld.shared.u32 	%r182, [%r191];
	xor.b32  	%r183, %r181, %r182;
	and.b32  	%r218, %r183, 2147483584;
	cvt.u64.u32	%rd126, %r217;
	add.s64 	%rd127, %rd14, %rd126;
	add.s64 	%rd128, %rd16, %rd127;
	ld.global.u64 	%rd129, [%rd128];
	ld.shared.u64 	%rd130, [%r19];
	xor.b64  	%rd153, %rd129, %rd130;
	ld.shared.u64 	%rd131, [%r19+64];
	ld.shared.u64 	%rd132, [%r19+128];
	st.shared.u64 	[%r19], %rd153;
	xor.b64  	%rd133, %rd132, %rd131;
	st.global.u64 	[%rd22], %rd153;
	st.global.u64 	[%rd21], %rd133;
	add.s32 	%r208, %r208, 1;
	setp.lt.u32	%p55, %r208, %r188;
	mov.u32 	%r209, 0;
	mov.u32 	%r210, %r209;
	@%p55 bra 	BB5_5;
	bra.uni 	BB5_54;

BB5_53:
	ld.shared.u64 	%rd153, [%r19];

BB5_54:
	mov.u32 	%r197, %tid.x;
	mov.u32 	%r196, %ntid.x;
	mov.u32 	%r195, %ctaid.x;
	mad.lo.s32 	%r194, %r195, %r196, %r197;
	shr.u32 	%r193, %r194, 3;
	cvt.u64.u32	%rd142, %r193;
	ld.param.u64 	%rd141, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_0];
	shl.b64 	%rd49, %rd142, 8;
	add.s64 	%rd134, %rd49, %rd10;
	cvta.to.global.u64 	%rd50, %rd141;
	shl.b64 	%rd135, %rd134, 3;
	add.s64 	%rd51, %rd50, %rd135;
	st.global.u64 	[%rd51], %rd153;
	setp.ne.s32	%p56, %r13, 0;
	@%p56 bra 	BB5_56;

	ld.param.u64 	%rd146, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_1];
	mov.u32 	%r202, %tid.x;
	mov.u32 	%r201, %ntid.x;
	mov.u32 	%r200, %ctaid.x;
	mad.lo.s32 	%r199, %r200, %r201, %r202;
	shr.u32 	%r198, %r199, 3;
	mul.wide.u32 	%rd145, %r198, 4;
	cvta.to.global.u64 	%rd144, %rd146;
	add.s64 	%rd143, %rd144, %rd145;
	st.global.u32 	[%rd143], %r40;

BB5_56:
	ld.param.s8 	%rs5, [_Z10execute_vmILi4ELb0EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs4, %rs5, 255;
	setp.eq.s16	%p57, %rs4, 0;
	@%p57 bra 	BB5_58;

	ld.shared.u64 	%rd136, [%r19+64];
	ld.shared.f64 	%fd46, [%r19+128];
	mov.b64 	 %rd137, %fd46;
	xor.b64  	%rd138, %rd137, %rd136;
	st.global.u64 	[%rd51+64], %rd138;
	st.global.f64 	[%rd51+128], %fd46;
	bra.uni 	BB5_60;

BB5_58:
	@%p56 bra 	BB5_60;

	shl.b64 	%rd139, %rd49, 3;
	add.s64 	%rd140, %rd50, %rd139;
	st.global.u32 	[%rd140+128], %r218;
	st.global.u32 	[%rd140+132], %r217;

BB5_60:
	ret;
}

	// .globl	_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 16, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<58>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<221>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<154>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd52, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd53, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd55, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd54, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r59, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_6];
	cvta.to.global.u64 	%rd1, %rd55;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r209, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r209, 4095;
	@%p1 bra 	BB6_3;

	mov.u32 	%r61, _ZZ10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r208, %r61, %r209;
	cvt.u64.u32	%rd2, %r4;
	cvta.to.global.u64 	%rd56, %rd52;
	cvt.u64.u32	%rd57, %r209;
	mul.wide.u32 	%rd58, %r5, 4096;
	add.s64 	%rd59, %rd58, %rd57;
	add.s64 	%rd147, %rd56, %rd59;

BB6_2:
	ld.global.u64 	%rd60, [%rd147];
	st.shared.u64 	[%r208], %rd60;
	add.s64 	%rd147, %rd147, %rd2;
	add.s32 	%r208, %r208, %r4;
	add.s32 	%r209, %r209, %r4;
	setp.lt.u32	%p2, %r209, 4096;
	@%p2 bra 	BB6_2;

BB6_3:
	bar.warp.sync 	-1;
	shr.u32 	%r62, %r1, 3;
	mul.wide.u32 	%rd6, %r62, 256;
	cvt.u32.u64	%r63, %rd6;
	shl.b32 	%r64, %r63, 3;
	mov.u32 	%r65, _ZZ10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r65, %r64;
	mad.lo.s32 	%r12, %r5, %r3, %r1;
	shr.u32 	%r66, %r12, 3;
	and.b32  	%r13, %r12, 7;
	ld.shared.v2.u32 	{%r219, %r215}, [%r11+128];
	ld.shared.u32 	%r17, [%r11+160];
	cvta.to.global.u64 	%rd61, %rd53;
	mul.wide.u32 	%rd62, %r66, 4;
	add.s64 	%rd8, %rd61, %rd62;
	ld.global.u32 	%r40, [%rd8];
	mul.lo.s32 	%r69, %r66, 2097216;
	cvt.u64.u32	%rd9, %r69;
	cvt.u64.u32	%rd10, %r13;
	or.b32  	%r70, %r13, %r63;
	shl.b32 	%r71, %r70, 3;
	add.s32 	%r19, %r65, %r71;
	setp.eq.s32	%p3, %r59, 0;
	@%p3 bra 	BB6_52;

	add.s32 	%r184, %r11, 128;
	ld.shared.u32 	%r73, [%r184+8];
	and.b32  	%r74, %r12, 1;
	shl.b32 	%r75, %r12, 3;
	and.b32  	%r76, %r75, 8;
	mov.u32 	%r77, 3;
	add.s32 	%r20, %r76, 64;
	add.s32 	%r185, %r11, 128;
	ld.shared.v2.u64 	{%rd63, %rd64}, [%r185+16];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p4, %rs3, 0;
	setp.lt.u32	%p5, %r13, 4;
	add.s32 	%r78, %r13, -4;
	shl.b32 	%r79, %r78, 1;
	cvt.s64.s32	%rd67, %r79;
	add.s64 	%rd68, %rd6, %rd67;
	add.s64 	%rd69, %rd68, 16;
	shl.b32 	%r80, %r13, 1;
	cvt.u64.u32	%rd70, %r80;
	add.s64 	%rd71, %rd6, %rd70;
	add.s64 	%rd72, %rd71, 8;
	selp.b64	%rd73, %rd72, %rd69, %p5;
	selp.b64	%rd11, -1, 72057594037927935, %p5;
	and.b32  	%r81, %r1, -8;
	and.b32  	%r82, %r12, 6;
	add.s32 	%r83, %r82, %r81;
	shl.b32 	%r21, %r77, %r83;
	ld.shared.u32 	%rd74, [%r11+140];
	shl.b32 	%r84, %r13, 3;
	cvt.u64.u32	%rd75, %r84;
	add.s64 	%rd12, %rd75, %rd9;
	add.s64 	%rd76, %rd73, 1;
	cvt.u32.u64	%r85, %rd76;
	shl.b32 	%r86, %r85, 3;
	add.s32 	%r22, %r65, %r86;
	add.s64 	%rd13, %rd74, %rd75;
	shl.b32 	%r88, %r74, 5;
	cvt.u64.u32	%rd14, %r88;
	ld.shared.u64 	%rd153, [%r19];
	cvta.to.global.u64 	%rd16, %rd54;
	mov.u32 	%r89, 255;
	shl.b32 	%r25, %r89, %r81;
	setp.eq.s32	%p6, %r74, 0;
	selp.b64	%rd17, %rd63, %rd64, %p6;
	selp.b64	%rd18, 0, %rd64, %p5;
	selp.b64	%rd19, 0, %rd63, %p5;
	cvt.u32.u64	%r90, %rd73;
	shl.b32 	%r91, %r90, 3;
	add.s32 	%r26, %r65, %r91;
	selp.b32	%r211, 0, %r219, %p4;
	selp.b32	%r212, 0, %r215, %p4;
	and.b32  	%r95, %r73, 255;
	add.s32 	%r32, %r11, %r95;
	mov.u32 	%r210, 0;

BB6_5:
	.pragma "nounroll";
	mov.u32 	%r218, %r219;
	bfe.u32 	%r205, %r73, 8, 8;
	add.s32 	%r204, %r11, %r205;
	mov.u32 	%r216, 0;
	ld.shared.u64 	%rd77, [%r204];
	ld.shared.u64 	%rd78, [%r32];
	xor.b64  	%rd79, %rd77, %rd78;
	mov.b64	{%r97, %r98}, %rd79;
	xor.b32  	%r99, %r97, %r212;
	xor.b32  	%r100, %r98, %r211;
	and.b32  	%r101, %r99, 2097088;
	and.b32  	%r102, %r100, 2097088;
	cvt.u64.u32	%rd80, %r101;
	add.s64 	%rd81, %rd12, %rd80;
	add.s64 	%rd21, %rd1, %rd81;
	cvt.u64.u32	%rd82, %r102;
	add.s64 	%rd83, %rd12, %rd82;
	add.s64 	%rd22, %rd1, %rd83;
	ld.global.u64 	%rd84, [%rd21];
	xor.b64  	%rd85, %rd153, %rd84;
	st.shared.u64 	[%r19], %rd85;
	ld.global.v2.u32 	{%r103, %r104}, [%rd22];
	cvt.rn.f64.s32	%fd21, %r103;
	mov.b64 	 %rd86, %fd21;
	and.b64  	%rd87, %rd86, %rd11;
	or.b64  	%rd88, %rd87, %rd19;
	st.shared.u64 	[%r26], %rd88;
	cvt.rn.f64.s32	%fd22, %r104;
	mov.b64 	 %rd89, %fd22;
	and.b64  	%rd90, %rd89, %rd11;
	or.b64  	%rd91, %rd90, %rd18;
	st.shared.u64 	[%r22], %rd91;
	add.s32 	%r186, %r11, 128;
	st.shared.u32 	[%r186+892], %r40;
	setp.eq.s32	%p7, %r17, 0;
	@%p7 bra 	BB6_51;
	bra.uni 	BB6_6;

BB6_16:
	setp.eq.s32	%p20, %r44, 14;
	@%p20 bra 	BB6_30;
	bra.uni 	BB6_17;

BB6_30:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd33, %rd23;
	// inline asm
	rsqrt.approx.ftz.f64 %fd32, %fd33;
	// inline asm
	mul.f64 	%fd34, %fd33, %fd32;
	mul.f64 	%fd35, %fd32, 0dBFE0000000000000;
	mov.f64 	%fd36, 0d3FE0000000000000;
	fma.rn.f64 	%fd37, %fd35, %fd34, %fd36;
	fma.rn.f64 	%fd7, %fd34, %fd37, %fd34;
	mul.f64 	%fd38, %fd32, 0d3FE0000000000000;
	fma.rn.f64 	%fd8, %fd38, %fd37, %fd38;
	neg.f64 	%fd39, %fd7;
	fma.rn.f64 	%fd9, %fd39, %fd7, %fd33;
	fma.rn.f64 	%fd47, %fd9, %fd8, %fd7;
	setp.eq.s32	%p33, %r40, 0;
	@%p33 bra 	BB6_32;

	fma.rm.f64 	%fd40, %fd9, %fd8, %fd7;
	and.b32  	%r150, %r40, 1;
	setp.eq.b32	%p34, %r150, 1;
	not.pred 	%p35, %p34;
	selp.f64	%fd41, %fd47, %fd40, %p35;
	setp.eq.s32	%p36, %r40, 2;
	fma.rp.f64 	%fd42, %fd9, %fd8, %fd7;
	selp.f64	%fd47, %fd42, %fd41, %p36;

BB6_32:
	setp.eq.s64	%p37, %rd23, 9218868437227405312;
	selp.f64	%fd43, %fd33, %fd47, %p37;
	mov.b64 	 %rd152, %fd43;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_17:
	setp.eq.s32	%p21, %r44, 6;
	@%p21 bra 	BB6_29;
	bra.uni 	BB6_18;

BB6_29:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd152, %rd23, %rd152;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_18:
	setp.eq.s32	%p22, %r44, 4;
	@%p22 bra 	BB6_28;
	bra.uni 	BB6_19;

BB6_28:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd152, %rd23, %rd152;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_19:
	setp.eq.s32	%p23, %r44, 8;
	@%p23 bra 	BB6_27;
	bra.uni 	BB6_20;

BB6_27:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r47], %rd23;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_20:
	setp.eq.s32	%p24, %r44, 15;
	@%p24 bra 	BB6_24;
	bra.uni 	BB6_21;

BB6_24:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	cvt.u32.u64	%r146, %rd14;
	shr.u64 	%rd96, %rd152, %r146;
	cvt.u32.u64	%r147, %rd96;
	cvt.rn.f64.s32	%fd25, %r147;
	mov.b64 	 %rd97, %fd25;
	and.b64  	%rd98, %rd97, 72057594037927935;
	or.b64  	%rd99, %rd98, %rd17;
	mov.b64 	 %fd24, %rd99;
	// inline asm
	rcp.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	neg.f64 	%fd26, %fd24;
	mov.f64 	%fd27, 0d3FF0000000000000;
	fma.rn.f64 	%fd28, %fd26, %fd23, %fd27;
	fma.rn.f64 	%fd2, %fd23, %fd28, %fd23;
	mov.b64 	 %fd3, %rd23;
	mul.f64 	%fd4, %fd3, %fd2;
	fma.rn.f64 	%fd5, %fd26, %fd4, %fd3;
	fma.rn.f64 	%fd29, %fd2, %fd5, %fd4;
	mov.b64 	 %rd150, %fd29;
	setp.eq.s32	%p26, %r40, 0;
	@%p26 bra 	BB6_26;

	and.b32  	%r148, %r40, 1;
	setp.eq.b32	%p27, %r148, 1;
	not.pred 	%p28, %p27;
	fma.rm.f64 	%fd30, %fd2, %fd5, %fd4;
	mov.b64 	 %rd100, %fd30;
	selp.b64	%rd101, %rd150, %rd100, %p28;
	fma.rp.f64 	%fd31, %fd2, %fd5, %fd4;
	mov.b64 	 %rd102, %fd31;
	setp.eq.s32	%p29, %r40, 2;
	selp.b64	%rd150, %rd102, %rd101, %p29;

BB6_26:
	and.b64  	%rd103, %rd150, 9218868437227405312;
	mov.u64 	%rd104, 9218868437227405312;
	setp.eq.s64	%p30, %rd103, 9218868437227405312;
	and.b32  	%r149, %r40, 1;
	cvt.u64.u32	%rd105, %r149;
	sub.s64 	%rd106, %rd104, %rd105;
	selp.b64	%rd107, %rd106, %rd150, %p30;
	setp.eq.s64	%p31, %rd23, 9218868437227405312;
	selp.b64	%rd108, 9218868437227405312, %rd107, %p31;
	setp.eq.f64	%p32, %fd3, %fd24;
	selp.b64	%rd152, 4607182418800017408, %rd108, %p32;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_21:
	setp.eq.s32	%p25, %r44, 5;
	@%p25 bra 	BB6_23;
	bra.uni 	BB6_22;

BB6_23:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd152, %rd23;
	bra.uni 	BB6_49;

BB6_22:
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd94, %rd152, %r48;
	mov.u32 	%r140, 64;
	sub.s32 	%r141, %r140, %r48;
	shl.b64 	%rd95, %rd152, %r141;
	cvt.u32.u64	%r142, %rd94;
	cvt.u32.u64	%r143, %rd95;
	or.b32  	%r144, %r142, %r143;
	and.b32  	%r145, %r144, 3;
	add.s32 	%r187, %r11, 128;
	st.shared.u32 	[%r187+892], %r145;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB6_50;

BB6_6:
	shl.b32 	%r107, %r216, 2;
	add.s32 	%r108, %r11, %r107;
	st.shared.u32 	[%r11+1016], %r216;
	ld.shared.u32 	%r109, [%r108+1024];
	bfe.u32 	%r110, %r109, 24, 3;
	bfe.u32 	%r41, %r109, 28, 3;
	sub.s32 	%r42, %r110, %r41;
	cvt.u32.u64	%r111, %rd10;
	setp.lt.u32	%p8, %r110, %r111;
	@%p8 bra 	BB6_50;

	shr.u32 	%r206, %r13, 1;
	sub.s32 	%r112, %r13, %r41;
	setp.lt.s32	%p9, %r112, %r41;
	selp.b32	%r113, %r206, %r112, %p9;
	add.s32 	%r114, %r113, %r216;
	shl.b32 	%r115, %r114, 2;
	add.s32 	%r116, %r11, %r115;
	ld.shared.u32 	%r43, [%r116+1024];
	bfe.u32 	%r48, %r43, 6, 8;
	shl.b32 	%r117, %r48, 2;
	add.s32 	%r118, %r11, %r117;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r44, %r43, 20, 4;
	and.b32  	%r45, %r43, 16384;
	selp.b32	%r119, 4, 3, %p9;
	selp.b32	%r120, %r20, 0, %p9;
	and.b32  	%r121, %r43, 7;
	shl.b32 	%r122, %r121, %r119;
	add.s32 	%r123, %r122, %r120;
	and.b32  	%r124, %r43, 56;
	setp.eq.s32	%p10, %r45, 0;
	and.pred  	%p11, %p9, %p10;
	add.s32 	%r125, %r20, 128;
	selp.b32	%r126, %r125, 0, %p11;
	add.s32 	%r127, %r126, %r124;
	add.s32 	%r46, %r11, %r123;
	add.s32 	%r47, %r11, %r127;
	ld.shared.u64 	%rd23, [%r46];
	ld.shared.u64 	%rd149, [%r47];
	ld.shared.u32 	%r49, [%r118+256];
	cvt.s64.s32	%rd25, %r49;
	ld.shared.u32 	%r50, [%r118+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p10 bra 	BB6_11;

	cvt.u32.u64	%r129, %rd25;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r128, %r129, 21, 5;
	// inline asm
	mov.u32 	%r130, -1;
	shr.u32 	%r131, %r130, %r128;
	add.s32 	%r132, %r131, -7;
	setp.eq.s32	%p12, %r128, 11;
	cvt.u32.u64	%r133, %rd149;
	selp.b32	%r134, 0, %r133, %p12;
	cvt.u32.u64	%r135, %rd23;
	setp.ne.s32	%p13, %r44, 10;
	selp.b32	%r136, %r134, %r135, %p13;
	add.s32 	%r137, %r136, %r49;
	and.b32  	%r138, %r137, %r132;
	cvt.u64.u32	%rd92, %r138;
	add.s64 	%rd93, %rd92, %rd9;
	add.s64 	%rd26, %rd1, %rd93;
	@%p13 bra 	BB6_10;
	bra.uni 	BB6_9;

BB6_10:
	ld.global.u64 	%rd149, [%rd26];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB6_11:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r139, %r43, 131072;
	setp.eq.s32	%p14, %r139, 0;
	selp.b64	%rd152, %rd149, %rd25, %p14;
	setp.lt.u32	%p15, %r44, 4;
	@%p15 bra 	BB6_48;
	bra.uni 	BB6_12;

BB6_48:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r171, %r43, 524288;
	setp.eq.s32	%p47, %r171, 0;
	neg.s64 	%rd114, %rd152;
	selp.b64	%rd115, %rd152, %rd114, %p47;
	setp.eq.s32	%p48, %r44, 0;
	selp.b64	%rd116, %rd25, 0, %p48;
	add.s64 	%rd117, %rd116, %rd23;
	bfe.u32 	%r172, %r43, 15, 2;
	shl.b64 	%rd118, %rd115, %r172;
	setp.lt.u32	%p49, %r44, 2;
	selp.b64	%rd119, %rd118, 0, %p49;
	add.s64 	%rd120, %rd117, %rd119;
	mov.b64	%rd121, {%r49, %r50};
	and.b32  	%r173, %r43, 262144;
	setp.eq.s32	%p50, %r173, 0;
	selp.b64	%rd122, %rd115, %rd121, %p50;
	setp.eq.s32	%p51, %r44, 2;
	selp.b64	%rd123, %rd122, 1, %p51;
	mul.lo.s64 	%rd124, %rd120, %rd123;
	setp.eq.s32	%p52, %r44, 3;
	selp.b64	%rd125, %rd122, 0, %p52;
	xor.b64  	%rd152, %rd124, %rd125;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_12:
	setp.eq.s32	%p16, %r44, 12;
	@%p16 bra 	BB6_38;
	bra.uni 	BB6_13;

BB6_38:
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p10 bra 	BB6_40;

	cvt.u32.u64	%r166, %rd14;
	shr.u64 	%rd111, %rd152, %r166;
	cvt.u32.u64	%r167, %rd111;
	cvt.rn.f64.s32	%fd44, %r167;
	mov.b64 	 %rd152, %fd44;

BB6_40:
	and.b32  	%r168, %r43, 524288;
	setp.eq.s32	%p42, %r168, 0;
	xor.b64  	%rd112, %rd152, -9223372036854775808;
	selp.b64	%rd113, %rd152, %rd112, %p42;
	shr.u32 	%r169, %r43, 15;
	and.b32  	%r170, %r169, 1;
	setp.eq.b32	%p43, %r170, 1;
	mov.b64 	 %fd13, %rd23;
	mov.b64 	 %fd45, %rd113;
	selp.f64	%fd14, %fd45, 0d3FF0000000000000, %p43;
	selp.f64	%fd15, 0d0000000000000000, %fd45, %p43;
	setp.eq.s32	%p44, %r40, 0;
	@%p44 bra 	BB6_46;
	bra.uni 	BB6_41;

BB6_46:
	fma.rn.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB6_47;

BB6_9:
	st.global.u64 	[%rd26], %rd149;
	bra.uni 	BB6_50;

BB6_13:
	setp.eq.s32	%p17, %r44, 9;
	@%p17 bra 	BB6_35;
	bra.uni 	BB6_14;

BB6_35:
	mov.u32 	%r207, 255;
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd152, %rd25, %rd23;
	cvt.u32.u64	%r159, %rd152;
	and.b32  	%r160, %r50, 31;
	shl.b32 	%r162, %r207, %r160;
	and.b32  	%r163, %r159, %r162;
	setp.ne.s32	%p40, %r163, 0;
	@%p40 bra 	BB6_37;

	shr.s32 	%r164, %r50, 5;
	sub.s32 	%r165, %r164, %r42;
	st.shared.u32 	[%r11+1016], %r165;

BB6_37:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_41:
	setp.eq.s32	%p45, %r40, 1;
	@%p45 bra 	BB6_45;
	bra.uni 	BB6_42;

BB6_45:
	fma.rm.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB6_47;

BB6_14:
	setp.eq.s32	%p18, %r44, 7;
	@%p18 bra 	BB6_34;
	bra.uni 	BB6_15;

BB6_34:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r157, %rd152;
	and.b32  	%r158, %r157, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd23, %r158;
	sub.u32 	%amt2, 64, %r158;
	shl.b64 	%rhs, %rd23, %amt2;
	add.u64 	%rd152, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_42:
	setp.eq.s32	%p46, %r40, 2;
	@%p46 bra 	BB6_44;
	bra.uni 	BB6_43;

BB6_44:
	fma.rp.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB6_47;

BB6_15:
	setp.eq.s32	%p19, %r44, 11;
	@%p19 bra 	BB6_33;
	bra.uni 	BB6_16;

BB6_33:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r151,%r152}, %rd23;
	// inline asm
	mov.u32 	%r155, 6175;
	mov.u32 	%r156, 1;
	shfl.sync.bfly.b32 	%r154|%p38, %r152, %r156, %r155, %r21;
	shfl.sync.bfly.b32 	%r153|%p39, %r151, %r156, %r155, %r21;
	// inline asm
	mov.b64 %rd152, {%r153,%r154};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB6_49;

BB6_43:
	fma.rz.f64 	%fd48, %fd13, %fd14, %fd15;

BB6_47:
	mov.b64 	 %rd152, %fd48;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB6_49:
	st.shared.u64 	[%r46], %rd152;
	// inline asm
	// EXECUTION END
	// inline asm

BB6_50:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	bar.warp.sync 	%r25;
	ld.shared.v2.u32 	{%r174, %r40}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r177, %r42, %r174;
	add.s32 	%r216, %r177, 1;
	setp.lt.u32	%p53, %r216, %r17;
	@%p53 bra 	BB6_6;

BB6_51:
	shr.u32 	%r192, %r73, 24;
	add.s32 	%r191, %r11, %r192;
	bfe.u32 	%r190, %r73, 16, 8;
	add.s32 	%r189, %r11, %r190;
	ld.param.u32 	%r188, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_5];
	ld.shared.u32 	%r180, [%r189];
	xor.b32  	%r181, %r180, %r215;
	ld.shared.u32 	%r182, [%r191];
	xor.b32  	%r183, %r181, %r182;
	and.b32  	%r219, %r183, 2147483584;
	cvt.u64.u32	%rd126, %r218;
	add.s64 	%rd127, %rd13, %rd126;
	add.s64 	%rd128, %rd16, %rd127;
	ld.global.u64 	%rd129, [%rd128];
	ld.shared.u64 	%rd130, [%r19];
	xor.b64  	%rd153, %rd129, %rd130;
	ld.shared.u64 	%rd131, [%r19+64];
	ld.shared.u64 	%rd132, [%r19+128];
	st.shared.u64 	[%r19], %rd153;
	xor.b64  	%rd133, %rd132, %rd131;
	st.global.u64 	[%rd22], %rd153;
	st.global.u64 	[%rd21], %rd133;
	add.s32 	%r210, %r210, 1;
	setp.lt.u32	%p54, %r210, %r188;
	mov.u32 	%r211, 0;
	mov.u32 	%r212, %r211;
	mov.u32 	%r215, %r218;
	@%p54 bra 	BB6_5;
	bra.uni 	BB6_53;

BB6_52:
	ld.shared.u64 	%rd153, [%r19];
	mov.u32 	%r218, %r215;

BB6_53:
	mov.u32 	%r197, %tid.x;
	mov.u32 	%r196, %ntid.x;
	mov.u32 	%r195, %ctaid.x;
	mad.lo.s32 	%r194, %r195, %r196, %r197;
	shr.u32 	%r193, %r194, 3;
	cvt.u64.u32	%rd142, %r193;
	ld.param.u64 	%rd141, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_0];
	shl.b64 	%rd49, %rd142, 8;
	add.s64 	%rd134, %rd49, %rd10;
	cvta.to.global.u64 	%rd50, %rd141;
	shl.b64 	%rd135, %rd134, 3;
	add.s64 	%rd51, %rd50, %rd135;
	st.global.u64 	[%rd51], %rd153;
	setp.ne.s32	%p55, %r13, 0;
	@%p55 bra 	BB6_55;

	ld.param.u64 	%rd146, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_1];
	mov.u32 	%r202, %tid.x;
	mov.u32 	%r201, %ntid.x;
	mov.u32 	%r200, %ctaid.x;
	mad.lo.s32 	%r199, %r200, %r201, %r202;
	shr.u32 	%r198, %r199, 3;
	mul.wide.u32 	%rd145, %r198, 4;
	cvta.to.global.u64 	%rd144, %rd146;
	add.s64 	%rd143, %rd144, %rd145;
	st.global.u32 	[%rd143], %r40;

BB6_55:
	ld.param.s8 	%rs5, [_Z10execute_vmILi8ELb0EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs4, %rs5, 255;
	setp.eq.s16	%p56, %rs4, 0;
	@%p56 bra 	BB6_57;

	ld.shared.u64 	%rd136, [%r19+64];
	ld.shared.f64 	%fd46, [%r19+128];
	mov.b64 	 %rd137, %fd46;
	xor.b64  	%rd138, %rd137, %rd136;
	st.global.u64 	[%rd51+64], %rd138;
	st.global.f64 	[%rd51+128], %fd46;
	bra.uni 	BB6_59;

BB6_57:
	@%p55 bra 	BB6_59;

	shl.b64 	%rd139, %rd49, 3;
	add.s64 	%rd140, %rd50, %rd139;
	st.global.u32 	[%rd140+128], %r219;
	st.global.u32 	[%rd140+132], %r218;

BB6_59:
	ret;
}

	// .globl	_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 32, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<328>;
	.reg .f64 	%fd<49>;
	.reg .b64 	%rd<169>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd40, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd41, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd42, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd43, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r57, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_6];
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r308, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r308, 4095;
	@%p1 bra 	BB7_3;

	mov.u32 	%r59, _ZZ10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r307, %r59, %r308;
	cvt.u64.u32	%rd1, %r4;
	cvt.u64.u32	%rd44, %r308;
	mul.wide.u32 	%rd45, %r5, 4096;
	add.s64 	%rd46, %rd45, %rd44;
	cvta.to.global.u64 	%rd47, %rd40;
	add.s64 	%rd160, %rd47, %rd46;

BB7_2:
	ld.global.u64 	%rd48, [%rd160];
	st.shared.u64 	[%r307], %rd48;
	add.s64 	%rd160, %rd160, %rd1;
	add.s32 	%r307, %r307, %r4;
	add.s32 	%r308, %r308, %r4;
	setp.lt.u32	%p2, %r308, 4096;
	@%p2 bra 	BB7_2;

BB7_3:
	bar.warp.sync 	-1;
	shl.b32 	%r60, %r1, 7;
	and.b32  	%r61, %r60, -2048;
	mov.u32 	%r62, _ZZ10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r62, %r61;
	mad.lo.s32 	%r63, %r5, %r3, %r1;
	shr.u32 	%r64, %r63, 4;
	ld.shared.v2.u32 	{%r314, %r313}, [%r11+128];
	ld.shared.u32 	%r14, [%r11+136];
	ld.shared.v2.u64 	{%rd49, %rd50}, [%r11+144];
	ld.shared.u32 	%r15, [%r11+160];
	cvta.to.global.u64 	%rd51, %rd41;
	mul.wide.u32 	%rd52, %r64, 4;
	add.s64 	%rd53, %rd51, %rd52;
	ld.global.u32 	%r33, [%rd53];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p3, %rs3, 0;
	selp.b32	%r311, 0, %r314, %p3;
	setp.eq.s32	%p4, %r57, 0;
	@%p4 bra 	BB7_4;

	ld.shared.u32 	%rd55, [%r11+140];
	and.b32  	%r73, %r63, 15;
	shl.b32 	%r74, %r73, 3;
	cvt.u64.u32	%rd56, %r74;
	add.s64 	%rd7, %rd55, %rd56;
	selp.b32	%r312, 0, %r313, %p3;
	mov.u32 	%r310, 0;
	cvta.to.global.u64 	%rd130, %rd43;

BB7_6:
	.pragma "nounroll";
	setp.gt.u32	%p6, %r73, 7;
	@%p6 bra 	BB7_8;

	setp.lt.u32	%p7, %r73, 4;
	shl.b32 	%r85, %r63, 1;
	and.b32  	%r86, %r85, 30;
	cvt.u64.u32	%rd57, %r86;
	shr.u32 	%r87, %r1, 4;
	mul.wide.u32 	%rd58, %r87, 256;
	add.s64 	%rd59, %rd58, %rd57;
	add.s64 	%rd60, %rd59, 8;
	add.s32 	%r88, %r73, -4;
	shl.b32 	%r89, %r88, 1;
	cvt.u64.u32	%rd61, %r89;
	add.s64 	%rd62, %rd58, %rd61;
	add.s64 	%rd63, %rd62, 16;
	selp.b64	%rd64, %rd60, %rd63, %p7;
	cvt.u32.u64	%r90, %rd64;
	shl.b32 	%r91, %r90, 3;
	or.b32  	%r92, %r91, 8;
	add.s32 	%r94, %r62, %r92;
	add.s32 	%r95, %r62, %r91;
	bfe.u32 	%r96, %r14, 8, 8;
	cvt.u32.u64	%r97, %rd58;
	shl.b32 	%r98, %r97, 3;
	add.s32 	%r99, %r62, %r98;
	add.s32 	%r100, %r99, %r96;
	and.b32  	%r101, %r14, 255;
	add.s32 	%r102, %r99, %r101;
	ld.shared.u64 	%rd65, [%r100];
	ld.shared.u64 	%rd66, [%r102];
	xor.b64  	%rd67, %rd65, %rd66;
	mov.b64	{%r103, %r104}, %rd67;
	xor.b32  	%r105, %r103, %r312;
	xor.b32  	%r106, %r104, %r311;
	and.b32  	%r312, %r105, 2097088;
	and.b32  	%r311, %r106, 2097088;
	cvt.u64.u32	%rd68, %r312;
	shl.b32 	%r107, %r63, 3;
	and.b32  	%r108, %r107, 120;
	cvt.u64.u32	%rd69, %r108;
	mul.lo.s32 	%r110, %r64, 2097216;
	cvt.u64.u32	%rd70, %r110;
	add.s64 	%rd71, %rd69, %rd70;
	add.s64 	%rd72, %rd71, %rd68;
	cvta.to.global.u64 	%rd73, %rd42;
	add.s64 	%rd162, %rd73, %rd72;
	cvt.u64.u32	%rd74, %r311;
	add.s64 	%rd75, %rd71, %rd74;
	add.s64 	%rd161, %rd73, %rd75;
	cvt.u64.u32	%rd76, %r73;
	add.s64 	%rd77, %rd76, %rd58;
	cvt.u32.u64	%r111, %rd77;
	shl.b32 	%r112, %r111, 3;
	add.s32 	%r309, %r62, %r112;
	ld.shared.u64 	%rd78, [%r309];
	ld.global.u64 	%rd79, [%rd162];
	xor.b64  	%rd80, %rd78, %rd79;
	st.shared.u64 	[%r309], %rd80;
	ld.global.v2.u32 	{%r113, %r114}, [%rd161];
	cvt.rn.f64.s32	%fd21, %r113;
	mov.b64 	 %rd81, %fd21;
	selp.b64	%rd82, -1, 72057594037927935, %p7;
	and.b64  	%rd83, %rd81, %rd82;
	selp.b64	%rd84, 0, %rd49, %p7;
	or.b64  	%rd85, %rd83, %rd84;
	st.shared.u64 	[%r95], %rd85;
	cvt.rn.f64.s32	%fd22, %r114;
	mov.b64 	 %rd86, %fd22;
	and.b64  	%rd87, %rd86, %rd82;
	selp.b64	%rd88, 0, %rd50, %p7;
	or.b64  	%rd89, %rd87, %rd88;
	st.shared.u64 	[%r94], %rd89;

BB7_8:
	st.shared.u32 	[%r11+1020], %r33;
	setp.eq.s32	%p8, %r15, 0;
	mov.u32 	%r319, 0;
	@%p8 bra 	BB7_54;
	bra.uni 	BB7_9;

BB7_19:
	setp.eq.s32	%p21, %r37, 14;
	@%p21 bra 	BB7_33;
	bra.uni 	BB7_20;

BB7_33:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd33, %rd14;
	// inline asm
	rsqrt.approx.ftz.f64 %fd32, %fd33;
	// inline asm
	mul.f64 	%fd34, %fd33, %fd32;
	mul.f64 	%fd35, %fd32, 0dBFE0000000000000;
	mov.f64 	%fd36, 0d3FE0000000000000;
	fma.rn.f64 	%fd37, %fd35, %fd34, %fd36;
	fma.rn.f64 	%fd7, %fd34, %fd37, %fd34;
	mul.f64 	%fd38, %fd32, 0d3FE0000000000000;
	fma.rn.f64 	%fd8, %fd38, %fd37, %fd38;
	neg.f64 	%fd39, %fd7;
	fma.rn.f64 	%fd9, %fd39, %fd7, %fd33;
	fma.rn.f64 	%fd47, %fd9, %fd8, %fd7;
	setp.eq.s32	%p35, %r33, 0;
	@%p35 bra 	BB7_35;

	fma.rm.f64 	%fd40, %fd9, %fd8, %fd7;
	and.b32  	%r195, %r33, 1;
	setp.eq.b32	%p36, %r195, 1;
	not.pred 	%p37, %p36;
	selp.f64	%fd41, %fd47, %fd40, %p37;
	setp.eq.s32	%p38, %r33, 2;
	fma.rp.f64 	%fd42, %fd9, %fd8, %fd7;
	selp.f64	%fd47, %fd42, %fd41, %p38;

BB7_35:
	setp.eq.s64	%p39, %rd14, 9218868437227405312;
	selp.f64	%fd43, %fd33, %fd47, %p39;
	mov.b64 	 %rd168, %fd43;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_20:
	setp.eq.s32	%p22, %r37, 6;
	@%p22 bra 	BB7_32;
	bra.uni 	BB7_21;

BB7_32:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd168, %rd14, %rd168;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_21:
	setp.eq.s32	%p23, %r37, 4;
	@%p23 bra 	BB7_31;
	bra.uni 	BB7_22;

BB7_31:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd168, %rd14, %rd168;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_22:
	setp.eq.s32	%p24, %r37, 8;
	@%p24 bra 	BB7_30;
	bra.uni 	BB7_23;

BB7_30:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r40], %rd14;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_23:
	setp.eq.s32	%p25, %r37, 15;
	@%p25 bra 	BB7_27;
	bra.uni 	BB7_24;

BB7_27:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	shl.b32 	%r191, %r146, 5;
	shr.u64 	%rd96, %rd168, %r191;
	cvt.u32.u64	%r192, %rd96;
	cvt.rn.f64.s32	%fd25, %r192;
	mov.b64 	 %rd97, %fd25;
	and.b64  	%rd98, %rd97, 72057594037927935;
	setp.eq.s32	%p27, %r146, 0;
	selp.b64	%rd99, %rd49, %rd50, %p27;
	or.b64  	%rd100, %rd98, %rd99;
	mov.b64 	 %fd24, %rd100;
	// inline asm
	rcp.approx.ftz.f64 %fd23, %fd24;
	// inline asm
	neg.f64 	%fd26, %fd24;
	mov.f64 	%fd27, 0d3FF0000000000000;
	fma.rn.f64 	%fd28, %fd26, %fd23, %fd27;
	fma.rn.f64 	%fd2, %fd23, %fd28, %fd23;
	mov.b64 	 %fd3, %rd14;
	mul.f64 	%fd4, %fd3, %fd2;
	fma.rn.f64 	%fd5, %fd26, %fd4, %fd3;
	fma.rn.f64 	%fd29, %fd2, %fd5, %fd4;
	mov.b64 	 %rd166, %fd29;
	setp.eq.s32	%p28, %r33, 0;
	@%p28 bra 	BB7_29;

	and.b32  	%r193, %r33, 1;
	setp.eq.b32	%p29, %r193, 1;
	not.pred 	%p30, %p29;
	fma.rm.f64 	%fd30, %fd2, %fd5, %fd4;
	mov.b64 	 %rd101, %fd30;
	selp.b64	%rd102, %rd166, %rd101, %p30;
	fma.rp.f64 	%fd31, %fd2, %fd5, %fd4;
	mov.b64 	 %rd103, %fd31;
	setp.eq.s32	%p31, %r33, 2;
	selp.b64	%rd166, %rd103, %rd102, %p31;

BB7_29:
	and.b64  	%rd104, %rd166, 9218868437227405312;
	mov.u64 	%rd105, 9218868437227405312;
	setp.eq.s64	%p32, %rd104, 9218868437227405312;
	and.b32  	%r194, %r33, 1;
	cvt.u64.u32	%rd106, %r194;
	sub.s64 	%rd107, %rd105, %rd106;
	selp.b64	%rd108, %rd107, %rd166, %p32;
	setp.eq.s64	%p33, %rd14, 9218868437227405312;
	selp.b64	%rd109, 9218868437227405312, %rd108, %p33;
	setp.eq.f64	%p34, %fd3, %fd24;
	selp.b64	%rd168, 4607182418800017408, %rd109, %p34;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_24:
	setp.eq.s32	%p26, %r37, 5;
	@%p26 bra 	BB7_26;
	bra.uni 	BB7_25;

BB7_26:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd168, %rd14;
	bra.uni 	BB7_52;

BB7_25:
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd94, %rd168, %r41;
	mov.u32 	%r175, 64;
	sub.s32 	%r176, %r175, %r41;
	shl.b64 	%rd95, %rd168, %r176;
	cvt.u32.u64	%r177, %rd94;
	cvt.u32.u64	%r178, %rd95;
	or.b32  	%r179, %r177, %r178;
	and.b32  	%r180, %r179, 3;
	st.shared.u32 	[%r11+1020], %r180;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB7_53;

BB7_9:
	shl.b32 	%r123, %r319, 2;
	add.s32 	%r124, %r11, %r123;
	st.shared.u32 	[%r11+1016], %r319;
	ld.shared.u32 	%r130, [%r124+1024];
	bfe.u32 	%r131, %r130, 24, 4;
	shr.u32 	%r34, %r130, 28;
	sub.s32 	%r35, %r131, %r34;
	setp.lt.u32	%p9, %r131, %r73;
	@%p9 bra 	BB7_53;

	sub.s32 	%r137, %r73, %r34;
	setp.lt.s32	%p10, %r137, %r34;
	bfe.u32 	%r138, %r63, 1, 3;
	selp.b32	%r139, %r138, %r137, %p10;
	add.s32 	%r140, %r139, %r319;
	shl.b32 	%r141, %r140, 2;
	add.s32 	%r142, %r11, %r141;
	ld.shared.u32 	%r36, [%r142+1024];
	bfe.u32 	%r41, %r36, 6, 8;
	shl.b32 	%r143, %r41, 2;
	add.s32 	%r144, %r11, %r143;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r37, %r36, 20, 4;
	and.b32  	%r38, %r36, 16384;
	selp.b32	%r145, 4, 3, %p10;
	and.b32  	%r146, %r63, 1;
	shl.b32 	%r147, %r146, 3;
	add.s32 	%r148, %r147, 64;
	selp.b32	%r149, %r148, 0, %p10;
	and.b32  	%r150, %r36, 7;
	shl.b32 	%r151, %r150, %r145;
	add.s32 	%r152, %r151, %r149;
	and.b32  	%r153, %r36, 56;
	setp.eq.s32	%p11, %r38, 0;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r154, %r147, 192;
	selp.b32	%r155, %r154, 0, %p12;
	add.s32 	%r156, %r155, %r153;
	add.s32 	%r39, %r11, %r152;
	add.s32 	%r40, %r11, %r156;
	ld.shared.u64 	%rd14, [%r39];
	ld.shared.u64 	%rd165, [%r40];
	ld.shared.u32 	%r42, [%r144+256];
	cvt.s64.s32	%rd16, %r42;
	ld.shared.u32 	%r43, [%r144+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p11 bra 	BB7_14;

	cvt.u32.u64	%r158, %rd16;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r157, %r158, 21, 5;
	// inline asm
	mov.u32 	%r159, -1;
	shr.u32 	%r160, %r159, %r157;
	add.s32 	%r161, %r160, -7;
	setp.eq.s32	%p13, %r157, 11;
	cvt.u32.u64	%r162, %rd165;
	selp.b32	%r163, 0, %r162, %p13;
	cvt.u32.u64	%r164, %rd14;
	setp.ne.s32	%p14, %r37, 10;
	selp.b32	%r165, %r163, %r164, %p14;
	add.s32 	%r166, %r165, %r42;
	and.b32  	%r167, %r166, %r161;
	cvt.u64.u32	%rd90, %r167;
	mul.lo.s32 	%r173, %r64, 2097216;
	cvt.u64.u32	%rd91, %r173;
	add.s64 	%rd92, %rd90, %rd91;
	cvta.to.global.u64 	%rd93, %rd42;
	add.s64 	%rd17, %rd93, %rd92;
	@%p14 bra 	BB7_13;
	bra.uni 	BB7_12;

BB7_13:
	ld.global.u64 	%rd165, [%rd17];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB7_14:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r174, %r36, 131072;
	setp.eq.s32	%p15, %r174, 0;
	selp.b64	%rd168, %rd165, %rd16, %p15;
	setp.lt.u32	%p16, %r37, 4;
	@%p16 bra 	BB7_51;
	bra.uni 	BB7_15;

BB7_51:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r230, %r36, 524288;
	setp.eq.s32	%p49, %r230, 0;
	neg.s64 	%rd115, %rd168;
	selp.b64	%rd116, %rd168, %rd115, %p49;
	setp.eq.s32	%p50, %r37, 0;
	selp.b64	%rd117, %rd16, 0, %p50;
	add.s64 	%rd118, %rd117, %rd14;
	bfe.u32 	%r231, %r36, 15, 2;
	shl.b64 	%rd119, %rd116, %r231;
	setp.lt.u32	%p51, %r37, 2;
	selp.b64	%rd120, %rd119, 0, %p51;
	add.s64 	%rd121, %rd118, %rd120;
	mov.b64	%rd122, {%r42, %r43};
	and.b32  	%r232, %r36, 262144;
	setp.eq.s32	%p52, %r232, 0;
	selp.b64	%rd123, %rd116, %rd122, %p52;
	setp.eq.s32	%p53, %r37, 2;
	selp.b64	%rd124, %rd123, 1, %p53;
	mul.lo.s64 	%rd125, %rd121, %rd124;
	setp.eq.s32	%p54, %r37, 3;
	selp.b64	%rd126, %rd123, 0, %p54;
	xor.b64  	%rd168, %rd125, %rd126;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_15:
	setp.eq.s32	%p17, %r37, 12;
	@%p17 bra 	BB7_41;
	bra.uni 	BB7_16;

BB7_41:
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p11 bra 	BB7_43;

	shl.b32 	%r225, %r146, 5;
	shr.u64 	%rd112, %rd168, %r225;
	cvt.u32.u64	%r226, %rd112;
	cvt.rn.f64.s32	%fd44, %r226;
	mov.b64 	 %rd168, %fd44;

BB7_43:
	and.b32  	%r227, %r36, 524288;
	setp.eq.s32	%p44, %r227, 0;
	xor.b64  	%rd113, %rd168, -9223372036854775808;
	selp.b64	%rd114, %rd168, %rd113, %p44;
	shr.u32 	%r228, %r36, 15;
	and.b32  	%r229, %r228, 1;
	setp.eq.b32	%p45, %r229, 1;
	mov.b64 	 %fd13, %rd14;
	mov.b64 	 %fd45, %rd114;
	selp.f64	%fd14, %fd45, 0d3FF0000000000000, %p45;
	selp.f64	%fd15, 0d0000000000000000, %fd45, %p45;
	setp.eq.s32	%p46, %r33, 0;
	@%p46 bra 	BB7_49;
	bra.uni 	BB7_44;

BB7_49:
	fma.rn.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB7_50;

BB7_12:
	st.global.u64 	[%rd17], %rd165;
	bra.uni 	BB7_53;

BB7_16:
	setp.eq.s32	%p18, %r37, 9;
	@%p18 bra 	BB7_38;
	bra.uni 	BB7_17;

BB7_38:
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd168, %rd16, %rd14;
	cvt.u32.u64	%r213, %rd168;
	and.b32  	%r214, %r43, 31;
	mov.u32 	%r215, 255;
	shl.b32 	%r216, %r215, %r214;
	and.b32  	%r217, %r213, %r216;
	setp.ne.s32	%p42, %r217, 0;
	@%p42 bra 	BB7_40;

	shr.s32 	%r218, %r43, 5;
	sub.s32 	%r219, %r218, %r35;
	st.shared.u32 	[%r11+1016], %r219;

BB7_40:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_44:
	setp.eq.s32	%p47, %r33, 1;
	@%p47 bra 	BB7_48;
	bra.uni 	BB7_45;

BB7_48:
	fma.rm.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB7_50;

BB7_17:
	setp.eq.s32	%p19, %r37, 7;
	@%p19 bra 	BB7_37;
	bra.uni 	BB7_18;

BB7_37:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r211, %rd168;
	and.b32  	%r212, %r211, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd14, %r212;
	sub.u32 	%amt2, 64, %r212;
	shl.b64 	%rhs, %rd14, %amt2;
	add.u64 	%rd168, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_45:
	setp.eq.s32	%p48, %r33, 2;
	@%p48 bra 	BB7_47;
	bra.uni 	BB7_46;

BB7_47:
	fma.rp.f64 	%fd48, %fd13, %fd14, %fd15;
	bra.uni 	BB7_50;

BB7_18:
	setp.eq.s32	%p20, %r37, 11;
	@%p20 bra 	BB7_36;
	bra.uni 	BB7_19;

BB7_36:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r196,%r197}, %rd14;
	// inline asm
	and.b32  	%r204, %r63, 14;
	and.b32  	%r205, %r1, -16;
	add.s32 	%r206, %r204, %r205;
	mov.u32 	%r207, 3;
	shl.b32 	%r208, %r207, %r206;
	mov.u32 	%r209, 6175;
	mov.u32 	%r210, 1;
	shfl.sync.bfly.b32 	%r199|%p40, %r197, %r210, %r209, %r208;
	shfl.sync.bfly.b32 	%r198|%p41, %r196, %r210, %r209, %r208;
	// inline asm
	mov.b64 %rd168, {%r198,%r199};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB7_52;

BB7_46:
	fma.rz.f64 	%fd48, %fd13, %fd14, %fd15;

BB7_50:
	mov.b64 	 %rd168, %fd48;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB7_52:
	st.shared.u64 	[%r39], %rd168;
	// inline asm
	// EXECUTION END
	// inline asm

BB7_53:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	and.b32  	%r234, %r1, -16;
	mov.u32 	%r235, 65535;
	shl.b32 	%r44, %r235, %r234;
	bar.warp.sync 	%r44;
	ld.shared.v2.u32 	{%r236, %r33}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r239, %r35, %r236;
	add.s32 	%r319, %r239, 1;
	setp.lt.u32	%p55, %r319, %r15;
	@%p55 bra 	BB7_9;

BB7_54:
	setp.gt.u32	%p62, %r73, 7;
	@%p62 bra 	BB7_55;

	shr.u32 	%r247, %r14, 24;
	shr.u32 	%r249, %r1, 4;
	mul.wide.u32 	%rd127, %r249, 256;
	cvt.u32.u64	%r250, %rd127;
	shl.b32 	%r251, %r250, 3;
	add.s32 	%r253, %r62, %r251;
	add.s32 	%r254, %r253, %r247;
	bfe.u32 	%r255, %r14, 16, 8;
	add.s32 	%r256, %r253, %r255;
	ld.shared.u32 	%r257, [%r256];
	xor.b32  	%r258, %r257, %r313;
	ld.shared.u32 	%r259, [%r254];
	xor.b32  	%r260, %r258, %r259;
	and.b32  	%r326, %r260, 2147483584;
	cvt.u64.u32	%rd128, %r314;
	add.s64 	%rd129, %rd7, %rd128;
	add.s64 	%rd131, %rd130, %rd129;
	ld.global.u64 	%rd132, [%rd131];
	ld.shared.u64 	%rd133, [%r309];
	xor.b64  	%rd134, %rd132, %rd133;
	st.shared.u64 	[%r309], %rd134;
	st.global.u64 	[%rd161], %rd134;
	cvt.u64.u32	%rd135, %r73;
	add.s64 	%rd136, %rd135, %rd127;
	cvt.u32.u64	%r265, %rd136;
	shl.b32 	%r266, %r265, 3;
	add.s32 	%r267, %r62, %r266;
	ld.shared.u64 	%rd137, [%r267+64];
	ld.shared.u64 	%rd138, [%r267+128];
	xor.b64  	%rd139, %rd138, %rd137;
	st.global.u64 	[%rd162], %rd139;
	mov.u32 	%r312, 0;
	mov.u32 	%r313, %r314;
	mov.u32 	%r311, %r312;
	bra.uni 	BB7_57;

BB7_55:
	mov.u32 	%r326, %r314;

BB7_57:
	ld.param.u32 	%r306, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_5];
	add.s32 	%r310, %r310, 1;
	setp.lt.u32	%p57, %r310, %r306;
	mov.u32 	%r314, %r326;
	@%p57 bra 	BB7_6;
	bra.uni 	BB7_58;

BB7_4:
	mov.u32 	%r326, %r314;

BB7_58:
	and.b32  	%r272, %r63, 15;
	setp.gt.u32	%p58, %r272, 7;
	@%p58 bra 	BB7_65;

	ld.param.u64 	%rd155, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_0];
	cvta.to.global.u64 	%rd37, %rd155;
	mul.wide.u32 	%rd38, %r64, 256;
	cvt.u64.u32	%rd140, %r272;
	shl.b32 	%r279, %r1, 4;
	and.b32  	%r280, %r279, 536870656;
	or.b32  	%r281, %r272, %r280;
	shl.b32 	%r282, %r281, 3;
	add.s32 	%r284, %r62, %r282;
	ld.shared.u64 	%rd141, [%r284];
	or.b64  	%rd142, %rd38, %rd140;
	shl.b64 	%rd143, %rd142, 3;
	add.s64 	%rd39, %rd37, %rd143;
	st.global.u64 	[%rd39], %rd141;
	setp.ne.s32	%p59, %r272, 0;
	@%p59 bra 	BB7_61;

	ld.param.u64 	%rd159, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_1];
	mul.wide.u32 	%rd158, %r64, 4;
	cvta.to.global.u64 	%rd157, %rd159;
	add.s64 	%rd156, %rd157, %rd158;
	st.global.u32 	[%rd156], %r33;

BB7_61:
	ld.param.s8 	%rs6, [_Z10execute_vmILi16ELb0EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs5, %rs6, 255;
	setp.eq.s16	%p60, %rs5, 0;
	@%p60 bra 	BB7_63;

	and.b32  	%r296, %r279, -256;
	cvt.u64.u32	%rd148, %r296;
	add.s64 	%rd149, %rd140, %rd148;
	cvt.u32.u64	%r297, %rd149;
	shl.b32 	%r298, %r297, 3;
	add.s32 	%r300, %r62, %r298;
	ld.shared.u64 	%rd150, [%r300+64];
	ld.shared.f64 	%fd46, [%r300+128];
	mov.b64 	 %rd151, %fd46;
	xor.b64  	%rd152, %rd151, %rd150;
	st.global.u64 	[%rd39+64], %rd152;
	st.global.f64 	[%rd39+128], %fd46;
	bra.uni 	BB7_65;

BB7_63:
	@%p59 bra 	BB7_65;

	shl.b64 	%rd153, %rd38, 3;
	add.s64 	%rd154, %rd37, %rd153;
	st.global.u32 	[%rd154+128], %r326;
	st.global.u32 	[%rd154+132], %r313;

BB7_65:
	ret;
}

	// .globl	_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 16, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<212>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<141>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd49, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd50, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd52, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd51, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r59, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_6];
	cvta.to.global.u64 	%rd1, %rd52;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r199, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r199, 4095;
	@%p1 bra 	BB8_3;

	mov.u32 	%r61, _ZZ10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r198, %r61, %r199;
	cvt.u64.u32	%rd2, %r4;
	cvta.to.global.u64 	%rd53, %rd49;
	cvt.u64.u32	%rd54, %r199;
	mul.wide.u32 	%rd55, %r5, 4096;
	add.s64 	%rd56, %rd55, %rd54;
	add.s64 	%rd135, %rd53, %rd56;

BB8_2:
	ld.global.u64 	%rd57, [%rd135];
	st.shared.u64 	[%r198], %rd57;
	add.s64 	%rd135, %rd135, %rd2;
	add.s32 	%r198, %r198, %r4;
	add.s32 	%r199, %r199, %r4;
	setp.lt.u32	%p2, %r199, 4096;
	@%p2 bra 	BB8_2;

BB8_3:
	bar.warp.sync 	-1;
	shr.u32 	%r62, %r1, 3;
	mul.wide.u32 	%rd6, %r62, 256;
	cvt.u32.u64	%r63, %rd6;
	shl.b32 	%r64, %r63, 3;
	mov.u32 	%r65, _ZZ10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r65, %r64;
	mad.lo.s32 	%r12, %r5, %r3, %r1;
	shr.u32 	%r66, %r12, 3;
	and.b32  	%r13, %r12, 7;
	ld.shared.v2.u32 	{%r210, %r209}, [%r11+128];
	ld.shared.u32 	%r17, [%r11+160];
	cvta.to.global.u64 	%rd58, %rd50;
	mul.wide.u32 	%rd59, %r66, 4;
	add.s64 	%rd8, %rd58, %rd59;
	ld.global.u32 	%r40, [%rd8];
	mul.lo.s32 	%r69, %r66, 2097216;
	cvt.u64.u32	%rd9, %r69;
	cvt.u64.u32	%rd10, %r13;
	or.b32  	%r70, %r13, %r63;
	shl.b32 	%r71, %r70, 3;
	add.s32 	%r19, %r65, %r71;
	setp.eq.s32	%p3, %r59, 0;
	@%p3 bra 	BB8_59;

	add.s32 	%r180, %r11, 128;
	ld.shared.u32 	%r73, [%r180+8];
	and.b32  	%r74, %r12, 1;
	shl.b32 	%r75, %r12, 3;
	and.b32  	%r76, %r75, 8;
	mov.u32 	%r77, 3;
	add.s32 	%r20, %r76, 64;
	add.s32 	%r181, %r11, 128;
	ld.shared.v2.u64 	{%rd60, %rd61}, [%r181+16];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p4, %rs3, 0;
	setp.lt.u32	%p5, %r13, 4;
	add.s32 	%r78, %r13, -4;
	shl.b32 	%r79, %r78, 1;
	cvt.s64.s32	%rd64, %r79;
	add.s64 	%rd65, %rd6, %rd64;
	add.s64 	%rd66, %rd65, 16;
	shl.b32 	%r80, %r13, 1;
	cvt.u64.u32	%rd67, %r80;
	add.s64 	%rd68, %rd6, %rd67;
	add.s64 	%rd69, %rd68, 8;
	selp.b64	%rd70, %rd69, %rd66, %p5;
	selp.b64	%rd11, -1, 72057594037927935, %p5;
	and.b32  	%r81, %r1, -8;
	and.b32  	%r82, %r12, 6;
	add.s32 	%r83, %r82, %r81;
	shl.b32 	%r21, %r77, %r83;
	ld.shared.u32 	%rd71, [%r11+140];
	shl.b32 	%r84, %r13, 3;
	cvt.u64.u32	%rd72, %r84;
	add.s64 	%rd12, %rd72, %rd9;
	add.s64 	%rd73, %rd70, 1;
	cvt.u32.u64	%r85, %rd73;
	shl.b32 	%r86, %r85, 3;
	add.s32 	%r22, %r65, %r86;
	shl.b32 	%r88, %r74, 5;
	cvt.u64.u32	%rd13, %r88;
	add.s64 	%rd14, %rd71, %rd72;
	ld.shared.u64 	%rd140, [%r19];
	cvta.to.global.u64 	%rd16, %rd51;
	shl.b32 	%r25, %r77, %r81;
	setp.eq.s32	%p6, %r74, 0;
	selp.b64	%rd17, %rd60, %rd61, %p6;
	selp.b64	%rd18, 0, %rd61, %p5;
	selp.b64	%rd19, 0, %rd60, %p5;
	cvt.u32.u64	%r89, %rd70;
	shl.b32 	%r90, %r89, 3;
	add.s32 	%r26, %r65, %r90;
	selp.b32	%r201, 0, %r210, %p4;
	selp.b32	%r202, 0, %r209, %p4;
	shr.u32 	%r91, %r73, 24;
	add.s32 	%r29, %r11, %r91;
	bfe.u32 	%r92, %r73, 16, 8;
	add.s32 	%r30, %r11, %r92;
	bfe.u32 	%r93, %r73, 8, 8;
	add.s32 	%r31, %r11, %r93;
	and.b32  	%r94, %r73, 255;
	add.s32 	%r32, %r11, %r94;
	mov.u32 	%r200, 0;

BB8_5:
	.pragma "nounroll";
	mov.u32 	%r36, %r209;
	mov.u32 	%r209, %r210;
	ld.shared.u64 	%rd74, [%r31];
	ld.shared.u64 	%rd75, [%r32];
	xor.b64  	%rd76, %rd74, %rd75;
	mov.b64	{%r95, %r96}, %rd76;
	xor.b32  	%r97, %r95, %r202;
	xor.b32  	%r98, %r96, %r201;
	and.b32  	%r99, %r97, 2097088;
	and.b32  	%r100, %r98, 2097088;
	cvt.u64.u32	%rd77, %r99;
	add.s64 	%rd78, %rd12, %rd77;
	add.s64 	%rd21, %rd1, %rd78;
	cvt.u64.u32	%rd79, %r100;
	add.s64 	%rd80, %rd12, %rd79;
	add.s64 	%rd22, %rd1, %rd80;
	ld.global.u64 	%rd81, [%rd21];
	xor.b64  	%rd82, %rd140, %rd81;
	st.shared.u64 	[%r19], %rd82;
	ld.global.v2.u32 	{%r101, %r102}, [%rd22];
	cvt.rn.f64.s32	%fd20, %r101;
	mov.b64 	 %rd83, %fd20;
	and.b64  	%rd84, %rd83, %rd11;
	or.b64  	%rd85, %rd84, %rd19;
	st.shared.u64 	[%r26], %rd85;
	cvt.rn.f64.s32	%fd21, %r102;
	mov.b64 	 %rd86, %fd21;
	and.b64  	%rd87, %rd86, %rd11;
	or.b64  	%rd88, %rd87, %rd18;
	st.shared.u64 	[%r22], %rd88;
	setp.gt.u32	%p7, %r13, 1;
	@%p7 bra 	BB8_58;

	setp.eq.s32	%p8, %r17, 0;
	mov.u32 	%r206, 0;
	add.s32 	%r182, %r11, 1016;
	st.shared.u32 	[%r182+4], %r40;
	@%p8 bra 	BB8_58;
	bra.uni 	BB8_7;

BB8_17:
	setp.eq.s32	%p21, %r44, 14;
	@%p21 bra 	BB8_34;
	bra.uni 	BB8_18;

BB8_34:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd7, %rd23;
	setp.eq.s32	%p29, %r40, 0;
	@%p29 bra 	BB8_38;
	bra.uni 	BB8_35;

BB8_38:
	sqrt.rn.f64 	%fd27, %fd7;
	bra.uni 	BB8_39;

BB8_18:
	setp.eq.s32	%p22, %r44, 6;
	@%p22 bra 	BB8_33;
	bra.uni 	BB8_19;

BB8_33:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd139, %rd23, %rd139;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_35:
	setp.eq.s32	%p30, %r40, 2;
	@%p30 bra 	BB8_37;
	bra.uni 	BB8_36;

BB8_37:
	sqrt.rp.f64 	%fd27, %fd7;
	bra.uni 	BB8_39;

BB8_19:
	setp.eq.s32	%p23, %r44, 4;
	@%p23 bra 	BB8_32;
	bra.uni 	BB8_20;

BB8_32:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd139, %rd23, %rd139;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_36:
	sqrt.rm.f64 	%fd27, %fd7;

BB8_39:
	mov.b64 	 %rd139, %fd27;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_20:
	setp.eq.s32	%p24, %r44, 8;
	@%p24 bra 	BB8_31;
	bra.uni 	BB8_21;

BB8_31:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r47], %rd23;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_21:
	setp.eq.s32	%p25, %r44, 15;
	@%p25 bra 	BB8_25;
	bra.uni 	BB8_22;

BB8_25:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	cvt.u32.u64	%r145, %rd13;
	shr.u64 	%rd93, %rd139, %r145;
	cvt.u32.u64	%r146, %rd93;
	cvt.rn.f64.s32	%fd22, %r146;
	mov.b64 	 %rd94, %fd22;
	and.b64  	%rd95, %rd94, 72057594037927935;
	or.b64  	%rd96, %rd95, %rd17;
	mov.b64 	 %fd1, %rd23;
	mov.b64 	 %fd2, %rd96;
	setp.eq.s32	%p27, %r40, 0;
	@%p27 bra 	BB8_29;
	bra.uni 	BB8_26;

BB8_29:
	div.rn.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB8_30;

BB8_22:
	setp.eq.s32	%p26, %r44, 5;
	@%p26 bra 	BB8_24;
	bra.uni 	BB8_23;

BB8_24:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd139, %rd23;
	bra.uni 	BB8_56;

BB8_26:
	setp.eq.s32	%p28, %r40, 2;
	@%p28 bra 	BB8_28;
	bra.uni 	BB8_27;

BB8_28:
	div.rp.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB8_30;

BB8_23:
	bfe.u32 	%r196, %r43, 6, 8;
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd91, %rd139, %r196;
	mov.u32 	%r139, 64;
	sub.s32 	%r140, %r139, %r196;
	shl.b64 	%rd92, %rd139, %r140;
	cvt.u32.u64	%r141, %rd91;
	cvt.u32.u64	%r142, %rd92;
	or.b32  	%r143, %r141, %r142;
	and.b32  	%r144, %r143, 3;
	add.s32 	%r183, %r11, 1016;
	st.shared.u32 	[%r183+4], %r144;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB8_57;

BB8_27:
	div.rm.f64 	%fd26, %fd1, %fd2;

BB8_30:
	mov.b64 	 %rd139, %fd26;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_7:
	shl.b32 	%r106, %r206, 2;
	add.s32 	%r107, %r11, %r106;
	st.shared.u32 	[%r11+1016], %r206;
	ld.shared.u32 	%r108, [%r107+1024];
	bfe.u32 	%r109, %r108, 24, 1;
	bfe.u32 	%r41, %r108, 28, 1;
	sub.s32 	%r42, %r109, %r41;
	cvt.u32.u64	%r110, %rd10;
	setp.lt.u32	%p9, %r109, %r110;
	@%p9 bra 	BB8_57;

	shr.u32 	%r195, %r13, 1;
	sub.s32 	%r111, %r13, %r41;
	setp.lt.s32	%p10, %r111, %r41;
	selp.b32	%r112, %r195, %r111, %p10;
	add.s32 	%r113, %r112, %r206;
	shl.b32 	%r114, %r113, 2;
	add.s32 	%r115, %r11, %r114;
	ld.shared.u32 	%r43, [%r115+1024];
	bfe.u32 	%r48, %r43, 6, 8;
	shl.b32 	%r116, %r48, 2;
	add.s32 	%r117, %r11, %r116;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r44, %r43, 20, 4;
	and.b32  	%r45, %r43, 16384;
	selp.b32	%r118, 4, 3, %p10;
	selp.b32	%r119, %r20, 0, %p10;
	and.b32  	%r120, %r43, 7;
	shl.b32 	%r121, %r120, %r118;
	add.s32 	%r122, %r121, %r119;
	and.b32  	%r123, %r43, 56;
	setp.eq.s32	%p11, %r45, 0;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r124, %r20, 128;
	selp.b32	%r125, %r124, 0, %p12;
	add.s32 	%r126, %r125, %r123;
	add.s32 	%r46, %r11, %r122;
	add.s32 	%r47, %r11, %r126;
	ld.shared.u64 	%rd23, [%r46];
	ld.shared.u64 	%rd137, [%r47];
	ld.shared.u32 	%r49, [%r117+256];
	cvt.s64.s32	%rd25, %r49;
	ld.shared.u32 	%r50, [%r117+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p11 bra 	BB8_12;

	cvt.u32.u64	%r128, %rd25;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r127, %r128, 21, 5;
	// inline asm
	mov.u32 	%r129, -1;
	shr.u32 	%r130, %r129, %r127;
	add.s32 	%r131, %r130, -7;
	setp.eq.s32	%p13, %r127, 11;
	cvt.u32.u64	%r132, %rd137;
	selp.b32	%r133, 0, %r132, %p13;
	cvt.u32.u64	%r134, %rd23;
	setp.ne.s32	%p14, %r44, 10;
	selp.b32	%r135, %r133, %r134, %p14;
	add.s32 	%r136, %r135, %r49;
	and.b32  	%r137, %r136, %r131;
	cvt.u64.u32	%rd89, %r137;
	add.s64 	%rd90, %rd89, %rd9;
	add.s64 	%rd26, %rd1, %rd90;
	@%p14 bra 	BB8_11;
	bra.uni 	BB8_10;

BB8_11:
	ld.global.u64 	%rd137, [%rd26];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB8_12:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r138, %r43, 131072;
	setp.eq.s32	%p15, %r138, 0;
	selp.b64	%rd139, %rd137, %rd25, %p15;
	setp.lt.u32	%p16, %r44, 4;
	@%p16 bra 	BB8_55;
	bra.uni 	BB8_13;

BB8_55:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r167, %r43, 524288;
	setp.eq.s32	%p40, %r167, 0;
	neg.s64 	%rd102, %rd139;
	selp.b64	%rd103, %rd139, %rd102, %p40;
	setp.eq.s32	%p41, %r44, 0;
	selp.b64	%rd104, %rd25, 0, %p41;
	add.s64 	%rd105, %rd104, %rd23;
	bfe.u32 	%r168, %r43, 15, 2;
	shl.b64 	%rd106, %rd103, %r168;
	setp.lt.u32	%p42, %r44, 2;
	selp.b64	%rd107, %rd106, 0, %p42;
	add.s64 	%rd108, %rd105, %rd107;
	mov.b64	%rd109, {%r49, %r50};
	and.b32  	%r169, %r43, 262144;
	setp.eq.s32	%p43, %r169, 0;
	selp.b64	%rd110, %rd103, %rd109, %p43;
	setp.eq.s32	%p44, %r44, 2;
	selp.b64	%rd111, %rd110, 1, %p44;
	mul.lo.s64 	%rd112, %rd108, %rd111;
	setp.eq.s32	%p45, %r44, 3;
	selp.b64	%rd113, %rd110, 0, %p45;
	xor.b64  	%rd139, %rd112, %rd113;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_13:
	setp.eq.s32	%p17, %r44, 12;
	@%p17 bra 	BB8_45;
	bra.uni 	BB8_14;

BB8_45:
	and.b32  	%r197, %r43, 16384;
	setp.eq.s32	%p51, %r197, 0;
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p51 bra 	BB8_47;

	cvt.u32.u64	%r162, %rd13;
	shr.u64 	%rd99, %rd139, %r162;
	cvt.u32.u64	%r163, %rd99;
	cvt.rn.f64.s32	%fd23, %r163;
	mov.b64 	 %rd139, %fd23;

BB8_47:
	and.b32  	%r164, %r43, 524288;
	setp.eq.s32	%p35, %r164, 0;
	xor.b64  	%rd100, %rd139, -9223372036854775808;
	selp.b64	%rd101, %rd139, %rd100, %p35;
	shr.u32 	%r165, %r43, 15;
	and.b32  	%r166, %r165, 1;
	setp.eq.b32	%p36, %r166, 1;
	mov.b64 	 %fd12, %rd23;
	mov.b64 	 %fd24, %rd101;
	selp.f64	%fd13, %fd24, 0d3FF0000000000000, %p36;
	selp.f64	%fd14, 0d0000000000000000, %fd24, %p36;
	setp.eq.s32	%p37, %r40, 0;
	@%p37 bra 	BB8_53;
	bra.uni 	BB8_48;

BB8_53:
	fma.rn.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB8_54;

BB8_10:
	st.global.u64 	[%rd26], %rd137;
	bra.uni 	BB8_57;

BB8_14:
	setp.eq.s32	%p18, %r44, 9;
	@%p18 bra 	BB8_42;
	bra.uni 	BB8_15;

BB8_42:
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd139, %rd25, %rd23;
	cvt.u32.u64	%r155, %rd139;
	and.b32  	%r156, %r50, 31;
	mov.u32 	%r157, 255;
	shl.b32 	%r158, %r157, %r156;
	and.b32  	%r159, %r155, %r158;
	setp.ne.s32	%p33, %r159, 0;
	@%p33 bra 	BB8_44;

	shr.s32 	%r160, %r50, 5;
	sub.s32 	%r161, %r160, %r42;
	st.shared.u32 	[%r11+1016], %r161;

BB8_44:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_48:
	setp.eq.s32	%p38, %r40, 1;
	@%p38 bra 	BB8_52;
	bra.uni 	BB8_49;

BB8_52:
	fma.rm.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB8_54;

BB8_15:
	setp.eq.s32	%p19, %r44, 7;
	@%p19 bra 	BB8_41;
	bra.uni 	BB8_16;

BB8_41:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r153, %rd139;
	and.b32  	%r154, %r153, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd23, %r154;
	sub.u32 	%amt2, 64, %r154;
	shl.b64 	%rhs, %rd23, %amt2;
	add.u64 	%rd139, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_49:
	setp.eq.s32	%p39, %r40, 2;
	@%p39 bra 	BB8_51;
	bra.uni 	BB8_50;

BB8_51:
	fma.rp.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB8_54;

BB8_16:
	setp.eq.s32	%p20, %r44, 11;
	@%p20 bra 	BB8_40;
	bra.uni 	BB8_17;

BB8_40:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r147,%r148}, %rd23;
	// inline asm
	mov.u32 	%r151, 6175;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r150|%p31, %r148, %r152, %r151, %r21;
	shfl.sync.bfly.b32 	%r149|%p32, %r147, %r152, %r151, %r21;
	// inline asm
	mov.b64 %rd139, {%r149,%r150};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB8_56;

BB8_50:
	fma.rz.f64 	%fd28, %fd12, %fd13, %fd14;

BB8_54:
	mov.b64 	 %rd139, %fd28;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB8_56:
	st.shared.u64 	[%r46], %rd139;
	// inline asm
	// EXECUTION END
	// inline asm

BB8_57:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	bar.warp.sync 	%r25;
	ld.shared.v2.u32 	{%r170, %r40}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r173, %r42, %r170;
	add.s32 	%r206, %r173, 1;
	setp.lt.u32	%p46, %r206, %r17;
	@%p46 bra 	BB8_7;

BB8_58:
	ld.param.u32 	%r184, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_5];
	ld.shared.u32 	%r176, [%r30];
	xor.b32  	%r177, %r176, %r36;
	ld.shared.u32 	%r178, [%r29];
	xor.b32  	%r179, %r177, %r178;
	and.b32  	%r210, %r179, 2147483584;
	cvt.u64.u32	%rd114, %r209;
	add.s64 	%rd115, %rd14, %rd114;
	add.s64 	%rd116, %rd16, %rd115;
	ld.global.u64 	%rd117, [%rd116];
	ld.shared.u64 	%rd118, [%r19];
	xor.b64  	%rd140, %rd117, %rd118;
	ld.shared.u64 	%rd119, [%r19+64];
	ld.shared.u64 	%rd120, [%r19+128];
	st.shared.u64 	[%r19], %rd140;
	xor.b64  	%rd121, %rd120, %rd119;
	st.global.u64 	[%rd22], %rd140;
	st.global.u64 	[%rd21], %rd121;
	add.s32 	%r200, %r200, 1;
	setp.lt.u32	%p47, %r200, %r184;
	mov.u32 	%r201, 0;
	mov.u32 	%r202, %r201;
	@%p47 bra 	BB8_5;
	bra.uni 	BB8_60;

BB8_59:
	ld.shared.u64 	%rd140, [%r19];

BB8_60:
	mov.u32 	%r189, %tid.x;
	mov.u32 	%r188, %ntid.x;
	mov.u32 	%r187, %ctaid.x;
	mad.lo.s32 	%r186, %r187, %r188, %r189;
	shr.u32 	%r185, %r186, 3;
	cvt.u64.u32	%rd130, %r185;
	ld.param.u64 	%rd129, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_0];
	shl.b64 	%rd46, %rd130, 8;
	add.s64 	%rd122, %rd46, %rd10;
	cvta.to.global.u64 	%rd47, %rd129;
	shl.b64 	%rd123, %rd122, 3;
	add.s64 	%rd48, %rd47, %rd123;
	st.global.u64 	[%rd48], %rd140;
	setp.ne.s32	%p48, %r13, 0;
	@%p48 bra 	BB8_62;

	ld.param.u64 	%rd134, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_1];
	mov.u32 	%r194, %tid.x;
	mov.u32 	%r193, %ntid.x;
	mov.u32 	%r192, %ctaid.x;
	mad.lo.s32 	%r191, %r192, %r193, %r194;
	shr.u32 	%r190, %r191, 3;
	mul.wide.u32 	%rd133, %r190, 4;
	cvta.to.global.u64 	%rd132, %rd134;
	add.s64 	%rd131, %rd132, %rd133;
	st.global.u32 	[%rd131], %r40;

BB8_62:
	ld.param.s8 	%rs5, [_Z10execute_vmILi2ELb1EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs4, %rs5, 255;
	setp.eq.s16	%p49, %rs4, 0;
	@%p49 bra 	BB8_64;

	ld.shared.u64 	%rd124, [%r19+64];
	ld.shared.f64 	%fd25, [%r19+128];
	mov.b64 	 %rd125, %fd25;
	xor.b64  	%rd126, %rd125, %rd124;
	st.global.u64 	[%rd48+64], %rd126;
	st.global.f64 	[%rd48+128], %fd25;
	bra.uni 	BB8_66;

BB8_64:
	@%p48 bra 	BB8_66;

	shl.b64 	%rd127, %rd46, 3;
	add.s64 	%rd128, %rd47, %rd127;
	st.global.u32 	[%rd128+128], %r210;
	st.global.u32 	[%rd128+132], %r209;

BB8_66:
	ret;
}

	// .globl	_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 16, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<213>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<141>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd49, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd50, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd52, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd51, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r59, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_6];
	cvta.to.global.u64 	%rd1, %rd52;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r200, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r200, 4095;
	@%p1 bra 	BB9_3;

	mov.u32 	%r61, _ZZ10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r199, %r61, %r200;
	cvt.u64.u32	%rd2, %r4;
	cvta.to.global.u64 	%rd53, %rd49;
	cvt.u64.u32	%rd54, %r200;
	mul.wide.u32 	%rd55, %r5, 4096;
	add.s64 	%rd56, %rd55, %rd54;
	add.s64 	%rd135, %rd53, %rd56;

BB9_2:
	ld.global.u64 	%rd57, [%rd135];
	st.shared.u64 	[%r199], %rd57;
	add.s64 	%rd135, %rd135, %rd2;
	add.s32 	%r199, %r199, %r4;
	add.s32 	%r200, %r200, %r4;
	setp.lt.u32	%p2, %r200, 4096;
	@%p2 bra 	BB9_2;

BB9_3:
	bar.warp.sync 	-1;
	shr.u32 	%r62, %r1, 3;
	mul.wide.u32 	%rd6, %r62, 256;
	cvt.u32.u64	%r63, %rd6;
	shl.b32 	%r64, %r63, 3;
	mov.u32 	%r65, _ZZ10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r65, %r64;
	mad.lo.s32 	%r12, %r5, %r3, %r1;
	shr.u32 	%r66, %r12, 3;
	and.b32  	%r13, %r12, 7;
	ld.shared.v2.u32 	{%r211, %r210}, [%r11+128];
	ld.shared.u32 	%r17, [%r11+160];
	cvta.to.global.u64 	%rd58, %rd50;
	mul.wide.u32 	%rd59, %r66, 4;
	add.s64 	%rd8, %rd58, %rd59;
	ld.global.u32 	%r40, [%rd8];
	mul.lo.s32 	%r69, %r66, 2097216;
	cvt.u64.u32	%rd9, %r69;
	cvt.u64.u32	%rd10, %r13;
	or.b32  	%r70, %r13, %r63;
	shl.b32 	%r71, %r70, 3;
	add.s32 	%r19, %r65, %r71;
	setp.eq.s32	%p3, %r59, 0;
	@%p3 bra 	BB9_59;

	add.s32 	%r181, %r11, 128;
	ld.shared.u32 	%r73, [%r181+8];
	and.b32  	%r74, %r12, 1;
	shl.b32 	%r75, %r12, 3;
	and.b32  	%r76, %r75, 8;
	mov.u32 	%r77, 3;
	add.s32 	%r20, %r76, 64;
	add.s32 	%r182, %r11, 128;
	ld.shared.v2.u64 	{%rd60, %rd61}, [%r182+16];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p4, %rs3, 0;
	setp.lt.u32	%p5, %r13, 4;
	add.s32 	%r78, %r13, -4;
	shl.b32 	%r79, %r78, 1;
	cvt.s64.s32	%rd64, %r79;
	add.s64 	%rd65, %rd6, %rd64;
	add.s64 	%rd66, %rd65, 16;
	shl.b32 	%r80, %r13, 1;
	cvt.u64.u32	%rd67, %r80;
	add.s64 	%rd68, %rd6, %rd67;
	add.s64 	%rd69, %rd68, 8;
	selp.b64	%rd70, %rd69, %rd66, %p5;
	selp.b64	%rd11, -1, 72057594037927935, %p5;
	and.b32  	%r81, %r1, -8;
	and.b32  	%r82, %r12, 6;
	add.s32 	%r83, %r82, %r81;
	shl.b32 	%r21, %r77, %r83;
	ld.shared.u32 	%rd71, [%r11+140];
	shl.b32 	%r84, %r13, 3;
	cvt.u64.u32	%rd72, %r84;
	add.s64 	%rd12, %rd72, %rd9;
	add.s64 	%rd73, %rd70, 1;
	cvt.u32.u64	%r85, %rd73;
	shl.b32 	%r86, %r85, 3;
	add.s32 	%r22, %r65, %r86;
	shl.b32 	%r88, %r74, 5;
	cvt.u64.u32	%rd13, %r88;
	add.s64 	%rd14, %rd71, %rd72;
	ld.shared.u64 	%rd140, [%r19];
	cvta.to.global.u64 	%rd16, %rd51;
	mov.u32 	%r89, 15;
	shl.b32 	%r25, %r89, %r81;
	setp.eq.s32	%p6, %r74, 0;
	selp.b64	%rd17, %rd60, %rd61, %p6;
	selp.b64	%rd18, 0, %rd61, %p5;
	selp.b64	%rd19, 0, %rd60, %p5;
	cvt.u32.u64	%r90, %rd70;
	shl.b32 	%r91, %r90, 3;
	add.s32 	%r26, %r65, %r91;
	selp.b32	%r202, 0, %r211, %p4;
	selp.b32	%r203, 0, %r210, %p4;
	shr.u32 	%r92, %r73, 24;
	add.s32 	%r29, %r11, %r92;
	bfe.u32 	%r93, %r73, 16, 8;
	add.s32 	%r30, %r11, %r93;
	bfe.u32 	%r94, %r73, 8, 8;
	add.s32 	%r31, %r11, %r94;
	and.b32  	%r95, %r73, 255;
	add.s32 	%r32, %r11, %r95;
	mov.u32 	%r201, 0;

BB9_5:
	.pragma "nounroll";
	mov.u32 	%r36, %r210;
	mov.u32 	%r210, %r211;
	cvt.u32.u64	%r96, %rd10;
	ld.shared.u64 	%rd74, [%r31];
	ld.shared.u64 	%rd75, [%r32];
	xor.b64  	%rd76, %rd74, %rd75;
	mov.b64	{%r97, %r98}, %rd76;
	xor.b32  	%r99, %r97, %r203;
	xor.b32  	%r100, %r98, %r202;
	and.b32  	%r101, %r99, 2097088;
	and.b32  	%r102, %r100, 2097088;
	cvt.u64.u32	%rd77, %r101;
	add.s64 	%rd78, %rd12, %rd77;
	add.s64 	%rd21, %rd1, %rd78;
	cvt.u64.u32	%rd79, %r102;
	add.s64 	%rd80, %rd12, %rd79;
	add.s64 	%rd22, %rd1, %rd80;
	ld.global.u64 	%rd81, [%rd21];
	xor.b64  	%rd82, %rd140, %rd81;
	st.shared.u64 	[%r19], %rd82;
	ld.global.v2.u32 	{%r103, %r104}, [%rd22];
	cvt.rn.f64.s32	%fd20, %r103;
	mov.b64 	 %rd83, %fd20;
	and.b64  	%rd84, %rd83, %rd11;
	or.b64  	%rd85, %rd84, %rd19;
	st.shared.u64 	[%r26], %rd85;
	cvt.rn.f64.s32	%fd21, %r104;
	mov.b64 	 %rd86, %fd21;
	and.b64  	%rd87, %rd86, %rd11;
	or.b64  	%rd88, %rd87, %rd18;
	st.shared.u64 	[%r22], %rd88;
	setp.gt.u32	%p7, %r96, 3;
	@%p7 bra 	BB9_58;

	setp.eq.s32	%p8, %r17, 0;
	mov.u32 	%r207, 0;
	add.s32 	%r183, %r11, 1016;
	st.shared.u32 	[%r183+4], %r40;
	@%p8 bra 	BB9_58;
	bra.uni 	BB9_7;

BB9_17:
	setp.eq.s32	%p21, %r44, 14;
	@%p21 bra 	BB9_34;
	bra.uni 	BB9_18;

BB9_34:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd7, %rd23;
	setp.eq.s32	%p29, %r40, 0;
	@%p29 bra 	BB9_38;
	bra.uni 	BB9_35;

BB9_38:
	sqrt.rn.f64 	%fd27, %fd7;
	bra.uni 	BB9_39;

BB9_18:
	setp.eq.s32	%p22, %r44, 6;
	@%p22 bra 	BB9_33;
	bra.uni 	BB9_19;

BB9_33:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd139, %rd23, %rd139;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_35:
	setp.eq.s32	%p30, %r40, 2;
	@%p30 bra 	BB9_37;
	bra.uni 	BB9_36;

BB9_37:
	sqrt.rp.f64 	%fd27, %fd7;
	bra.uni 	BB9_39;

BB9_19:
	setp.eq.s32	%p23, %r44, 4;
	@%p23 bra 	BB9_32;
	bra.uni 	BB9_20;

BB9_32:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd139, %rd23, %rd139;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_36:
	sqrt.rm.f64 	%fd27, %fd7;

BB9_39:
	mov.b64 	 %rd139, %fd27;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_20:
	setp.eq.s32	%p24, %r44, 8;
	@%p24 bra 	BB9_31;
	bra.uni 	BB9_21;

BB9_31:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r47], %rd23;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_21:
	setp.eq.s32	%p25, %r44, 15;
	@%p25 bra 	BB9_25;
	bra.uni 	BB9_22;

BB9_25:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	cvt.u32.u64	%r146, %rd13;
	shr.u64 	%rd93, %rd139, %r146;
	cvt.u32.u64	%r147, %rd93;
	cvt.rn.f64.s32	%fd22, %r147;
	mov.b64 	 %rd94, %fd22;
	and.b64  	%rd95, %rd94, 72057594037927935;
	or.b64  	%rd96, %rd95, %rd17;
	mov.b64 	 %fd1, %rd23;
	mov.b64 	 %fd2, %rd96;
	setp.eq.s32	%p27, %r40, 0;
	@%p27 bra 	BB9_29;
	bra.uni 	BB9_26;

BB9_29:
	div.rn.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB9_30;

BB9_22:
	setp.eq.s32	%p26, %r44, 5;
	@%p26 bra 	BB9_24;
	bra.uni 	BB9_23;

BB9_24:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd139, %rd23;
	bra.uni 	BB9_56;

BB9_26:
	setp.eq.s32	%p28, %r40, 2;
	@%p28 bra 	BB9_28;
	bra.uni 	BB9_27;

BB9_28:
	div.rp.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB9_30;

BB9_23:
	bfe.u32 	%r197, %r43, 6, 8;
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd91, %rd139, %r197;
	mov.u32 	%r140, 64;
	sub.s32 	%r141, %r140, %r197;
	shl.b64 	%rd92, %rd139, %r141;
	cvt.u32.u64	%r142, %rd91;
	cvt.u32.u64	%r143, %rd92;
	or.b32  	%r144, %r142, %r143;
	and.b32  	%r145, %r144, 3;
	add.s32 	%r184, %r11, 1016;
	st.shared.u32 	[%r184+4], %r145;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB9_57;

BB9_27:
	div.rm.f64 	%fd26, %fd1, %fd2;

BB9_30:
	mov.b64 	 %rd139, %fd26;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_7:
	shl.b32 	%r108, %r207, 2;
	add.s32 	%r109, %r11, %r108;
	st.shared.u32 	[%r11+1016], %r207;
	ld.shared.u32 	%r110, [%r109+1024];
	bfe.u32 	%r111, %r110, 24, 2;
	bfe.u32 	%r41, %r110, 28, 2;
	sub.s32 	%r42, %r111, %r41;
	setp.lt.u32	%p9, %r111, %r13;
	@%p9 bra 	BB9_57;

	shr.u32 	%r196, %r13, 1;
	sub.s32 	%r112, %r13, %r41;
	setp.lt.s32	%p10, %r112, %r41;
	selp.b32	%r113, %r196, %r112, %p10;
	add.s32 	%r114, %r113, %r207;
	shl.b32 	%r115, %r114, 2;
	add.s32 	%r116, %r11, %r115;
	ld.shared.u32 	%r43, [%r116+1024];
	bfe.u32 	%r48, %r43, 6, 8;
	shl.b32 	%r117, %r48, 2;
	add.s32 	%r118, %r11, %r117;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r44, %r43, 20, 4;
	and.b32  	%r45, %r43, 16384;
	selp.b32	%r119, 4, 3, %p10;
	selp.b32	%r120, %r20, 0, %p10;
	and.b32  	%r121, %r43, 7;
	shl.b32 	%r122, %r121, %r119;
	add.s32 	%r123, %r122, %r120;
	and.b32  	%r124, %r43, 56;
	setp.eq.s32	%p11, %r45, 0;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r125, %r20, 128;
	selp.b32	%r126, %r125, 0, %p12;
	add.s32 	%r127, %r126, %r124;
	add.s32 	%r46, %r11, %r123;
	add.s32 	%r47, %r11, %r127;
	ld.shared.u64 	%rd23, [%r46];
	ld.shared.u64 	%rd137, [%r47];
	ld.shared.u32 	%r49, [%r118+256];
	cvt.s64.s32	%rd25, %r49;
	ld.shared.u32 	%r50, [%r118+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p11 bra 	BB9_12;

	cvt.u32.u64	%r129, %rd25;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r128, %r129, 21, 5;
	// inline asm
	mov.u32 	%r130, -1;
	shr.u32 	%r131, %r130, %r128;
	add.s32 	%r132, %r131, -7;
	setp.eq.s32	%p13, %r128, 11;
	cvt.u32.u64	%r133, %rd137;
	selp.b32	%r134, 0, %r133, %p13;
	cvt.u32.u64	%r135, %rd23;
	setp.ne.s32	%p14, %r44, 10;
	selp.b32	%r136, %r134, %r135, %p14;
	add.s32 	%r137, %r136, %r49;
	and.b32  	%r138, %r137, %r132;
	cvt.u64.u32	%rd89, %r138;
	add.s64 	%rd90, %rd89, %rd9;
	add.s64 	%rd26, %rd1, %rd90;
	@%p14 bra 	BB9_11;
	bra.uni 	BB9_10;

BB9_11:
	ld.global.u64 	%rd137, [%rd26];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB9_12:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r139, %r43, 131072;
	setp.eq.s32	%p15, %r139, 0;
	selp.b64	%rd139, %rd137, %rd25, %p15;
	setp.lt.u32	%p16, %r44, 4;
	@%p16 bra 	BB9_55;
	bra.uni 	BB9_13;

BB9_55:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r168, %r43, 524288;
	setp.eq.s32	%p40, %r168, 0;
	neg.s64 	%rd102, %rd139;
	selp.b64	%rd103, %rd139, %rd102, %p40;
	setp.eq.s32	%p41, %r44, 0;
	selp.b64	%rd104, %rd25, 0, %p41;
	add.s64 	%rd105, %rd104, %rd23;
	bfe.u32 	%r169, %r43, 15, 2;
	shl.b64 	%rd106, %rd103, %r169;
	setp.lt.u32	%p42, %r44, 2;
	selp.b64	%rd107, %rd106, 0, %p42;
	add.s64 	%rd108, %rd105, %rd107;
	mov.b64	%rd109, {%r49, %r50};
	and.b32  	%r170, %r43, 262144;
	setp.eq.s32	%p43, %r170, 0;
	selp.b64	%rd110, %rd103, %rd109, %p43;
	setp.eq.s32	%p44, %r44, 2;
	selp.b64	%rd111, %rd110, 1, %p44;
	mul.lo.s64 	%rd112, %rd108, %rd111;
	setp.eq.s32	%p45, %r44, 3;
	selp.b64	%rd113, %rd110, 0, %p45;
	xor.b64  	%rd139, %rd112, %rd113;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_13:
	setp.eq.s32	%p17, %r44, 12;
	@%p17 bra 	BB9_45;
	bra.uni 	BB9_14;

BB9_45:
	and.b32  	%r198, %r43, 16384;
	setp.eq.s32	%p51, %r198, 0;
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p51 bra 	BB9_47;

	cvt.u32.u64	%r163, %rd13;
	shr.u64 	%rd99, %rd139, %r163;
	cvt.u32.u64	%r164, %rd99;
	cvt.rn.f64.s32	%fd23, %r164;
	mov.b64 	 %rd139, %fd23;

BB9_47:
	and.b32  	%r165, %r43, 524288;
	setp.eq.s32	%p35, %r165, 0;
	xor.b64  	%rd100, %rd139, -9223372036854775808;
	selp.b64	%rd101, %rd139, %rd100, %p35;
	shr.u32 	%r166, %r43, 15;
	and.b32  	%r167, %r166, 1;
	setp.eq.b32	%p36, %r167, 1;
	mov.b64 	 %fd12, %rd23;
	mov.b64 	 %fd24, %rd101;
	selp.f64	%fd13, %fd24, 0d3FF0000000000000, %p36;
	selp.f64	%fd14, 0d0000000000000000, %fd24, %p36;
	setp.eq.s32	%p37, %r40, 0;
	@%p37 bra 	BB9_53;
	bra.uni 	BB9_48;

BB9_53:
	fma.rn.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB9_54;

BB9_10:
	st.global.u64 	[%rd26], %rd137;
	bra.uni 	BB9_57;

BB9_14:
	setp.eq.s32	%p18, %r44, 9;
	@%p18 bra 	BB9_42;
	bra.uni 	BB9_15;

BB9_42:
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd139, %rd25, %rd23;
	cvt.u32.u64	%r156, %rd139;
	and.b32  	%r157, %r50, 31;
	mov.u32 	%r158, 255;
	shl.b32 	%r159, %r158, %r157;
	and.b32  	%r160, %r156, %r159;
	setp.ne.s32	%p33, %r160, 0;
	@%p33 bra 	BB9_44;

	shr.s32 	%r161, %r50, 5;
	sub.s32 	%r162, %r161, %r42;
	st.shared.u32 	[%r11+1016], %r162;

BB9_44:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_48:
	setp.eq.s32	%p38, %r40, 1;
	@%p38 bra 	BB9_52;
	bra.uni 	BB9_49;

BB9_52:
	fma.rm.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB9_54;

BB9_15:
	setp.eq.s32	%p19, %r44, 7;
	@%p19 bra 	BB9_41;
	bra.uni 	BB9_16;

BB9_41:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r154, %rd139;
	and.b32  	%r155, %r154, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd23, %r155;
	sub.u32 	%amt2, 64, %r155;
	shl.b64 	%rhs, %rd23, %amt2;
	add.u64 	%rd139, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_49:
	setp.eq.s32	%p39, %r40, 2;
	@%p39 bra 	BB9_51;
	bra.uni 	BB9_50;

BB9_51:
	fma.rp.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB9_54;

BB9_16:
	setp.eq.s32	%p20, %r44, 11;
	@%p20 bra 	BB9_40;
	bra.uni 	BB9_17;

BB9_40:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r148,%r149}, %rd23;
	// inline asm
	mov.u32 	%r152, 6175;
	mov.u32 	%r153, 1;
	shfl.sync.bfly.b32 	%r151|%p31, %r149, %r153, %r152, %r21;
	shfl.sync.bfly.b32 	%r150|%p32, %r148, %r153, %r152, %r21;
	// inline asm
	mov.b64 %rd139, {%r150,%r151};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB9_56;

BB9_50:
	fma.rz.f64 	%fd28, %fd12, %fd13, %fd14;

BB9_54:
	mov.b64 	 %rd139, %fd28;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB9_56:
	st.shared.u64 	[%r46], %rd139;
	// inline asm
	// EXECUTION END
	// inline asm

BB9_57:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	bar.warp.sync 	%r25;
	ld.shared.v2.u32 	{%r171, %r40}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r174, %r42, %r171;
	add.s32 	%r207, %r174, 1;
	setp.lt.u32	%p46, %r207, %r17;
	@%p46 bra 	BB9_7;

BB9_58:
	ld.param.u32 	%r185, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_5];
	ld.shared.u32 	%r177, [%r30];
	xor.b32  	%r178, %r177, %r36;
	ld.shared.u32 	%r179, [%r29];
	xor.b32  	%r180, %r178, %r179;
	and.b32  	%r211, %r180, 2147483584;
	cvt.u64.u32	%rd114, %r210;
	add.s64 	%rd115, %rd14, %rd114;
	add.s64 	%rd116, %rd16, %rd115;
	ld.global.u64 	%rd117, [%rd116];
	ld.shared.u64 	%rd118, [%r19];
	xor.b64  	%rd140, %rd117, %rd118;
	ld.shared.u64 	%rd119, [%r19+64];
	ld.shared.u64 	%rd120, [%r19+128];
	st.shared.u64 	[%r19], %rd140;
	xor.b64  	%rd121, %rd120, %rd119;
	st.global.u64 	[%rd22], %rd140;
	st.global.u64 	[%rd21], %rd121;
	add.s32 	%r201, %r201, 1;
	setp.lt.u32	%p47, %r201, %r185;
	mov.u32 	%r202, 0;
	mov.u32 	%r203, %r202;
	@%p47 bra 	BB9_5;
	bra.uni 	BB9_60;

BB9_59:
	ld.shared.u64 	%rd140, [%r19];

BB9_60:
	mov.u32 	%r190, %tid.x;
	mov.u32 	%r189, %ntid.x;
	mov.u32 	%r188, %ctaid.x;
	mad.lo.s32 	%r187, %r188, %r189, %r190;
	shr.u32 	%r186, %r187, 3;
	cvt.u64.u32	%rd130, %r186;
	ld.param.u64 	%rd129, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_0];
	shl.b64 	%rd46, %rd130, 8;
	add.s64 	%rd122, %rd46, %rd10;
	cvta.to.global.u64 	%rd47, %rd129;
	shl.b64 	%rd123, %rd122, 3;
	add.s64 	%rd48, %rd47, %rd123;
	st.global.u64 	[%rd48], %rd140;
	setp.ne.s32	%p48, %r13, 0;
	@%p48 bra 	BB9_62;

	ld.param.u64 	%rd134, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_1];
	mov.u32 	%r195, %tid.x;
	mov.u32 	%r194, %ntid.x;
	mov.u32 	%r193, %ctaid.x;
	mad.lo.s32 	%r192, %r193, %r194, %r195;
	shr.u32 	%r191, %r192, 3;
	mul.wide.u32 	%rd133, %r191, 4;
	cvta.to.global.u64 	%rd132, %rd134;
	add.s64 	%rd131, %rd132, %rd133;
	st.global.u32 	[%rd131], %r40;

BB9_62:
	ld.param.s8 	%rs5, [_Z10execute_vmILi4ELb1EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs4, %rs5, 255;
	setp.eq.s16	%p49, %rs4, 0;
	@%p49 bra 	BB9_64;

	ld.shared.u64 	%rd124, [%r19+64];
	ld.shared.f64 	%fd25, [%r19+128];
	mov.b64 	 %rd125, %fd25;
	xor.b64  	%rd126, %rd125, %rd124;
	st.global.u64 	[%rd48+64], %rd126;
	st.global.f64 	[%rd48+128], %fd25;
	bra.uni 	BB9_66;

BB9_64:
	@%p48 bra 	BB9_66;

	shl.b64 	%rd127, %rd46, 3;
	add.s64 	%rd128, %rd47, %rd127;
	st.global.u32 	[%rd128+128], %r211;
	st.global.u32 	[%rd128+132], %r210;

BB9_66:
	ret;
}

	// .globl	_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 16, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<51>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<214>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<141>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd49, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd50, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd52, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd51, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r59, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_6];
	cvta.to.global.u64 	%rd1, %rd52;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r202, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r202, 4095;
	@%p1 bra 	BB10_3;

	mov.u32 	%r61, _ZZ10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r201, %r61, %r202;
	cvt.u64.u32	%rd2, %r4;
	cvta.to.global.u64 	%rd53, %rd49;
	cvt.u64.u32	%rd54, %r202;
	mul.wide.u32 	%rd55, %r5, 4096;
	add.s64 	%rd56, %rd55, %rd54;
	add.s64 	%rd135, %rd53, %rd56;

BB10_2:
	ld.global.u64 	%rd57, [%rd135];
	st.shared.u64 	[%r201], %rd57;
	add.s64 	%rd135, %rd135, %rd2;
	add.s32 	%r201, %r201, %r4;
	add.s32 	%r202, %r202, %r4;
	setp.lt.u32	%p2, %r202, 4096;
	@%p2 bra 	BB10_2;

BB10_3:
	bar.warp.sync 	-1;
	shr.u32 	%r62, %r1, 3;
	mul.wide.u32 	%rd6, %r62, 256;
	cvt.u32.u64	%r63, %rd6;
	shl.b32 	%r64, %r63, 3;
	mov.u32 	%r65, _ZZ10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r65, %r64;
	mad.lo.s32 	%r12, %r5, %r3, %r1;
	shr.u32 	%r66, %r12, 3;
	and.b32  	%r13, %r12, 7;
	ld.shared.v2.u32 	{%r212, %r208}, [%r11+128];
	ld.shared.u32 	%r17, [%r11+160];
	cvta.to.global.u64 	%rd58, %rd50;
	mul.wide.u32 	%rd59, %r66, 4;
	add.s64 	%rd8, %rd58, %rd59;
	ld.global.u32 	%r40, [%rd8];
	mul.lo.s32 	%r69, %r66, 2097216;
	cvt.u64.u32	%rd9, %r69;
	cvt.u64.u32	%rd10, %r13;
	or.b32  	%r70, %r13, %r63;
	shl.b32 	%r71, %r70, 3;
	add.s32 	%r19, %r65, %r71;
	setp.eq.s32	%p3, %r59, 0;
	@%p3 bra 	BB10_58;

	add.s32 	%r181, %r11, 128;
	ld.shared.u32 	%r73, [%r181+8];
	and.b32  	%r74, %r12, 1;
	shl.b32 	%r75, %r12, 3;
	and.b32  	%r76, %r75, 8;
	mov.u32 	%r77, 3;
	add.s32 	%r20, %r76, 64;
	add.s32 	%r182, %r11, 128;
	ld.shared.v2.u64 	{%rd60, %rd61}, [%r182+16];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p4, %rs3, 0;
	setp.lt.u32	%p5, %r13, 4;
	add.s32 	%r78, %r13, -4;
	shl.b32 	%r79, %r78, 1;
	cvt.s64.s32	%rd64, %r79;
	add.s64 	%rd65, %rd6, %rd64;
	add.s64 	%rd66, %rd65, 16;
	shl.b32 	%r80, %r13, 1;
	cvt.u64.u32	%rd67, %r80;
	add.s64 	%rd68, %rd6, %rd67;
	add.s64 	%rd69, %rd68, 8;
	selp.b64	%rd70, %rd69, %rd66, %p5;
	selp.b64	%rd11, -1, 72057594037927935, %p5;
	and.b32  	%r81, %r1, -8;
	and.b32  	%r82, %r12, 6;
	add.s32 	%r83, %r82, %r81;
	shl.b32 	%r21, %r77, %r83;
	ld.shared.u32 	%rd71, [%r11+140];
	shl.b32 	%r84, %r13, 3;
	cvt.u64.u32	%rd72, %r84;
	add.s64 	%rd12, %rd72, %rd9;
	add.s64 	%rd73, %rd70, 1;
	cvt.u32.u64	%r85, %rd73;
	shl.b32 	%r86, %r85, 3;
	add.s32 	%r22, %r65, %r86;
	add.s64 	%rd13, %rd71, %rd72;
	shl.b32 	%r88, %r74, 5;
	cvt.u64.u32	%rd14, %r88;
	ld.shared.u64 	%rd140, [%r19];
	cvta.to.global.u64 	%rd16, %rd51;
	mov.u32 	%r89, 255;
	shl.b32 	%r25, %r89, %r81;
	setp.eq.s32	%p6, %r74, 0;
	selp.b64	%rd17, %rd60, %rd61, %p6;
	selp.b64	%rd18, 0, %rd61, %p5;
	selp.b64	%rd19, 0, %rd60, %p5;
	cvt.u32.u64	%r90, %rd70;
	shl.b32 	%r91, %r90, 3;
	add.s32 	%r26, %r65, %r91;
	selp.b32	%r204, 0, %r212, %p4;
	selp.b32	%r205, 0, %r208, %p4;
	shr.u32 	%r92, %r73, 24;
	add.s32 	%r29, %r11, %r92;
	bfe.u32 	%r93, %r73, 16, 8;
	add.s32 	%r30, %r11, %r93;
	bfe.u32 	%r94, %r73, 8, 8;
	add.s32 	%r31, %r11, %r94;
	and.b32  	%r95, %r73, 255;
	add.s32 	%r32, %r11, %r95;
	mov.u32 	%r203, 0;

BB10_5:
	.pragma "nounroll";
	mov.u32 	%r211, %r212;
	mov.u32 	%r209, 0;
	ld.shared.u64 	%rd74, [%r31];
	ld.shared.u64 	%rd75, [%r32];
	xor.b64  	%rd76, %rd74, %rd75;
	mov.b64	{%r97, %r98}, %rd76;
	xor.b32  	%r99, %r97, %r205;
	xor.b32  	%r100, %r98, %r204;
	and.b32  	%r101, %r99, 2097088;
	and.b32  	%r102, %r100, 2097088;
	cvt.u64.u32	%rd77, %r101;
	add.s64 	%rd78, %rd12, %rd77;
	add.s64 	%rd21, %rd1, %rd78;
	cvt.u64.u32	%rd79, %r102;
	add.s64 	%rd80, %rd12, %rd79;
	add.s64 	%rd22, %rd1, %rd80;
	ld.global.u64 	%rd81, [%rd21];
	xor.b64  	%rd82, %rd140, %rd81;
	st.shared.u64 	[%r19], %rd82;
	ld.global.v2.u32 	{%r103, %r104}, [%rd22];
	cvt.rn.f64.s32	%fd20, %r103;
	mov.b64 	 %rd83, %fd20;
	and.b64  	%rd84, %rd83, %rd11;
	or.b64  	%rd85, %rd84, %rd19;
	st.shared.u64 	[%r26], %rd85;
	cvt.rn.f64.s32	%fd21, %r104;
	mov.b64 	 %rd86, %fd21;
	and.b64  	%rd87, %rd86, %rd11;
	or.b64  	%rd88, %rd87, %rd18;
	st.shared.u64 	[%r22], %rd88;
	add.s32 	%r183, %r11, 128;
	st.shared.u32 	[%r183+892], %r40;
	setp.eq.s32	%p7, %r17, 0;
	@%p7 bra 	BB10_57;
	bra.uni 	BB10_6;

BB10_16:
	setp.eq.s32	%p20, %r44, 14;
	@%p20 bra 	BB10_33;
	bra.uni 	BB10_17;

BB10_33:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd7, %rd23;
	setp.eq.s32	%p28, %r40, 0;
	@%p28 bra 	BB10_37;
	bra.uni 	BB10_34;

BB10_37:
	sqrt.rn.f64 	%fd27, %fd7;
	bra.uni 	BB10_38;

BB10_17:
	setp.eq.s32	%p21, %r44, 6;
	@%p21 bra 	BB10_32;
	bra.uni 	BB10_18;

BB10_32:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd139, %rd23, %rd139;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_34:
	setp.eq.s32	%p29, %r40, 2;
	@%p29 bra 	BB10_36;
	bra.uni 	BB10_35;

BB10_36:
	sqrt.rp.f64 	%fd27, %fd7;
	bra.uni 	BB10_38;

BB10_18:
	setp.eq.s32	%p22, %r44, 4;
	@%p22 bra 	BB10_31;
	bra.uni 	BB10_19;

BB10_31:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd139, %rd23, %rd139;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_35:
	sqrt.rm.f64 	%fd27, %fd7;

BB10_38:
	mov.b64 	 %rd139, %fd27;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_19:
	setp.eq.s32	%p23, %r44, 8;
	@%p23 bra 	BB10_30;
	bra.uni 	BB10_20;

BB10_30:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r47], %rd23;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_20:
	setp.eq.s32	%p24, %r44, 15;
	@%p24 bra 	BB10_24;
	bra.uni 	BB10_21;

BB10_24:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	cvt.u32.u64	%r146, %rd14;
	shr.u64 	%rd93, %rd139, %r146;
	cvt.u32.u64	%r147, %rd93;
	cvt.rn.f64.s32	%fd22, %r147;
	mov.b64 	 %rd94, %fd22;
	and.b64  	%rd95, %rd94, 72057594037927935;
	or.b64  	%rd96, %rd95, %rd17;
	mov.b64 	 %fd1, %rd23;
	mov.b64 	 %fd2, %rd96;
	setp.eq.s32	%p26, %r40, 0;
	@%p26 bra 	BB10_28;
	bra.uni 	BB10_25;

BB10_28:
	div.rn.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB10_29;

BB10_21:
	setp.eq.s32	%p25, %r44, 5;
	@%p25 bra 	BB10_23;
	bra.uni 	BB10_22;

BB10_23:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd139, %rd23;
	bra.uni 	BB10_55;

BB10_25:
	setp.eq.s32	%p27, %r40, 2;
	@%p27 bra 	BB10_27;
	bra.uni 	BB10_26;

BB10_27:
	div.rp.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB10_29;

BB10_22:
	bfe.u32 	%r198, %r43, 6, 8;
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd91, %rd139, %r198;
	mov.u32 	%r140, 64;
	sub.s32 	%r141, %r140, %r198;
	shl.b64 	%rd92, %rd139, %r141;
	cvt.u32.u64	%r142, %rd91;
	cvt.u32.u64	%r143, %rd92;
	or.b32  	%r144, %r142, %r143;
	and.b32  	%r145, %r144, 3;
	add.s32 	%r184, %r11, 128;
	st.shared.u32 	[%r184+892], %r145;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB10_56;

BB10_26:
	div.rm.f64 	%fd26, %fd1, %fd2;

BB10_29:
	mov.b64 	 %rd139, %fd26;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_6:
	shl.b32 	%r107, %r209, 2;
	add.s32 	%r108, %r11, %r107;
	st.shared.u32 	[%r11+1016], %r209;
	ld.shared.u32 	%r109, [%r108+1024];
	bfe.u32 	%r110, %r109, 24, 3;
	bfe.u32 	%r41, %r109, 28, 3;
	sub.s32 	%r42, %r110, %r41;
	cvt.u32.u64	%r111, %rd10;
	setp.lt.u32	%p8, %r110, %r111;
	@%p8 bra 	BB10_56;

	shr.u32 	%r197, %r13, 1;
	sub.s32 	%r112, %r13, %r41;
	setp.lt.s32	%p9, %r112, %r41;
	selp.b32	%r113, %r197, %r112, %p9;
	add.s32 	%r114, %r113, %r209;
	shl.b32 	%r115, %r114, 2;
	add.s32 	%r116, %r11, %r115;
	ld.shared.u32 	%r43, [%r116+1024];
	bfe.u32 	%r48, %r43, 6, 8;
	shl.b32 	%r117, %r48, 2;
	add.s32 	%r118, %r11, %r117;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r44, %r43, 20, 4;
	and.b32  	%r45, %r43, 16384;
	selp.b32	%r119, 4, 3, %p9;
	selp.b32	%r120, %r20, 0, %p9;
	and.b32  	%r121, %r43, 7;
	shl.b32 	%r122, %r121, %r119;
	add.s32 	%r123, %r122, %r120;
	and.b32  	%r124, %r43, 56;
	setp.eq.s32	%p10, %r45, 0;
	and.pred  	%p11, %p9, %p10;
	add.s32 	%r125, %r20, 128;
	selp.b32	%r126, %r125, 0, %p11;
	add.s32 	%r127, %r126, %r124;
	add.s32 	%r46, %r11, %r123;
	add.s32 	%r47, %r11, %r127;
	ld.shared.u64 	%rd23, [%r46];
	ld.shared.u64 	%rd137, [%r47];
	ld.shared.u32 	%r49, [%r118+256];
	cvt.s64.s32	%rd25, %r49;
	ld.shared.u32 	%r50, [%r118+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p10 bra 	BB10_11;

	cvt.u32.u64	%r129, %rd25;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r128, %r129, 21, 5;
	// inline asm
	mov.u32 	%r130, -1;
	shr.u32 	%r131, %r130, %r128;
	add.s32 	%r132, %r131, -7;
	setp.eq.s32	%p12, %r128, 11;
	cvt.u32.u64	%r133, %rd137;
	selp.b32	%r134, 0, %r133, %p12;
	cvt.u32.u64	%r135, %rd23;
	setp.ne.s32	%p13, %r44, 10;
	selp.b32	%r136, %r134, %r135, %p13;
	add.s32 	%r137, %r136, %r49;
	and.b32  	%r138, %r137, %r132;
	cvt.u64.u32	%rd89, %r138;
	add.s64 	%rd90, %rd89, %rd9;
	add.s64 	%rd26, %rd1, %rd90;
	@%p13 bra 	BB10_10;
	bra.uni 	BB10_9;

BB10_10:
	ld.global.u64 	%rd137, [%rd26];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB10_11:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r139, %r43, 131072;
	setp.eq.s32	%p14, %r139, 0;
	selp.b64	%rd139, %rd137, %rd25, %p14;
	setp.lt.u32	%p15, %r44, 4;
	@%p15 bra 	BB10_54;
	bra.uni 	BB10_12;

BB10_54:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r168, %r43, 524288;
	setp.eq.s32	%p39, %r168, 0;
	neg.s64 	%rd102, %rd139;
	selp.b64	%rd103, %rd139, %rd102, %p39;
	setp.eq.s32	%p40, %r44, 0;
	selp.b64	%rd104, %rd25, 0, %p40;
	add.s64 	%rd105, %rd104, %rd23;
	bfe.u32 	%r169, %r43, 15, 2;
	shl.b64 	%rd106, %rd103, %r169;
	setp.lt.u32	%p41, %r44, 2;
	selp.b64	%rd107, %rd106, 0, %p41;
	add.s64 	%rd108, %rd105, %rd107;
	mov.b64	%rd109, {%r49, %r50};
	and.b32  	%r170, %r43, 262144;
	setp.eq.s32	%p42, %r170, 0;
	selp.b64	%rd110, %rd103, %rd109, %p42;
	setp.eq.s32	%p43, %r44, 2;
	selp.b64	%rd111, %rd110, 1, %p43;
	mul.lo.s64 	%rd112, %rd108, %rd111;
	setp.eq.s32	%p44, %r44, 3;
	selp.b64	%rd113, %rd110, 0, %p44;
	xor.b64  	%rd139, %rd112, %rd113;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_12:
	setp.eq.s32	%p16, %r44, 12;
	@%p16 bra 	BB10_44;
	bra.uni 	BB10_13;

BB10_44:
	and.b32  	%r200, %r43, 16384;
	setp.eq.s32	%p50, %r200, 0;
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p50 bra 	BB10_46;

	cvt.u32.u64	%r163, %rd14;
	shr.u64 	%rd99, %rd139, %r163;
	cvt.u32.u64	%r164, %rd99;
	cvt.rn.f64.s32	%fd23, %r164;
	mov.b64 	 %rd139, %fd23;

BB10_46:
	and.b32  	%r165, %r43, 524288;
	setp.eq.s32	%p34, %r165, 0;
	xor.b64  	%rd100, %rd139, -9223372036854775808;
	selp.b64	%rd101, %rd139, %rd100, %p34;
	shr.u32 	%r166, %r43, 15;
	and.b32  	%r167, %r166, 1;
	setp.eq.b32	%p35, %r167, 1;
	mov.b64 	 %fd12, %rd23;
	mov.b64 	 %fd24, %rd101;
	selp.f64	%fd13, %fd24, 0d3FF0000000000000, %p35;
	selp.f64	%fd14, 0d0000000000000000, %fd24, %p35;
	setp.eq.s32	%p36, %r40, 0;
	@%p36 bra 	BB10_52;
	bra.uni 	BB10_47;

BB10_52:
	fma.rn.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB10_53;

BB10_9:
	st.global.u64 	[%rd26], %rd137;
	bra.uni 	BB10_56;

BB10_13:
	setp.eq.s32	%p17, %r44, 9;
	@%p17 bra 	BB10_41;
	bra.uni 	BB10_14;

BB10_41:
	mov.u32 	%r199, 255;
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd139, %rd25, %rd23;
	cvt.u32.u64	%r156, %rd139;
	and.b32  	%r157, %r50, 31;
	shl.b32 	%r159, %r199, %r157;
	and.b32  	%r160, %r156, %r159;
	setp.ne.s32	%p32, %r160, 0;
	@%p32 bra 	BB10_43;

	shr.s32 	%r161, %r50, 5;
	sub.s32 	%r162, %r161, %r42;
	st.shared.u32 	[%r11+1016], %r162;

BB10_43:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_47:
	setp.eq.s32	%p37, %r40, 1;
	@%p37 bra 	BB10_51;
	bra.uni 	BB10_48;

BB10_51:
	fma.rm.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB10_53;

BB10_14:
	setp.eq.s32	%p18, %r44, 7;
	@%p18 bra 	BB10_40;
	bra.uni 	BB10_15;

BB10_40:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r154, %rd139;
	and.b32  	%r155, %r154, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd23, %r155;
	sub.u32 	%amt2, 64, %r155;
	shl.b64 	%rhs, %rd23, %amt2;
	add.u64 	%rd139, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_48:
	setp.eq.s32	%p38, %r40, 2;
	@%p38 bra 	BB10_50;
	bra.uni 	BB10_49;

BB10_50:
	fma.rp.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB10_53;

BB10_15:
	setp.eq.s32	%p19, %r44, 11;
	@%p19 bra 	BB10_39;
	bra.uni 	BB10_16;

BB10_39:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r148,%r149}, %rd23;
	// inline asm
	mov.u32 	%r152, 6175;
	mov.u32 	%r153, 1;
	shfl.sync.bfly.b32 	%r151|%p30, %r149, %r153, %r152, %r21;
	shfl.sync.bfly.b32 	%r150|%p31, %r148, %r153, %r152, %r21;
	// inline asm
	mov.b64 %rd139, {%r150,%r151};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB10_55;

BB10_49:
	fma.rz.f64 	%fd28, %fd12, %fd13, %fd14;

BB10_53:
	mov.b64 	 %rd139, %fd28;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB10_55:
	st.shared.u64 	[%r46], %rd139;
	// inline asm
	// EXECUTION END
	// inline asm

BB10_56:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	bar.warp.sync 	%r25;
	ld.shared.v2.u32 	{%r171, %r40}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r174, %r42, %r171;
	add.s32 	%r209, %r174, 1;
	setp.lt.u32	%p45, %r209, %r17;
	@%p45 bra 	BB10_6;

BB10_57:
	ld.param.u32 	%r185, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_5];
	ld.shared.u32 	%r177, [%r30];
	xor.b32  	%r178, %r177, %r208;
	ld.shared.u32 	%r179, [%r29];
	xor.b32  	%r180, %r178, %r179;
	and.b32  	%r212, %r180, 2147483584;
	cvt.u64.u32	%rd114, %r211;
	add.s64 	%rd115, %rd13, %rd114;
	add.s64 	%rd116, %rd16, %rd115;
	ld.global.u64 	%rd117, [%rd116];
	ld.shared.u64 	%rd118, [%r19];
	xor.b64  	%rd140, %rd117, %rd118;
	ld.shared.u64 	%rd119, [%r19+64];
	ld.shared.u64 	%rd120, [%r19+128];
	st.shared.u64 	[%r19], %rd140;
	xor.b64  	%rd121, %rd120, %rd119;
	st.global.u64 	[%rd22], %rd140;
	st.global.u64 	[%rd21], %rd121;
	add.s32 	%r203, %r203, 1;
	setp.lt.u32	%p46, %r203, %r185;
	mov.u32 	%r204, 0;
	mov.u32 	%r205, %r204;
	mov.u32 	%r208, %r211;
	@%p46 bra 	BB10_5;
	bra.uni 	BB10_59;

BB10_58:
	ld.shared.u64 	%rd140, [%r19];
	mov.u32 	%r211, %r208;

BB10_59:
	mov.u32 	%r190, %tid.x;
	mov.u32 	%r189, %ntid.x;
	mov.u32 	%r188, %ctaid.x;
	mad.lo.s32 	%r187, %r188, %r189, %r190;
	shr.u32 	%r186, %r187, 3;
	cvt.u64.u32	%rd130, %r186;
	ld.param.u64 	%rd129, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_0];
	shl.b64 	%rd46, %rd130, 8;
	add.s64 	%rd122, %rd46, %rd10;
	cvta.to.global.u64 	%rd47, %rd129;
	shl.b64 	%rd123, %rd122, 3;
	add.s64 	%rd48, %rd47, %rd123;
	st.global.u64 	[%rd48], %rd140;
	setp.ne.s32	%p47, %r13, 0;
	@%p47 bra 	BB10_61;

	ld.param.u64 	%rd134, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_1];
	mov.u32 	%r195, %tid.x;
	mov.u32 	%r194, %ntid.x;
	mov.u32 	%r193, %ctaid.x;
	mad.lo.s32 	%r192, %r193, %r194, %r195;
	shr.u32 	%r191, %r192, 3;
	mul.wide.u32 	%rd133, %r191, 4;
	cvta.to.global.u64 	%rd132, %rd134;
	add.s64 	%rd131, %rd132, %rd133;
	st.global.u32 	[%rd131], %r40;

BB10_61:
	ld.param.s8 	%rs5, [_Z10execute_vmILi8ELb1EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs4, %rs5, 255;
	setp.eq.s16	%p48, %rs4, 0;
	@%p48 bra 	BB10_63;

	ld.shared.u64 	%rd124, [%r19+64];
	ld.shared.f64 	%fd25, [%r19+128];
	mov.b64 	 %rd125, %fd25;
	xor.b64  	%rd126, %rd125, %rd124;
	st.global.u64 	[%rd48+64], %rd126;
	st.global.f64 	[%rd48+128], %fd25;
	bra.uni 	BB10_65;

BB10_63:
	@%p47 bra 	BB10_65;

	shl.b64 	%rd127, %rd46, 3;
	add.s64 	%rd128, %rd47, %rd127;
	st.global.u32 	[%rd128+128], %r212;
	st.global.u32 	[%rd128+132], %r211;

BB10_65:
	ret;
}

	// .globl	_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb
.visible .entry _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb(
	.param .u64 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_2,
	.param .u64 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_4,
	.param .u32 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_6,
	.param .u8 _Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_7
)
.maxntid 32, 1, 1
.minnctapersm 16
{
	.reg .pred 	%p<55>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<326>;
	.reg .f64 	%fd<29>;
	.reg .b64 	%rd<156>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd37, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_0];
	ld.param.u64 	%rd38, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_1];
	ld.param.u64 	%rd39, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_2];
	ld.param.u64 	%rd40, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_3];
	ld.param.u32 	%r57, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_5];
	ld.param.s8 	%rs1, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_6];
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r306, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r306, 4095;
	@%p1 bra 	BB11_3;

	mov.u32 	%r59, _ZZ10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r305, %r59, %r306;
	cvt.u64.u32	%rd1, %r4;
	cvt.u64.u32	%rd41, %r306;
	mul.wide.u32 	%rd42, %r5, 4096;
	add.s64 	%rd43, %rd42, %rd41;
	cvta.to.global.u64 	%rd44, %rd37;
	add.s64 	%rd148, %rd44, %rd43;

BB11_2:
	ld.global.u64 	%rd45, [%rd148];
	st.shared.u64 	[%r305], %rd45;
	add.s64 	%rd148, %rd148, %rd1;
	add.s32 	%r305, %r305, %r4;
	add.s32 	%r306, %r306, %r4;
	setp.lt.u32	%p2, %r306, 4096;
	@%p2 bra 	BB11_2;

BB11_3:
	bar.warp.sync 	-1;
	shl.b32 	%r60, %r1, 7;
	and.b32  	%r61, %r60, -2048;
	mov.u32 	%r62, _ZZ10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbbE15vm_states_local;
	add.s32 	%r11, %r62, %r61;
	mad.lo.s32 	%r63, %r5, %r3, %r1;
	shr.u32 	%r64, %r63, 4;
	ld.shared.v2.u32 	{%r312, %r311}, [%r11+128];
	ld.shared.u32 	%r14, [%r11+136];
	ld.shared.v2.u64 	{%rd46, %rd47}, [%r11+144];
	ld.shared.u32 	%r15, [%r11+160];
	cvta.to.global.u64 	%rd48, %rd38;
	mul.wide.u32 	%rd49, %r64, 4;
	add.s64 	%rd50, %rd48, %rd49;
	ld.global.u32 	%r33, [%rd50];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p3, %rs3, 0;
	selp.b32	%r309, 0, %r312, %p3;
	setp.eq.s32	%p4, %r57, 0;
	@%p4 bra 	BB11_4;

	ld.shared.u32 	%rd52, [%r11+140];
	and.b32  	%r73, %r63, 15;
	shl.b32 	%r74, %r73, 3;
	cvt.u64.u32	%rd53, %r74;
	add.s64 	%rd7, %rd52, %rd53;
	selp.b32	%r310, 0, %r311, %p3;
	mov.u32 	%r308, 0;
	cvta.to.global.u64 	%rd118, %rd40;

BB11_6:
	.pragma "nounroll";
	setp.gt.u32	%p6, %r73, 7;
	@%p6 bra 	BB11_8;

	setp.lt.u32	%p7, %r73, 4;
	shl.b32 	%r85, %r63, 1;
	and.b32  	%r86, %r85, 30;
	cvt.u64.u32	%rd54, %r86;
	shr.u32 	%r87, %r1, 4;
	mul.wide.u32 	%rd55, %r87, 256;
	add.s64 	%rd56, %rd55, %rd54;
	add.s64 	%rd57, %rd56, 8;
	add.s32 	%r88, %r73, -4;
	shl.b32 	%r89, %r88, 1;
	cvt.u64.u32	%rd58, %r89;
	add.s64 	%rd59, %rd55, %rd58;
	add.s64 	%rd60, %rd59, 16;
	selp.b64	%rd61, %rd57, %rd60, %p7;
	cvt.u32.u64	%r90, %rd61;
	shl.b32 	%r91, %r90, 3;
	or.b32  	%r92, %r91, 8;
	add.s32 	%r94, %r62, %r92;
	add.s32 	%r95, %r62, %r91;
	bfe.u32 	%r96, %r14, 8, 8;
	cvt.u32.u64	%r97, %rd55;
	shl.b32 	%r98, %r97, 3;
	add.s32 	%r99, %r62, %r98;
	add.s32 	%r100, %r99, %r96;
	and.b32  	%r101, %r14, 255;
	add.s32 	%r102, %r99, %r101;
	ld.shared.u64 	%rd62, [%r100];
	ld.shared.u64 	%rd63, [%r102];
	xor.b64  	%rd64, %rd62, %rd63;
	mov.b64	{%r103, %r104}, %rd64;
	xor.b32  	%r105, %r103, %r310;
	xor.b32  	%r106, %r104, %r309;
	and.b32  	%r310, %r105, 2097088;
	and.b32  	%r309, %r106, 2097088;
	cvt.u64.u32	%rd65, %r310;
	shl.b32 	%r107, %r63, 3;
	and.b32  	%r108, %r107, 120;
	cvt.u64.u32	%rd66, %r108;
	mul.lo.s32 	%r110, %r64, 2097216;
	cvt.u64.u32	%rd67, %r110;
	add.s64 	%rd68, %rd66, %rd67;
	add.s64 	%rd69, %rd68, %rd65;
	cvta.to.global.u64 	%rd70, %rd39;
	add.s64 	%rd150, %rd70, %rd69;
	cvt.u64.u32	%rd71, %r309;
	add.s64 	%rd72, %rd68, %rd71;
	add.s64 	%rd149, %rd70, %rd72;
	cvt.u64.u32	%rd73, %r73;
	add.s64 	%rd74, %rd73, %rd55;
	cvt.u32.u64	%r111, %rd74;
	shl.b32 	%r112, %r111, 3;
	add.s32 	%r307, %r62, %r112;
	ld.shared.u64 	%rd75, [%r307];
	ld.global.u64 	%rd76, [%rd150];
	xor.b64  	%rd77, %rd75, %rd76;
	st.shared.u64 	[%r307], %rd77;
	ld.global.v2.u32 	{%r113, %r114}, [%rd149];
	cvt.rn.f64.s32	%fd20, %r113;
	mov.b64 	 %rd78, %fd20;
	selp.b64	%rd79, -1, 72057594037927935, %p7;
	and.b64  	%rd80, %rd78, %rd79;
	selp.b64	%rd81, 0, %rd46, %p7;
	or.b64  	%rd82, %rd80, %rd81;
	st.shared.u64 	[%r95], %rd82;
	cvt.rn.f64.s32	%fd21, %r114;
	mov.b64 	 %rd83, %fd21;
	and.b64  	%rd84, %rd83, %rd79;
	selp.b64	%rd85, 0, %rd47, %p7;
	or.b64  	%rd86, %rd84, %rd85;
	st.shared.u64 	[%r94], %rd86;

BB11_8:
	st.shared.u32 	[%r11+1020], %r33;
	setp.eq.s32	%p8, %r15, 0;
	mov.u32 	%r317, 0;
	@%p8 bra 	BB11_60;
	bra.uni 	BB11_9;

BB11_19:
	setp.eq.s32	%p21, %r37, 14;
	@%p21 bra 	BB11_36;
	bra.uni 	BB11_20;

BB11_36:
	// inline asm
	// FSQRT_R (6/256) ------>
	// inline asm
	mov.b64 	 %fd7, %rd14;
	setp.eq.s32	%p30, %r33, 0;
	@%p30 bra 	BB11_40;
	bra.uni 	BB11_37;

BB11_40:
	sqrt.rn.f64 	%fd27, %fd7;
	bra.uni 	BB11_41;

BB11_20:
	setp.eq.s32	%p22, %r37, 6;
	@%p22 bra 	BB11_35;
	bra.uni 	BB11_21;

BB11_35:
	// inline asm
	// IMULH_R, IMULH_M (5/256) ------>
	// inline asm
	mul.hi.u64 	%rd155, %rd14, %rd155;
	// inline asm
	// <------ IMULH_R, IMULH_M (5/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_37:
	setp.eq.s32	%p31, %r33, 2;
	@%p31 bra 	BB11_39;
	bra.uni 	BB11_38;

BB11_39:
	sqrt.rp.f64 	%fd27, %fd7;
	bra.uni 	BB11_41;

BB11_21:
	setp.eq.s32	%p23, %r37, 4;
	@%p23 bra 	BB11_34;
	bra.uni 	BB11_22;

BB11_34:
	// inline asm
	// ISMULH_R, ISMULH_M (5/256) ------>
	// inline asm
	mul.hi.s64 	%rd155, %rd14, %rd155;
	// inline asm
	// <------ ISMULH_R, ISMULH_M (5/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_38:
	sqrt.rm.f64 	%fd27, %fd7;

BB11_41:
	mov.b64 	 %rd155, %fd27;
	// inline asm
	// <------ FSQRT_R (6/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_22:
	setp.eq.s32	%p24, %r37, 8;
	@%p24 bra 	BB11_33;
	bra.uni 	BB11_23;

BB11_33:
	// inline asm
	// ISWAP_R (4/256) ------>
	// inline asm
	st.shared.u64 	[%r40], %rd14;
	// inline asm
	// <------ ISWAP_R (4/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_23:
	setp.eq.s32	%p25, %r37, 15;
	@%p25 bra 	BB11_27;
	bra.uni 	BB11_24;

BB11_27:
	// inline asm
	// FDIV_M (4/256) ------>
	// inline asm
	shl.b32 	%r191, %r146, 5;
	shr.u64 	%rd93, %rd155, %r191;
	cvt.u32.u64	%r192, %rd93;
	cvt.rn.f64.s32	%fd22, %r192;
	mov.b64 	 %rd94, %fd22;
	and.b64  	%rd95, %rd94, 72057594037927935;
	setp.eq.s32	%p27, %r146, 0;
	selp.b64	%rd96, %rd46, %rd47, %p27;
	or.b64  	%rd97, %rd95, %rd96;
	mov.b64 	 %fd1, %rd14;
	mov.b64 	 %fd2, %rd97;
	setp.eq.s32	%p28, %r33, 0;
	@%p28 bra 	BB11_31;
	bra.uni 	BB11_28;

BB11_31:
	div.rn.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB11_32;

BB11_24:
	setp.eq.s32	%p26, %r37, 5;
	@%p26 bra 	BB11_26;
	bra.uni 	BB11_25;

BB11_26:
	// inline asm
	// INEG_R (2/256) ------>
	// inline asm
	// inline asm
	// <------ INEG_R (2/256)
	// inline asm
	neg.s64 	%rd155, %rd14;
	bra.uni 	BB11_58;

BB11_28:
	setp.eq.s32	%p29, %r33, 2;
	@%p29 bra 	BB11_30;
	bra.uni 	BB11_29;

BB11_30:
	div.rp.f64 	%fd26, %fd1, %fd2;
	bra.uni 	BB11_32;

BB11_25:
	bfe.u32 	%r304, %r36, 6, 8;
	// inline asm
	// CFROUND (1/256) ------>
	// inline asm
	shr.u64 	%rd91, %rd155, %r304;
	mov.u32 	%r175, 64;
	sub.s32 	%r176, %r175, %r304;
	shl.b64 	%rd92, %rd155, %r176;
	cvt.u32.u64	%r177, %rd91;
	cvt.u32.u64	%r178, %rd92;
	or.b32  	%r179, %r177, %r178;
	and.b32  	%r180, %r179, 3;
	st.shared.u32 	[%r11+1020], %r180;
	// inline asm
	// <------ CFROUND (1/256)
	// inline asm
	bra.uni 	BB11_59;

BB11_29:
	div.rm.f64 	%fd26, %fd1, %fd2;

BB11_32:
	mov.b64 	 %rd155, %fd26;
	// inline asm
	// <------ FDIV_M (4/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_9:
	shl.b32 	%r123, %r317, 2;
	add.s32 	%r124, %r11, %r123;
	st.shared.u32 	[%r11+1016], %r317;
	ld.shared.u32 	%r130, [%r124+1024];
	bfe.u32 	%r131, %r130, 24, 4;
	shr.u32 	%r34, %r130, 28;
	sub.s32 	%r35, %r131, %r34;
	setp.lt.u32	%p9, %r131, %r73;
	@%p9 bra 	BB11_59;

	sub.s32 	%r137, %r73, %r34;
	setp.lt.s32	%p10, %r137, %r34;
	bfe.u32 	%r138, %r63, 1, 3;
	selp.b32	%r139, %r138, %r137, %p10;
	add.s32 	%r140, %r139, %r317;
	shl.b32 	%r141, %r140, 2;
	add.s32 	%r142, %r11, %r141;
	ld.shared.u32 	%r36, [%r142+1024];
	bfe.u32 	%r41, %r36, 6, 8;
	shl.b32 	%r143, %r41, 2;
	add.s32 	%r144, %r11, %r143;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r37, %r36, 20, 4;
	and.b32  	%r38, %r36, 16384;
	selp.b32	%r145, 4, 3, %p10;
	and.b32  	%r146, %r63, 1;
	shl.b32 	%r147, %r146, 3;
	add.s32 	%r148, %r147, 64;
	selp.b32	%r149, %r148, 0, %p10;
	and.b32  	%r150, %r36, 7;
	shl.b32 	%r151, %r150, %r145;
	add.s32 	%r152, %r151, %r149;
	and.b32  	%r153, %r36, 56;
	setp.eq.s32	%p11, %r38, 0;
	and.pred  	%p12, %p10, %p11;
	add.s32 	%r154, %r147, 192;
	selp.b32	%r155, %r154, 0, %p12;
	add.s32 	%r156, %r155, %r153;
	add.s32 	%r39, %r11, %r152;
	add.s32 	%r40, %r11, %r156;
	ld.shared.u64 	%rd14, [%r39];
	ld.shared.u64 	%rd153, [%r40];
	ld.shared.u32 	%r42, [%r144+256];
	cvt.s64.s32	%rd16, %r42;
	ld.shared.u32 	%r43, [%r144+260];
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	@%p11 bra 	BB11_14;

	cvt.u32.u64	%r158, %rd16;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	// inline asm
	bfe.u32 %r157, %r158, 21, 5;
	// inline asm
	mov.u32 	%r159, -1;
	shr.u32 	%r160, %r159, %r157;
	add.s32 	%r161, %r160, -7;
	setp.eq.s32	%p13, %r157, 11;
	cvt.u32.u64	%r162, %rd153;
	selp.b32	%r163, 0, %r162, %p13;
	cvt.u32.u64	%r164, %rd14;
	setp.ne.s32	%p14, %r37, 10;
	selp.b32	%r165, %r163, %r164, %p14;
	add.s32 	%r166, %r165, %r42;
	and.b32  	%r167, %r166, %r161;
	cvt.u64.u32	%rd87, %r167;
	mul.lo.s32 	%r173, %r64, 2097216;
	cvt.u64.u32	%rd88, %r173;
	add.s64 	%rd89, %rd87, %rd88;
	cvta.to.global.u64 	%rd90, %rd39;
	add.s64 	%rd17, %rd90, %rd89;
	@%p14 bra 	BB11_13;
	bra.uni 	BB11_12;

BB11_13:
	ld.global.u64 	%rd153, [%rd17];
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB11_14:
	// inline asm
	// EXECUTION BEGIN
	// inline asm
	and.b32  	%r174, %r36, 131072;
	setp.eq.s32	%p15, %r174, 0;
	selp.b64	%rd155, %rd153, %rd16, %p15;
	setp.lt.u32	%p16, %r37, 4;
	@%p16 bra 	BB11_57;
	bra.uni 	BB11_15;

BB11_57:
	// inline asm
	// IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256) ------>
	// inline asm
	and.b32  	%r227, %r36, 524288;
	setp.eq.s32	%p41, %r227, 0;
	neg.s64 	%rd103, %rd155;
	selp.b64	%rd104, %rd155, %rd103, %p41;
	setp.eq.s32	%p42, %r37, 0;
	selp.b64	%rd105, %rd16, 0, %p42;
	add.s64 	%rd106, %rd105, %rd14;
	bfe.u32 	%r228, %r36, 15, 2;
	shl.b64 	%rd107, %rd104, %r228;
	setp.lt.u32	%p43, %r37, 2;
	selp.b64	%rd108, %rd107, 0, %p43;
	add.s64 	%rd109, %rd106, %rd108;
	mov.b64	%rd110, {%r42, %r43};
	and.b32  	%r229, %r36, 262144;
	setp.eq.s32	%p44, %r229, 0;
	selp.b64	%rd111, %rd104, %rd110, %p44;
	setp.eq.s32	%p45, %r37, 2;
	selp.b64	%rd112, %rd111, 1, %p45;
	mul.lo.s64 	%rd113, %rd109, %rd112;
	setp.eq.s32	%p46, %r37, 3;
	selp.b64	%rd114, %rd111, 0, %p46;
	xor.b64  	%rd155, %rd113, %rd114;
	// inline asm
	// <------ IADD_RS, IADD_M, ISUB_R, ISUB_M, IMUL_R, IMUL_M, IMUL_RCP, IXOR_R, IXOR_M, FSCAL_R (109/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_15:
	setp.eq.s32	%p17, %r37, 12;
	@%p17 bra 	BB11_47;
	bra.uni 	BB11_16;

BB11_47:
	// inline asm
	// FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256) ------>
	// inline asm
	@%p11 bra 	BB11_49;

	shl.b32 	%r222, %r146, 5;
	shr.u64 	%rd100, %rd155, %r222;
	cvt.u32.u64	%r223, %rd100;
	cvt.rn.f64.s32	%fd23, %r223;
	mov.b64 	 %rd155, %fd23;

BB11_49:
	and.b32  	%r224, %r36, 524288;
	setp.eq.s32	%p36, %r224, 0;
	xor.b64  	%rd101, %rd155, -9223372036854775808;
	selp.b64	%rd102, %rd155, %rd101, %p36;
	shr.u32 	%r225, %r36, 15;
	and.b32  	%r226, %r225, 1;
	setp.eq.b32	%p37, %r226, 1;
	mov.b64 	 %fd12, %rd14;
	mov.b64 	 %fd24, %rd102;
	selp.f64	%fd13, %fd24, 0d3FF0000000000000, %p37;
	selp.f64	%fd14, 0d0000000000000000, %fd24, %p37;
	setp.eq.s32	%p38, %r33, 0;
	@%p38 bra 	BB11_55;
	bra.uni 	BB11_50;

BB11_55:
	fma.rn.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB11_56;

BB11_12:
	st.global.u64 	[%rd17], %rd153;
	bra.uni 	BB11_59;

BB11_16:
	setp.eq.s32	%p18, %r37, 9;
	@%p18 bra 	BB11_44;
	bra.uni 	BB11_17;

BB11_44:
	// inline asm
	// CBRANCH (16/256) ------>
	// inline asm
	add.s64 	%rd155, %rd16, %rd14;
	cvt.u32.u64	%r210, %rd155;
	and.b32  	%r211, %r43, 31;
	mov.u32 	%r212, 255;
	shl.b32 	%r213, %r212, %r211;
	and.b32  	%r214, %r210, %r213;
	setp.ne.s32	%p34, %r214, 0;
	@%p34 bra 	BB11_46;

	shr.s32 	%r215, %r43, 5;
	sub.s32 	%r216, %r215, %r35;
	st.shared.u32 	[%r11+1016], %r216;

BB11_46:
	// inline asm
	// <------ CBRANCH (16/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_50:
	setp.eq.s32	%p39, %r33, 1;
	@%p39 bra 	BB11_54;
	bra.uni 	BB11_51;

BB11_54:
	fma.rm.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB11_56;

BB11_17:
	setp.eq.s32	%p19, %r37, 7;
	@%p19 bra 	BB11_43;
	bra.uni 	BB11_18;

BB11_43:
	// inline asm
	// IROR_R (10/256) ------>
	// inline asm
	cvt.u32.u64	%r208, %rd155;
	and.b32  	%r209, %r208, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd14, %r209;
	sub.u32 	%amt2, 64, %r209;
	shl.b64 	%rhs, %rd14, %amt2;
	add.u64 	%rd155, %lhs, %rhs;
	}
	// inline asm
	// <------ IROR_R (10/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_51:
	setp.eq.s32	%p40, %r33, 2;
	@%p40 bra 	BB11_53;
	bra.uni 	BB11_52;

BB11_53:
	fma.rp.f64 	%fd28, %fd12, %fd13, %fd14;
	bra.uni 	BB11_56;

BB11_18:
	setp.eq.s32	%p20, %r37, 11;
	@%p20 bra 	BB11_42;
	bra.uni 	BB11_19;

BB11_42:
	// inline asm
	// FSWAP_R (8/256) ------>
	// inline asm
	// inline asm
	mov.b64 {%r193,%r194}, %rd14;
	// inline asm
	and.b32  	%r201, %r63, 14;
	and.b32  	%r202, %r1, -16;
	add.s32 	%r203, %r201, %r202;
	mov.u32 	%r204, 3;
	shl.b32 	%r205, %r204, %r203;
	mov.u32 	%r206, 6175;
	mov.u32 	%r207, 1;
	shfl.sync.bfly.b32 	%r196|%p32, %r194, %r207, %r206, %r205;
	shfl.sync.bfly.b32 	%r195|%p33, %r193, %r207, %r206, %r205;
	// inline asm
	mov.b64 %rd155, {%r195,%r196};
	// inline asm
	// inline asm
	// <------ FSWAP_R (8/256)
	// inline asm
	bra.uni 	BB11_58;

BB11_52:
	fma.rz.f64 	%fd28, %fd12, %fd13, %fd14;

BB11_56:
	mov.b64 	 %rd155, %fd28;
	// inline asm
	// <------ FADD_R, FADD_M, FSUB_R, FSUB_M, FMUL_R (70/256)
	// inline asm

BB11_58:
	st.shared.u64 	[%r39], %rd155;
	// inline asm
	// EXECUTION END
	// inline asm

BB11_59:
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE BEGIN
	// inline asm
	and.b32  	%r231, %r1, -16;
	mov.u32 	%r232, 65535;
	shl.b32 	%r44, %r232, %r231;
	bar.warp.sync 	%r44;
	ld.shared.v2.u32 	{%r233, %r33}, [%r11+1016];
	// inline asm
	// SYNCHRONIZATION OF INSTRUCTION POINTER AND ROUNDING MODE END
	// inline asm
	add.s32 	%r236, %r35, %r233;
	add.s32 	%r317, %r236, 1;
	setp.lt.u32	%p47, %r317, %r15;
	@%p47 bra 	BB11_9;

BB11_60:
	setp.gt.u32	%p54, %r73, 7;
	@%p54 bra 	BB11_61;

	shr.u32 	%r244, %r14, 24;
	shr.u32 	%r246, %r1, 4;
	mul.wide.u32 	%rd115, %r246, 256;
	cvt.u32.u64	%r247, %rd115;
	shl.b32 	%r248, %r247, 3;
	add.s32 	%r250, %r62, %r248;
	add.s32 	%r251, %r250, %r244;
	bfe.u32 	%r252, %r14, 16, 8;
	add.s32 	%r253, %r250, %r252;
	ld.shared.u32 	%r254, [%r253];
	xor.b32  	%r255, %r254, %r311;
	ld.shared.u32 	%r256, [%r251];
	xor.b32  	%r257, %r255, %r256;
	and.b32  	%r324, %r257, 2147483584;
	cvt.u64.u32	%rd116, %r312;
	add.s64 	%rd117, %rd7, %rd116;
	add.s64 	%rd119, %rd118, %rd117;
	ld.global.u64 	%rd120, [%rd119];
	ld.shared.u64 	%rd121, [%r307];
	xor.b64  	%rd122, %rd120, %rd121;
	st.shared.u64 	[%r307], %rd122;
	st.global.u64 	[%rd149], %rd122;
	cvt.u64.u32	%rd123, %r73;
	add.s64 	%rd124, %rd123, %rd115;
	cvt.u32.u64	%r262, %rd124;
	shl.b32 	%r263, %r262, 3;
	add.s32 	%r264, %r62, %r263;
	ld.shared.u64 	%rd125, [%r264+64];
	ld.shared.u64 	%rd126, [%r264+128];
	xor.b64  	%rd127, %rd126, %rd125;
	st.global.u64 	[%rd150], %rd127;
	mov.u32 	%r310, 0;
	mov.u32 	%r311, %r312;
	mov.u32 	%r309, %r310;
	bra.uni 	BB11_63;

BB11_61:
	mov.u32 	%r324, %r312;

BB11_63:
	ld.param.u32 	%r303, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_5];
	add.s32 	%r308, %r308, 1;
	setp.lt.u32	%p49, %r308, %r303;
	mov.u32 	%r312, %r324;
	@%p49 bra 	BB11_6;
	bra.uni 	BB11_64;

BB11_4:
	mov.u32 	%r324, %r312;

BB11_64:
	and.b32  	%r269, %r63, 15;
	setp.gt.u32	%p50, %r269, 7;
	@%p50 bra 	BB11_71;

	ld.param.u64 	%rd143, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_0];
	cvta.to.global.u64 	%rd34, %rd143;
	mul.wide.u32 	%rd35, %r64, 256;
	cvt.u64.u32	%rd128, %r269;
	shl.b32 	%r276, %r1, 4;
	and.b32  	%r277, %r276, 536870656;
	or.b32  	%r278, %r269, %r277;
	shl.b32 	%r279, %r278, 3;
	add.s32 	%r281, %r62, %r279;
	ld.shared.u64 	%rd129, [%r281];
	or.b64  	%rd130, %rd35, %rd128;
	shl.b64 	%rd131, %rd130, 3;
	add.s64 	%rd36, %rd34, %rd131;
	st.global.u64 	[%rd36], %rd129;
	setp.ne.s32	%p51, %r269, 0;
	@%p51 bra 	BB11_67;

	ld.param.u64 	%rd147, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_1];
	mul.wide.u32 	%rd146, %r64, 4;
	cvta.to.global.u64 	%rd145, %rd147;
	add.s64 	%rd144, %rd145, %rd146;
	st.global.u32 	[%rd144], %r33;

BB11_67:
	ld.param.s8 	%rs6, [_Z10execute_vmILi16ELb1EEvPvS0_S0_PKvjjbb_param_7];
	and.b16  	%rs5, %rs6, 255;
	setp.eq.s16	%p52, %rs5, 0;
	@%p52 bra 	BB11_69;

	and.b32  	%r293, %r276, -256;
	cvt.u64.u32	%rd136, %r293;
	add.s64 	%rd137, %rd128, %rd136;
	cvt.u32.u64	%r294, %rd137;
	shl.b32 	%r295, %r294, 3;
	add.s32 	%r297, %r62, %r295;
	ld.shared.u64 	%rd138, [%r297+64];
	ld.shared.f64 	%fd25, [%r297+128];
	mov.b64 	 %rd139, %fd25;
	xor.b64  	%rd140, %rd139, %rd138;
	st.global.u64 	[%rd36+64], %rd140;
	st.global.f64 	[%rd36+128], %fd25;
	bra.uni 	BB11_71;

BB11_69:
	@%p51 bra 	BB11_71;

	shl.b64 	%rd141, %rd35, 3;
	add.s64 	%rd142, %rd34, %rd141;
	st.global.u32 	[%rd142+128], %r324;
	st.global.u32 	[%rd142+132], %r311;

BB11_71:
	ret;
}

	// .globl	_Z20blake2b_initial_hashILj76EEvPvPKvj
.visible .entry _Z20blake2b_initial_hashILj76EEvPvPKvj(
	.param .u64 _Z20blake2b_initial_hashILj76EEvPvPKvj_param_0,
	.param .u64 _Z20blake2b_initial_hashILj76EEvPvPKvj_param_1,
	.param .u32 _Z20blake2b_initial_hashILj76EEvPvPKvj_param_2
)
{
	.reg .b32 	%r<1740>;
	.reg .b64 	%rd<1308>;


	ld.param.u64 	%rd1, [_Z20blake2b_initial_hashILj76EEvPvPKvj_param_0];
	ld.param.u64 	%rd2, [_Z20blake2b_initial_hashILj76EEvPvPKvj_param_1];
	ld.param.u32 	%r769, [_Z20blake2b_initial_hashILj76EEvPvPKvj_param_2];
	mov.u32 	%r770, %ctaid.x;
	mov.u32 	%r771, %ntid.x;
	mov.u32 	%r772, %tid.x;
	mad.lo.s32 	%r773, %r771, %r770, %r772;
	cvta.to.global.u64 	%rd3, %rd2;
	ld.global.u64 	%rd4, [%rd3];
	ld.global.u64 	%rd5, [%rd3+8];
	ld.global.u64 	%rd6, [%rd3+16];
	ld.global.u64 	%rd7, [%rd3+24];
	ld.global.u64 	%rd8, [%rd3+32];
	ld.global.u64 	%rd9, [%rd3+40];
	ld.global.u64 	%rd10, [%rd3+48];
	ld.global.u64 	%rd11, [%rd3+56];
	ld.global.u64 	%rd12, [%rd3+64];
	ld.global.u32 	%rd13, [%rd3+72];
	add.s32 	%r774, %r773, %r769;
	cvt.u64.u32	%rd14, %r774;
	and.b64  	%rd15, %rd8, 72057594037927935;
	shl.b64 	%rd16, %rd14, 56;
	or.b64  	%rd17, %rd15, %rd16;
	and.b64  	%rd18, %rd9, -16777216;
	shr.u64 	%rd19, %rd14, 8;
	or.b64  	%rd20, %rd18, %rd19;
	add.s64 	%rd21, %rd4, -4965156021692249063;
	xor.b64  	%rd22, %rd21, 5840696475078001309;
	mov.b64	{%r775, %r776}, %rd22;
	mov.b64	%rd23, {%r776, %r775};
	add.s64 	%rd24, %rd23, 7640891576956012808;
	xor.b64  	%rd25, %rd24, 5840696475078001361;
	mov.b64	{%r777, %r778}, %rd25;
	mov.u32 	%r768, 1;
	mov.u32 	%r779, 25923;
	mov.u32 	%r780, 8455;
	prmt.b32 	%r781, %r777, %r778, %r780;
	prmt.b32 	%r782, %r777, %r778, %r779;
	mov.b64	%rd26, {%r782, %r781};
	add.s64 	%rd27, %rd5, %rd21;
	add.s64 	%rd28, %rd27, %rd26;
	xor.b64  	%rd29, %rd28, %rd23;
	mov.b64	{%r783, %r784}, %rd29;
	mov.u32 	%r785, 21554;
	mov.u32 	%r786, 4214;
	prmt.b32 	%r787, %r783, %r784, %r786;
	prmt.b32 	%r788, %r783, %r784, %r785;
	mov.b64	%rd30, {%r788, %r787};
	add.s64 	%rd31, %rd30, %rd24;
	xor.b64  	%rd32, %rd31, %rd26;
	mov.b64	{%r6, %r7}, %rd32;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r768;
	// inline asm
	mov.b64	%rd33, {%r1, %r5};
	add.s64 	%rd34, %rd6, 6227659224458531674;
	xor.b64  	%rd35, %rd34, -7276294671716946913;
	mov.b64	{%r789, %r790}, %rd35;
	mov.b64	%rd36, {%r790, %r789};
	add.s64 	%rd37, %rd36, -4942790177534073029;
	xor.b64  	%rd38, %rd37, -7276294671716946913;
	mov.b64	{%r791, %r792}, %rd38;
	prmt.b32 	%r793, %r791, %r792, %r780;
	prmt.b32 	%r794, %r791, %r792, %r779;
	mov.b64	%rd39, {%r794, %r793};
	add.s64 	%rd40, %rd7, %rd34;
	add.s64 	%rd41, %rd40, %rd39;
	xor.b64  	%rd42, %rd41, %rd36;
	mov.b64	{%r795, %r796}, %rd42;
	prmt.b32 	%r797, %r795, %r796, %r786;
	prmt.b32 	%r798, %r795, %r796, %r785;
	mov.b64	%rd43, {%r798, %r797};
	add.s64 	%rd44, %rd43, %rd37;
	xor.b64  	%rd45, %rd44, %rd39;
	mov.b64	{%r14, %r15}, %rd45;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r768;
	// inline asm
	mov.b64	%rd46, {%r9, %r13};
	add.s64 	%rd47, %rd17, 6625583534739731862;
	xor.b64  	%rd48, %rd47, -2270897969802886508;
	mov.b64	{%r799, %r800}, %rd48;
	mov.b64	%rd49, {%r800, %r799};
	add.s64 	%rd50, %rd49, 4354685564936845355;
	xor.b64  	%rd51, %rd50, 2270897969802886507;
	mov.b64	{%r801, %r802}, %rd51;
	prmt.b32 	%r803, %r801, %r802, %r780;
	prmt.b32 	%r804, %r801, %r802, %r779;
	mov.b64	%rd52, {%r804, %r803};
	add.s64 	%rd53, %rd20, %rd47;
	add.s64 	%rd54, %rd53, %rd52;
	xor.b64  	%rd55, %rd54, %rd49;
	mov.b64	{%r805, %r806}, %rd55;
	prmt.b32 	%r807, %r805, %r806, %r786;
	prmt.b32 	%r808, %r805, %r806, %r785;
	mov.b64	%rd56, {%r808, %r807};
	add.s64 	%rd57, %rd56, %rd50;
	xor.b64  	%rd58, %rd57, %rd52;
	mov.b64	{%r22, %r23}, %rd58;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r768;
	// inline asm
	mov.b64	%rd59, {%r17, %r21};
	add.s64 	%rd60, %rd10, 85782056580896874;
	xor.b64  	%rd61, %rd60, 6620516959819538809;
	mov.b64	{%r809, %r810}, %rd61;
	mov.b64	%rd62, {%r810, %r809};
	add.s64 	%rd63, %rd62, -6534734903238641935;
	xor.b64  	%rd64, %rd63, 6620516959819538809;
	mov.b64	{%r811, %r812}, %rd64;
	prmt.b32 	%r813, %r811, %r812, %r780;
	prmt.b32 	%r814, %r811, %r812, %r779;
	mov.b64	%rd65, {%r814, %r813};
	add.s64 	%rd66, %rd11, %rd60;
	add.s64 	%rd67, %rd66, %rd65;
	xor.b64  	%rd68, %rd67, %rd62;
	mov.b64	{%r815, %r816}, %rd68;
	prmt.b32 	%r817, %r815, %r816, %r786;
	prmt.b32 	%r818, %r815, %r816, %r785;
	mov.b64	%rd69, {%r818, %r817};
	add.s64 	%rd70, %rd69, %rd63;
	xor.b64  	%rd71, %rd70, %rd65;
	mov.b64	{%r30, %r31}, %rd71;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r768;
	// inline asm
	mov.b64	%rd72, {%r25, %r29};
	add.s64 	%rd73, %rd28, %rd12;
	add.s64 	%rd74, %rd73, %rd46;
	xor.b64  	%rd75, %rd69, %rd74;
	mov.b64	{%r819, %r820}, %rd75;
	mov.b64	%rd76, {%r820, %r819};
	add.s64 	%rd77, %rd76, %rd57;
	xor.b64  	%rd78, %rd77, %rd46;
	mov.b64	{%r821, %r822}, %rd78;
	prmt.b32 	%r823, %r821, %r822, %r780;
	prmt.b32 	%r824, %r821, %r822, %r779;
	mov.b64	%rd79, {%r824, %r823};
	add.s64 	%rd80, %rd74, %rd13;
	add.s64 	%rd81, %rd80, %rd79;
	xor.b64  	%rd82, %rd76, %rd81;
	mov.b64	{%r825, %r826}, %rd82;
	prmt.b32 	%r827, %r825, %r826, %r786;
	prmt.b32 	%r828, %r825, %r826, %r785;
	mov.b64	%rd83, {%r828, %r827};
	add.s64 	%rd84, %rd77, %rd83;
	xor.b64  	%rd85, %rd84, %rd79;
	mov.b64	{%r38, %r39}, %rd85;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r768;
	// inline asm
	mov.b64	%rd86, {%r33, %r37};
	add.s64 	%rd87, %rd59, %rd41;
	xor.b64  	%rd88, %rd87, %rd30;
	mov.b64	{%r829, %r830}, %rd88;
	mov.b64	%rd89, {%r830, %r829};
	add.s64 	%rd90, %rd89, %rd70;
	xor.b64  	%rd91, %rd90, %rd59;
	mov.b64	{%r831, %r832}, %rd91;
	prmt.b32 	%r833, %r831, %r832, %r780;
	prmt.b32 	%r834, %r831, %r832, %r779;
	mov.b64	%rd92, {%r834, %r833};
	add.s64 	%rd93, %rd92, %rd87;
	xor.b64  	%rd94, %rd93, %rd89;
	mov.b64	{%r835, %r836}, %rd94;
	prmt.b32 	%r837, %r835, %r836, %r786;
	prmt.b32 	%r838, %r835, %r836, %r785;
	mov.b64	%rd95, {%r838, %r837};
	add.s64 	%rd96, %rd95, %rd90;
	xor.b64  	%rd97, %rd96, %rd92;
	mov.b64	{%r46, %r47}, %rd97;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r768;
	// inline asm
	mov.b64	%rd98, {%r41, %r45};
	add.s64 	%rd99, %rd72, %rd54;
	xor.b64  	%rd100, %rd99, %rd43;
	mov.b64	{%r839, %r840}, %rd100;
	mov.b64	%rd101, {%r840, %r839};
	add.s64 	%rd102, %rd101, %rd31;
	xor.b64  	%rd103, %rd102, %rd72;
	mov.b64	{%r841, %r842}, %rd103;
	prmt.b32 	%r843, %r841, %r842, %r780;
	prmt.b32 	%r844, %r841, %r842, %r779;
	mov.b64	%rd104, {%r844, %r843};
	add.s64 	%rd105, %rd104, %rd99;
	xor.b64  	%rd106, %rd105, %rd101;
	mov.b64	{%r845, %r846}, %rd106;
	prmt.b32 	%r847, %r845, %r846, %r786;
	prmt.b32 	%r848, %r845, %r846, %r785;
	mov.b64	%rd107, {%r848, %r847};
	add.s64 	%rd108, %rd107, %rd102;
	xor.b64  	%rd109, %rd108, %rd104;
	mov.b64	{%r54, %r55}, %rd109;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r768;
	// inline asm
	mov.b64	%rd110, {%r49, %r53};
	add.s64 	%rd111, %rd67, %rd33;
	xor.b64  	%rd112, %rd111, %rd56;
	mov.b64	{%r849, %r850}, %rd112;
	mov.b64	%rd113, {%r850, %r849};
	add.s64 	%rd114, %rd113, %rd44;
	xor.b64  	%rd115, %rd114, %rd33;
	mov.b64	{%r851, %r852}, %rd115;
	prmt.b32 	%r853, %r851, %r852, %r780;
	prmt.b32 	%r854, %r851, %r852, %r779;
	mov.b64	%rd116, {%r854, %r853};
	add.s64 	%rd117, %rd116, %rd111;
	xor.b64  	%rd118, %rd117, %rd113;
	mov.b64	{%r855, %r856}, %rd118;
	prmt.b32 	%r857, %r855, %r856, %r786;
	prmt.b32 	%r858, %r855, %r856, %r785;
	mov.b64	%rd119, {%r858, %r857};
	add.s64 	%rd120, %rd119, %rd114;
	xor.b64  	%rd121, %rd120, %rd116;
	mov.b64	{%r62, %r63}, %rd121;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r768;
	// inline asm
	mov.b64	%rd122, {%r57, %r61};
	add.s64 	%rd123, %rd122, %rd81;
	xor.b64  	%rd124, %rd123, %rd95;
	mov.b64	{%r859, %r860}, %rd124;
	mov.b64	%rd125, {%r860, %r859};
	add.s64 	%rd126, %rd125, %rd108;
	xor.b64  	%rd127, %rd126, %rd122;
	mov.b64	{%r861, %r862}, %rd127;
	prmt.b32 	%r863, %r861, %r862, %r780;
	prmt.b32 	%r864, %r861, %r862, %r779;
	mov.b64	%rd128, {%r864, %r863};
	add.s64 	%rd129, %rd128, %rd123;
	xor.b64  	%rd130, %rd125, %rd129;
	mov.b64	{%r865, %r866}, %rd130;
	prmt.b32 	%r867, %r865, %r866, %r786;
	prmt.b32 	%r868, %r865, %r866, %r785;
	mov.b64	%rd131, {%r868, %r867};
	add.s64 	%rd132, %rd126, %rd131;
	xor.b64  	%rd133, %rd132, %rd128;
	mov.b64	{%r70, %r71}, %rd133;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r768;
	// inline asm
	mov.b64	%rd134, {%r65, %r69};
	add.s64 	%rd135, %rd86, %rd17;
	add.s64 	%rd136, %rd135, %rd93;
	xor.b64  	%rd137, %rd107, %rd136;
	mov.b64	{%r869, %r870}, %rd137;
	mov.b64	%rd138, {%r870, %r869};
	add.s64 	%rd139, %rd120, %rd138;
	xor.b64  	%rd140, %rd139, %rd86;
	mov.b64	{%r871, %r872}, %rd140;
	prmt.b32 	%r873, %r871, %r872, %r780;
	prmt.b32 	%r874, %r871, %r872, %r779;
	mov.b64	%rd141, {%r874, %r873};
	add.s64 	%rd142, %rd136, %rd12;
	add.s64 	%rd143, %rd142, %rd141;
	xor.b64  	%rd144, %rd143, %rd138;
	mov.b64	{%r875, %r876}, %rd144;
	prmt.b32 	%r877, %r875, %r876, %r786;
	prmt.b32 	%r878, %r875, %r876, %r785;
	mov.b64	%rd145, {%r878, %r877};
	add.s64 	%rd146, %rd145, %rd139;
	xor.b64  	%rd147, %rd146, %rd141;
	mov.b64	{%r78, %r79}, %rd147;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r768;
	// inline asm
	mov.b64	%rd148, {%r73, %r77};
	add.s64 	%rd149, %rd98, %rd13;
	add.s64 	%rd150, %rd149, %rd105;
	xor.b64  	%rd151, %rd119, %rd150;
	mov.b64	{%r879, %r880}, %rd151;
	mov.b64	%rd152, {%r880, %r879};
	add.s64 	%rd153, %rd152, %rd84;
	xor.b64  	%rd154, %rd153, %rd98;
	mov.b64	{%r881, %r882}, %rd154;
	prmt.b32 	%r883, %r881, %r882, %r780;
	prmt.b32 	%r884, %r881, %r882, %r779;
	mov.b64	%rd155, {%r884, %r883};
	add.s64 	%rd156, %rd155, %rd150;
	xor.b64  	%rd157, %rd156, %rd152;
	mov.b64	{%r885, %r886}, %rd157;
	prmt.b32 	%r887, %r885, %r886, %r786;
	prmt.b32 	%r888, %r885, %r886, %r785;
	mov.b64	%rd158, {%r888, %r887};
	add.s64 	%rd159, %rd158, %rd153;
	xor.b64  	%rd160, %rd159, %rd155;
	mov.b64	{%r86, %r87}, %rd160;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r768;
	// inline asm
	mov.b64	%rd161, {%r81, %r85};
	add.s64 	%rd162, %rd117, %rd110;
	xor.b64  	%rd163, %rd162, %rd83;
	mov.b64	{%r889, %r890}, %rd163;
	mov.b64	%rd164, {%r890, %r889};
	add.s64 	%rd165, %rd164, %rd96;
	xor.b64  	%rd166, %rd165, %rd110;
	mov.b64	{%r891, %r892}, %rd166;
	prmt.b32 	%r893, %r891, %r892, %r780;
	prmt.b32 	%r894, %r891, %r892, %r779;
	mov.b64	%rd167, {%r894, %r893};
	add.s64 	%rd168, %rd162, %rd10;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r895, %r896}, %rd170;
	prmt.b32 	%r897, %r895, %r896, %r786;
	prmt.b32 	%r898, %r895, %r896, %r785;
	mov.b64	%rd171, {%r898, %r897};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r94, %r95}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r768;
	// inline asm
	mov.b64	%rd174, {%r89, %r93};
	add.s64 	%rd175, %rd129, %rd5;
	add.s64 	%rd176, %rd175, %rd148;
	xor.b64  	%rd177, %rd171, %rd176;
	mov.b64	{%r899, %r900}, %rd177;
	mov.b64	%rd178, {%r900, %r899};
	add.s64 	%rd179, %rd178, %rd159;
	xor.b64  	%rd180, %rd179, %rd148;
	mov.b64	{%r901, %r902}, %rd180;
	prmt.b32 	%r903, %r901, %r902, %r780;
	prmt.b32 	%r904, %r901, %r902, %r779;
	mov.b64	%rd181, {%r904, %r903};
	add.s64 	%rd182, %rd181, %rd176;
	xor.b64  	%rd183, %rd178, %rd182;
	mov.b64	{%r905, %r906}, %rd183;
	prmt.b32 	%r907, %r905, %r906, %r786;
	prmt.b32 	%r908, %r905, %r906, %r785;
	mov.b64	%rd184, {%r908, %r907};
	add.s64 	%rd185, %rd179, %rd184;
	xor.b64  	%rd186, %rd185, %rd181;
	mov.b64	{%r102, %r103}, %rd186;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r768;
	// inline asm
	mov.b64	%rd187, {%r97, %r101};
	add.s64 	%rd188, %rd143, %rd4;
	add.s64 	%rd189, %rd188, %rd161;
	xor.b64  	%rd190, %rd189, %rd131;
	mov.b64	{%r909, %r910}, %rd190;
	mov.b64	%rd191, {%r910, %r909};
	add.s64 	%rd192, %rd191, %rd172;
	xor.b64  	%rd193, %rd192, %rd161;
	mov.b64	{%r911, %r912}, %rd193;
	prmt.b32 	%r913, %r911, %r912, %r780;
	prmt.b32 	%r914, %r911, %r912, %r779;
	mov.b64	%rd194, {%r914, %r913};
	add.s64 	%rd195, %rd189, %rd6;
	add.s64 	%rd196, %rd195, %rd194;
	xor.b64  	%rd197, %rd196, %rd191;
	mov.b64	{%r915, %r916}, %rd197;
	prmt.b32 	%r917, %r915, %r916, %r786;
	prmt.b32 	%r918, %r915, %r916, %r785;
	mov.b64	%rd198, {%r918, %r917};
	add.s64 	%rd199, %rd198, %rd192;
	xor.b64  	%rd200, %rd199, %rd194;
	mov.b64	{%r110, %r111}, %rd200;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r768;
	// inline asm
	mov.b64	%rd201, {%r105, %r109};
	add.s64 	%rd202, %rd174, %rd156;
	xor.b64  	%rd203, %rd202, %rd145;
	mov.b64	{%r919, %r920}, %rd203;
	mov.b64	%rd204, {%r920, %r919};
	add.s64 	%rd205, %rd204, %rd132;
	xor.b64  	%rd206, %rd205, %rd174;
	mov.b64	{%r921, %r922}, %rd206;
	prmt.b32 	%r923, %r921, %r922, %r780;
	prmt.b32 	%r924, %r921, %r922, %r779;
	mov.b64	%rd207, {%r924, %r923};
	add.s64 	%rd208, %rd202, %rd11;
	add.s64 	%rd209, %rd208, %rd207;
	xor.b64  	%rd210, %rd209, %rd204;
	mov.b64	{%r925, %r926}, %rd210;
	prmt.b32 	%r927, %r925, %r926, %r786;
	prmt.b32 	%r928, %r925, %r926, %r785;
	mov.b64	%rd211, {%r928, %r927};
	add.s64 	%rd212, %rd211, %rd205;
	xor.b64  	%rd213, %rd212, %rd207;
	mov.b64	{%r118, %r119}, %rd213;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r768;
	// inline asm
	mov.b64	%rd214, {%r113, %r117};
	add.s64 	%rd215, %rd134, %rd20;
	add.s64 	%rd216, %rd215, %rd169;
	xor.b64  	%rd217, %rd216, %rd158;
	mov.b64	{%r929, %r930}, %rd217;
	mov.b64	%rd218, {%r930, %r929};
	add.s64 	%rd219, %rd218, %rd146;
	xor.b64  	%rd220, %rd219, %rd134;
	mov.b64	{%r931, %r932}, %rd220;
	prmt.b32 	%r933, %r931, %r932, %r780;
	prmt.b32 	%r934, %r931, %r932, %r779;
	mov.b64	%rd221, {%r934, %r933};
	add.s64 	%rd222, %rd216, %rd7;
	add.s64 	%rd223, %rd222, %rd221;
	xor.b64  	%rd224, %rd223, %rd218;
	mov.b64	{%r935, %r936}, %rd224;
	prmt.b32 	%r937, %r935, %r936, %r786;
	prmt.b32 	%r938, %r935, %r936, %r785;
	mov.b64	%rd225, {%r938, %r937};
	add.s64 	%rd226, %rd225, %rd219;
	xor.b64  	%rd227, %rd226, %rd221;
	mov.b64	{%r126, %r127}, %rd227;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r768;
	// inline asm
	mov.b64	%rd228, {%r121, %r125};
	add.s64 	%rd229, %rd228, %rd182;
	xor.b64  	%rd230, %rd229, %rd198;
	mov.b64	{%r939, %r940}, %rd230;
	mov.b64	%rd231, {%r940, %r939};
	add.s64 	%rd232, %rd231, %rd212;
	xor.b64  	%rd233, %rd232, %rd228;
	mov.b64	{%r941, %r942}, %rd233;
	prmt.b32 	%r943, %r941, %r942, %r780;
	prmt.b32 	%r944, %r941, %r942, %r779;
	mov.b64	%rd234, {%r944, %r943};
	add.s64 	%rd235, %rd229, %rd12;
	add.s64 	%rd236, %rd235, %rd234;
	xor.b64  	%rd237, %rd231, %rd236;
	mov.b64	{%r945, %r946}, %rd237;
	prmt.b32 	%r947, %r945, %r946, %r786;
	prmt.b32 	%r948, %r945, %r946, %r785;
	mov.b64	%rd238, {%r948, %r947};
	add.s64 	%rd239, %rd232, %rd238;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r134, %r135}, %rd240;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r768;
	// inline asm
	mov.b64	%rd241, {%r129, %r133};
	add.s64 	%rd242, %rd196, %rd187;
	xor.b64  	%rd243, %rd211, %rd242;
	mov.b64	{%r949, %r950}, %rd243;
	mov.b64	%rd244, {%r950, %r949};
	add.s64 	%rd245, %rd226, %rd244;
	xor.b64  	%rd246, %rd245, %rd187;
	mov.b64	{%r951, %r952}, %rd246;
	prmt.b32 	%r953, %r951, %r952, %r780;
	prmt.b32 	%r954, %r951, %r952, %r779;
	mov.b64	%rd247, {%r954, %r953};
	add.s64 	%rd248, %rd242, %rd4;
	add.s64 	%rd249, %rd248, %rd247;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r955, %r956}, %rd250;
	prmt.b32 	%r957, %r955, %r956, %r786;
	prmt.b32 	%r958, %r955, %r956, %r785;
	mov.b64	%rd251, {%r958, %r957};
	add.s64 	%rd252, %rd251, %rd245;
	xor.b64  	%rd253, %rd252, %rd247;
	mov.b64	{%r142, %r143}, %rd253;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r768;
	// inline asm
	mov.b64	%rd254, {%r137, %r141};
	add.s64 	%rd255, %rd201, %rd20;
	add.s64 	%rd256, %rd255, %rd209;
	xor.b64  	%rd257, %rd225, %rd256;
	mov.b64	{%r959, %r960}, %rd257;
	mov.b64	%rd258, {%r960, %r959};
	add.s64 	%rd259, %rd258, %rd185;
	xor.b64  	%rd260, %rd259, %rd201;
	mov.b64	{%r961, %r962}, %rd260;
	prmt.b32 	%r963, %r961, %r962, %r780;
	prmt.b32 	%r964, %r961, %r962, %r779;
	mov.b64	%rd261, {%r964, %r963};
	add.s64 	%rd262, %rd256, %rd6;
	add.s64 	%rd263, %rd262, %rd261;
	xor.b64  	%rd264, %rd263, %rd258;
	mov.b64	{%r965, %r966}, %rd264;
	prmt.b32 	%r967, %r965, %r966, %r786;
	prmt.b32 	%r968, %r965, %r966, %r785;
	mov.b64	%rd265, {%r968, %r967};
	add.s64 	%rd266, %rd265, %rd259;
	xor.b64  	%rd267, %rd266, %rd261;
	mov.b64	{%r150, %r151}, %rd267;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r768;
	// inline asm
	mov.b64	%rd268, {%r145, %r149};
	add.s64 	%rd269, %rd223, %rd214;
	xor.b64  	%rd270, %rd269, %rd184;
	mov.b64	{%r969, %r970}, %rd270;
	mov.b64	%rd271, {%r970, %r969};
	add.s64 	%rd272, %rd271, %rd199;
	xor.b64  	%rd273, %rd272, %rd214;
	mov.b64	{%r971, %r972}, %rd273;
	prmt.b32 	%r973, %r971, %r972, %r780;
	prmt.b32 	%r974, %r971, %r972, %r779;
	mov.b64	%rd274, {%r974, %r973};
	add.s64 	%rd275, %rd274, %rd269;
	xor.b64  	%rd276, %rd275, %rd271;
	mov.b64	{%r975, %r976}, %rd276;
	prmt.b32 	%r977, %r975, %r976, %r786;
	prmt.b32 	%r978, %r975, %r976, %r785;
	mov.b64	%rd277, {%r978, %r977};
	add.s64 	%rd278, %rd277, %rd272;
	xor.b64  	%rd279, %rd278, %rd274;
	mov.b64	{%r158, %r159}, %rd279;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r768;
	// inline asm
	mov.b64	%rd280, {%r153, %r157};
	add.s64 	%rd281, %rd254, %rd236;
	xor.b64  	%rd282, %rd277, %rd281;
	mov.b64	{%r979, %r980}, %rd282;
	mov.b64	%rd283, {%r980, %r979};
	add.s64 	%rd284, %rd283, %rd266;
	xor.b64  	%rd285, %rd284, %rd254;
	mov.b64	{%r981, %r982}, %rd285;
	prmt.b32 	%r983, %r981, %r982, %r780;
	prmt.b32 	%r984, %r981, %r982, %r779;
	mov.b64	%rd286, {%r984, %r983};
	add.s64 	%rd287, %rd286, %rd281;
	xor.b64  	%rd288, %rd283, %rd287;
	mov.b64	{%r985, %r986}, %rd288;
	prmt.b32 	%r987, %r985, %r986, %r786;
	prmt.b32 	%r988, %r985, %r986, %r785;
	mov.b64	%rd289, {%r988, %r987};
	add.s64 	%rd290, %rd284, %rd289;
	xor.b64  	%rd291, %rd290, %rd286;
	mov.b64	{%r166, %r167}, %rd291;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r768;
	// inline asm
	mov.b64	%rd292, {%r161, %r165};
	add.s64 	%rd293, %rd249, %rd7;
	add.s64 	%rd294, %rd293, %rd268;
	xor.b64  	%rd295, %rd294, %rd238;
	mov.b64	{%r989, %r990}, %rd295;
	mov.b64	%rd296, {%r990, %r989};
	add.s64 	%rd297, %rd296, %rd278;
	xor.b64  	%rd298, %rd297, %rd268;
	mov.b64	{%r991, %r992}, %rd298;
	prmt.b32 	%r993, %r991, %r992, %r780;
	prmt.b32 	%r994, %r991, %r992, %r779;
	mov.b64	%rd299, {%r994, %r993};
	add.s64 	%rd300, %rd294, %rd10;
	add.s64 	%rd301, %rd300, %rd299;
	xor.b64  	%rd302, %rd301, %rd296;
	mov.b64	{%r995, %r996}, %rd302;
	prmt.b32 	%r997, %r995, %r996, %r786;
	prmt.b32 	%r998, %r995, %r996, %r785;
	mov.b64	%rd303, {%r998, %r997};
	add.s64 	%rd304, %rd303, %rd297;
	xor.b64  	%rd305, %rd304, %rd299;
	mov.b64	{%r174, %r175}, %rd305;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r768;
	// inline asm
	mov.b64	%rd306, {%r169, %r173};
	add.s64 	%rd307, %rd263, %rd11;
	add.s64 	%rd308, %rd307, %rd280;
	xor.b64  	%rd309, %rd308, %rd251;
	mov.b64	{%r999, %r1000}, %rd309;
	mov.b64	%rd310, {%r1000, %r999};
	add.s64 	%rd311, %rd310, %rd239;
	xor.b64  	%rd312, %rd311, %rd280;
	mov.b64	{%r1001, %r1002}, %rd312;
	prmt.b32 	%r1003, %r1001, %r1002, %r780;
	prmt.b32 	%r1004, %r1001, %r1002, %r779;
	mov.b64	%rd313, {%r1004, %r1003};
	add.s64 	%rd314, %rd308, %rd5;
	add.s64 	%rd315, %rd314, %rd313;
	xor.b64  	%rd316, %rd315, %rd310;
	mov.b64	{%r1005, %r1006}, %rd316;
	prmt.b32 	%r1007, %r1005, %r1006, %r786;
	prmt.b32 	%r1008, %r1005, %r1006, %r785;
	mov.b64	%rd317, {%r1008, %r1007};
	add.s64 	%rd318, %rd317, %rd311;
	xor.b64  	%rd319, %rd318, %rd313;
	mov.b64	{%r182, %r183}, %rd319;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r768;
	// inline asm
	mov.b64	%rd320, {%r177, %r181};
	add.s64 	%rd321, %rd241, %rd13;
	add.s64 	%rd322, %rd321, %rd275;
	xor.b64  	%rd323, %rd322, %rd265;
	mov.b64	{%r1009, %r1010}, %rd323;
	mov.b64	%rd324, {%r1010, %r1009};
	add.s64 	%rd325, %rd324, %rd252;
	xor.b64  	%rd326, %rd325, %rd241;
	mov.b64	{%r1011, %r1012}, %rd326;
	prmt.b32 	%r1013, %r1011, %r1012, %r780;
	prmt.b32 	%r1014, %r1011, %r1012, %r779;
	mov.b64	%rd327, {%r1014, %r1013};
	add.s64 	%rd328, %rd322, %rd17;
	add.s64 	%rd329, %rd328, %rd327;
	xor.b64  	%rd330, %rd329, %rd324;
	mov.b64	{%r1015, %r1016}, %rd330;
	prmt.b32 	%r1017, %r1015, %r1016, %r786;
	prmt.b32 	%r1018, %r1015, %r1016, %r785;
	mov.b64	%rd331, {%r1018, %r1017};
	add.s64 	%rd332, %rd331, %rd325;
	xor.b64  	%rd333, %rd332, %rd327;
	mov.b64	{%r190, %r191}, %rd333;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r768;
	// inline asm
	mov.b64	%rd334, {%r185, %r189};
	add.s64 	%rd335, %rd287, %rd11;
	add.s64 	%rd336, %rd335, %rd334;
	xor.b64  	%rd337, %rd336, %rd303;
	mov.b64	{%r1019, %r1020}, %rd337;
	mov.b64	%rd338, {%r1020, %r1019};
	add.s64 	%rd339, %rd338, %rd318;
	xor.b64  	%rd340, %rd339, %rd334;
	mov.b64	{%r1021, %r1022}, %rd340;
	prmt.b32 	%r1023, %r1021, %r1022, %r780;
	prmt.b32 	%r1024, %r1021, %r1022, %r779;
	mov.b64	%rd341, {%r1024, %r1023};
	add.s64 	%rd342, %rd336, %rd13;
	add.s64 	%rd343, %rd342, %rd341;
	xor.b64  	%rd344, %rd338, %rd343;
	mov.b64	{%r1025, %r1026}, %rd344;
	prmt.b32 	%r1027, %r1025, %r1026, %r786;
	prmt.b32 	%r1028, %r1025, %r1026, %r785;
	mov.b64	%rd345, {%r1028, %r1027};
	add.s64 	%rd346, %rd339, %rd345;
	xor.b64  	%rd347, %rd346, %rd341;
	mov.b64	{%r198, %r199}, %rd347;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r768;
	// inline asm
	mov.b64	%rd348, {%r193, %r197};
	add.s64 	%rd349, %rd292, %rd7;
	add.s64 	%rd350, %rd349, %rd301;
	xor.b64  	%rd351, %rd317, %rd350;
	mov.b64	{%r1029, %r1030}, %rd351;
	mov.b64	%rd352, {%r1030, %r1029};
	add.s64 	%rd353, %rd332, %rd352;
	xor.b64  	%rd354, %rd353, %rd292;
	mov.b64	{%r1031, %r1032}, %rd354;
	prmt.b32 	%r1033, %r1031, %r1032, %r780;
	prmt.b32 	%r1034, %r1031, %r1032, %r779;
	mov.b64	%rd355, {%r1034, %r1033};
	add.s64 	%rd356, %rd350, %rd5;
	add.s64 	%rd357, %rd356, %rd355;
	xor.b64  	%rd358, %rd357, %rd352;
	mov.b64	{%r1035, %r1036}, %rd358;
	prmt.b32 	%r1037, %r1035, %r1036, %r786;
	prmt.b32 	%r1038, %r1035, %r1036, %r785;
	mov.b64	%rd359, {%r1038, %r1037};
	add.s64 	%rd360, %rd359, %rd353;
	xor.b64  	%rd361, %rd360, %rd355;
	mov.b64	{%r206, %r207}, %rd361;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r768;
	// inline asm
	mov.b64	%rd362, {%r201, %r205};
	add.s64 	%rd363, %rd315, %rd306;
	xor.b64  	%rd364, %rd331, %rd363;
	mov.b64	{%r1039, %r1040}, %rd364;
	mov.b64	%rd365, {%r1040, %r1039};
	add.s64 	%rd366, %rd365, %rd290;
	xor.b64  	%rd367, %rd366, %rd306;
	mov.b64	{%r1041, %r1042}, %rd367;
	prmt.b32 	%r1043, %r1041, %r1042, %r780;
	prmt.b32 	%r1044, %r1041, %r1042, %r779;
	mov.b64	%rd368, {%r1044, %r1043};
	add.s64 	%rd369, %rd368, %rd363;
	xor.b64  	%rd370, %rd369, %rd365;
	mov.b64	{%r1045, %r1046}, %rd370;
	prmt.b32 	%r1047, %r1045, %r1046, %r786;
	prmt.b32 	%r1048, %r1045, %r1046, %r785;
	mov.b64	%rd371, {%r1048, %r1047};
	add.s64 	%rd372, %rd371, %rd366;
	xor.b64  	%rd373, %rd372, %rd368;
	mov.b64	{%r214, %r215}, %rd373;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r768;
	// inline asm
	mov.b64	%rd374, {%r209, %r213};
	add.s64 	%rd375, %rd329, %rd320;
	xor.b64  	%rd376, %rd375, %rd289;
	mov.b64	{%r1049, %r1050}, %rd376;
	mov.b64	%rd377, {%r1050, %r1049};
	add.s64 	%rd378, %rd377, %rd304;
	xor.b64  	%rd379, %rd378, %rd320;
	mov.b64	{%r1051, %r1052}, %rd379;
	prmt.b32 	%r1053, %r1051, %r1052, %r780;
	prmt.b32 	%r1054, %r1051, %r1052, %r779;
	mov.b64	%rd380, {%r1054, %r1053};
	add.s64 	%rd381, %rd380, %rd375;
	xor.b64  	%rd382, %rd381, %rd377;
	mov.b64	{%r1055, %r1056}, %rd382;
	prmt.b32 	%r1057, %r1055, %r1056, %r786;
	prmt.b32 	%r1058, %r1055, %r1056, %r785;
	mov.b64	%rd383, {%r1058, %r1057};
	add.s64 	%rd384, %rd383, %rd378;
	xor.b64  	%rd385, %rd384, %rd380;
	mov.b64	{%r222, %r223}, %rd385;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r768;
	// inline asm
	mov.b64	%rd386, {%r217, %r221};
	add.s64 	%rd387, %rd343, %rd6;
	add.s64 	%rd388, %rd387, %rd362;
	xor.b64  	%rd389, %rd383, %rd388;
	mov.b64	{%r1059, %r1060}, %rd389;
	mov.b64	%rd390, {%r1060, %r1059};
	add.s64 	%rd391, %rd390, %rd372;
	xor.b64  	%rd392, %rd391, %rd362;
	mov.b64	{%r1061, %r1062}, %rd392;
	prmt.b32 	%r1063, %r1061, %r1062, %r780;
	prmt.b32 	%r1064, %r1061, %r1062, %r779;
	mov.b64	%rd393, {%r1064, %r1063};
	add.s64 	%rd394, %rd388, %rd10;
	add.s64 	%rd395, %rd394, %rd393;
	xor.b64  	%rd396, %rd390, %rd395;
	mov.b64	{%r1065, %r1066}, %rd396;
	prmt.b32 	%r1067, %r1065, %r1066, %r786;
	prmt.b32 	%r1068, %r1065, %r1066, %r785;
	mov.b64	%rd397, {%r1068, %r1067};
	add.s64 	%rd398, %rd391, %rd397;
	xor.b64  	%rd399, %rd398, %rd393;
	mov.b64	{%r230, %r231}, %rd399;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r768;
	// inline asm
	mov.b64	%rd400, {%r225, %r229};
	add.s64 	%rd401, %rd357, %rd20;
	add.s64 	%rd402, %rd401, %rd374;
	xor.b64  	%rd403, %rd402, %rd345;
	mov.b64	{%r1069, %r1070}, %rd403;
	mov.b64	%rd404, {%r1070, %r1069};
	add.s64 	%rd405, %rd404, %rd384;
	xor.b64  	%rd406, %rd405, %rd374;
	mov.b64	{%r1071, %r1072}, %rd406;
	prmt.b32 	%r1073, %r1071, %r1072, %r780;
	prmt.b32 	%r1074, %r1071, %r1072, %r779;
	mov.b64	%rd407, {%r1074, %r1073};
	add.s64 	%rd408, %rd407, %rd402;
	xor.b64  	%rd409, %rd408, %rd404;
	mov.b64	{%r1075, %r1076}, %rd409;
	prmt.b32 	%r1077, %r1075, %r1076, %r786;
	prmt.b32 	%r1078, %r1075, %r1076, %r785;
	mov.b64	%rd410, {%r1078, %r1077};
	add.s64 	%rd411, %rd410, %rd405;
	xor.b64  	%rd412, %rd411, %rd407;
	mov.b64	{%r238, %r239}, %rd412;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r768;
	// inline asm
	mov.b64	%rd413, {%r233, %r237};
	add.s64 	%rd414, %rd369, %rd17;
	add.s64 	%rd415, %rd414, %rd386;
	xor.b64  	%rd416, %rd415, %rd359;
	mov.b64	{%r1079, %r1080}, %rd416;
	mov.b64	%rd417, {%r1080, %r1079};
	add.s64 	%rd418, %rd417, %rd346;
	xor.b64  	%rd419, %rd418, %rd386;
	mov.b64	{%r1081, %r1082}, %rd419;
	prmt.b32 	%r1083, %r1081, %r1082, %r780;
	prmt.b32 	%r1084, %r1081, %r1082, %r779;
	mov.b64	%rd420, {%r1084, %r1083};
	add.s64 	%rd421, %rd415, %rd4;
	add.s64 	%rd422, %rd421, %rd420;
	xor.b64  	%rd423, %rd422, %rd417;
	mov.b64	{%r1085, %r1086}, %rd423;
	prmt.b32 	%r1087, %r1085, %r1086, %r786;
	prmt.b32 	%r1088, %r1085, %r1086, %r785;
	mov.b64	%rd424, {%r1088, %r1087};
	add.s64 	%rd425, %rd424, %rd418;
	xor.b64  	%rd426, %rd425, %rd420;
	mov.b64	{%r246, %r247}, %rd426;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r768;
	// inline asm
	mov.b64	%rd427, {%r241, %r245};
	add.s64 	%rd428, %rd381, %rd348;
	xor.b64  	%rd429, %rd428, %rd371;
	mov.b64	{%r1089, %r1090}, %rd429;
	mov.b64	%rd430, {%r1090, %r1089};
	add.s64 	%rd431, %rd430, %rd360;
	xor.b64  	%rd432, %rd431, %rd348;
	mov.b64	{%r1091, %r1092}, %rd432;
	prmt.b32 	%r1093, %r1091, %r1092, %r780;
	prmt.b32 	%r1094, %r1091, %r1092, %r779;
	mov.b64	%rd433, {%r1094, %r1093};
	add.s64 	%rd434, %rd428, %rd12;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1095, %r1096}, %rd436;
	prmt.b32 	%r1097, %r1095, %r1096, %r786;
	prmt.b32 	%r1098, %r1095, %r1096, %r785;
	mov.b64	%rd437, {%r1098, %r1097};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r254, %r255}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r768;
	// inline asm
	mov.b64	%rd440, {%r249, %r253};
	add.s64 	%rd441, %rd395, %rd13;
	add.s64 	%rd442, %rd441, %rd440;
	xor.b64  	%rd443, %rd442, %rd410;
	mov.b64	{%r1099, %r1100}, %rd443;
	mov.b64	%rd444, {%r1100, %r1099};
	add.s64 	%rd445, %rd444, %rd425;
	xor.b64  	%rd446, %rd445, %rd440;
	mov.b64	{%r1101, %r1102}, %rd446;
	prmt.b32 	%r1103, %r1101, %r1102, %r780;
	prmt.b32 	%r1104, %r1101, %r1102, %r779;
	mov.b64	%rd447, {%r1104, %r1103};
	add.s64 	%rd448, %rd442, %rd4;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd444, %rd449;
	mov.b64	{%r1105, %r1106}, %rd450;
	prmt.b32 	%r1107, %r1105, %r1106, %r786;
	prmt.b32 	%r1108, %r1105, %r1106, %r785;
	mov.b64	%rd451, {%r1108, %r1107};
	add.s64 	%rd452, %rd445, %rd451;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r262, %r263}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r768;
	// inline asm
	mov.b64	%rd454, {%r257, %r261};
	add.s64 	%rd455, %rd400, %rd20;
	add.s64 	%rd456, %rd455, %rd408;
	xor.b64  	%rd457, %rd424, %rd456;
	mov.b64	{%r1109, %r1110}, %rd457;
	mov.b64	%rd458, {%r1110, %r1109};
	add.s64 	%rd459, %rd438, %rd458;
	xor.b64  	%rd460, %rd459, %rd400;
	mov.b64	{%r1111, %r1112}, %rd460;
	prmt.b32 	%r1113, %r1111, %r1112, %r780;
	prmt.b32 	%r1114, %r1111, %r1112, %r779;
	mov.b64	%rd461, {%r1114, %r1113};
	add.s64 	%rd462, %rd456, %rd11;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1115, %r1116}, %rd464;
	prmt.b32 	%r1117, %r1115, %r1116, %r786;
	prmt.b32 	%r1118, %r1115, %r1116, %r785;
	mov.b64	%rd465, {%r1118, %r1117};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r270, %r271}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r768;
	// inline asm
	mov.b64	%rd468, {%r265, %r269};
	add.s64 	%rd469, %rd413, %rd6;
	add.s64 	%rd470, %rd469, %rd422;
	xor.b64  	%rd471, %rd437, %rd470;
	mov.b64	{%r1119, %r1120}, %rd471;
	mov.b64	%rd472, {%r1120, %r1119};
	add.s64 	%rd473, %rd472, %rd398;
	xor.b64  	%rd474, %rd473, %rd413;
	mov.b64	{%r1121, %r1122}, %rd474;
	prmt.b32 	%r1123, %r1121, %r1122, %r780;
	prmt.b32 	%r1124, %r1121, %r1122, %r779;
	mov.b64	%rd475, {%r1124, %r1123};
	add.s64 	%rd476, %rd470, %rd17;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd477, %rd472;
	mov.b64	{%r1125, %r1126}, %rd478;
	prmt.b32 	%r1127, %r1125, %r1126, %r786;
	prmt.b32 	%r1128, %r1125, %r1126, %r785;
	mov.b64	%rd479, {%r1128, %r1127};
	add.s64 	%rd480, %rd479, %rd473;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r278, %r279}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r768;
	// inline asm
	mov.b64	%rd482, {%r273, %r277};
	add.s64 	%rd483, %rd435, %rd427;
	xor.b64  	%rd484, %rd483, %rd397;
	mov.b64	{%r1129, %r1130}, %rd484;
	mov.b64	%rd485, {%r1130, %r1129};
	add.s64 	%rd486, %rd485, %rd411;
	xor.b64  	%rd487, %rd486, %rd427;
	mov.b64	{%r1131, %r1132}, %rd487;
	prmt.b32 	%r1133, %r1131, %r1132, %r780;
	prmt.b32 	%r1134, %r1131, %r1132, %r779;
	mov.b64	%rd488, {%r1134, %r1133};
	add.s64 	%rd489, %rd488, %rd483;
	xor.b64  	%rd490, %rd489, %rd485;
	mov.b64	{%r1135, %r1136}, %rd490;
	prmt.b32 	%r1137, %r1135, %r1136, %r786;
	prmt.b32 	%r1138, %r1135, %r1136, %r785;
	mov.b64	%rd491, {%r1138, %r1137};
	add.s64 	%rd492, %rd491, %rd486;
	xor.b64  	%rd493, %rd492, %rd488;
	mov.b64	{%r286, %r287}, %rd493;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r768;
	// inline asm
	mov.b64	%rd494, {%r281, %r285};
	add.s64 	%rd495, %rd468, %rd449;
	xor.b64  	%rd496, %rd491, %rd495;
	mov.b64	{%r1139, %r1140}, %rd496;
	mov.b64	%rd497, {%r1140, %r1139};
	add.s64 	%rd498, %rd497, %rd480;
	xor.b64  	%rd499, %rd498, %rd468;
	mov.b64	{%r1141, %r1142}, %rd499;
	prmt.b32 	%r1143, %r1141, %r1142, %r780;
	prmt.b32 	%r1144, %r1141, %r1142, %r779;
	mov.b64	%rd500, {%r1144, %r1143};
	add.s64 	%rd501, %rd495, %rd5;
	add.s64 	%rd502, %rd501, %rd500;
	xor.b64  	%rd503, %rd497, %rd502;
	mov.b64	{%r1145, %r1146}, %rd503;
	prmt.b32 	%r1147, %r1145, %r1146, %r786;
	prmt.b32 	%r1148, %r1145, %r1146, %r785;
	mov.b64	%rd504, {%r1148, %r1147};
	add.s64 	%rd505, %rd498, %rd504;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r294, %r295}, %rd506;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r768;
	// inline asm
	mov.b64	%rd507, {%r289, %r293};
	add.s64 	%rd508, %rd482, %rd463;
	xor.b64  	%rd509, %rd508, %rd451;
	mov.b64	{%r1149, %r1150}, %rd509;
	mov.b64	%rd510, {%r1150, %r1149};
	add.s64 	%rd511, %rd510, %rd492;
	xor.b64  	%rd512, %rd511, %rd482;
	mov.b64	{%r1151, %r1152}, %rd512;
	prmt.b32 	%r1153, %r1151, %r1152, %r780;
	prmt.b32 	%r1154, %r1151, %r1152, %r779;
	mov.b64	%rd513, {%r1154, %r1153};
	add.s64 	%rd514, %rd513, %rd508;
	xor.b64  	%rd515, %rd514, %rd510;
	mov.b64	{%r1155, %r1156}, %rd515;
	prmt.b32 	%r1157, %r1155, %r1156, %r786;
	prmt.b32 	%r1158, %r1155, %r1156, %r785;
	mov.b64	%rd516, {%r1158, %r1157};
	add.s64 	%rd517, %rd516, %rd511;
	xor.b64  	%rd518, %rd517, %rd513;
	mov.b64	{%r302, %r303}, %rd518;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r768;
	// inline asm
	mov.b64	%rd519, {%r297, %r301};
	add.s64 	%rd520, %rd477, %rd10;
	add.s64 	%rd521, %rd520, %rd494;
	xor.b64  	%rd522, %rd521, %rd465;
	mov.b64	{%r1159, %r1160}, %rd522;
	mov.b64	%rd523, {%r1160, %r1159};
	add.s64 	%rd524, %rd523, %rd452;
	xor.b64  	%rd525, %rd524, %rd494;
	mov.b64	{%r1161, %r1162}, %rd525;
	prmt.b32 	%r1163, %r1161, %r1162, %r780;
	prmt.b32 	%r1164, %r1161, %r1162, %r779;
	mov.b64	%rd526, {%r1164, %r1163};
	add.s64 	%rd527, %rd521, %rd12;
	add.s64 	%rd528, %rd527, %rd526;
	xor.b64  	%rd529, %rd528, %rd523;
	mov.b64	{%r1165, %r1166}, %rd529;
	prmt.b32 	%r1167, %r1165, %r1166, %r786;
	prmt.b32 	%r1168, %r1165, %r1166, %r785;
	mov.b64	%rd530, {%r1168, %r1167};
	add.s64 	%rd531, %rd530, %rd524;
	xor.b64  	%rd532, %rd531, %rd526;
	mov.b64	{%r310, %r311}, %rd532;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r768;
	// inline asm
	mov.b64	%rd533, {%r305, %r309};
	add.s64 	%rd534, %rd454, %rd7;
	add.s64 	%rd535, %rd534, %rd489;
	xor.b64  	%rd536, %rd535, %rd479;
	mov.b64	{%r1169, %r1170}, %rd536;
	mov.b64	%rd537, {%r1170, %r1169};
	add.s64 	%rd538, %rd537, %rd466;
	xor.b64  	%rd539, %rd538, %rd454;
	mov.b64	{%r1171, %r1172}, %rd539;
	prmt.b32 	%r1173, %r1171, %r1172, %r780;
	prmt.b32 	%r1174, %r1171, %r1172, %r779;
	mov.b64	%rd540, {%r1174, %r1173};
	add.s64 	%rd541, %rd540, %rd535;
	xor.b64  	%rd542, %rd541, %rd537;
	mov.b64	{%r1175, %r1176}, %rd542;
	prmt.b32 	%r1177, %r1175, %r1176, %r786;
	prmt.b32 	%r1178, %r1175, %r1176, %r785;
	mov.b64	%rd543, {%r1178, %r1177};
	add.s64 	%rd544, %rd543, %rd538;
	xor.b64  	%rd545, %rd544, %rd540;
	mov.b64	{%r318, %r319}, %rd545;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r768;
	// inline asm
	mov.b64	%rd546, {%r313, %r317};
	add.s64 	%rd547, %rd502, %rd6;
	add.s64 	%rd548, %rd547, %rd546;
	xor.b64  	%rd549, %rd548, %rd516;
	mov.b64	{%r1179, %r1180}, %rd549;
	mov.b64	%rd550, {%r1180, %r1179};
	add.s64 	%rd551, %rd550, %rd531;
	xor.b64  	%rd552, %rd551, %rd546;
	mov.b64	{%r1181, %r1182}, %rd552;
	prmt.b32 	%r1183, %r1181, %r1182, %r780;
	prmt.b32 	%r1184, %r1181, %r1182, %r779;
	mov.b64	%rd553, {%r1184, %r1183};
	add.s64 	%rd554, %rd553, %rd548;
	xor.b64  	%rd555, %rd550, %rd554;
	mov.b64	{%r1185, %r1186}, %rd555;
	prmt.b32 	%r1187, %r1185, %r1186, %r786;
	prmt.b32 	%r1188, %r1185, %r1186, %r785;
	mov.b64	%rd556, {%r1188, %r1187};
	add.s64 	%rd557, %rd551, %rd556;
	xor.b64  	%rd558, %rd557, %rd553;
	mov.b64	{%r326, %r327}, %rd558;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r768;
	// inline asm
	mov.b64	%rd559, {%r321, %r325};
	add.s64 	%rd560, %rd507, %rd10;
	add.s64 	%rd561, %rd560, %rd514;
	xor.b64  	%rd562, %rd530, %rd561;
	mov.b64	{%r1189, %r1190}, %rd562;
	mov.b64	%rd563, {%r1190, %r1189};
	add.s64 	%rd564, %rd544, %rd563;
	xor.b64  	%rd565, %rd564, %rd507;
	mov.b64	{%r1191, %r1192}, %rd565;
	prmt.b32 	%r1193, %r1191, %r1192, %r780;
	prmt.b32 	%r1194, %r1191, %r1192, %r779;
	mov.b64	%rd566, {%r1194, %r1193};
	add.s64 	%rd567, %rd566, %rd561;
	xor.b64  	%rd568, %rd567, %rd563;
	mov.b64	{%r1195, %r1196}, %rd568;
	prmt.b32 	%r1197, %r1195, %r1196, %r786;
	prmt.b32 	%r1198, %r1195, %r1196, %r785;
	mov.b64	%rd569, {%r1198, %r1197};
	add.s64 	%rd570, %rd569, %rd564;
	xor.b64  	%rd571, %rd570, %rd566;
	mov.b64	{%r334, %r335}, %rd571;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r768;
	// inline asm
	mov.b64	%rd572, {%r329, %r333};
	add.s64 	%rd573, %rd519, %rd4;
	add.s64 	%rd574, %rd573, %rd528;
	xor.b64  	%rd575, %rd543, %rd574;
	mov.b64	{%r1199, %r1200}, %rd575;
	mov.b64	%rd576, {%r1200, %r1199};
	add.s64 	%rd577, %rd576, %rd505;
	xor.b64  	%rd578, %rd577, %rd519;
	mov.b64	{%r1201, %r1202}, %rd578;
	prmt.b32 	%r1203, %r1201, %r1202, %r780;
	prmt.b32 	%r1204, %r1201, %r1202, %r779;
	mov.b64	%rd579, {%r1204, %r1203};
	add.s64 	%rd580, %rd579, %rd574;
	xor.b64  	%rd581, %rd580, %rd576;
	mov.b64	{%r1205, %r1206}, %rd581;
	prmt.b32 	%r1207, %r1205, %r1206, %r786;
	prmt.b32 	%r1208, %r1205, %r1206, %r785;
	mov.b64	%rd582, {%r1208, %r1207};
	add.s64 	%rd583, %rd582, %rd577;
	xor.b64  	%rd584, %rd583, %rd579;
	mov.b64	{%r342, %r343}, %rd584;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r768;
	// inline asm
	mov.b64	%rd585, {%r337, %r341};
	add.s64 	%rd586, %rd533, %rd12;
	add.s64 	%rd587, %rd586, %rd541;
	xor.b64  	%rd588, %rd587, %rd504;
	mov.b64	{%r1209, %r1210}, %rd588;
	mov.b64	%rd589, {%r1210, %r1209};
	add.s64 	%rd590, %rd589, %rd517;
	xor.b64  	%rd591, %rd590, %rd533;
	mov.b64	{%r1211, %r1212}, %rd591;
	prmt.b32 	%r1213, %r1211, %r1212, %r780;
	prmt.b32 	%r1214, %r1211, %r1212, %r779;
	mov.b64	%rd592, {%r1214, %r1213};
	add.s64 	%rd593, %rd587, %rd7;
	add.s64 	%rd594, %rd593, %rd592;
	xor.b64  	%rd595, %rd594, %rd589;
	mov.b64	{%r1215, %r1216}, %rd595;
	prmt.b32 	%r1217, %r1215, %r1216, %r786;
	prmt.b32 	%r1218, %r1215, %r1216, %r785;
	mov.b64	%rd596, {%r1218, %r1217};
	add.s64 	%rd597, %rd596, %rd590;
	xor.b64  	%rd598, %rd597, %rd592;
	mov.b64	{%r350, %r351}, %rd598;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r768;
	// inline asm
	mov.b64	%rd599, {%r345, %r349};
	add.s64 	%rd600, %rd554, %rd17;
	add.s64 	%rd601, %rd600, %rd572;
	xor.b64  	%rd602, %rd596, %rd601;
	mov.b64	{%r1219, %r1220}, %rd602;
	mov.b64	%rd603, {%r1220, %r1219};
	add.s64 	%rd604, %rd603, %rd583;
	xor.b64  	%rd605, %rd604, %rd572;
	mov.b64	{%r1221, %r1222}, %rd605;
	prmt.b32 	%r1223, %r1221, %r1222, %r780;
	prmt.b32 	%r1224, %r1221, %r1222, %r779;
	mov.b64	%rd606, {%r1224, %r1223};
	add.s64 	%rd607, %rd606, %rd601;
	xor.b64  	%rd608, %rd603, %rd607;
	mov.b64	{%r1225, %r1226}, %rd608;
	prmt.b32 	%r1227, %r1225, %r1226, %r786;
	prmt.b32 	%r1228, %r1225, %r1226, %r785;
	mov.b64	%rd609, {%r1228, %r1227};
	add.s64 	%rd610, %rd604, %rd609;
	xor.b64  	%rd611, %rd610, %rd606;
	mov.b64	{%r358, %r359}, %rd611;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r768;
	// inline asm
	mov.b64	%rd612, {%r353, %r357};
	add.s64 	%rd613, %rd567, %rd11;
	add.s64 	%rd614, %rd613, %rd585;
	xor.b64  	%rd615, %rd614, %rd556;
	mov.b64	{%r1229, %r1230}, %rd615;
	mov.b64	%rd616, {%r1230, %r1229};
	add.s64 	%rd617, %rd616, %rd597;
	xor.b64  	%rd618, %rd617, %rd585;
	mov.b64	{%r1231, %r1232}, %rd618;
	prmt.b32 	%r1233, %r1231, %r1232, %r780;
	prmt.b32 	%r1234, %r1231, %r1232, %r779;
	mov.b64	%rd619, {%r1234, %r1233};
	add.s64 	%rd620, %rd614, %rd20;
	add.s64 	%rd621, %rd620, %rd619;
	xor.b64  	%rd622, %rd621, %rd616;
	mov.b64	{%r1235, %r1236}, %rd622;
	prmt.b32 	%r1237, %r1235, %r1236, %r786;
	prmt.b32 	%r1238, %r1235, %r1236, %r785;
	mov.b64	%rd623, {%r1238, %r1237};
	add.s64 	%rd624, %rd623, %rd617;
	xor.b64  	%rd625, %rd624, %rd619;
	mov.b64	{%r366, %r367}, %rd625;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r768;
	// inline asm
	mov.b64	%rd626, {%r361, %r365};
	add.s64 	%rd627, %rd599, %rd580;
	xor.b64  	%rd628, %rd627, %rd569;
	mov.b64	{%r1239, %r1240}, %rd628;
	mov.b64	%rd629, {%r1240, %r1239};
	add.s64 	%rd630, %rd629, %rd557;
	xor.b64  	%rd631, %rd630, %rd599;
	mov.b64	{%r1241, %r1242}, %rd631;
	prmt.b32 	%r1243, %r1241, %r1242, %r780;
	prmt.b32 	%r1244, %r1241, %r1242, %r779;
	mov.b64	%rd632, {%r1244, %r1243};
	add.s64 	%rd633, %rd632, %rd627;
	xor.b64  	%rd634, %rd633, %rd629;
	mov.b64	{%r1245, %r1246}, %rd634;
	prmt.b32 	%r1247, %r1245, %r1246, %r786;
	prmt.b32 	%r1248, %r1245, %r1246, %r785;
	mov.b64	%rd635, {%r1248, %r1247};
	add.s64 	%rd636, %rd635, %rd630;
	xor.b64  	%rd637, %rd636, %rd632;
	mov.b64	{%r374, %r375}, %rd637;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r768;
	// inline asm
	mov.b64	%rd638, {%r369, %r373};
	add.s64 	%rd639, %rd559, %rd5;
	add.s64 	%rd640, %rd639, %rd594;
	xor.b64  	%rd641, %rd640, %rd582;
	mov.b64	{%r1249, %r1250}, %rd641;
	mov.b64	%rd642, {%r1250, %r1249};
	add.s64 	%rd643, %rd642, %rd570;
	xor.b64  	%rd644, %rd643, %rd559;
	mov.b64	{%r1251, %r1252}, %rd644;
	prmt.b32 	%r1253, %r1251, %r1252, %r780;
	prmt.b32 	%r1254, %r1251, %r1252, %r779;
	mov.b64	%rd645, {%r1254, %r1253};
	add.s64 	%rd646, %rd640, %rd13;
	add.s64 	%rd647, %rd646, %rd645;
	xor.b64  	%rd648, %rd647, %rd642;
	mov.b64	{%r1255, %r1256}, %rd648;
	prmt.b32 	%r1257, %r1255, %r1256, %r786;
	prmt.b32 	%r1258, %r1255, %r1256, %r785;
	mov.b64	%rd649, {%r1258, %r1257};
	add.s64 	%rd650, %rd649, %rd643;
	xor.b64  	%rd651, %rd650, %rd645;
	mov.b64	{%r382, %r383}, %rd651;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r768;
	// inline asm
	mov.b64	%rd652, {%r377, %r381};
	add.s64 	%rd653, %rd652, %rd607;
	xor.b64  	%rd654, %rd653, %rd623;
	mov.b64	{%r1259, %r1260}, %rd654;
	mov.b64	%rd655, {%r1260, %r1259};
	add.s64 	%rd656, %rd655, %rd636;
	xor.b64  	%rd657, %rd656, %rd652;
	mov.b64	{%r1261, %r1262}, %rd657;
	prmt.b32 	%r1263, %r1261, %r1262, %r780;
	prmt.b32 	%r1264, %r1261, %r1262, %r779;
	mov.b64	%rd658, {%r1264, %r1263};
	add.s64 	%rd659, %rd653, %rd20;
	add.s64 	%rd660, %rd659, %rd658;
	xor.b64  	%rd661, %rd655, %rd660;
	mov.b64	{%r1265, %r1266}, %rd661;
	prmt.b32 	%r1267, %r1265, %r1266, %r786;
	prmt.b32 	%r1268, %r1265, %r1266, %r785;
	mov.b64	%rd662, {%r1268, %r1267};
	add.s64 	%rd663, %rd656, %rd662;
	xor.b64  	%rd664, %rd663, %rd658;
	mov.b64	{%r390, %r391}, %rd664;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r768;
	// inline asm
	mov.b64	%rd665, {%r385, %r389};
	add.s64 	%rd666, %rd612, %rd5;
	add.s64 	%rd667, %rd666, %rd621;
	xor.b64  	%rd668, %rd635, %rd667;
	mov.b64	{%r1269, %r1270}, %rd668;
	mov.b64	%rd669, {%r1270, %r1269};
	add.s64 	%rd670, %rd650, %rd669;
	xor.b64  	%rd671, %rd670, %rd612;
	mov.b64	{%r1271, %r1272}, %rd671;
	prmt.b32 	%r1273, %r1271, %r1272, %r780;
	prmt.b32 	%r1274, %r1271, %r1272, %r779;
	mov.b64	%rd672, {%r1274, %r1273};
	add.s64 	%rd673, %rd672, %rd667;
	xor.b64  	%rd674, %rd673, %rd669;
	mov.b64	{%r1275, %r1276}, %rd674;
	prmt.b32 	%r1277, %r1275, %r1276, %r786;
	prmt.b32 	%r1278, %r1275, %r1276, %r785;
	mov.b64	%rd675, {%r1278, %r1277};
	add.s64 	%rd676, %rd675, %rd670;
	xor.b64  	%rd677, %rd676, %rd672;
	mov.b64	{%r398, %r399}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r768;
	// inline asm
	mov.b64	%rd678, {%r393, %r397};
	add.s64 	%rd679, %rd633, %rd626;
	xor.b64  	%rd680, %rd649, %rd679;
	mov.b64	{%r1279, %r1280}, %rd680;
	mov.b64	%rd681, {%r1280, %r1279};
	add.s64 	%rd682, %rd681, %rd610;
	xor.b64  	%rd683, %rd682, %rd626;
	mov.b64	{%r1281, %r1282}, %rd683;
	prmt.b32 	%r1283, %r1281, %r1282, %r780;
	prmt.b32 	%r1284, %r1281, %r1282, %r779;
	mov.b64	%rd684, {%r1284, %r1283};
	add.s64 	%rd685, %rd684, %rd679;
	xor.b64  	%rd686, %rd685, %rd681;
	mov.b64	{%r1285, %r1286}, %rd686;
	prmt.b32 	%r1287, %r1285, %r1286, %r786;
	prmt.b32 	%r1288, %r1285, %r1286, %r785;
	mov.b64	%rd687, {%r1288, %r1287};
	add.s64 	%rd688, %rd687, %rd682;
	xor.b64  	%rd689, %rd688, %rd684;
	mov.b64	{%r406, %r407}, %rd689;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r768;
	// inline asm
	mov.b64	%rd690, {%r401, %r405};
	add.s64 	%rd691, %rd638, %rd17;
	add.s64 	%rd692, %rd691, %rd647;
	xor.b64  	%rd693, %rd692, %rd609;
	mov.b64	{%r1289, %r1290}, %rd693;
	mov.b64	%rd694, {%r1290, %r1289};
	add.s64 	%rd695, %rd694, %rd624;
	xor.b64  	%rd696, %rd695, %rd638;
	mov.b64	{%r1291, %r1292}, %rd696;
	prmt.b32 	%r1293, %r1291, %r1292, %r780;
	prmt.b32 	%r1294, %r1291, %r1292, %r779;
	mov.b64	%rd697, {%r1294, %r1293};
	add.s64 	%rd698, %rd697, %rd692;
	xor.b64  	%rd699, %rd698, %rd694;
	mov.b64	{%r1295, %r1296}, %rd699;
	prmt.b32 	%r1297, %r1295, %r1296, %r786;
	prmt.b32 	%r1298, %r1295, %r1296, %r785;
	mov.b64	%rd700, {%r1298, %r1297};
	add.s64 	%rd701, %rd700, %rd695;
	xor.b64  	%rd702, %rd701, %rd697;
	mov.b64	{%r414, %r415}, %rd702;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r768;
	// inline asm
	mov.b64	%rd703, {%r409, %r413};
	add.s64 	%rd704, %rd660, %rd4;
	add.s64 	%rd705, %rd704, %rd678;
	xor.b64  	%rd706, %rd700, %rd705;
	mov.b64	{%r1299, %r1300}, %rd706;
	mov.b64	%rd707, {%r1300, %r1299};
	add.s64 	%rd708, %rd707, %rd688;
	xor.b64  	%rd709, %rd708, %rd678;
	mov.b64	{%r1301, %r1302}, %rd709;
	prmt.b32 	%r1303, %r1301, %r1302, %r780;
	prmt.b32 	%r1304, %r1301, %r1302, %r779;
	mov.b64	%rd710, {%r1304, %r1303};
	add.s64 	%rd711, %rd705, %rd11;
	add.s64 	%rd712, %rd711, %rd710;
	xor.b64  	%rd713, %rd707, %rd712;
	mov.b64	{%r1305, %r1306}, %rd713;
	prmt.b32 	%r1307, %r1305, %r1306, %r786;
	prmt.b32 	%r1308, %r1305, %r1306, %r785;
	mov.b64	%rd714, {%r1308, %r1307};
	add.s64 	%rd715, %rd708, %rd714;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r422, %r423}, %rd716;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r768;
	// inline asm
	mov.b64	%rd717, {%r417, %r421};
	add.s64 	%rd718, %rd673, %rd10;
	add.s64 	%rd719, %rd718, %rd690;
	xor.b64  	%rd720, %rd719, %rd662;
	mov.b64	{%r1309, %r1310}, %rd720;
	mov.b64	%rd721, {%r1310, %r1309};
	add.s64 	%rd722, %rd721, %rd701;
	xor.b64  	%rd723, %rd722, %rd690;
	mov.b64	{%r1311, %r1312}, %rd723;
	prmt.b32 	%r1313, %r1311, %r1312, %r780;
	prmt.b32 	%r1314, %r1311, %r1312, %r779;
	mov.b64	%rd724, {%r1314, %r1313};
	add.s64 	%rd725, %rd719, %rd7;
	add.s64 	%rd726, %rd725, %rd724;
	xor.b64  	%rd727, %rd726, %rd721;
	mov.b64	{%r1315, %r1316}, %rd727;
	prmt.b32 	%r1317, %r1315, %r1316, %r786;
	prmt.b32 	%r1318, %r1315, %r1316, %r785;
	mov.b64	%rd728, {%r1318, %r1317};
	add.s64 	%rd729, %rd728, %rd722;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r430, %r431}, %rd730;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r768;
	// inline asm
	mov.b64	%rd731, {%r425, %r429};
	add.s64 	%rd732, %rd685, %rd13;
	add.s64 	%rd733, %rd732, %rd703;
	xor.b64  	%rd734, %rd733, %rd675;
	mov.b64	{%r1319, %r1320}, %rd734;
	mov.b64	%rd735, {%r1320, %r1319};
	add.s64 	%rd736, %rd735, %rd663;
	xor.b64  	%rd737, %rd736, %rd703;
	mov.b64	{%r1321, %r1322}, %rd737;
	prmt.b32 	%r1323, %r1321, %r1322, %r780;
	prmt.b32 	%r1324, %r1321, %r1322, %r779;
	mov.b64	%rd738, {%r1324, %r1323};
	add.s64 	%rd739, %rd733, %rd6;
	add.s64 	%rd740, %rd739, %rd738;
	xor.b64  	%rd741, %rd740, %rd735;
	mov.b64	{%r1325, %r1326}, %rd741;
	prmt.b32 	%r1327, %r1325, %r1326, %r786;
	prmt.b32 	%r1328, %r1325, %r1326, %r785;
	mov.b64	%rd742, {%r1328, %r1327};
	add.s64 	%rd743, %rd742, %rd736;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r438, %r439}, %rd744;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r768;
	// inline asm
	mov.b64	%rd745, {%r433, %r437};
	add.s64 	%rd746, %rd665, %rd12;
	add.s64 	%rd747, %rd746, %rd698;
	xor.b64  	%rd748, %rd747, %rd687;
	mov.b64	{%r1329, %r1330}, %rd748;
	mov.b64	%rd749, {%r1330, %r1329};
	add.s64 	%rd750, %rd749, %rd676;
	xor.b64  	%rd751, %rd750, %rd665;
	mov.b64	{%r1331, %r1332}, %rd751;
	prmt.b32 	%r1333, %r1331, %r1332, %r780;
	prmt.b32 	%r1334, %r1331, %r1332, %r779;
	mov.b64	%rd752, {%r1334, %r1333};
	add.s64 	%rd753, %rd752, %rd747;
	xor.b64  	%rd754, %rd753, %rd749;
	mov.b64	{%r1335, %r1336}, %rd754;
	prmt.b32 	%r1337, %r1335, %r1336, %r786;
	prmt.b32 	%r1338, %r1335, %r1336, %r785;
	mov.b64	%rd755, {%r1338, %r1337};
	add.s64 	%rd756, %rd755, %rd750;
	xor.b64  	%rd757, %rd756, %rd752;
	mov.b64	{%r446, %r447}, %rd757;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r768;
	// inline asm
	mov.b64	%rd758, {%r441, %r445};
	add.s64 	%rd759, %rd758, %rd712;
	xor.b64  	%rd760, %rd759, %rd728;
	mov.b64	{%r1339, %r1340}, %rd760;
	mov.b64	%rd761, {%r1340, %r1339};
	add.s64 	%rd762, %rd761, %rd743;
	xor.b64  	%rd763, %rd762, %rd758;
	mov.b64	{%r1341, %r1342}, %rd763;
	prmt.b32 	%r1343, %r1341, %r1342, %r780;
	prmt.b32 	%r1344, %r1341, %r1342, %r779;
	mov.b64	%rd764, {%r1344, %r1343};
	add.s64 	%rd765, %rd764, %rd759;
	xor.b64  	%rd766, %rd761, %rd765;
	mov.b64	{%r1345, %r1346}, %rd766;
	prmt.b32 	%r1347, %r1345, %r1346, %r786;
	prmt.b32 	%r1348, %r1345, %r1346, %r785;
	mov.b64	%rd767, {%r1348, %r1347};
	add.s64 	%rd768, %rd762, %rd767;
	xor.b64  	%rd769, %rd768, %rd764;
	mov.b64	{%r454, %r455}, %rd769;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r768;
	// inline asm
	mov.b64	%rd770, {%r449, %r453};
	add.s64 	%rd771, %rd717, %rd11;
	add.s64 	%rd772, %rd771, %rd726;
	xor.b64  	%rd773, %rd742, %rd772;
	mov.b64	{%r1349, %r1350}, %rd773;
	mov.b64	%rd774, {%r1350, %r1349};
	add.s64 	%rd775, %rd756, %rd774;
	xor.b64  	%rd776, %rd775, %rd717;
	mov.b64	{%r1351, %r1352}, %rd776;
	prmt.b32 	%r1353, %r1351, %r1352, %r780;
	prmt.b32 	%r1354, %r1351, %r1352, %r779;
	mov.b64	%rd777, {%r1354, %r1353};
	add.s64 	%rd778, %rd777, %rd772;
	xor.b64  	%rd779, %rd778, %rd774;
	mov.b64	{%r1355, %r1356}, %rd779;
	prmt.b32 	%r1357, %r1355, %r1356, %r786;
	prmt.b32 	%r1358, %r1355, %r1356, %r785;
	mov.b64	%rd780, {%r1358, %r1357};
	add.s64 	%rd781, %rd780, %rd775;
	xor.b64  	%rd782, %rd781, %rd777;
	mov.b64	{%r462, %r463}, %rd782;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r768;
	// inline asm
	mov.b64	%rd783, {%r457, %r461};
	add.s64 	%rd784, %rd740, %rd731;
	xor.b64  	%rd785, %rd755, %rd784;
	mov.b64	{%r1359, %r1360}, %rd785;
	mov.b64	%rd786, {%r1360, %r1359};
	add.s64 	%rd787, %rd786, %rd715;
	xor.b64  	%rd788, %rd787, %rd731;
	mov.b64	{%r1361, %r1362}, %rd788;
	prmt.b32 	%r1363, %r1361, %r1362, %r780;
	prmt.b32 	%r1364, %r1361, %r1362, %r779;
	mov.b64	%rd789, {%r1364, %r1363};
	add.s64 	%rd790, %rd784, %rd5;
	add.s64 	%rd791, %rd790, %rd789;
	xor.b64  	%rd792, %rd791, %rd786;
	mov.b64	{%r1365, %r1366}, %rd792;
	prmt.b32 	%r1367, %r1365, %r1366, %r786;
	prmt.b32 	%r1368, %r1365, %r1366, %r785;
	mov.b64	%rd793, {%r1368, %r1367};
	add.s64 	%rd794, %rd793, %rd787;
	xor.b64  	%rd795, %rd794, %rd789;
	mov.b64	{%r470, %r471}, %rd795;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r768;
	// inline asm
	mov.b64	%rd796, {%r465, %r469};
	add.s64 	%rd797, %rd745, %rd7;
	add.s64 	%rd798, %rd797, %rd753;
	xor.b64  	%rd799, %rd798, %rd714;
	mov.b64	{%r1369, %r1370}, %rd799;
	mov.b64	%rd800, {%r1370, %r1369};
	add.s64 	%rd801, %rd800, %rd729;
	xor.b64  	%rd802, %rd801, %rd745;
	mov.b64	{%r1371, %r1372}, %rd802;
	prmt.b32 	%r1373, %r1371, %r1372, %r780;
	prmt.b32 	%r1374, %r1371, %r1372, %r779;
	mov.b64	%rd803, {%r1374, %r1373};
	add.s64 	%rd804, %rd798, %rd13;
	add.s64 	%rd805, %rd804, %rd803;
	xor.b64  	%rd806, %rd805, %rd800;
	mov.b64	{%r1375, %r1376}, %rd806;
	prmt.b32 	%r1377, %r1375, %r1376, %r786;
	prmt.b32 	%r1378, %r1375, %r1376, %r785;
	mov.b64	%rd807, {%r1378, %r1377};
	add.s64 	%rd808, %rd807, %rd801;
	xor.b64  	%rd809, %rd808, %rd803;
	mov.b64	{%r478, %r479}, %rd809;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r768;
	// inline asm
	mov.b64	%rd810, {%r473, %r477};
	add.s64 	%rd811, %rd765, %rd20;
	add.s64 	%rd812, %rd811, %rd783;
	xor.b64  	%rd813, %rd807, %rd812;
	mov.b64	{%r1379, %r1380}, %rd813;
	mov.b64	%rd814, {%r1380, %r1379};
	add.s64 	%rd815, %rd814, %rd794;
	xor.b64  	%rd816, %rd815, %rd783;
	mov.b64	{%r1381, %r1382}, %rd816;
	prmt.b32 	%r1383, %r1381, %r1382, %r780;
	prmt.b32 	%r1384, %r1381, %r1382, %r779;
	mov.b64	%rd817, {%r1384, %r1383};
	add.s64 	%rd818, %rd812, %rd4;
	add.s64 	%rd819, %rd818, %rd817;
	xor.b64  	%rd820, %rd814, %rd819;
	mov.b64	{%r1385, %r1386}, %rd820;
	prmt.b32 	%r1387, %r1385, %r1386, %r786;
	prmt.b32 	%r1388, %r1385, %r1386, %r785;
	mov.b64	%rd821, {%r1388, %r1387};
	add.s64 	%rd822, %rd815, %rd821;
	xor.b64  	%rd823, %rd822, %rd817;
	mov.b64	{%r486, %r487}, %rd823;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r768;
	// inline asm
	mov.b64	%rd824, {%r481, %r485};
	add.s64 	%rd825, %rd796, %rd778;
	xor.b64  	%rd826, %rd825, %rd767;
	mov.b64	{%r1389, %r1390}, %rd826;
	mov.b64	%rd827, {%r1390, %r1389};
	add.s64 	%rd828, %rd827, %rd808;
	xor.b64  	%rd829, %rd828, %rd796;
	mov.b64	{%r1391, %r1392}, %rd829;
	prmt.b32 	%r1393, %r1391, %r1392, %r780;
	prmt.b32 	%r1394, %r1391, %r1392, %r779;
	mov.b64	%rd830, {%r1394, %r1393};
	add.s64 	%rd831, %rd825, %rd17;
	add.s64 	%rd832, %rd831, %rd830;
	xor.b64  	%rd833, %rd832, %rd827;
	mov.b64	{%r1395, %r1396}, %rd833;
	prmt.b32 	%r1397, %r1395, %r1396, %r786;
	prmt.b32 	%r1398, %r1395, %r1396, %r785;
	mov.b64	%rd834, {%r1398, %r1397};
	add.s64 	%rd835, %rd834, %rd828;
	xor.b64  	%rd836, %rd835, %rd830;
	mov.b64	{%r494, %r495}, %rd836;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r768;
	// inline asm
	mov.b64	%rd837, {%r489, %r493};
	add.s64 	%rd838, %rd791, %rd12;
	add.s64 	%rd839, %rd838, %rd810;
	xor.b64  	%rd840, %rd839, %rd780;
	mov.b64	{%r1399, %r1400}, %rd840;
	mov.b64	%rd841, {%r1400, %r1399};
	add.s64 	%rd842, %rd841, %rd768;
	xor.b64  	%rd843, %rd842, %rd810;
	mov.b64	{%r1401, %r1402}, %rd843;
	prmt.b32 	%r1403, %r1401, %r1402, %r780;
	prmt.b32 	%r1404, %r1401, %r1402, %r779;
	mov.b64	%rd844, {%r1404, %r1403};
	add.s64 	%rd845, %rd839, %rd10;
	add.s64 	%rd846, %rd845, %rd844;
	xor.b64  	%rd847, %rd846, %rd841;
	mov.b64	{%r1405, %r1406}, %rd847;
	prmt.b32 	%r1407, %r1405, %r1406, %r786;
	prmt.b32 	%r1408, %r1405, %r1406, %r785;
	mov.b64	%rd848, {%r1408, %r1407};
	add.s64 	%rd849, %rd848, %rd842;
	xor.b64  	%rd850, %rd849, %rd844;
	mov.b64	{%r502, %r503}, %rd850;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r768;
	// inline asm
	mov.b64	%rd851, {%r497, %r501};
	add.s64 	%rd852, %rd770, %rd6;
	add.s64 	%rd853, %rd852, %rd805;
	xor.b64  	%rd854, %rd853, %rd793;
	mov.b64	{%r1409, %r1410}, %rd854;
	mov.b64	%rd855, {%r1410, %r1409};
	add.s64 	%rd856, %rd855, %rd781;
	xor.b64  	%rd857, %rd856, %rd770;
	mov.b64	{%r1411, %r1412}, %rd857;
	prmt.b32 	%r1413, %r1411, %r1412, %r780;
	prmt.b32 	%r1414, %r1411, %r1412, %r779;
	mov.b64	%rd858, {%r1414, %r1413};
	add.s64 	%rd859, %rd858, %rd853;
	xor.b64  	%rd860, %rd859, %rd855;
	mov.b64	{%r1415, %r1416}, %rd860;
	prmt.b32 	%r1417, %r1415, %r1416, %r786;
	prmt.b32 	%r1418, %r1415, %r1416, %r785;
	mov.b64	%rd861, {%r1418, %r1417};
	add.s64 	%rd862, %rd861, %rd856;
	xor.b64  	%rd863, %rd862, %rd858;
	mov.b64	{%r510, %r511}, %rd863;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r768;
	// inline asm
	mov.b64	%rd864, {%r505, %r509};
	add.s64 	%rd865, %rd819, %rd10;
	add.s64 	%rd866, %rd865, %rd864;
	xor.b64  	%rd867, %rd866, %rd834;
	mov.b64	{%r1419, %r1420}, %rd867;
	mov.b64	%rd868, {%r1420, %r1419};
	add.s64 	%rd869, %rd868, %rd849;
	xor.b64  	%rd870, %rd869, %rd864;
	mov.b64	{%r1421, %r1422}, %rd870;
	prmt.b32 	%r1423, %r1421, %r1422, %r780;
	prmt.b32 	%r1424, %r1421, %r1422, %r779;
	mov.b64	%rd871, {%r1424, %r1423};
	add.s64 	%rd872, %rd871, %rd866;
	xor.b64  	%rd873, %rd868, %rd872;
	mov.b64	{%r1425, %r1426}, %rd873;
	prmt.b32 	%r1427, %r1425, %r1426, %r786;
	prmt.b32 	%r1428, %r1425, %r1426, %r785;
	mov.b64	%rd874, {%r1428, %r1427};
	add.s64 	%rd875, %rd869, %rd874;
	xor.b64  	%rd876, %rd875, %rd871;
	mov.b64	{%r518, %r519}, %rd876;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r768;
	// inline asm
	mov.b64	%rd877, {%r513, %r517};
	add.s64 	%rd878, %rd832, %rd824;
	xor.b64  	%rd879, %rd848, %rd878;
	mov.b64	{%r1429, %r1430}, %rd879;
	mov.b64	%rd880, {%r1430, %r1429};
	add.s64 	%rd881, %rd862, %rd880;
	xor.b64  	%rd882, %rd881, %rd824;
	mov.b64	{%r1431, %r1432}, %rd882;
	prmt.b32 	%r1433, %r1431, %r1432, %r780;
	prmt.b32 	%r1434, %r1431, %r1432, %r779;
	mov.b64	%rd883, {%r1434, %r1433};
	add.s64 	%rd884, %rd878, %rd13;
	add.s64 	%rd885, %rd884, %rd883;
	xor.b64  	%rd886, %rd885, %rd880;
	mov.b64	{%r1435, %r1436}, %rd886;
	prmt.b32 	%r1437, %r1435, %r1436, %r786;
	prmt.b32 	%r1438, %r1435, %r1436, %r785;
	mov.b64	%rd887, {%r1438, %r1437};
	add.s64 	%rd888, %rd887, %rd881;
	xor.b64  	%rd889, %rd888, %rd883;
	mov.b64	{%r526, %r527}, %rd889;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r768;
	// inline asm
	mov.b64	%rd890, {%r521, %r525};
	add.s64 	%rd891, %rd846, %rd837;
	xor.b64  	%rd892, %rd861, %rd891;
	mov.b64	{%r1439, %r1440}, %rd892;
	mov.b64	%rd893, {%r1440, %r1439};
	add.s64 	%rd894, %rd893, %rd822;
	xor.b64  	%rd895, %rd894, %rd837;
	mov.b64	{%r1441, %r1442}, %rd895;
	prmt.b32 	%r1443, %r1441, %r1442, %r780;
	prmt.b32 	%r1444, %r1441, %r1442, %r779;
	mov.b64	%rd896, {%r1444, %r1443};
	add.s64 	%rd897, %rd891, %rd7;
	add.s64 	%rd898, %rd897, %rd896;
	xor.b64  	%rd899, %rd898, %rd893;
	mov.b64	{%r1445, %r1446}, %rd899;
	prmt.b32 	%r1447, %r1445, %r1446, %r786;
	prmt.b32 	%r1448, %r1445, %r1446, %r785;
	mov.b64	%rd900, {%r1448, %r1447};
	add.s64 	%rd901, %rd900, %rd894;
	xor.b64  	%rd902, %rd901, %rd896;
	mov.b64	{%r534, %r535}, %rd902;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r768;
	// inline asm
	mov.b64	%rd903, {%r529, %r533};
	add.s64 	%rd904, %rd851, %rd4;
	add.s64 	%rd905, %rd904, %rd859;
	xor.b64  	%rd906, %rd905, %rd821;
	mov.b64	{%r1449, %r1450}, %rd906;
	mov.b64	%rd907, {%r1450, %r1449};
	add.s64 	%rd908, %rd907, %rd835;
	xor.b64  	%rd909, %rd908, %rd851;
	mov.b64	{%r1451, %r1452}, %rd909;
	prmt.b32 	%r1453, %r1451, %r1452, %r780;
	prmt.b32 	%r1454, %r1451, %r1452, %r779;
	mov.b64	%rd910, {%r1454, %r1453};
	add.s64 	%rd911, %rd905, %rd12;
	add.s64 	%rd912, %rd911, %rd910;
	xor.b64  	%rd913, %rd912, %rd907;
	mov.b64	{%r1455, %r1456}, %rd913;
	prmt.b32 	%r1457, %r1455, %r1456, %r786;
	prmt.b32 	%r1458, %r1455, %r1456, %r785;
	mov.b64	%rd914, {%r1458, %r1457};
	add.s64 	%rd915, %rd914, %rd908;
	xor.b64  	%rd916, %rd915, %rd910;
	mov.b64	{%r542, %r543}, %rd916;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r768;
	// inline asm
	mov.b64	%rd917, {%r537, %r541};
	add.s64 	%rd918, %rd890, %rd872;
	xor.b64  	%rd919, %rd914, %rd918;
	mov.b64	{%r1459, %r1460}, %rd919;
	mov.b64	%rd920, {%r1460, %r1459};
	add.s64 	%rd921, %rd920, %rd901;
	xor.b64  	%rd922, %rd921, %rd890;
	mov.b64	{%r1461, %r1462}, %rd922;
	prmt.b32 	%r1463, %r1461, %r1462, %r780;
	prmt.b32 	%r1464, %r1461, %r1462, %r779;
	mov.b64	%rd923, {%r1464, %r1463};
	add.s64 	%rd924, %rd918, %rd6;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r1465, %r1466}, %rd926;
	prmt.b32 	%r1467, %r1465, %r1466, %r786;
	prmt.b32 	%r1468, %r1465, %r1466, %r785;
	mov.b64	%rd927, {%r1468, %r1467};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r550, %r551}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r768;
	// inline asm
	mov.b64	%rd930, {%r545, %r549};
	add.s64 	%rd931, %rd903, %rd885;
	xor.b64  	%rd932, %rd931, %rd874;
	mov.b64	{%r1469, %r1470}, %rd932;
	mov.b64	%rd933, {%r1470, %r1469};
	add.s64 	%rd934, %rd933, %rd915;
	xor.b64  	%rd935, %rd934, %rd903;
	mov.b64	{%r1471, %r1472}, %rd935;
	prmt.b32 	%r1473, %r1471, %r1472, %r780;
	prmt.b32 	%r1474, %r1471, %r1472, %r779;
	mov.b64	%rd936, {%r1474, %r1473};
	add.s64 	%rd937, %rd931, %rd11;
	add.s64 	%rd938, %rd937, %rd936;
	xor.b64  	%rd939, %rd938, %rd933;
	mov.b64	{%r1475, %r1476}, %rd939;
	prmt.b32 	%r1477, %r1475, %r1476, %r786;
	prmt.b32 	%r1478, %r1475, %r1476, %r785;
	mov.b64	%rd940, {%r1478, %r1477};
	add.s64 	%rd941, %rd940, %rd934;
	xor.b64  	%rd942, %rd941, %rd936;
	mov.b64	{%r558, %r559}, %rd942;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r768;
	// inline asm
	mov.b64	%rd943, {%r553, %r557};
	add.s64 	%rd944, %rd898, %rd5;
	add.s64 	%rd945, %rd944, %rd917;
	xor.b64  	%rd946, %rd945, %rd887;
	mov.b64	{%r1479, %r1480}, %rd946;
	mov.b64	%rd947, {%r1480, %r1479};
	add.s64 	%rd948, %rd947, %rd875;
	xor.b64  	%rd949, %rd948, %rd917;
	mov.b64	{%r1481, %r1482}, %rd949;
	prmt.b32 	%r1483, %r1481, %r1482, %r780;
	prmt.b32 	%r1484, %r1481, %r1482, %r779;
	mov.b64	%rd950, {%r1484, %r1483};
	add.s64 	%rd951, %rd945, %rd17;
	add.s64 	%rd952, %rd951, %rd950;
	xor.b64  	%rd953, %rd952, %rd947;
	mov.b64	{%r1485, %r1486}, %rd953;
	prmt.b32 	%r1487, %r1485, %r1486, %r786;
	prmt.b32 	%r1488, %r1485, %r1486, %r785;
	mov.b64	%rd954, {%r1488, %r1487};
	add.s64 	%rd955, %rd954, %rd948;
	xor.b64  	%rd956, %rd955, %rd950;
	mov.b64	{%r566, %r567}, %rd956;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r768;
	// inline asm
	mov.b64	%rd957, {%r561, %r565};
	add.s64 	%rd958, %rd912, %rd877;
	xor.b64  	%rd959, %rd958, %rd900;
	mov.b64	{%r1489, %r1490}, %rd959;
	mov.b64	%rd960, {%r1490, %r1489};
	add.s64 	%rd961, %rd960, %rd888;
	xor.b64  	%rd962, %rd961, %rd877;
	mov.b64	{%r1491, %r1492}, %rd962;
	prmt.b32 	%r1493, %r1491, %r1492, %r780;
	prmt.b32 	%r1494, %r1491, %r1492, %r779;
	mov.b64	%rd963, {%r1494, %r1493};
	add.s64 	%rd964, %rd958, %rd20;
	add.s64 	%rd965, %rd964, %rd963;
	xor.b64  	%rd966, %rd965, %rd960;
	mov.b64	{%r1495, %r1496}, %rd966;
	prmt.b32 	%r1497, %r1495, %r1496, %r786;
	prmt.b32 	%r1498, %r1495, %r1496, %r785;
	mov.b64	%rd967, {%r1498, %r1497};
	add.s64 	%rd968, %rd967, %rd961;
	xor.b64  	%rd969, %rd968, %rd963;
	mov.b64	{%r574, %r575}, %rd969;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r768;
	// inline asm
	mov.b64	%rd970, {%r569, %r573};
	add.s64 	%rd971, %rd970, %rd925;
	xor.b64  	%rd972, %rd971, %rd940;
	mov.b64	{%r1499, %r1500}, %rd972;
	mov.b64	%rd973, {%r1500, %r1499};
	add.s64 	%rd974, %rd973, %rd955;
	xor.b64  	%rd975, %rd974, %rd970;
	mov.b64	{%r1501, %r1502}, %rd975;
	prmt.b32 	%r1503, %r1501, %r1502, %r780;
	prmt.b32 	%r1504, %r1501, %r1502, %r779;
	mov.b64	%rd976, {%r1504, %r1503};
	add.s64 	%rd977, %rd971, %rd6;
	add.s64 	%rd978, %rd977, %rd976;
	xor.b64  	%rd979, %rd973, %rd978;
	mov.b64	{%r1505, %r1506}, %rd979;
	prmt.b32 	%r1507, %r1505, %r1506, %r786;
	prmt.b32 	%r1508, %r1505, %r1506, %r785;
	mov.b64	%rd980, {%r1508, %r1507};
	add.s64 	%rd981, %rd974, %rd980;
	xor.b64  	%rd982, %rd981, %rd976;
	mov.b64	{%r582, %r583}, %rd982;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r768;
	// inline asm
	mov.b64	%rd983, {%r577, %r581};
	add.s64 	%rd984, %rd930, %rd12;
	add.s64 	%rd985, %rd984, %rd938;
	xor.b64  	%rd986, %rd954, %rd985;
	mov.b64	{%r1509, %r1510}, %rd986;
	mov.b64	%rd987, {%r1510, %r1509};
	add.s64 	%rd988, %rd968, %rd987;
	xor.b64  	%rd989, %rd988, %rd930;
	mov.b64	{%r1511, %r1512}, %rd989;
	prmt.b32 	%r1513, %r1511, %r1512, %r780;
	prmt.b32 	%r1514, %r1511, %r1512, %r779;
	mov.b64	%rd990, {%r1514, %r1513};
	add.s64 	%rd991, %rd985, %rd17;
	add.s64 	%rd992, %rd991, %rd990;
	xor.b64  	%rd993, %rd992, %rd987;
	mov.b64	{%r1515, %r1516}, %rd993;
	prmt.b32 	%r1517, %r1515, %r1516, %r786;
	prmt.b32 	%r1518, %r1515, %r1516, %r785;
	mov.b64	%rd994, {%r1518, %r1517};
	add.s64 	%rd995, %rd994, %rd988;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r590, %r591}, %rd996;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r768;
	// inline asm
	mov.b64	%rd997, {%r585, %r589};
	add.s64 	%rd998, %rd943, %rd11;
	add.s64 	%rd999, %rd998, %rd952;
	xor.b64  	%rd1000, %rd967, %rd999;
	mov.b64	{%r1519, %r1520}, %rd1000;
	mov.b64	%rd1001, {%r1520, %r1519};
	add.s64 	%rd1002, %rd1001, %rd928;
	xor.b64  	%rd1003, %rd1002, %rd943;
	mov.b64	{%r1521, %r1522}, %rd1003;
	prmt.b32 	%r1523, %r1521, %r1522, %r780;
	prmt.b32 	%r1524, %r1521, %r1522, %r779;
	mov.b64	%rd1004, {%r1524, %r1523};
	add.s64 	%rd1005, %rd999, %rd10;
	add.s64 	%rd1006, %rd1005, %rd1004;
	xor.b64  	%rd1007, %rd1006, %rd1001;
	mov.b64	{%r1525, %r1526}, %rd1007;
	prmt.b32 	%r1527, %r1525, %r1526, %r786;
	prmt.b32 	%r1528, %r1525, %r1526, %r785;
	mov.b64	%rd1008, {%r1528, %r1527};
	add.s64 	%rd1009, %rd1008, %rd1002;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r598, %r599}, %rd1010;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r768;
	// inline asm
	mov.b64	%rd1011, {%r593, %r597};
	add.s64 	%rd1012, %rd957, %rd5;
	add.s64 	%rd1013, %rd1012, %rd965;
	xor.b64  	%rd1014, %rd1013, %rd927;
	mov.b64	{%r1529, %r1530}, %rd1014;
	mov.b64	%rd1015, {%r1530, %r1529};
	add.s64 	%rd1016, %rd1015, %rd941;
	xor.b64  	%rd1017, %rd1016, %rd957;
	mov.b64	{%r1531, %r1532}, %rd1017;
	prmt.b32 	%r1533, %r1531, %r1532, %r780;
	prmt.b32 	%r1534, %r1531, %r1532, %r779;
	mov.b64	%rd1018, {%r1534, %r1533};
	add.s64 	%rd1019, %rd1013, %rd20;
	add.s64 	%rd1020, %rd1019, %rd1018;
	xor.b64  	%rd1021, %rd1020, %rd1015;
	mov.b64	{%r1535, %r1536}, %rd1021;
	prmt.b32 	%r1537, %r1535, %r1536, %r786;
	prmt.b32 	%r1538, %r1535, %r1536, %r785;
	mov.b64	%rd1022, {%r1538, %r1537};
	add.s64 	%rd1023, %rd1022, %rd1016;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r606, %r607}, %rd1024;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r768;
	// inline asm
	mov.b64	%rd1025, {%r601, %r605};
	add.s64 	%rd1026, %rd997, %rd978;
	xor.b64  	%rd1027, %rd1022, %rd1026;
	mov.b64	{%r1539, %r1540}, %rd1027;
	mov.b64	%rd1028, {%r1540, %r1539};
	add.s64 	%rd1029, %rd1028, %rd1009;
	xor.b64  	%rd1030, %rd1029, %rd997;
	mov.b64	{%r1541, %r1542}, %rd1030;
	prmt.b32 	%r1543, %r1541, %r1542, %r780;
	prmt.b32 	%r1544, %r1541, %r1542, %r779;
	mov.b64	%rd1031, {%r1544, %r1543};
	add.s64 	%rd1032, %rd1031, %rd1026;
	xor.b64  	%rd1033, %rd1028, %rd1032;
	mov.b64	{%r1545, %r1546}, %rd1033;
	prmt.b32 	%r1547, %r1545, %r1546, %r786;
	prmt.b32 	%r1548, %r1545, %r1546, %r785;
	mov.b64	%rd1034, {%r1548, %r1547};
	add.s64 	%rd1035, %rd1029, %rd1034;
	xor.b64  	%rd1036, %rd1035, %rd1031;
	mov.b64	{%r614, %r615}, %rd1036;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r768;
	// inline asm
	mov.b64	%rd1037, {%r609, %r613};
	add.s64 	%rd1038, %rd992, %rd13;
	add.s64 	%rd1039, %rd1038, %rd1011;
	xor.b64  	%rd1040, %rd1039, %rd980;
	mov.b64	{%r1549, %r1550}, %rd1040;
	mov.b64	%rd1041, {%r1550, %r1549};
	add.s64 	%rd1042, %rd1041, %rd1023;
	xor.b64  	%rd1043, %rd1042, %rd1011;
	mov.b64	{%r1551, %r1552}, %rd1043;
	prmt.b32 	%r1553, %r1551, %r1552, %r780;
	prmt.b32 	%r1554, %r1551, %r1552, %r779;
	mov.b64	%rd1044, {%r1554, %r1553};
	add.s64 	%rd1045, %rd1044, %rd1039;
	xor.b64  	%rd1046, %rd1045, %rd1041;
	mov.b64	{%r1555, %r1556}, %rd1046;
	prmt.b32 	%r1557, %r1555, %r1556, %r786;
	prmt.b32 	%r1558, %r1555, %r1556, %r785;
	mov.b64	%rd1047, {%r1558, %r1557};
	add.s64 	%rd1048, %rd1047, %rd1042;
	xor.b64  	%rd1049, %rd1048, %rd1044;
	mov.b64	{%r622, %r623}, %rd1049;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r768;
	// inline asm
	mov.b64	%rd1050, {%r617, %r621};
	add.s64 	%rd1051, %rd1006, %rd7;
	add.s64 	%rd1052, %rd1051, %rd1025;
	xor.b64  	%rd1053, %rd1052, %rd994;
	mov.b64	{%r1559, %r1560}, %rd1053;
	mov.b64	%rd1054, {%r1560, %r1559};
	add.s64 	%rd1055, %rd1054, %rd981;
	xor.b64  	%rd1056, %rd1055, %rd1025;
	mov.b64	{%r1561, %r1562}, %rd1056;
	prmt.b32 	%r1563, %r1561, %r1562, %r780;
	prmt.b32 	%r1564, %r1561, %r1562, %r779;
	mov.b64	%rd1057, {%r1564, %r1563};
	add.s64 	%rd1058, %rd1057, %rd1052;
	xor.b64  	%rd1059, %rd1058, %rd1054;
	mov.b64	{%r1565, %r1566}, %rd1059;
	prmt.b32 	%r1567, %r1565, %r1566, %r786;
	prmt.b32 	%r1568, %r1565, %r1566, %r785;
	mov.b64	%rd1060, {%r1568, %r1567};
	add.s64 	%rd1061, %rd1060, %rd1055;
	xor.b64  	%rd1062, %rd1061, %rd1057;
	mov.b64	{%r630, %r631}, %rd1062;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r768;
	// inline asm
	mov.b64	%rd1063, {%r625, %r629};
	add.s64 	%rd1064, %rd1020, %rd983;
	xor.b64  	%rd1065, %rd1064, %rd1008;
	mov.b64	{%r1569, %r1570}, %rd1065;
	mov.b64	%rd1066, {%r1570, %r1569};
	add.s64 	%rd1067, %rd1066, %rd995;
	xor.b64  	%rd1068, %rd1067, %rd983;
	mov.b64	{%r1571, %r1572}, %rd1068;
	prmt.b32 	%r1573, %r1571, %r1572, %r780;
	prmt.b32 	%r1574, %r1571, %r1572, %r779;
	mov.b64	%rd1069, {%r1574, %r1573};
	add.s64 	%rd1070, %rd1064, %rd4;
	add.s64 	%rd1071, %rd1070, %rd1069;
	xor.b64  	%rd1072, %rd1071, %rd1066;
	mov.b64	{%r1575, %r1576}, %rd1072;
	prmt.b32 	%r1577, %r1575, %r1576, %r786;
	prmt.b32 	%r1578, %r1575, %r1576, %r785;
	mov.b64	%rd1073, {%r1578, %r1577};
	add.s64 	%rd1074, %rd1073, %rd1067;
	xor.b64  	%rd1075, %rd1074, %rd1069;
	mov.b64	{%r638, %r639}, %rd1075;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r768;
	// inline asm
	mov.b64	%rd1076, {%r633, %r637};
	add.s64 	%rd1077, %rd1032, %rd4;
	add.s64 	%rd1078, %rd1077, %rd1076;
	xor.b64  	%rd1079, %rd1078, %rd1047;
	mov.b64	{%r1579, %r1580}, %rd1079;
	mov.b64	%rd1080, {%r1580, %r1579};
	add.s64 	%rd1081, %rd1080, %rd1061;
	xor.b64  	%rd1082, %rd1081, %rd1076;
	mov.b64	{%r1581, %r1582}, %rd1082;
	prmt.b32 	%r1583, %r1581, %r1582, %r780;
	prmt.b32 	%r1584, %r1581, %r1582, %r779;
	mov.b64	%rd1083, {%r1584, %r1583};
	add.s64 	%rd1084, %rd1078, %rd5;
	add.s64 	%rd1085, %rd1084, %rd1083;
	xor.b64  	%rd1086, %rd1080, %rd1085;
	mov.b64	{%r1585, %r1586}, %rd1086;
	prmt.b32 	%r1587, %r1585, %r1586, %r786;
	prmt.b32 	%r1588, %r1585, %r1586, %r785;
	mov.b64	%rd1087, {%r1588, %r1587};
	add.s64 	%rd1088, %rd1081, %rd1087;
	xor.b64  	%rd1089, %rd1088, %rd1083;
	mov.b64	{%r646, %r647}, %rd1089;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r768;
	// inline asm
	mov.b64	%rd1090, {%r641, %r645};
	add.s64 	%rd1091, %rd1037, %rd6;
	add.s64 	%rd1092, %rd1091, %rd1045;
	xor.b64  	%rd1093, %rd1060, %rd1092;
	mov.b64	{%r1589, %r1590}, %rd1093;
	mov.b64	%rd1094, {%r1590, %r1589};
	add.s64 	%rd1095, %rd1074, %rd1094;
	xor.b64  	%rd1096, %rd1095, %rd1037;
	mov.b64	{%r1591, %r1592}, %rd1096;
	prmt.b32 	%r1593, %r1591, %r1592, %r780;
	prmt.b32 	%r1594, %r1591, %r1592, %r779;
	mov.b64	%rd1097, {%r1594, %r1593};
	add.s64 	%rd1098, %rd1092, %rd7;
	add.s64 	%rd1099, %rd1098, %rd1097;
	xor.b64  	%rd1100, %rd1099, %rd1094;
	mov.b64	{%r1595, %r1596}, %rd1100;
	prmt.b32 	%r1597, %r1595, %r1596, %r786;
	prmt.b32 	%r1598, %r1595, %r1596, %r785;
	mov.b64	%rd1101, {%r1598, %r1597};
	add.s64 	%rd1102, %rd1101, %rd1095;
	xor.b64  	%rd1103, %rd1102, %rd1097;
	mov.b64	{%r654, %r655}, %rd1103;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r768;
	// inline asm
	mov.b64	%rd1104, {%r649, %r653};
	add.s64 	%rd1105, %rd1050, %rd17;
	add.s64 	%rd1106, %rd1105, %rd1058;
	xor.b64  	%rd1107, %rd1073, %rd1106;
	mov.b64	{%r1599, %r1600}, %rd1107;
	mov.b64	%rd1108, {%r1600, %r1599};
	add.s64 	%rd1109, %rd1108, %rd1035;
	xor.b64  	%rd1110, %rd1109, %rd1050;
	mov.b64	{%r1601, %r1602}, %rd1110;
	prmt.b32 	%r1603, %r1601, %r1602, %r780;
	prmt.b32 	%r1604, %r1601, %r1602, %r779;
	mov.b64	%rd1111, {%r1604, %r1603};
	add.s64 	%rd1112, %rd1106, %rd20;
	add.s64 	%rd1113, %rd1112, %rd1111;
	xor.b64  	%rd1114, %rd1113, %rd1108;
	mov.b64	{%r1605, %r1606}, %rd1114;
	prmt.b32 	%r1607, %r1605, %r1606, %r786;
	prmt.b32 	%r1608, %r1605, %r1606, %r785;
	mov.b64	%rd1115, {%r1608, %r1607};
	add.s64 	%rd1116, %rd1115, %rd1109;
	xor.b64  	%rd1117, %rd1116, %rd1111;
	mov.b64	{%r662, %r663}, %rd1117;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r768;
	// inline asm
	mov.b64	%rd1118, {%r657, %r661};
	add.s64 	%rd1119, %rd1063, %rd10;
	add.s64 	%rd1120, %rd1119, %rd1071;
	xor.b64  	%rd1121, %rd1120, %rd1034;
	mov.b64	{%r1609, %r1610}, %rd1121;
	mov.b64	%rd1122, {%r1610, %r1609};
	add.s64 	%rd1123, %rd1122, %rd1048;
	xor.b64  	%rd1124, %rd1123, %rd1063;
	mov.b64	{%r1611, %r1612}, %rd1124;
	prmt.b32 	%r1613, %r1611, %r1612, %r780;
	prmt.b32 	%r1614, %r1611, %r1612, %r779;
	mov.b64	%rd1125, {%r1614, %r1613};
	add.s64 	%rd1126, %rd1120, %rd11;
	add.s64 	%rd1127, %rd1126, %rd1125;
	xor.b64  	%rd1128, %rd1127, %rd1122;
	mov.b64	{%r1615, %r1616}, %rd1128;
	prmt.b32 	%r1617, %r1615, %r1616, %r786;
	prmt.b32 	%r1618, %r1615, %r1616, %r785;
	mov.b64	%rd1129, {%r1618, %r1617};
	add.s64 	%rd1130, %rd1129, %rd1123;
	xor.b64  	%rd1131, %rd1130, %rd1125;
	mov.b64	{%r670, %r671}, %rd1131;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r768;
	// inline asm
	mov.b64	%rd1132, {%r665, %r669};
	add.s64 	%rd1133, %rd1085, %rd12;
	add.s64 	%rd1134, %rd1133, %rd1104;
	xor.b64  	%rd1135, %rd1129, %rd1134;
	mov.b64	{%r1619, %r1620}, %rd1135;
	mov.b64	%rd1136, {%r1620, %r1619};
	add.s64 	%rd1137, %rd1136, %rd1116;
	xor.b64  	%rd1138, %rd1137, %rd1104;
	mov.b64	{%r1621, %r1622}, %rd1138;
	prmt.b32 	%r1623, %r1621, %r1622, %r780;
	prmt.b32 	%r1624, %r1621, %r1622, %r779;
	mov.b64	%rd1139, {%r1624, %r1623};
	add.s64 	%rd1140, %rd1134, %rd13;
	add.s64 	%rd1141, %rd1140, %rd1139;
	xor.b64  	%rd1142, %rd1136, %rd1141;
	mov.b64	{%r1625, %r1626}, %rd1142;
	prmt.b32 	%r1627, %r1625, %r1626, %r786;
	prmt.b32 	%r1628, %r1625, %r1626, %r785;
	mov.b64	%rd1143, {%r1628, %r1627};
	add.s64 	%rd1144, %rd1137, %rd1143;
	xor.b64  	%rd1145, %rd1144, %rd1139;
	mov.b64	{%r678, %r679}, %rd1145;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r768;
	// inline asm
	mov.b64	%rd1146, {%r673, %r677};
	add.s64 	%rd1147, %rd1118, %rd1099;
	xor.b64  	%rd1148, %rd1147, %rd1087;
	mov.b64	{%r1629, %r1630}, %rd1148;
	mov.b64	%rd1149, {%r1630, %r1629};
	add.s64 	%rd1150, %rd1149, %rd1130;
	xor.b64  	%rd1151, %rd1150, %rd1118;
	mov.b64	{%r1631, %r1632}, %rd1151;
	prmt.b32 	%r1633, %r1631, %r1632, %r780;
	prmt.b32 	%r1634, %r1631, %r1632, %r779;
	mov.b64	%rd1152, {%r1634, %r1633};
	add.s64 	%rd1153, %rd1152, %rd1147;
	xor.b64  	%rd1154, %rd1153, %rd1149;
	mov.b64	{%r1635, %r1636}, %rd1154;
	prmt.b32 	%r1637, %r1635, %r1636, %r786;
	prmt.b32 	%r1638, %r1635, %r1636, %r785;
	mov.b64	%rd1155, {%r1638, %r1637};
	add.s64 	%rd1156, %rd1155, %rd1150;
	xor.b64  	%rd1157, %rd1156, %rd1152;
	mov.b64	{%r686, %r687}, %rd1157;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r768;
	// inline asm
	mov.b64	%rd1158, {%r681, %r685};
	add.s64 	%rd1159, %rd1132, %rd1113;
	xor.b64  	%rd1160, %rd1159, %rd1101;
	mov.b64	{%r1639, %r1640}, %rd1160;
	mov.b64	%rd1161, {%r1640, %r1639};
	add.s64 	%rd1162, %rd1161, %rd1088;
	xor.b64  	%rd1163, %rd1162, %rd1132;
	mov.b64	{%r1641, %r1642}, %rd1163;
	prmt.b32 	%r1643, %r1641, %r1642, %r780;
	prmt.b32 	%r1644, %r1641, %r1642, %r779;
	mov.b64	%rd1164, {%r1644, %r1643};
	add.s64 	%rd1165, %rd1164, %rd1159;
	xor.b64  	%rd1166, %rd1165, %rd1161;
	mov.b64	{%r1645, %r1646}, %rd1166;
	prmt.b32 	%r1647, %r1645, %r1646, %r786;
	prmt.b32 	%r1648, %r1645, %r1646, %r785;
	mov.b64	%rd1167, {%r1648, %r1647};
	add.s64 	%rd1168, %rd1167, %rd1162;
	xor.b64  	%rd1169, %rd1168, %rd1164;
	mov.b64	{%r694, %r695}, %rd1169;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r768;
	// inline asm
	mov.b64	%rd1170, {%r689, %r693};
	add.s64 	%rd1171, %rd1127, %rd1090;
	xor.b64  	%rd1172, %rd1171, %rd1115;
	mov.b64	{%r1649, %r1650}, %rd1172;
	mov.b64	%rd1173, {%r1650, %r1649};
	add.s64 	%rd1174, %rd1173, %rd1102;
	xor.b64  	%rd1175, %rd1174, %rd1090;
	mov.b64	{%r1651, %r1652}, %rd1175;
	prmt.b32 	%r1653, %r1651, %r1652, %r780;
	prmt.b32 	%r1654, %r1651, %r1652, %r779;
	mov.b64	%rd1176, {%r1654, %r1653};
	add.s64 	%rd1177, %rd1176, %rd1171;
	xor.b64  	%rd1178, %rd1177, %rd1173;
	mov.b64	{%r1655, %r1656}, %rd1178;
	prmt.b32 	%r1657, %r1655, %r1656, %r786;
	prmt.b32 	%r1658, %r1655, %r1656, %r785;
	mov.b64	%rd1179, {%r1658, %r1657};
	add.s64 	%rd1180, %rd1179, %rd1174;
	xor.b64  	%rd1181, %rd1180, %rd1176;
	mov.b64	{%r702, %r703}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r768;
	// inline asm
	mov.b64	%rd1182, {%r697, %r701};
	add.s64 	%rd1183, %rd1182, %rd1141;
	xor.b64  	%rd1184, %rd1183, %rd1155;
	mov.b64	{%r1659, %r1660}, %rd1184;
	mov.b64	%rd1185, {%r1660, %r1659};
	add.s64 	%rd1186, %rd1185, %rd1168;
	xor.b64  	%rd1187, %rd1186, %rd1182;
	mov.b64	{%r1661, %r1662}, %rd1187;
	prmt.b32 	%r1663, %r1661, %r1662, %r780;
	prmt.b32 	%r1664, %r1661, %r1662, %r779;
	mov.b64	%rd1188, {%r1664, %r1663};
	add.s64 	%rd1189, %rd1188, %rd1183;
	xor.b64  	%rd1190, %rd1185, %rd1189;
	mov.b64	{%r1665, %r1666}, %rd1190;
	prmt.b32 	%r1667, %r1665, %r1666, %r786;
	prmt.b32 	%r1668, %r1665, %r1666, %r785;
	mov.b64	%rd1191, {%r1668, %r1667};
	add.s64 	%rd1192, %rd1186, %rd1191;
	xor.b64  	%rd1193, %rd1192, %rd1188;
	mov.b64	{%r710, %r711}, %rd1193;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r768;
	// inline asm
	mov.b64	%rd1194, {%r705, %r709};
	add.s64 	%rd1195, %rd1146, %rd17;
	add.s64 	%rd1196, %rd1195, %rd1153;
	xor.b64  	%rd1197, %rd1167, %rd1196;
	mov.b64	{%r1669, %r1670}, %rd1197;
	mov.b64	%rd1198, {%r1670, %r1669};
	add.s64 	%rd1199, %rd1180, %rd1198;
	xor.b64  	%rd1200, %rd1199, %rd1146;
	mov.b64	{%r1671, %r1672}, %rd1200;
	prmt.b32 	%r1673, %r1671, %r1672, %r780;
	prmt.b32 	%r1674, %r1671, %r1672, %r779;
	mov.b64	%rd1201, {%r1674, %r1673};
	add.s64 	%rd1202, %rd1196, %rd12;
	add.s64 	%rd1203, %rd1202, %rd1201;
	xor.b64  	%rd1204, %rd1203, %rd1198;
	mov.b64	{%r1675, %r1676}, %rd1204;
	prmt.b32 	%r1677, %r1675, %r1676, %r786;
	prmt.b32 	%r1678, %r1675, %r1676, %r785;
	mov.b64	%rd1205, {%r1678, %r1677};
	add.s64 	%rd1206, %rd1205, %rd1199;
	xor.b64  	%rd1207, %rd1206, %rd1201;
	mov.b64	{%r718, %r719}, %rd1207;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r768;
	// inline asm
	mov.b64	%rd1208, {%r713, %r717};
	add.s64 	%rd1209, %rd1158, %rd13;
	add.s64 	%rd1210, %rd1209, %rd1165;
	xor.b64  	%rd1211, %rd1179, %rd1210;
	mov.b64	{%r1679, %r1680}, %rd1211;
	mov.b64	%rd1212, {%r1680, %r1679};
	add.s64 	%rd1213, %rd1212, %rd1144;
	xor.b64  	%rd1214, %rd1213, %rd1158;
	mov.b64	{%r1681, %r1682}, %rd1214;
	prmt.b32 	%r1683, %r1681, %r1682, %r780;
	prmt.b32 	%r1684, %r1681, %r1682, %r779;
	mov.b64	%rd1215, {%r1684, %r1683};
	add.s64 	%rd1216, %rd1215, %rd1210;
	xor.b64  	%rd1217, %rd1216, %rd1212;
	mov.b64	{%r1685, %r1686}, %rd1217;
	prmt.b32 	%r1687, %r1685, %r1686, %r786;
	prmt.b32 	%r1688, %r1685, %r1686, %r785;
	mov.b64	%rd1218, {%r1688, %r1687};
	add.s64 	%rd1219, %rd1218, %rd1213;
	xor.b64  	%rd1220, %rd1219, %rd1215;
	mov.b64	{%r726, %r727}, %rd1220;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r768;
	// inline asm
	mov.b64	%rd1221, {%r721, %r725};
	add.s64 	%rd1222, %rd1177, %rd1170;
	xor.b64  	%rd1223, %rd1222, %rd1143;
	mov.b64	{%r1689, %r1690}, %rd1223;
	mov.b64	%rd1224, {%r1690, %r1689};
	add.s64 	%rd1225, %rd1224, %rd1156;
	xor.b64  	%rd1226, %rd1225, %rd1170;
	mov.b64	{%r1691, %r1692}, %rd1226;
	prmt.b32 	%r1693, %r1691, %r1692, %r780;
	prmt.b32 	%r1694, %r1691, %r1692, %r779;
	mov.b64	%rd1227, {%r1694, %r1693};
	add.s64 	%rd1228, %rd1222, %rd10;
	add.s64 	%rd1229, %rd1228, %rd1227;
	xor.b64  	%rd1230, %rd1229, %rd1224;
	mov.b64	{%r1695, %r1696}, %rd1230;
	prmt.b32 	%r1697, %r1695, %r1696, %r786;
	prmt.b32 	%r1698, %r1695, %r1696, %r785;
	mov.b64	%rd1231, {%r1698, %r1697};
	add.s64 	%rd1232, %rd1231, %rd1225;
	xor.b64  	%rd1233, %rd1232, %rd1227;
	mov.b64	{%r734, %r735}, %rd1233;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r768;
	// inline asm
	mov.b64	%rd1234, {%r729, %r733};
	add.s64 	%rd1235, %rd1189, %rd5;
	add.s64 	%rd1236, %rd1235, %rd1208;
	xor.b64  	%rd1237, %rd1231, %rd1236;
	mov.b64	{%r1699, %r1700}, %rd1237;
	mov.b64	%rd1238, {%r1700, %r1699};
	add.s64 	%rd1239, %rd1238, %rd1219;
	xor.b64  	%rd1240, %rd1239, %rd1208;
	mov.b64	{%r1701, %r1702}, %rd1240;
	prmt.b32 	%r1703, %r1701, %r1702, %r780;
	prmt.b32 	%r1704, %r1701, %r1702, %r779;
	mov.b64	%rd1241, {%r1704, %r1703};
	add.s64 	%rd1242, %rd1241, %rd1236;
	xor.b64  	%rd1243, %rd1238, %rd1242;
	mov.b64	{%r1705, %r1706}, %rd1243;
	prmt.b32 	%r1707, %r1705, %r1706, %r786;
	prmt.b32 	%r1708, %r1705, %r1706, %r785;
	mov.b64	%rd1244, {%r1708, %r1707};
	add.s64 	%rd1245, %rd1239, %rd1244;
	xor.b64  	%rd1246, %rd1245, %rd1241;
	mov.b64	{%r742, %r743}, %rd1246;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r768;
	// inline asm
	mov.b64	%rd1247, {%r737, %r741};
	add.s64 	%rd1248, %rd1203, %rd4;
	add.s64 	%rd1249, %rd1248, %rd1221;
	xor.b64  	%rd1250, %rd1249, %rd1191;
	mov.b64	{%r1709, %r1710}, %rd1250;
	mov.b64	%rd1251, {%r1710, %r1709};
	add.s64 	%rd1252, %rd1251, %rd1232;
	xor.b64  	%rd1253, %rd1252, %rd1221;
	mov.b64	{%r1711, %r1712}, %rd1253;
	prmt.b32 	%r1713, %r1711, %r1712, %r780;
	prmt.b32 	%r1714, %r1711, %r1712, %r779;
	mov.b64	%rd1254, {%r1714, %r1713};
	add.s64 	%rd1255, %rd1249, %rd6;
	add.s64 	%rd1256, %rd1255, %rd1254;
	xor.b64  	%rd1257, %rd1256, %rd1251;
	mov.b64	{%r1715, %r1716}, %rd1257;
	prmt.b32 	%r1717, %r1715, %r1716, %r786;
	prmt.b32 	%r1718, %r1715, %r1716, %r785;
	mov.b64	%rd1258, {%r1718, %r1717};
	add.s64 	%rd1259, %rd1258, %rd1252;
	xor.b64  	%rd1260, %rd1259, %rd1254;
	mov.b64	{%r750, %r751}, %rd1260;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r768;
	// inline asm
	mov.b64	%rd1261, {%r745, %r749};
	add.s64 	%rd1262, %rd1234, %rd1216;
	xor.b64  	%rd1263, %rd1262, %rd1205;
	mov.b64	{%r1719, %r1720}, %rd1263;
	mov.b64	%rd1264, {%r1720, %r1719};
	add.s64 	%rd1265, %rd1264, %rd1192;
	xor.b64  	%rd1266, %rd1265, %rd1234;
	mov.b64	{%r1721, %r1722}, %rd1266;
	prmt.b32 	%r1723, %r1721, %r1722, %r780;
	prmt.b32 	%r1724, %r1721, %r1722, %r779;
	mov.b64	%rd1267, {%r1724, %r1723};
	add.s64 	%rd1268, %rd1262, %rd11;
	add.s64 	%rd1269, %rd1268, %rd1267;
	xor.b64  	%rd1270, %rd1269, %rd1264;
	mov.b64	{%r1725, %r1726}, %rd1270;
	prmt.b32 	%r1727, %r1725, %r1726, %r786;
	prmt.b32 	%r1728, %r1725, %r1726, %r785;
	mov.b64	%rd1271, {%r1728, %r1727};
	add.s64 	%rd1272, %rd1271, %rd1265;
	xor.b64  	%rd1273, %rd1272, %rd1267;
	mov.b64	{%r758, %r759}, %rd1273;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r768;
	// inline asm
	mov.b64	%rd1274, {%r753, %r757};
	add.s64 	%rd1275, %rd1194, %rd20;
	add.s64 	%rd1276, %rd1275, %rd1229;
	xor.b64  	%rd1277, %rd1276, %rd1218;
	mov.b64	{%r1729, %r1730}, %rd1277;
	mov.b64	%rd1278, {%r1730, %r1729};
	add.s64 	%rd1279, %rd1278, %rd1206;
	xor.b64  	%rd1280, %rd1279, %rd1194;
	mov.b64	{%r1731, %r1732}, %rd1280;
	prmt.b32 	%r1733, %r1731, %r1732, %r780;
	prmt.b32 	%r1734, %r1731, %r1732, %r779;
	mov.b64	%rd1281, {%r1734, %r1733};
	add.s64 	%rd1282, %rd1276, %rd7;
	add.s64 	%rd1283, %rd1282, %rd1281;
	xor.b64  	%rd1284, %rd1283, %rd1278;
	mov.b64	{%r1735, %r1736}, %rd1284;
	prmt.b32 	%r1737, %r1735, %r1736, %r786;
	prmt.b32 	%r1738, %r1735, %r1736, %r785;
	mov.b64	%rd1285, {%r1738, %r1737};
	add.s64 	%rd1286, %rd1285, %rd1279;
	xor.b64  	%rd1287, %rd1286, %rd1281;
	mov.b64	{%r766, %r767}, %rd1287;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r768;
	// inline asm
	mov.b64	%rd1288, {%r761, %r765};
	xor.b64  	%rd1289, %rd1242, %rd1272;
	xor.b64  	%rd1290, %rd1289, 7640891576939301192;
	xor.b64  	%rd1291, %rd1256, %rd1286;
	xor.b64  	%rd1292, %rd1291, -4942790177534073029;
	xor.b64  	%rd1293, %rd1245, %rd1269;
	xor.b64  	%rd1294, %rd1293, 4354685564936845355;
	xor.b64  	%rd1295, %rd1259, %rd1283;
	xor.b64  	%rd1296, %rd1295, -6534734903238641935;
	xor.b64  	%rd1297, %rd1258, %rd1288;
	xor.b64  	%rd1298, %rd1297, 5840696475078001361;
	xor.b64  	%rd1299, %rd1247, %rd1271;
	xor.b64  	%rd1300, %rd1299, -7276294671716946913;
	xor.b64  	%rd1301, %rd1261, %rd1285;
	xor.b64  	%rd1302, %rd1301, 2270897969802886507;
	xor.b64  	%rd1303, %rd1244, %rd1274;
	xor.b64  	%rd1304, %rd1303, 6620516959819538809;
	cvta.to.global.u64 	%rd1305, %rd1;
	shl.b32 	%r1739, %r773, 3;
	mul.wide.u32 	%rd1306, %r1739, 8;
	add.s64 	%rd1307, %rd1305, %rd1306;
	st.global.u64 	[%rd1307], %rd1290;
	st.global.u64 	[%rd1307+8], %rd1292;
	st.global.u64 	[%rd1307+16], %rd1294;
	st.global.u64 	[%rd1307+24], %rd1296;
	st.global.u64 	[%rd1307+32], %rd1298;
	st.global.u64 	[%rd1307+40], %rd1300;
	st.global.u64 	[%rd1307+48], %rd1302;
	st.global.u64 	[%rd1307+56], %rd1304;
	ret;
}

	// .globl	_Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j
.visible .entry _Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j(
	.param .u64 _Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j_param_0,
	.param .u64 _Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j_param_1,
	.param .u32 _Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<925>;
	.reg .b64 	%rd<25>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_jE1T[8192];

	ld.param.u64 	%rd5, [_Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j_param_0];
	ld.param.u64 	%rd6, [_Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j_param_1];
	ld.param.u32 	%r32, [_Z11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_j_param_2];
	shl.b32 	%r33, %r32, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r919, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r919;
	setp.ge.u32	%p1, %r4, %r33;
	@%p1 bra 	BB13_6;

	setp.gt.s32	%p2, %r919, 2047;
	@%p2 bra 	BB13_3;

BB13_2:
	mul.wide.s32 	%rd7, %r919, 4;
	mov.u64 	%rd8, AES_TABLE;
	add.s64 	%rd9, %rd8, %rd7;
	ld.const.u32 	%r34, [%rd9];
	shl.b32 	%r35, %r919, 2;
	mov.u32 	%r36, _ZZ11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_jE1T;
	add.s32 	%r37, %r36, %r35;
	st.shared.u32 	[%r37], %r34;
	add.s32 	%r919, %r919, %r1;
	setp.lt.s32	%p3, %r919, 2048;
	@%p3 bra 	BB13_2;

BB13_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r39, %r7, 2;
	mul.wide.u32 	%rd10, %r39, 4;
	mov.u64 	%rd11, AES_KEY_FILL;
	add.s64 	%rd12, %rd11, %rd10;
	ld.const.u32 	%r8, [%rd12];
	ld.const.u32 	%r9, [%rd12+4];
	ld.const.u32 	%r10, [%rd12+8];
	ld.const.u32 	%r11, [%rd12+12];
	shr.u32 	%r40, %r4, 2;
	mul.wide.u32 	%rd13, %r40, 16;
	mul.wide.u32 	%rd14, %r7, 4;
	add.s64 	%rd15, %rd13, %rd14;
	cvta.to.global.u64 	%rd16, %rd5;
	shl.b64 	%rd17, %rd15, 2;
	add.s64 	%rd1, %rd16, %rd17;
	ld.global.u32 	%r924, [%rd1];
	ld.global.u32 	%r923, [%rd1+4];
	ld.global.u32 	%r922, [%rd1+8];
	ld.global.u32 	%r921, [%rd1+12];
	and.b32  	%r41, %r4, 1;
	shl.b32 	%r42, %r4, 4;
	and.b32  	%r43, %r42, 16;
	xor.b32  	%r44, %r43, 16;
	add.s32 	%r16, %r44, 8;
	add.s32 	%r17, %r43, 8;
	setp.eq.s32	%p4, %r41, 0;
	mov.u32 	%r38, 0;
	mov.u32 	%r45, _ZZ11fillAes1Rx4ILy2097152ELb0ELy64EEvPvS0_jE1T;
	add.s32 	%r46, %r45, 4096;
	selp.b32	%r18, %r46, %r45, %p4;
	add.s32 	%r47, %r45, 1024;
	add.s32 	%r48, %r45, 7168;
	selp.b32	%r19, %r48, %r47, %p4;
	add.s32 	%r49, %r45, 2048;
	add.s32 	%r50, %r45, 6144;
	selp.b32	%r20, %r50, %r49, %p4;
	add.s32 	%r51, %r45, 3072;
	add.s32 	%r52, %r45, 5120;
	selp.b32	%r21, %r52, %r51, %p4;
	mul.wide.u32 	%rd18, %r40, 131076;
	cvt.u64.u32	%rd19, %r4;
	and.b64  	%rd20, %rd19, 3;
	or.b64  	%rd21, %rd18, %rd20;
	cvta.to.global.u64 	%rd22, %rd6;
	shl.b64 	%rd23, %rd21, 4;
	add.s64 	%rd24, %rd22, %rd23;
	mov.u32 	%r920, %r38;

BB13_4:
	.pragma "nounroll";
	// inline asm
	bfe.u32 %r55, %r924, %r38, 8;
	// inline asm
	shl.b32 	%r439, %r55, 2;
	add.s32 	%r440, %r18, %r439;
	ld.shared.u32 	%r441, [%r440];
	// inline asm
	bfe.u32 %r58, %r923, %r16, 8;
	// inline asm
	shl.b32 	%r442, %r58, 2;
	add.s32 	%r443, %r19, %r442;
	ld.shared.u32 	%r444, [%r443];
	mov.u32 	%r435, 16;
	// inline asm
	bfe.u32 %r61, %r922, %r435, 8;
	// inline asm
	shl.b32 	%r445, %r61, 2;
	add.s32 	%r446, %r20, %r445;
	ld.shared.u32 	%r447, [%r446];
	// inline asm
	bfe.u32 %r64, %r921, %r17, 8;
	// inline asm
	shl.b32 	%r448, %r64, 2;
	add.s32 	%r449, %r21, %r448;
	xor.b32  	%r450, %r441, %r8;
	xor.b32  	%r451, %r450, %r444;
	xor.b32  	%r452, %r451, %r447;
	ld.shared.u32 	%r453, [%r449];
	// inline asm
	bfe.u32 %r67, %r923, %r38, 8;
	// inline asm
	shl.b32 	%r454, %r67, 2;
	add.s32 	%r455, %r18, %r454;
	ld.shared.u32 	%r456, [%r455];
	// inline asm
	bfe.u32 %r70, %r922, %r16, 8;
	// inline asm
	shl.b32 	%r457, %r70, 2;
	add.s32 	%r458, %r19, %r457;
	ld.shared.u32 	%r459, [%r458];
	// inline asm
	bfe.u32 %r73, %r921, %r435, 8;
	// inline asm
	shl.b32 	%r460, %r73, 2;
	add.s32 	%r461, %r20, %r460;
	ld.shared.u32 	%r462, [%r461];
	// inline asm
	bfe.u32 %r76, %r924, %r17, 8;
	// inline asm
	shl.b32 	%r463, %r76, 2;
	add.s32 	%r464, %r21, %r463;
	xor.b32  	%r465, %r456, %r9;
	xor.b32  	%r466, %r465, %r459;
	xor.b32  	%r467, %r466, %r462;
	ld.shared.u32 	%r468, [%r464];
	// inline asm
	bfe.u32 %r79, %r922, %r38, 8;
	// inline asm
	shl.b32 	%r469, %r79, 2;
	add.s32 	%r470, %r18, %r469;
	ld.shared.u32 	%r471, [%r470];
	// inline asm
	bfe.u32 %r82, %r921, %r16, 8;
	// inline asm
	shl.b32 	%r472, %r82, 2;
	add.s32 	%r473, %r19, %r472;
	ld.shared.u32 	%r474, [%r473];
	// inline asm
	bfe.u32 %r85, %r924, %r435, 8;
	// inline asm
	shl.b32 	%r475, %r85, 2;
	add.s32 	%r476, %r20, %r475;
	ld.shared.u32 	%r477, [%r476];
	// inline asm
	bfe.u32 %r88, %r923, %r17, 8;
	// inline asm
	shl.b32 	%r478, %r88, 2;
	add.s32 	%r479, %r21, %r478;
	xor.b32  	%r480, %r471, %r10;
	xor.b32  	%r481, %r480, %r474;
	xor.b32  	%r482, %r481, %r477;
	ld.shared.u32 	%r483, [%r479];
	// inline asm
	bfe.u32 %r91, %r921, %r38, 8;
	// inline asm
	shl.b32 	%r484, %r91, 2;
	add.s32 	%r485, %r18, %r484;
	ld.shared.u32 	%r486, [%r485];
	// inline asm
	bfe.u32 %r94, %r924, %r16, 8;
	// inline asm
	shl.b32 	%r487, %r94, 2;
	add.s32 	%r488, %r19, %r487;
	ld.shared.u32 	%r489, [%r488];
	// inline asm
	bfe.u32 %r97, %r923, %r435, 8;
	// inline asm
	shl.b32 	%r490, %r97, 2;
	add.s32 	%r491, %r20, %r490;
	ld.shared.u32 	%r492, [%r491];
	// inline asm
	bfe.u32 %r100, %r922, %r17, 8;
	// inline asm
	shl.b32 	%r493, %r100, 2;
	add.s32 	%r494, %r21, %r493;
	xor.b32  	%r495, %r486, %r11;
	xor.b32  	%r496, %r495, %r489;
	xor.b32  	%r497, %r496, %r492;
	ld.shared.u32 	%r498, [%r494];
	xor.b32  	%r140, %r497, %r498;
	xor.b32  	%r149, %r482, %r483;
	xor.b32  	%r146, %r467, %r468;
	xor.b32  	%r143, %r452, %r453;
	st.global.v4.u32 	[%rd24], {%r143, %r146, %r149, %r140};
	// inline asm
	bfe.u32 %r103, %r143, %r38, 8;
	// inline asm
	shl.b32 	%r499, %r103, 2;
	add.s32 	%r500, %r18, %r499;
	ld.shared.u32 	%r501, [%r500];
	// inline asm
	bfe.u32 %r106, %r146, %r16, 8;
	// inline asm
	shl.b32 	%r502, %r106, 2;
	add.s32 	%r503, %r19, %r502;
	ld.shared.u32 	%r504, [%r503];
	// inline asm
	bfe.u32 %r109, %r149, %r435, 8;
	// inline asm
	shl.b32 	%r505, %r109, 2;
	add.s32 	%r506, %r20, %r505;
	ld.shared.u32 	%r507, [%r506];
	// inline asm
	bfe.u32 %r112, %r140, %r17, 8;
	// inline asm
	shl.b32 	%r508, %r112, 2;
	add.s32 	%r509, %r21, %r508;
	xor.b32  	%r510, %r501, %r8;
	xor.b32  	%r511, %r510, %r504;
	xor.b32  	%r512, %r511, %r507;
	ld.shared.u32 	%r513, [%r509];
	// inline asm
	bfe.u32 %r115, %r146, %r38, 8;
	// inline asm
	shl.b32 	%r514, %r115, 2;
	add.s32 	%r515, %r18, %r514;
	ld.shared.u32 	%r516, [%r515];
	// inline asm
	bfe.u32 %r118, %r149, %r16, 8;
	// inline asm
	shl.b32 	%r517, %r118, 2;
	add.s32 	%r518, %r19, %r517;
	ld.shared.u32 	%r519, [%r518];
	// inline asm
	bfe.u32 %r121, %r140, %r435, 8;
	// inline asm
	shl.b32 	%r520, %r121, 2;
	add.s32 	%r521, %r20, %r520;
	ld.shared.u32 	%r522, [%r521];
	// inline asm
	bfe.u32 %r124, %r143, %r17, 8;
	// inline asm
	shl.b32 	%r523, %r124, 2;
	add.s32 	%r524, %r21, %r523;
	xor.b32  	%r525, %r516, %r9;
	xor.b32  	%r526, %r525, %r519;
	xor.b32  	%r527, %r526, %r522;
	ld.shared.u32 	%r528, [%r524];
	// inline asm
	bfe.u32 %r127, %r149, %r38, 8;
	// inline asm
	shl.b32 	%r529, %r127, 2;
	add.s32 	%r530, %r18, %r529;
	ld.shared.u32 	%r531, [%r530];
	// inline asm
	bfe.u32 %r130, %r140, %r16, 8;
	// inline asm
	shl.b32 	%r532, %r130, 2;
	add.s32 	%r533, %r19, %r532;
	ld.shared.u32 	%r534, [%r533];
	// inline asm
	bfe.u32 %r133, %r143, %r435, 8;
	// inline asm
	shl.b32 	%r535, %r133, 2;
	add.s32 	%r536, %r20, %r535;
	ld.shared.u32 	%r537, [%r536];
	// inline asm
	bfe.u32 %r136, %r146, %r17, 8;
	// inline asm
	shl.b32 	%r538, %r136, 2;
	add.s32 	%r539, %r21, %r538;
	xor.b32  	%r540, %r531, %r10;
	xor.b32  	%r541, %r540, %r534;
	xor.b32  	%r542, %r541, %r537;
	ld.shared.u32 	%r543, [%r539];
	// inline asm
	bfe.u32 %r139, %r140, %r38, 8;
	// inline asm
	shl.b32 	%r544, %r139, 2;
	add.s32 	%r545, %r18, %r544;
	ld.shared.u32 	%r546, [%r545];
	// inline asm
	bfe.u32 %r142, %r143, %r16, 8;
	// inline asm
	shl.b32 	%r547, %r142, 2;
	add.s32 	%r548, %r19, %r547;
	ld.shared.u32 	%r549, [%r548];
	// inline asm
	bfe.u32 %r145, %r146, %r435, 8;
	// inline asm
	shl.b32 	%r550, %r145, 2;
	add.s32 	%r551, %r20, %r550;
	ld.shared.u32 	%r552, [%r551];
	// inline asm
	bfe.u32 %r148, %r149, %r17, 8;
	// inline asm
	shl.b32 	%r553, %r148, 2;
	add.s32 	%r554, %r21, %r553;
	xor.b32  	%r555, %r546, %r11;
	xor.b32  	%r556, %r555, %r549;
	xor.b32  	%r557, %r556, %r552;
	ld.shared.u32 	%r558, [%r554];
	xor.b32  	%r188, %r557, %r558;
	xor.b32  	%r197, %r542, %r543;
	xor.b32  	%r194, %r527, %r528;
	xor.b32  	%r191, %r512, %r513;
	st.global.v4.u32 	[%rd24+64], {%r191, %r194, %r197, %r188};
	// inline asm
	bfe.u32 %r151, %r191, %r38, 8;
	// inline asm
	shl.b32 	%r559, %r151, 2;
	add.s32 	%r560, %r18, %r559;
	ld.shared.u32 	%r561, [%r560];
	// inline asm
	bfe.u32 %r154, %r194, %r16, 8;
	// inline asm
	shl.b32 	%r562, %r154, 2;
	add.s32 	%r563, %r19, %r562;
	ld.shared.u32 	%r564, [%r563];
	// inline asm
	bfe.u32 %r157, %r197, %r435, 8;
	// inline asm
	shl.b32 	%r565, %r157, 2;
	add.s32 	%r566, %r20, %r565;
	ld.shared.u32 	%r567, [%r566];
	// inline asm
	bfe.u32 %r160, %r188, %r17, 8;
	// inline asm
	shl.b32 	%r568, %r160, 2;
	add.s32 	%r569, %r21, %r568;
	xor.b32  	%r570, %r561, %r8;
	xor.b32  	%r571, %r570, %r564;
	xor.b32  	%r572, %r571, %r567;
	ld.shared.u32 	%r573, [%r569];
	// inline asm
	bfe.u32 %r163, %r194, %r38, 8;
	// inline asm
	shl.b32 	%r574, %r163, 2;
	add.s32 	%r575, %r18, %r574;
	ld.shared.u32 	%r576, [%r575];
	// inline asm
	bfe.u32 %r166, %r197, %r16, 8;
	// inline asm
	shl.b32 	%r577, %r166, 2;
	add.s32 	%r578, %r19, %r577;
	ld.shared.u32 	%r579, [%r578];
	// inline asm
	bfe.u32 %r169, %r188, %r435, 8;
	// inline asm
	shl.b32 	%r580, %r169, 2;
	add.s32 	%r581, %r20, %r580;
	ld.shared.u32 	%r582, [%r581];
	// inline asm
	bfe.u32 %r172, %r191, %r17, 8;
	// inline asm
	shl.b32 	%r583, %r172, 2;
	add.s32 	%r584, %r21, %r583;
	xor.b32  	%r585, %r576, %r9;
	xor.b32  	%r586, %r585, %r579;
	xor.b32  	%r587, %r586, %r582;
	ld.shared.u32 	%r588, [%r584];
	// inline asm
	bfe.u32 %r175, %r197, %r38, 8;
	// inline asm
	shl.b32 	%r589, %r175, 2;
	add.s32 	%r590, %r18, %r589;
	ld.shared.u32 	%r591, [%r590];
	// inline asm
	bfe.u32 %r178, %r188, %r16, 8;
	// inline asm
	shl.b32 	%r592, %r178, 2;
	add.s32 	%r593, %r19, %r592;
	ld.shared.u32 	%r594, [%r593];
	// inline asm
	bfe.u32 %r181, %r191, %r435, 8;
	// inline asm
	shl.b32 	%r595, %r181, 2;
	add.s32 	%r596, %r20, %r595;
	ld.shared.u32 	%r597, [%r596];
	// inline asm
	bfe.u32 %r184, %r194, %r17, 8;
	// inline asm
	shl.b32 	%r598, %r184, 2;
	add.s32 	%r599, %r21, %r598;
	xor.b32  	%r600, %r591, %r10;
	xor.b32  	%r601, %r600, %r594;
	xor.b32  	%r602, %r601, %r597;
	ld.shared.u32 	%r603, [%r599];
	// inline asm
	bfe.u32 %r187, %r188, %r38, 8;
	// inline asm
	shl.b32 	%r604, %r187, 2;
	add.s32 	%r605, %r18, %r604;
	ld.shared.u32 	%r606, [%r605];
	// inline asm
	bfe.u32 %r190, %r191, %r16, 8;
	// inline asm
	shl.b32 	%r607, %r190, 2;
	add.s32 	%r608, %r19, %r607;
	ld.shared.u32 	%r609, [%r608];
	// inline asm
	bfe.u32 %r193, %r194, %r435, 8;
	// inline asm
	shl.b32 	%r610, %r193, 2;
	add.s32 	%r611, %r20, %r610;
	ld.shared.u32 	%r612, [%r611];
	// inline asm
	bfe.u32 %r196, %r197, %r17, 8;
	// inline asm
	shl.b32 	%r613, %r196, 2;
	add.s32 	%r614, %r21, %r613;
	xor.b32  	%r615, %r606, %r11;
	xor.b32  	%r616, %r615, %r609;
	xor.b32  	%r617, %r616, %r612;
	ld.shared.u32 	%r618, [%r614];
	xor.b32  	%r236, %r617, %r618;
	xor.b32  	%r245, %r602, %r603;
	xor.b32  	%r242, %r587, %r588;
	xor.b32  	%r239, %r572, %r573;
	st.global.v4.u32 	[%rd24+128], {%r239, %r242, %r245, %r236};
	// inline asm
	bfe.u32 %r199, %r239, %r38, 8;
	// inline asm
	shl.b32 	%r619, %r199, 2;
	add.s32 	%r620, %r18, %r619;
	ld.shared.u32 	%r621, [%r620];
	// inline asm
	bfe.u32 %r202, %r242, %r16, 8;
	// inline asm
	shl.b32 	%r622, %r202, 2;
	add.s32 	%r623, %r19, %r622;
	ld.shared.u32 	%r624, [%r623];
	// inline asm
	bfe.u32 %r205, %r245, %r435, 8;
	// inline asm
	shl.b32 	%r625, %r205, 2;
	add.s32 	%r626, %r20, %r625;
	ld.shared.u32 	%r627, [%r626];
	// inline asm
	bfe.u32 %r208, %r236, %r17, 8;
	// inline asm
	shl.b32 	%r628, %r208, 2;
	add.s32 	%r629, %r21, %r628;
	xor.b32  	%r630, %r621, %r8;
	xor.b32  	%r631, %r630, %r624;
	xor.b32  	%r632, %r631, %r627;
	ld.shared.u32 	%r633, [%r629];
	// inline asm
	bfe.u32 %r211, %r242, %r38, 8;
	// inline asm
	shl.b32 	%r634, %r211, 2;
	add.s32 	%r635, %r18, %r634;
	ld.shared.u32 	%r636, [%r635];
	// inline asm
	bfe.u32 %r214, %r245, %r16, 8;
	// inline asm
	shl.b32 	%r637, %r214, 2;
	add.s32 	%r638, %r19, %r637;
	ld.shared.u32 	%r639, [%r638];
	// inline asm
	bfe.u32 %r217, %r236, %r435, 8;
	// inline asm
	shl.b32 	%r640, %r217, 2;
	add.s32 	%r641, %r20, %r640;
	ld.shared.u32 	%r642, [%r641];
	// inline asm
	bfe.u32 %r220, %r239, %r17, 8;
	// inline asm
	shl.b32 	%r643, %r220, 2;
	add.s32 	%r644, %r21, %r643;
	xor.b32  	%r645, %r636, %r9;
	xor.b32  	%r646, %r645, %r639;
	xor.b32  	%r647, %r646, %r642;
	ld.shared.u32 	%r648, [%r644];
	// inline asm
	bfe.u32 %r223, %r245, %r38, 8;
	// inline asm
	shl.b32 	%r649, %r223, 2;
	add.s32 	%r650, %r18, %r649;
	ld.shared.u32 	%r651, [%r650];
	// inline asm
	bfe.u32 %r226, %r236, %r16, 8;
	// inline asm
	shl.b32 	%r652, %r226, 2;
	add.s32 	%r653, %r19, %r652;
	ld.shared.u32 	%r654, [%r653];
	// inline asm
	bfe.u32 %r229, %r239, %r435, 8;
	// inline asm
	shl.b32 	%r655, %r229, 2;
	add.s32 	%r656, %r20, %r655;
	ld.shared.u32 	%r657, [%r656];
	// inline asm
	bfe.u32 %r232, %r242, %r17, 8;
	// inline asm
	shl.b32 	%r658, %r232, 2;
	add.s32 	%r659, %r21, %r658;
	xor.b32  	%r660, %r651, %r10;
	xor.b32  	%r661, %r660, %r654;
	xor.b32  	%r662, %r661, %r657;
	ld.shared.u32 	%r663, [%r659];
	// inline asm
	bfe.u32 %r235, %r236, %r38, 8;
	// inline asm
	shl.b32 	%r664, %r235, 2;
	add.s32 	%r665, %r18, %r664;
	ld.shared.u32 	%r666, [%r665];
	// inline asm
	bfe.u32 %r238, %r239, %r16, 8;
	// inline asm
	shl.b32 	%r667, %r238, 2;
	add.s32 	%r668, %r19, %r667;
	ld.shared.u32 	%r669, [%r668];
	// inline asm
	bfe.u32 %r241, %r242, %r435, 8;
	// inline asm
	shl.b32 	%r670, %r241, 2;
	add.s32 	%r671, %r20, %r670;
	ld.shared.u32 	%r672, [%r671];
	// inline asm
	bfe.u32 %r244, %r245, %r17, 8;
	// inline asm
	shl.b32 	%r673, %r244, 2;
	add.s32 	%r674, %r21, %r673;
	xor.b32  	%r675, %r666, %r11;
	xor.b32  	%r676, %r675, %r669;
	xor.b32  	%r677, %r676, %r672;
	ld.shared.u32 	%r678, [%r674];
	xor.b32  	%r284, %r677, %r678;
	xor.b32  	%r293, %r662, %r663;
	xor.b32  	%r290, %r647, %r648;
	xor.b32  	%r287, %r632, %r633;
	st.global.v4.u32 	[%rd24+192], {%r287, %r290, %r293, %r284};
	// inline asm
	bfe.u32 %r247, %r287, %r38, 8;
	// inline asm
	shl.b32 	%r679, %r247, 2;
	add.s32 	%r680, %r18, %r679;
	ld.shared.u32 	%r681, [%r680];
	// inline asm
	bfe.u32 %r250, %r290, %r16, 8;
	// inline asm
	shl.b32 	%r682, %r250, 2;
	add.s32 	%r683, %r19, %r682;
	ld.shared.u32 	%r684, [%r683];
	// inline asm
	bfe.u32 %r253, %r293, %r435, 8;
	// inline asm
	shl.b32 	%r685, %r253, 2;
	add.s32 	%r686, %r20, %r685;
	ld.shared.u32 	%r687, [%r686];
	// inline asm
	bfe.u32 %r256, %r284, %r17, 8;
	// inline asm
	shl.b32 	%r688, %r256, 2;
	add.s32 	%r689, %r21, %r688;
	xor.b32  	%r690, %r681, %r8;
	xor.b32  	%r691, %r690, %r684;
	xor.b32  	%r692, %r691, %r687;
	ld.shared.u32 	%r693, [%r689];
	// inline asm
	bfe.u32 %r259, %r290, %r38, 8;
	// inline asm
	shl.b32 	%r694, %r259, 2;
	add.s32 	%r695, %r18, %r694;
	ld.shared.u32 	%r696, [%r695];
	// inline asm
	bfe.u32 %r262, %r293, %r16, 8;
	// inline asm
	shl.b32 	%r697, %r262, 2;
	add.s32 	%r698, %r19, %r697;
	ld.shared.u32 	%r699, [%r698];
	// inline asm
	bfe.u32 %r265, %r284, %r435, 8;
	// inline asm
	shl.b32 	%r700, %r265, 2;
	add.s32 	%r701, %r20, %r700;
	ld.shared.u32 	%r702, [%r701];
	// inline asm
	bfe.u32 %r268, %r287, %r17, 8;
	// inline asm
	shl.b32 	%r703, %r268, 2;
	add.s32 	%r704, %r21, %r703;
	xor.b32  	%r705, %r696, %r9;
	xor.b32  	%r706, %r705, %r699;
	xor.b32  	%r707, %r706, %r702;
	ld.shared.u32 	%r708, [%r704];
	// inline asm
	bfe.u32 %r271, %r293, %r38, 8;
	// inline asm
	shl.b32 	%r709, %r271, 2;
	add.s32 	%r710, %r18, %r709;
	ld.shared.u32 	%r711, [%r710];
	// inline asm
	bfe.u32 %r274, %r284, %r16, 8;
	// inline asm
	shl.b32 	%r712, %r274, 2;
	add.s32 	%r713, %r19, %r712;
	ld.shared.u32 	%r714, [%r713];
	// inline asm
	bfe.u32 %r277, %r287, %r435, 8;
	// inline asm
	shl.b32 	%r715, %r277, 2;
	add.s32 	%r716, %r20, %r715;
	ld.shared.u32 	%r717, [%r716];
	// inline asm
	bfe.u32 %r280, %r290, %r17, 8;
	// inline asm
	shl.b32 	%r718, %r280, 2;
	add.s32 	%r719, %r21, %r718;
	xor.b32  	%r720, %r711, %r10;
	xor.b32  	%r721, %r720, %r714;
	xor.b32  	%r722, %r721, %r717;
	ld.shared.u32 	%r723, [%r719];
	// inline asm
	bfe.u32 %r283, %r284, %r38, 8;
	// inline asm
	shl.b32 	%r724, %r283, 2;
	add.s32 	%r725, %r18, %r724;
	ld.shared.u32 	%r726, [%r725];
	// inline asm
	bfe.u32 %r286, %r287, %r16, 8;
	// inline asm
	shl.b32 	%r727, %r286, 2;
	add.s32 	%r728, %r19, %r727;
	ld.shared.u32 	%r729, [%r728];
	// inline asm
	bfe.u32 %r289, %r290, %r435, 8;
	// inline asm
	shl.b32 	%r730, %r289, 2;
	add.s32 	%r731, %r20, %r730;
	ld.shared.u32 	%r732, [%r731];
	// inline asm
	bfe.u32 %r292, %r293, %r17, 8;
	// inline asm
	shl.b32 	%r733, %r292, 2;
	add.s32 	%r734, %r21, %r733;
	xor.b32  	%r735, %r726, %r11;
	xor.b32  	%r736, %r735, %r729;
	xor.b32  	%r737, %r736, %r732;
	ld.shared.u32 	%r738, [%r734];
	xor.b32  	%r332, %r737, %r738;
	xor.b32  	%r341, %r722, %r723;
	xor.b32  	%r338, %r707, %r708;
	xor.b32  	%r335, %r692, %r693;
	st.global.v4.u32 	[%rd24+256], {%r335, %r338, %r341, %r332};
	// inline asm
	bfe.u32 %r295, %r335, %r38, 8;
	// inline asm
	shl.b32 	%r739, %r295, 2;
	add.s32 	%r740, %r18, %r739;
	ld.shared.u32 	%r741, [%r740];
	// inline asm
	bfe.u32 %r298, %r338, %r16, 8;
	// inline asm
	shl.b32 	%r742, %r298, 2;
	add.s32 	%r743, %r19, %r742;
	ld.shared.u32 	%r744, [%r743];
	// inline asm
	bfe.u32 %r301, %r341, %r435, 8;
	// inline asm
	shl.b32 	%r745, %r301, 2;
	add.s32 	%r746, %r20, %r745;
	ld.shared.u32 	%r747, [%r746];
	// inline asm
	bfe.u32 %r304, %r332, %r17, 8;
	// inline asm
	shl.b32 	%r748, %r304, 2;
	add.s32 	%r749, %r21, %r748;
	xor.b32  	%r750, %r741, %r8;
	xor.b32  	%r751, %r750, %r744;
	xor.b32  	%r752, %r751, %r747;
	ld.shared.u32 	%r753, [%r749];
	// inline asm
	bfe.u32 %r307, %r338, %r38, 8;
	// inline asm
	shl.b32 	%r754, %r307, 2;
	add.s32 	%r755, %r18, %r754;
	ld.shared.u32 	%r756, [%r755];
	// inline asm
	bfe.u32 %r310, %r341, %r16, 8;
	// inline asm
	shl.b32 	%r757, %r310, 2;
	add.s32 	%r758, %r19, %r757;
	ld.shared.u32 	%r759, [%r758];
	// inline asm
	bfe.u32 %r313, %r332, %r435, 8;
	// inline asm
	shl.b32 	%r760, %r313, 2;
	add.s32 	%r761, %r20, %r760;
	ld.shared.u32 	%r762, [%r761];
	// inline asm
	bfe.u32 %r316, %r335, %r17, 8;
	// inline asm
	shl.b32 	%r763, %r316, 2;
	add.s32 	%r764, %r21, %r763;
	xor.b32  	%r765, %r756, %r9;
	xor.b32  	%r766, %r765, %r759;
	xor.b32  	%r767, %r766, %r762;
	ld.shared.u32 	%r768, [%r764];
	// inline asm
	bfe.u32 %r319, %r341, %r38, 8;
	// inline asm
	shl.b32 	%r769, %r319, 2;
	add.s32 	%r770, %r18, %r769;
	ld.shared.u32 	%r771, [%r770];
	// inline asm
	bfe.u32 %r322, %r332, %r16, 8;
	// inline asm
	shl.b32 	%r772, %r322, 2;
	add.s32 	%r773, %r19, %r772;
	ld.shared.u32 	%r774, [%r773];
	// inline asm
	bfe.u32 %r325, %r335, %r435, 8;
	// inline asm
	shl.b32 	%r775, %r325, 2;
	add.s32 	%r776, %r20, %r775;
	ld.shared.u32 	%r777, [%r776];
	// inline asm
	bfe.u32 %r328, %r338, %r17, 8;
	// inline asm
	shl.b32 	%r778, %r328, 2;
	add.s32 	%r779, %r21, %r778;
	xor.b32  	%r780, %r771, %r10;
	xor.b32  	%r781, %r780, %r774;
	xor.b32  	%r782, %r781, %r777;
	ld.shared.u32 	%r783, [%r779];
	// inline asm
	bfe.u32 %r331, %r332, %r38, 8;
	// inline asm
	shl.b32 	%r784, %r331, 2;
	add.s32 	%r785, %r18, %r784;
	ld.shared.u32 	%r786, [%r785];
	// inline asm
	bfe.u32 %r334, %r335, %r16, 8;
	// inline asm
	shl.b32 	%r787, %r334, 2;
	add.s32 	%r788, %r19, %r787;
	ld.shared.u32 	%r789, [%r788];
	// inline asm
	bfe.u32 %r337, %r338, %r435, 8;
	// inline asm
	shl.b32 	%r790, %r337, 2;
	add.s32 	%r791, %r20, %r790;
	ld.shared.u32 	%r792, [%r791];
	// inline asm
	bfe.u32 %r340, %r341, %r17, 8;
	// inline asm
	shl.b32 	%r793, %r340, 2;
	add.s32 	%r794, %r21, %r793;
	xor.b32  	%r795, %r786, %r11;
	xor.b32  	%r796, %r795, %r789;
	xor.b32  	%r797, %r796, %r792;
	ld.shared.u32 	%r798, [%r794];
	xor.b32  	%r380, %r797, %r798;
	xor.b32  	%r389, %r782, %r783;
	xor.b32  	%r386, %r767, %r768;
	xor.b32  	%r383, %r752, %r753;
	st.global.v4.u32 	[%rd24+320], {%r383, %r386, %r389, %r380};
	// inline asm
	bfe.u32 %r343, %r383, %r38, 8;
	// inline asm
	shl.b32 	%r799, %r343, 2;
	add.s32 	%r800, %r18, %r799;
	ld.shared.u32 	%r801, [%r800];
	// inline asm
	bfe.u32 %r346, %r386, %r16, 8;
	// inline asm
	shl.b32 	%r802, %r346, 2;
	add.s32 	%r803, %r19, %r802;
	ld.shared.u32 	%r804, [%r803];
	// inline asm
	bfe.u32 %r349, %r389, %r435, 8;
	// inline asm
	shl.b32 	%r805, %r349, 2;
	add.s32 	%r806, %r20, %r805;
	ld.shared.u32 	%r807, [%r806];
	// inline asm
	bfe.u32 %r352, %r380, %r17, 8;
	// inline asm
	shl.b32 	%r808, %r352, 2;
	add.s32 	%r809, %r21, %r808;
	xor.b32  	%r810, %r801, %r8;
	xor.b32  	%r811, %r810, %r804;
	xor.b32  	%r812, %r811, %r807;
	ld.shared.u32 	%r813, [%r809];
	// inline asm
	bfe.u32 %r355, %r386, %r38, 8;
	// inline asm
	shl.b32 	%r814, %r355, 2;
	add.s32 	%r815, %r18, %r814;
	ld.shared.u32 	%r816, [%r815];
	// inline asm
	bfe.u32 %r358, %r389, %r16, 8;
	// inline asm
	shl.b32 	%r817, %r358, 2;
	add.s32 	%r818, %r19, %r817;
	ld.shared.u32 	%r819, [%r818];
	// inline asm
	bfe.u32 %r361, %r380, %r435, 8;
	// inline asm
	shl.b32 	%r820, %r361, 2;
	add.s32 	%r821, %r20, %r820;
	ld.shared.u32 	%r822, [%r821];
	// inline asm
	bfe.u32 %r364, %r383, %r17, 8;
	// inline asm
	shl.b32 	%r823, %r364, 2;
	add.s32 	%r824, %r21, %r823;
	xor.b32  	%r825, %r816, %r9;
	xor.b32  	%r826, %r825, %r819;
	xor.b32  	%r827, %r826, %r822;
	ld.shared.u32 	%r828, [%r824];
	// inline asm
	bfe.u32 %r367, %r389, %r38, 8;
	// inline asm
	shl.b32 	%r829, %r367, 2;
	add.s32 	%r830, %r18, %r829;
	ld.shared.u32 	%r831, [%r830];
	// inline asm
	bfe.u32 %r370, %r380, %r16, 8;
	// inline asm
	shl.b32 	%r832, %r370, 2;
	add.s32 	%r833, %r19, %r832;
	ld.shared.u32 	%r834, [%r833];
	// inline asm
	bfe.u32 %r373, %r383, %r435, 8;
	// inline asm
	shl.b32 	%r835, %r373, 2;
	add.s32 	%r836, %r20, %r835;
	ld.shared.u32 	%r837, [%r836];
	// inline asm
	bfe.u32 %r376, %r386, %r17, 8;
	// inline asm
	shl.b32 	%r838, %r376, 2;
	add.s32 	%r839, %r21, %r838;
	xor.b32  	%r840, %r831, %r10;
	xor.b32  	%r841, %r840, %r834;
	xor.b32  	%r842, %r841, %r837;
	ld.shared.u32 	%r843, [%r839];
	// inline asm
	bfe.u32 %r379, %r380, %r38, 8;
	// inline asm
	shl.b32 	%r844, %r379, 2;
	add.s32 	%r845, %r18, %r844;
	ld.shared.u32 	%r846, [%r845];
	// inline asm
	bfe.u32 %r382, %r383, %r16, 8;
	// inline asm
	shl.b32 	%r847, %r382, 2;
	add.s32 	%r848, %r19, %r847;
	ld.shared.u32 	%r849, [%r848];
	// inline asm
	bfe.u32 %r385, %r386, %r435, 8;
	// inline asm
	shl.b32 	%r850, %r385, 2;
	add.s32 	%r851, %r20, %r850;
	ld.shared.u32 	%r852, [%r851];
	// inline asm
	bfe.u32 %r388, %r389, %r17, 8;
	// inline asm
	shl.b32 	%r853, %r388, 2;
	add.s32 	%r854, %r21, %r853;
	xor.b32  	%r855, %r846, %r11;
	xor.b32  	%r856, %r855, %r849;
	xor.b32  	%r857, %r856, %r852;
	ld.shared.u32 	%r858, [%r854];
	xor.b32  	%r428, %r857, %r858;
	xor.b32  	%r437, %r842, %r843;
	xor.b32  	%r434, %r827, %r828;
	xor.b32  	%r431, %r812, %r813;
	st.global.v4.u32 	[%rd24+384], {%r431, %r434, %r437, %r428};
	// inline asm
	bfe.u32 %r391, %r431, %r38, 8;
	// inline asm
	shl.b32 	%r859, %r391, 2;
	add.s32 	%r860, %r18, %r859;
	ld.shared.u32 	%r861, [%r860];
	// inline asm
	bfe.u32 %r394, %r434, %r16, 8;
	// inline asm
	shl.b32 	%r862, %r394, 2;
	add.s32 	%r863, %r19, %r862;
	ld.shared.u32 	%r864, [%r863];
	// inline asm
	bfe.u32 %r397, %r437, %r435, 8;
	// inline asm
	shl.b32 	%r865, %r397, 2;
	add.s32 	%r866, %r20, %r865;
	ld.shared.u32 	%r867, [%r866];
	// inline asm
	bfe.u32 %r400, %r428, %r17, 8;
	// inline asm
	shl.b32 	%r868, %r400, 2;
	add.s32 	%r869, %r21, %r868;
	xor.b32  	%r870, %r861, %r8;
	xor.b32  	%r871, %r870, %r864;
	xor.b32  	%r872, %r871, %r867;
	ld.shared.u32 	%r873, [%r869];
	xor.b32  	%r924, %r872, %r873;
	// inline asm
	bfe.u32 %r403, %r434, %r38, 8;
	// inline asm
	shl.b32 	%r874, %r403, 2;
	add.s32 	%r875, %r18, %r874;
	ld.shared.u32 	%r876, [%r875];
	// inline asm
	bfe.u32 %r406, %r437, %r16, 8;
	// inline asm
	shl.b32 	%r877, %r406, 2;
	add.s32 	%r878, %r19, %r877;
	ld.shared.u32 	%r879, [%r878];
	// inline asm
	bfe.u32 %r409, %r428, %r435, 8;
	// inline asm
	shl.b32 	%r880, %r409, 2;
	add.s32 	%r881, %r20, %r880;
	ld.shared.u32 	%r882, [%r881];
	// inline asm
	bfe.u32 %r412, %r431, %r17, 8;
	// inline asm
	shl.b32 	%r883, %r412, 2;
	add.s32 	%r884, %r21, %r883;
	xor.b32  	%r885, %r876, %r9;
	xor.b32  	%r886, %r885, %r879;
	xor.b32  	%r887, %r886, %r882;
	ld.shared.u32 	%r888, [%r884];
	xor.b32  	%r923, %r887, %r888;
	// inline asm
	bfe.u32 %r415, %r437, %r38, 8;
	// inline asm
	shl.b32 	%r889, %r415, 2;
	add.s32 	%r890, %r18, %r889;
	ld.shared.u32 	%r891, [%r890];
	// inline asm
	bfe.u32 %r418, %r428, %r16, 8;
	// inline asm
	shl.b32 	%r892, %r418, 2;
	add.s32 	%r893, %r19, %r892;
	ld.shared.u32 	%r894, [%r893];
	// inline asm
	bfe.u32 %r421, %r431, %r435, 8;
	// inline asm
	shl.b32 	%r895, %r421, 2;
	add.s32 	%r896, %r20, %r895;
	ld.shared.u32 	%r897, [%r896];
	// inline asm
	bfe.u32 %r424, %r434, %r17, 8;
	// inline asm
	shl.b32 	%r898, %r424, 2;
	add.s32 	%r899, %r21, %r898;
	xor.b32  	%r900, %r891, %r10;
	xor.b32  	%r901, %r900, %r894;
	xor.b32  	%r902, %r901, %r897;
	ld.shared.u32 	%r903, [%r899];
	xor.b32  	%r922, %r902, %r903;
	// inline asm
	bfe.u32 %r427, %r428, %r38, 8;
	// inline asm
	shl.b32 	%r904, %r427, 2;
	add.s32 	%r905, %r18, %r904;
	ld.shared.u32 	%r906, [%r905];
	// inline asm
	bfe.u32 %r430, %r431, %r16, 8;
	// inline asm
	shl.b32 	%r907, %r430, 2;
	add.s32 	%r908, %r19, %r907;
	ld.shared.u32 	%r909, [%r908];
	// inline asm
	bfe.u32 %r433, %r434, %r435, 8;
	// inline asm
	shl.b32 	%r910, %r433, 2;
	add.s32 	%r911, %r20, %r910;
	ld.shared.u32 	%r912, [%r911];
	// inline asm
	bfe.u32 %r436, %r437, %r17, 8;
	// inline asm
	shl.b32 	%r913, %r436, 2;
	add.s32 	%r914, %r21, %r913;
	xor.b32  	%r915, %r906, %r11;
	xor.b32  	%r916, %r915, %r909;
	xor.b32  	%r917, %r916, %r912;
	ld.shared.u32 	%r918, [%r914];
	xor.b32  	%r921, %r917, %r918;
	st.global.v4.u32 	[%rd24+448], {%r924, %r923, %r922, %r921};
	add.s32 	%r920, %r920, 32;
	setp.lt.u32	%p5, %r920, 131072;
	add.s64 	%rd24, %rd24, 512;
	@%p5 bra 	BB13_4;

	st.global.v2.u32 	[%rd1], {%r924, %r923};
	st.global.v2.u32 	[%rd1+8], {%r922, %r921};

BB13_6:
	ret;
}

	// .globl	_Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j
.visible .entry _Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j(
	.param .u64 _Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j_param_0,
	.param .u64 _Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j_param_1,
	.param .u32 _Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<920>;
	.reg .b64 	%rd<22>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11fillAes4Rx4ILy2176ELb0EEvPvS0_jE1T[8192];

	ld.param.u64 	%rd5, [_Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j_param_0];
	ld.param.u64 	%rd6, [_Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j_param_1];
	ld.param.u32 	%r27, [_Z11fillAes4Rx4ILy2176ELb0EEvPvS0_j_param_2];
	shl.b32 	%r28, %r27, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r914, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r914;
	setp.ge.u32	%p1, %r4, %r28;
	@%p1 bra 	BB14_6;

	setp.gt.s32	%p2, %r914, 2047;
	@%p2 bra 	BB14_3;

BB14_2:
	mul.wide.s32 	%rd7, %r914, 4;
	mov.u64 	%rd8, AES_TABLE;
	add.s64 	%rd9, %rd8, %rd7;
	ld.const.u32 	%r29, [%rd9];
	shl.b32 	%r30, %r914, 2;
	mov.u32 	%r31, _ZZ11fillAes4Rx4ILy2176ELb0EEvPvS0_jE1T;
	add.s32 	%r32, %r31, %r30;
	st.shared.u32 	[%r32], %r29;
	add.s32 	%r914, %r914, %r1;
	setp.lt.s32	%p3, %r914, 2048;
	@%p3 bra 	BB14_2;

BB14_3:
	bar.sync 	0;
	shr.u32 	%r34, %r4, 2;
	mul.wide.u32 	%rd10, %r34, 16;
	and.b32  	%r35, %r4, 3;
	mul.wide.u32 	%rd11, %r35, 4;
	or.b64  	%rd12, %rd10, %rd11;
	cvta.to.global.u64 	%rd13, %rd5;
	shl.b64 	%rd14, %rd12, 2;
	add.s64 	%rd1, %rd13, %rd14;
	ld.global.u32 	%r919, [%rd1];
	ld.global.u32 	%r918, [%rd1+4];
	ld.global.u32 	%r917, [%rd1+8];
	ld.global.u32 	%r916, [%rd1+12];
	and.b32  	%r36, %r4, 1;
	shl.b32 	%r37, %r4, 4;
	and.b32  	%r38, %r37, 16;
	xor.b32  	%r39, %r38, 16;
	add.s32 	%r11, %r39, 8;
	add.s32 	%r12, %r38, 8;
	setp.eq.s32	%p4, %r36, 0;
	mov.u32 	%r33, 0;
	mov.u32 	%r40, _ZZ11fillAes4Rx4ILy2176ELb0EEvPvS0_jE1T;
	add.s32 	%r41, %r40, 4096;
	selp.b32	%r13, %r41, %r40, %p4;
	add.s32 	%r42, %r40, 1024;
	add.s32 	%r43, %r40, 7168;
	selp.b32	%r14, %r43, %r42, %p4;
	add.s32 	%r44, %r40, 2048;
	add.s32 	%r45, %r40, 6144;
	selp.b32	%r15, %r45, %r44, %p4;
	add.s32 	%r46, %r40, 3072;
	add.s32 	%r47, %r40, 5120;
	selp.b32	%r16, %r47, %r46, %p4;
	mul.wide.u32 	%rd15, %r34, 136;
	cvt.u64.u32	%rd16, %r4;
	and.b64  	%rd17, %rd16, 3;
	or.b64  	%rd18, %rd15, %rd17;
	cvta.to.global.u64 	%rd19, %rd6;
	shl.b64 	%rd20, %rd18, 4;
	add.s64 	%rd21, %rd19, %rd20;
	mov.u32 	%r915, %r33;

BB14_4:
	.pragma "nounroll";
	// inline asm
	bfe.u32 %r50, %r919, %r33, 8;
	// inline asm
	shl.b32 	%r434, %r50, 2;
	add.s32 	%r435, %r13, %r434;
	ld.shared.u32 	%r436, [%r435];
	// inline asm
	bfe.u32 %r53, %r918, %r11, 8;
	// inline asm
	shl.b32 	%r437, %r53, 2;
	add.s32 	%r438, %r14, %r437;
	ld.shared.u32 	%r439, [%r438];
	mov.u32 	%r430, 16;
	// inline asm
	bfe.u32 %r56, %r917, %r430, 8;
	// inline asm
	shl.b32 	%r440, %r56, 2;
	add.s32 	%r441, %r15, %r440;
	ld.shared.u32 	%r442, [%r441];
	// inline asm
	bfe.u32 %r59, %r916, %r12, 8;
	// inline asm
	shl.b32 	%r443, %r59, 2;
	add.s32 	%r444, %r16, %r443;
	xor.b32  	%r445, %r436, %r439;
	xor.b32  	%r446, %r445, %r442;
	ld.shared.u32 	%r447, [%r444];
	xor.b32  	%r448, %r446, %r447;
	xor.b32  	%r99, %r448, -124762531;
	// inline asm
	bfe.u32 %r62, %r918, %r33, 8;
	// inline asm
	shl.b32 	%r449, %r62, 2;
	add.s32 	%r450, %r13, %r449;
	ld.shared.u32 	%r451, [%r450];
	// inline asm
	bfe.u32 %r65, %r917, %r11, 8;
	// inline asm
	shl.b32 	%r452, %r65, 2;
	add.s32 	%r453, %r14, %r452;
	ld.shared.u32 	%r454, [%r453];
	// inline asm
	bfe.u32 %r68, %r916, %r430, 8;
	// inline asm
	shl.b32 	%r455, %r68, 2;
	add.s32 	%r456, %r15, %r455;
	ld.shared.u32 	%r457, [%r456];
	// inline asm
	bfe.u32 %r71, %r919, %r12, 8;
	// inline asm
	shl.b32 	%r458, %r71, 2;
	add.s32 	%r459, %r16, %r458;
	xor.b32  	%r460, %r451, %r454;
	xor.b32  	%r461, %r460, %r457;
	ld.shared.u32 	%r462, [%r459];
	xor.b32  	%r463, %r461, %r462;
	xor.b32  	%r102, %r463, 2147214502;
	// inline asm
	bfe.u32 %r74, %r917, %r33, 8;
	// inline asm
	shl.b32 	%r464, %r74, 2;
	add.s32 	%r465, %r13, %r464;
	ld.shared.u32 	%r466, [%r465];
	// inline asm
	bfe.u32 %r77, %r916, %r11, 8;
	// inline asm
	shl.b32 	%r467, %r77, 2;
	add.s32 	%r468, %r14, %r467;
	ld.shared.u32 	%r469, [%r468];
	// inline asm
	bfe.u32 %r80, %r919, %r430, 8;
	// inline asm
	shl.b32 	%r470, %r80, 2;
	add.s32 	%r471, %r15, %r470;
	ld.shared.u32 	%r472, [%r471];
	// inline asm
	bfe.u32 %r83, %r918, %r12, 8;
	// inline asm
	shl.b32 	%r473, %r83, 2;
	add.s32 	%r474, %r16, %r473;
	xor.b32  	%r475, %r466, %r469;
	xor.b32  	%r476, %r475, %r472;
	ld.shared.u32 	%r477, [%r474];
	xor.b32  	%r478, %r476, %r477;
	xor.b32  	%r105, %r478, 337609399;
	// inline asm
	bfe.u32 %r86, %r916, %r33, 8;
	// inline asm
	shl.b32 	%r479, %r86, 2;
	add.s32 	%r480, %r13, %r479;
	ld.shared.u32 	%r481, [%r480];
	// inline asm
	bfe.u32 %r89, %r919, %r11, 8;
	// inline asm
	shl.b32 	%r482, %r89, 2;
	add.s32 	%r483, %r14, %r482;
	ld.shared.u32 	%r484, [%r483];
	// inline asm
	bfe.u32 %r92, %r918, %r430, 8;
	// inline asm
	shl.b32 	%r485, %r92, 2;
	add.s32 	%r486, %r15, %r485;
	ld.shared.u32 	%r487, [%r486];
	// inline asm
	bfe.u32 %r95, %r917, %r12, 8;
	// inline asm
	shl.b32 	%r488, %r95, 2;
	add.s32 	%r489, %r16, %r488;
	xor.b32  	%r490, %r481, %r484;
	xor.b32  	%r491, %r490, %r487;
	ld.shared.u32 	%r492, [%r489];
	xor.b32  	%r493, %r491, %r492;
	xor.b32  	%r108, %r493, -818569579;
	// inline asm
	bfe.u32 %r98, %r99, %r33, 8;
	// inline asm
	shl.b32 	%r494, %r98, 2;
	add.s32 	%r495, %r13, %r494;
	ld.shared.u32 	%r496, [%r495];
	// inline asm
	bfe.u32 %r101, %r102, %r11, 8;
	// inline asm
	shl.b32 	%r497, %r101, 2;
	add.s32 	%r498, %r14, %r497;
	ld.shared.u32 	%r499, [%r498];
	// inline asm
	bfe.u32 %r104, %r105, %r430, 8;
	// inline asm
	shl.b32 	%r500, %r104, 2;
	add.s32 	%r501, %r15, %r500;
	ld.shared.u32 	%r502, [%r501];
	// inline asm
	bfe.u32 %r107, %r108, %r12, 8;
	// inline asm
	shl.b32 	%r503, %r107, 2;
	add.s32 	%r504, %r16, %r503;
	xor.b32  	%r505, %r496, %r499;
	xor.b32  	%r506, %r505, %r502;
	ld.shared.u32 	%r507, [%r504];
	xor.b32  	%r508, %r506, %r507;
	xor.b32  	%r147, %r508, 1784005712;
	// inline asm
	bfe.u32 %r110, %r102, %r33, 8;
	// inline asm
	shl.b32 	%r509, %r110, 2;
	add.s32 	%r510, %r13, %r509;
	ld.shared.u32 	%r511, [%r510];
	// inline asm
	bfe.u32 %r113, %r105, %r11, 8;
	// inline asm
	shl.b32 	%r512, %r113, 2;
	add.s32 	%r513, %r14, %r512;
	ld.shared.u32 	%r514, [%r513];
	// inline asm
	bfe.u32 %r116, %r108, %r430, 8;
	// inline asm
	shl.b32 	%r515, %r116, 2;
	add.s32 	%r516, %r15, %r515;
	ld.shared.u32 	%r517, [%r516];
	// inline asm
	bfe.u32 %r119, %r99, %r12, 8;
	// inline asm
	shl.b32 	%r518, %r119, 2;
	add.s32 	%r519, %r16, %r518;
	xor.b32  	%r520, %r511, %r514;
	xor.b32  	%r521, %r520, %r517;
	ld.shared.u32 	%r522, [%r519];
	xor.b32  	%r523, %r521, %r522;
	xor.b32  	%r150, %r523, -18339958;
	// inline asm
	bfe.u32 %r122, %r105, %r33, 8;
	// inline asm
	shl.b32 	%r524, %r122, 2;
	add.s32 	%r525, %r13, %r524;
	ld.shared.u32 	%r526, [%r525];
	// inline asm
	bfe.u32 %r125, %r108, %r11, 8;
	// inline asm
	shl.b32 	%r527, %r125, 2;
	add.s32 	%r528, %r14, %r527;
	ld.shared.u32 	%r529, [%r528];
	// inline asm
	bfe.u32 %r128, %r99, %r430, 8;
	// inline asm
	shl.b32 	%r530, %r128, 2;
	add.s32 	%r531, %r15, %r530;
	ld.shared.u32 	%r532, [%r531];
	// inline asm
	bfe.u32 %r131, %r102, %r12, 8;
	// inline asm
	shl.b32 	%r533, %r131, 2;
	add.s32 	%r534, %r16, %r533;
	xor.b32  	%r535, %r526, %r529;
	xor.b32  	%r536, %r535, %r532;
	ld.shared.u32 	%r537, [%r534];
	xor.b32  	%r538, %r536, %r537;
	xor.b32  	%r153, %r538, -1118020925;
	// inline asm
	bfe.u32 %r134, %r108, %r33, 8;
	// inline asm
	shl.b32 	%r539, %r134, 2;
	add.s32 	%r540, %r13, %r539;
	ld.shared.u32 	%r541, [%r540];
	// inline asm
	bfe.u32 %r137, %r99, %r11, 8;
	// inline asm
	shl.b32 	%r542, %r137, 2;
	add.s32 	%r543, %r14, %r542;
	ld.shared.u32 	%r544, [%r543];
	// inline asm
	bfe.u32 %r140, %r102, %r430, 8;
	// inline asm
	shl.b32 	%r545, %r140, 2;
	add.s32 	%r546, %r15, %r545;
	ld.shared.u32 	%r547, [%r546];
	// inline asm
	bfe.u32 %r143, %r105, %r12, 8;
	// inline asm
	shl.b32 	%r548, %r143, 2;
	add.s32 	%r549, %r16, %r548;
	xor.b32  	%r550, %r541, %r544;
	xor.b32  	%r551, %r550, %r547;
	ld.shared.u32 	%r552, [%r549];
	xor.b32  	%r553, %r551, %r552;
	xor.b32  	%r156, %r553, 1732378588;
	// inline asm
	bfe.u32 %r146, %r147, %r33, 8;
	// inline asm
	shl.b32 	%r554, %r146, 2;
	add.s32 	%r555, %r13, %r554;
	ld.shared.u32 	%r556, [%r555];
	// inline asm
	bfe.u32 %r149, %r150, %r11, 8;
	// inline asm
	shl.b32 	%r557, %r149, 2;
	add.s32 	%r558, %r14, %r557;
	ld.shared.u32 	%r559, [%r558];
	// inline asm
	bfe.u32 %r152, %r153, %r430, 8;
	// inline asm
	shl.b32 	%r560, %r152, 2;
	add.s32 	%r561, %r15, %r560;
	ld.shared.u32 	%r562, [%r561];
	// inline asm
	bfe.u32 %r155, %r156, %r12, 8;
	// inline asm
	shl.b32 	%r563, %r155, 2;
	add.s32 	%r564, %r16, %r563;
	xor.b32  	%r565, %r556, %r559;
	xor.b32  	%r566, %r565, %r562;
	ld.shared.u32 	%r567, [%r564];
	xor.b32  	%r568, %r566, %r567;
	xor.b32  	%r195, %r568, 290211748;
	// inline asm
	bfe.u32 %r158, %r150, %r33, 8;
	// inline asm
	shl.b32 	%r569, %r158, 2;
	add.s32 	%r570, %r13, %r569;
	ld.shared.u32 	%r571, [%r570];
	// inline asm
	bfe.u32 %r161, %r153, %r11, 8;
	// inline asm
	shl.b32 	%r572, %r161, 2;
	add.s32 	%r573, %r14, %r572;
	ld.shared.u32 	%r574, [%r573];
	// inline asm
	bfe.u32 %r164, %r156, %r430, 8;
	// inline asm
	shl.b32 	%r575, %r164, 2;
	add.s32 	%r576, %r15, %r575;
	ld.shared.u32 	%r577, [%r576];
	// inline asm
	bfe.u32 %r167, %r147, %r12, 8;
	// inline asm
	shl.b32 	%r578, %r167, 2;
	add.s32 	%r579, %r16, %r578;
	xor.b32  	%r580, %r571, %r574;
	xor.b32  	%r581, %r580, %r577;
	ld.shared.u32 	%r582, [%r579];
	xor.b32  	%r583, %r581, %r582;
	xor.b32  	%r198, %r583, -718995996;
	// inline asm
	bfe.u32 %r170, %r153, %r33, 8;
	// inline asm
	shl.b32 	%r584, %r170, 2;
	add.s32 	%r585, %r13, %r584;
	ld.shared.u32 	%r586, [%r585];
	// inline asm
	bfe.u32 %r173, %r156, %r11, 8;
	// inline asm
	shl.b32 	%r587, %r173, 2;
	add.s32 	%r588, %r14, %r587;
	ld.shared.u32 	%r589, [%r588];
	// inline asm
	bfe.u32 %r176, %r147, %r430, 8;
	// inline asm
	shl.b32 	%r590, %r176, 2;
	add.s32 	%r591, %r15, %r590;
	ld.shared.u32 	%r592, [%r591];
	// inline asm
	bfe.u32 %r179, %r150, %r12, 8;
	// inline asm
	shl.b32 	%r593, %r179, 2;
	add.s32 	%r594, %r16, %r593;
	xor.b32  	%r595, %r586, %r589;
	xor.b32  	%r596, %r595, %r592;
	ld.shared.u32 	%r597, [%r594];
	xor.b32  	%r598, %r596, %r597;
	xor.b32  	%r201, %r598, -1490576686;
	// inline asm
	bfe.u32 %r182, %r156, %r33, 8;
	// inline asm
	shl.b32 	%r599, %r182, 2;
	add.s32 	%r600, %r13, %r599;
	ld.shared.u32 	%r601, [%r600];
	// inline asm
	bfe.u32 %r185, %r147, %r11, 8;
	// inline asm
	shl.b32 	%r602, %r185, 2;
	add.s32 	%r603, %r14, %r602;
	ld.shared.u32 	%r604, [%r603];
	// inline asm
	bfe.u32 %r188, %r150, %r430, 8;
	// inline asm
	shl.b32 	%r605, %r188, 2;
	add.s32 	%r606, %r15, %r605;
	ld.shared.u32 	%r607, [%r606];
	// inline asm
	bfe.u32 %r191, %r153, %r12, 8;
	// inline asm
	shl.b32 	%r608, %r191, 2;
	add.s32 	%r609, %r16, %r608;
	xor.b32  	%r610, %r601, %r604;
	xor.b32  	%r611, %r610, %r607;
	ld.shared.u32 	%r612, [%r609];
	xor.b32  	%r613, %r611, %r612;
	xor.b32  	%r204, %r613, 1026706092;
	// inline asm
	bfe.u32 %r194, %r195, %r33, 8;
	// inline asm
	shl.b32 	%r614, %r194, 2;
	add.s32 	%r615, %r13, %r614;
	ld.shared.u32 	%r616, [%r615];
	// inline asm
	bfe.u32 %r197, %r198, %r11, 8;
	// inline asm
	shl.b32 	%r617, %r197, 2;
	add.s32 	%r618, %r14, %r617;
	ld.shared.u32 	%r619, [%r618];
	// inline asm
	bfe.u32 %r200, %r201, %r430, 8;
	// inline asm
	shl.b32 	%r620, %r200, 2;
	add.s32 	%r621, %r15, %r620;
	ld.shared.u32 	%r622, [%r621];
	// inline asm
	bfe.u32 %r203, %r204, %r12, 8;
	// inline asm
	shl.b32 	%r623, %r203, 2;
	add.s32 	%r624, %r16, %r623;
	xor.b32  	%r625, %r616, %r619;
	xor.b32  	%r626, %r625, %r622;
	ld.shared.u32 	%r627, [%r624];
	xor.b32  	%r628, %r626, %r627;
	// inline asm
	bfe.u32 %r206, %r198, %r33, 8;
	// inline asm
	shl.b32 	%r629, %r206, 2;
	add.s32 	%r630, %r13, %r629;
	ld.shared.u32 	%r631, [%r630];
	// inline asm
	bfe.u32 %r209, %r201, %r11, 8;
	// inline asm
	shl.b32 	%r632, %r209, 2;
	add.s32 	%r633, %r14, %r632;
	ld.shared.u32 	%r634, [%r633];
	// inline asm
	bfe.u32 %r212, %r204, %r430, 8;
	// inline asm
	shl.b32 	%r635, %r212, 2;
	add.s32 	%r636, %r15, %r635;
	ld.shared.u32 	%r637, [%r636];
	// inline asm
	bfe.u32 %r215, %r195, %r12, 8;
	// inline asm
	shl.b32 	%r638, %r215, 2;
	add.s32 	%r639, %r16, %r638;
	xor.b32  	%r640, %r631, %r634;
	xor.b32  	%r641, %r640, %r637;
	ld.shared.u32 	%r642, [%r639];
	xor.b32  	%r643, %r641, %r642;
	// inline asm
	bfe.u32 %r218, %r201, %r33, 8;
	// inline asm
	shl.b32 	%r644, %r218, 2;
	add.s32 	%r645, %r13, %r644;
	ld.shared.u32 	%r646, [%r645];
	// inline asm
	bfe.u32 %r221, %r204, %r11, 8;
	// inline asm
	shl.b32 	%r647, %r221, 2;
	add.s32 	%r648, %r14, %r647;
	ld.shared.u32 	%r649, [%r648];
	// inline asm
	bfe.u32 %r224, %r195, %r430, 8;
	// inline asm
	shl.b32 	%r650, %r224, 2;
	add.s32 	%r651, %r15, %r650;
	ld.shared.u32 	%r652, [%r651];
	// inline asm
	bfe.u32 %r227, %r198, %r12, 8;
	// inline asm
	shl.b32 	%r653, %r227, 2;
	add.s32 	%r654, %r16, %r653;
	xor.b32  	%r655, %r646, %r649;
	xor.b32  	%r656, %r655, %r652;
	ld.shared.u32 	%r657, [%r654];
	xor.b32  	%r658, %r656, %r657;
	// inline asm
	bfe.u32 %r230, %r204, %r33, 8;
	// inline asm
	shl.b32 	%r659, %r230, 2;
	add.s32 	%r660, %r13, %r659;
	ld.shared.u32 	%r661, [%r660];
	// inline asm
	bfe.u32 %r233, %r195, %r11, 8;
	// inline asm
	shl.b32 	%r662, %r233, 2;
	add.s32 	%r663, %r14, %r662;
	ld.shared.u32 	%r664, [%r663];
	// inline asm
	bfe.u32 %r236, %r198, %r430, 8;
	// inline asm
	shl.b32 	%r665, %r236, 2;
	add.s32 	%r666, %r15, %r665;
	ld.shared.u32 	%r667, [%r666];
	// inline asm
	bfe.u32 %r239, %r201, %r12, 8;
	// inline asm
	shl.b32 	%r668, %r239, 2;
	add.s32 	%r669, %r16, %r668;
	xor.b32  	%r670, %r661, %r664;
	xor.b32  	%r671, %r670, %r667;
	ld.shared.u32 	%r672, [%r669];
	xor.b32  	%r673, %r671, %r672;
	xor.b32  	%r252, %r673, 1995889416;
	xor.b32  	%r249, %r658, 1121180633;
	xor.b32  	%r246, %r643, -1716932865;
	xor.b32  	%r243, %r628, -2129905110;
	st.global.v4.u32 	[%rd21], {%r243, %r246, %r249, %r252};
	// inline asm
	bfe.u32 %r242, %r243, %r33, 8;
	// inline asm
	shl.b32 	%r674, %r242, 2;
	add.s32 	%r675, %r13, %r674;
	ld.shared.u32 	%r676, [%r675];
	// inline asm
	bfe.u32 %r245, %r246, %r11, 8;
	// inline asm
	shl.b32 	%r677, %r245, 2;
	add.s32 	%r678, %r14, %r677;
	ld.shared.u32 	%r679, [%r678];
	// inline asm
	bfe.u32 %r248, %r249, %r430, 8;
	// inline asm
	shl.b32 	%r680, %r248, 2;
	add.s32 	%r681, %r15, %r680;
	ld.shared.u32 	%r682, [%r681];
	// inline asm
	bfe.u32 %r251, %r252, %r12, 8;
	// inline asm
	shl.b32 	%r683, %r251, 2;
	add.s32 	%r684, %r16, %r683;
	xor.b32  	%r685, %r676, %r679;
	xor.b32  	%r686, %r685, %r682;
	ld.shared.u32 	%r687, [%r684];
	xor.b32  	%r688, %r686, %r687;
	xor.b32  	%r291, %r688, -124762531;
	// inline asm
	bfe.u32 %r254, %r246, %r33, 8;
	// inline asm
	shl.b32 	%r689, %r254, 2;
	add.s32 	%r690, %r13, %r689;
	ld.shared.u32 	%r691, [%r690];
	// inline asm
	bfe.u32 %r257, %r249, %r11, 8;
	// inline asm
	shl.b32 	%r692, %r257, 2;
	add.s32 	%r693, %r14, %r692;
	ld.shared.u32 	%r694, [%r693];
	// inline asm
	bfe.u32 %r260, %r252, %r430, 8;
	// inline asm
	shl.b32 	%r695, %r260, 2;
	add.s32 	%r696, %r15, %r695;
	ld.shared.u32 	%r697, [%r696];
	// inline asm
	bfe.u32 %r263, %r243, %r12, 8;
	// inline asm
	shl.b32 	%r698, %r263, 2;
	add.s32 	%r699, %r16, %r698;
	xor.b32  	%r700, %r691, %r694;
	xor.b32  	%r701, %r700, %r697;
	ld.shared.u32 	%r702, [%r699];
	xor.b32  	%r703, %r701, %r702;
	xor.b32  	%r294, %r703, 2147214502;
	// inline asm
	bfe.u32 %r266, %r249, %r33, 8;
	// inline asm
	shl.b32 	%r704, %r266, 2;
	add.s32 	%r705, %r13, %r704;
	ld.shared.u32 	%r706, [%r705];
	// inline asm
	bfe.u32 %r269, %r252, %r11, 8;
	// inline asm
	shl.b32 	%r707, %r269, 2;
	add.s32 	%r708, %r14, %r707;
	ld.shared.u32 	%r709, [%r708];
	// inline asm
	bfe.u32 %r272, %r243, %r430, 8;
	// inline asm
	shl.b32 	%r710, %r272, 2;
	add.s32 	%r711, %r15, %r710;
	ld.shared.u32 	%r712, [%r711];
	// inline asm
	bfe.u32 %r275, %r246, %r12, 8;
	// inline asm
	shl.b32 	%r713, %r275, 2;
	add.s32 	%r714, %r16, %r713;
	xor.b32  	%r715, %r706, %r709;
	xor.b32  	%r716, %r715, %r712;
	ld.shared.u32 	%r717, [%r714];
	xor.b32  	%r718, %r716, %r717;
	xor.b32  	%r297, %r718, 337609399;
	// inline asm
	bfe.u32 %r278, %r252, %r33, 8;
	// inline asm
	shl.b32 	%r719, %r278, 2;
	add.s32 	%r720, %r13, %r719;
	ld.shared.u32 	%r721, [%r720];
	// inline asm
	bfe.u32 %r281, %r243, %r11, 8;
	// inline asm
	shl.b32 	%r722, %r281, 2;
	add.s32 	%r723, %r14, %r722;
	ld.shared.u32 	%r724, [%r723];
	// inline asm
	bfe.u32 %r284, %r246, %r430, 8;
	// inline asm
	shl.b32 	%r725, %r284, 2;
	add.s32 	%r726, %r15, %r725;
	ld.shared.u32 	%r727, [%r726];
	// inline asm
	bfe.u32 %r287, %r249, %r12, 8;
	// inline asm
	shl.b32 	%r728, %r287, 2;
	add.s32 	%r729, %r16, %r728;
	xor.b32  	%r730, %r721, %r724;
	xor.b32  	%r731, %r730, %r727;
	ld.shared.u32 	%r732, [%r729];
	xor.b32  	%r733, %r731, %r732;
	xor.b32  	%r300, %r733, -818569579;
	// inline asm
	bfe.u32 %r290, %r291, %r33, 8;
	// inline asm
	shl.b32 	%r734, %r290, 2;
	add.s32 	%r735, %r13, %r734;
	ld.shared.u32 	%r736, [%r735];
	// inline asm
	bfe.u32 %r293, %r294, %r11, 8;
	// inline asm
	shl.b32 	%r737, %r293, 2;
	add.s32 	%r738, %r14, %r737;
	ld.shared.u32 	%r739, [%r738];
	// inline asm
	bfe.u32 %r296, %r297, %r430, 8;
	// inline asm
	shl.b32 	%r740, %r296, 2;
	add.s32 	%r741, %r15, %r740;
	ld.shared.u32 	%r742, [%r741];
	// inline asm
	bfe.u32 %r299, %r300, %r12, 8;
	// inline asm
	shl.b32 	%r743, %r299, 2;
	add.s32 	%r744, %r16, %r743;
	xor.b32  	%r745, %r736, %r739;
	xor.b32  	%r746, %r745, %r742;
	ld.shared.u32 	%r747, [%r744];
	xor.b32  	%r748, %r746, %r747;
	xor.b32  	%r339, %r748, 1784005712;
	// inline asm
	bfe.u32 %r302, %r294, %r33, 8;
	// inline asm
	shl.b32 	%r749, %r302, 2;
	add.s32 	%r750, %r13, %r749;
	ld.shared.u32 	%r751, [%r750];
	// inline asm
	bfe.u32 %r305, %r297, %r11, 8;
	// inline asm
	shl.b32 	%r752, %r305, 2;
	add.s32 	%r753, %r14, %r752;
	ld.shared.u32 	%r754, [%r753];
	// inline asm
	bfe.u32 %r308, %r300, %r430, 8;
	// inline asm
	shl.b32 	%r755, %r308, 2;
	add.s32 	%r756, %r15, %r755;
	ld.shared.u32 	%r757, [%r756];
	// inline asm
	bfe.u32 %r311, %r291, %r12, 8;
	// inline asm
	shl.b32 	%r758, %r311, 2;
	add.s32 	%r759, %r16, %r758;
	xor.b32  	%r760, %r751, %r754;
	xor.b32  	%r761, %r760, %r757;
	ld.shared.u32 	%r762, [%r759];
	xor.b32  	%r763, %r761, %r762;
	xor.b32  	%r342, %r763, -18339958;
	// inline asm
	bfe.u32 %r314, %r297, %r33, 8;
	// inline asm
	shl.b32 	%r764, %r314, 2;
	add.s32 	%r765, %r13, %r764;
	ld.shared.u32 	%r766, [%r765];
	// inline asm
	bfe.u32 %r317, %r300, %r11, 8;
	// inline asm
	shl.b32 	%r767, %r317, 2;
	add.s32 	%r768, %r14, %r767;
	ld.shared.u32 	%r769, [%r768];
	// inline asm
	bfe.u32 %r320, %r291, %r430, 8;
	// inline asm
	shl.b32 	%r770, %r320, 2;
	add.s32 	%r771, %r15, %r770;
	ld.shared.u32 	%r772, [%r771];
	// inline asm
	bfe.u32 %r323, %r294, %r12, 8;
	// inline asm
	shl.b32 	%r773, %r323, 2;
	add.s32 	%r774, %r16, %r773;
	xor.b32  	%r775, %r766, %r769;
	xor.b32  	%r776, %r775, %r772;
	ld.shared.u32 	%r777, [%r774];
	xor.b32  	%r778, %r776, %r777;
	xor.b32  	%r345, %r778, -1118020925;
	// inline asm
	bfe.u32 %r326, %r300, %r33, 8;
	// inline asm
	shl.b32 	%r779, %r326, 2;
	add.s32 	%r780, %r13, %r779;
	ld.shared.u32 	%r781, [%r780];
	// inline asm
	bfe.u32 %r329, %r291, %r11, 8;
	// inline asm
	shl.b32 	%r782, %r329, 2;
	add.s32 	%r783, %r14, %r782;
	ld.shared.u32 	%r784, [%r783];
	// inline asm
	bfe.u32 %r332, %r294, %r430, 8;
	// inline asm
	shl.b32 	%r785, %r332, 2;
	add.s32 	%r786, %r15, %r785;
	ld.shared.u32 	%r787, [%r786];
	// inline asm
	bfe.u32 %r335, %r297, %r12, 8;
	// inline asm
	shl.b32 	%r788, %r335, 2;
	add.s32 	%r789, %r16, %r788;
	xor.b32  	%r790, %r781, %r784;
	xor.b32  	%r791, %r790, %r787;
	ld.shared.u32 	%r792, [%r789];
	xor.b32  	%r793, %r791, %r792;
	xor.b32  	%r348, %r793, 1732378588;
	// inline asm
	bfe.u32 %r338, %r339, %r33, 8;
	// inline asm
	shl.b32 	%r794, %r338, 2;
	add.s32 	%r795, %r13, %r794;
	ld.shared.u32 	%r796, [%r795];
	// inline asm
	bfe.u32 %r341, %r342, %r11, 8;
	// inline asm
	shl.b32 	%r797, %r341, 2;
	add.s32 	%r798, %r14, %r797;
	ld.shared.u32 	%r799, [%r798];
	// inline asm
	bfe.u32 %r344, %r345, %r430, 8;
	// inline asm
	shl.b32 	%r800, %r344, 2;
	add.s32 	%r801, %r15, %r800;
	ld.shared.u32 	%r802, [%r801];
	// inline asm
	bfe.u32 %r347, %r348, %r12, 8;
	// inline asm
	shl.b32 	%r803, %r347, 2;
	add.s32 	%r804, %r16, %r803;
	xor.b32  	%r805, %r796, %r799;
	xor.b32  	%r806, %r805, %r802;
	ld.shared.u32 	%r807, [%r804];
	xor.b32  	%r808, %r806, %r807;
	xor.b32  	%r387, %r808, 290211748;
	// inline asm
	bfe.u32 %r350, %r342, %r33, 8;
	// inline asm
	shl.b32 	%r809, %r350, 2;
	add.s32 	%r810, %r13, %r809;
	ld.shared.u32 	%r811, [%r810];
	// inline asm
	bfe.u32 %r353, %r345, %r11, 8;
	// inline asm
	shl.b32 	%r812, %r353, 2;
	add.s32 	%r813, %r14, %r812;
	ld.shared.u32 	%r814, [%r813];
	// inline asm
	bfe.u32 %r356, %r348, %r430, 8;
	// inline asm
	shl.b32 	%r815, %r356, 2;
	add.s32 	%r816, %r15, %r815;
	ld.shared.u32 	%r817, [%r816];
	// inline asm
	bfe.u32 %r359, %r339, %r12, 8;
	// inline asm
	shl.b32 	%r818, %r359, 2;
	add.s32 	%r819, %r16, %r818;
	xor.b32  	%r820, %r811, %r814;
	xor.b32  	%r821, %r820, %r817;
	ld.shared.u32 	%r822, [%r819];
	xor.b32  	%r823, %r821, %r822;
	xor.b32  	%r390, %r823, -718995996;
	// inline asm
	bfe.u32 %r362, %r345, %r33, 8;
	// inline asm
	shl.b32 	%r824, %r362, 2;
	add.s32 	%r825, %r13, %r824;
	ld.shared.u32 	%r826, [%r825];
	// inline asm
	bfe.u32 %r365, %r348, %r11, 8;
	// inline asm
	shl.b32 	%r827, %r365, 2;
	add.s32 	%r828, %r14, %r827;
	ld.shared.u32 	%r829, [%r828];
	// inline asm
	bfe.u32 %r368, %r339, %r430, 8;
	// inline asm
	shl.b32 	%r830, %r368, 2;
	add.s32 	%r831, %r15, %r830;
	ld.shared.u32 	%r832, [%r831];
	// inline asm
	bfe.u32 %r371, %r342, %r12, 8;
	// inline asm
	shl.b32 	%r833, %r371, 2;
	add.s32 	%r834, %r16, %r833;
	xor.b32  	%r835, %r826, %r829;
	xor.b32  	%r836, %r835, %r832;
	ld.shared.u32 	%r837, [%r834];
	xor.b32  	%r838, %r836, %r837;
	xor.b32  	%r393, %r838, -1490576686;
	// inline asm
	bfe.u32 %r374, %r348, %r33, 8;
	// inline asm
	shl.b32 	%r839, %r374, 2;
	add.s32 	%r840, %r13, %r839;
	ld.shared.u32 	%r841, [%r840];
	// inline asm
	bfe.u32 %r377, %r339, %r11, 8;
	// inline asm
	shl.b32 	%r842, %r377, 2;
	add.s32 	%r843, %r14, %r842;
	ld.shared.u32 	%r844, [%r843];
	// inline asm
	bfe.u32 %r380, %r342, %r430, 8;
	// inline asm
	shl.b32 	%r845, %r380, 2;
	add.s32 	%r846, %r15, %r845;
	ld.shared.u32 	%r847, [%r846];
	// inline asm
	bfe.u32 %r383, %r345, %r12, 8;
	// inline asm
	shl.b32 	%r848, %r383, 2;
	add.s32 	%r849, %r16, %r848;
	xor.b32  	%r850, %r841, %r844;
	xor.b32  	%r851, %r850, %r847;
	ld.shared.u32 	%r852, [%r849];
	xor.b32  	%r853, %r851, %r852;
	xor.b32  	%r396, %r853, 1026706092;
	// inline asm
	bfe.u32 %r386, %r387, %r33, 8;
	// inline asm
	shl.b32 	%r854, %r386, 2;
	add.s32 	%r855, %r13, %r854;
	ld.shared.u32 	%r856, [%r855];
	// inline asm
	bfe.u32 %r389, %r390, %r11, 8;
	// inline asm
	shl.b32 	%r857, %r389, 2;
	add.s32 	%r858, %r14, %r857;
	ld.shared.u32 	%r859, [%r858];
	// inline asm
	bfe.u32 %r392, %r393, %r430, 8;
	// inline asm
	shl.b32 	%r860, %r392, 2;
	add.s32 	%r861, %r15, %r860;
	ld.shared.u32 	%r862, [%r861];
	// inline asm
	bfe.u32 %r395, %r396, %r12, 8;
	// inline asm
	shl.b32 	%r863, %r395, 2;
	add.s32 	%r864, %r16, %r863;
	xor.b32  	%r865, %r856, %r859;
	xor.b32  	%r866, %r865, %r862;
	ld.shared.u32 	%r867, [%r864];
	xor.b32  	%r868, %r866, %r867;
	xor.b32  	%r919, %r868, -2129905110;
	// inline asm
	bfe.u32 %r398, %r390, %r33, 8;
	// inline asm
	shl.b32 	%r869, %r398, 2;
	add.s32 	%r870, %r13, %r869;
	ld.shared.u32 	%r871, [%r870];
	// inline asm
	bfe.u32 %r401, %r393, %r11, 8;
	// inline asm
	shl.b32 	%r872, %r401, 2;
	add.s32 	%r873, %r14, %r872;
	ld.shared.u32 	%r874, [%r873];
	// inline asm
	bfe.u32 %r404, %r396, %r430, 8;
	// inline asm
	shl.b32 	%r875, %r404, 2;
	add.s32 	%r876, %r15, %r875;
	ld.shared.u32 	%r877, [%r876];
	// inline asm
	bfe.u32 %r407, %r387, %r12, 8;
	// inline asm
	shl.b32 	%r878, %r407, 2;
	add.s32 	%r879, %r16, %r878;
	xor.b32  	%r880, %r871, %r874;
	xor.b32  	%r881, %r880, %r877;
	ld.shared.u32 	%r882, [%r879];
	xor.b32  	%r883, %r881, %r882;
	xor.b32  	%r918, %r883, -1716932865;
	// inline asm
	bfe.u32 %r410, %r393, %r33, 8;
	// inline asm
	shl.b32 	%r884, %r410, 2;
	add.s32 	%r885, %r13, %r884;
	ld.shared.u32 	%r886, [%r885];
	// inline asm
	bfe.u32 %r413, %r396, %r11, 8;
	// inline asm
	shl.b32 	%r887, %r413, 2;
	add.s32 	%r888, %r14, %r887;
	ld.shared.u32 	%r889, [%r888];
	// inline asm
	bfe.u32 %r416, %r387, %r430, 8;
	// inline asm
	shl.b32 	%r890, %r416, 2;
	add.s32 	%r891, %r15, %r890;
	ld.shared.u32 	%r892, [%r891];
	// inline asm
	bfe.u32 %r419, %r390, %r12, 8;
	// inline asm
	shl.b32 	%r893, %r419, 2;
	add.s32 	%r894, %r16, %r893;
	xor.b32  	%r895, %r886, %r889;
	xor.b32  	%r896, %r895, %r892;
	ld.shared.u32 	%r897, [%r894];
	xor.b32  	%r898, %r896, %r897;
	xor.b32  	%r917, %r898, 1121180633;
	// inline asm
	bfe.u32 %r422, %r396, %r33, 8;
	// inline asm
	shl.b32 	%r899, %r422, 2;
	add.s32 	%r900, %r13, %r899;
	ld.shared.u32 	%r901, [%r900];
	// inline asm
	bfe.u32 %r425, %r387, %r11, 8;
	// inline asm
	shl.b32 	%r902, %r425, 2;
	add.s32 	%r903, %r14, %r902;
	ld.shared.u32 	%r904, [%r903];
	// inline asm
	bfe.u32 %r428, %r390, %r430, 8;
	// inline asm
	shl.b32 	%r905, %r428, 2;
	add.s32 	%r906, %r15, %r905;
	ld.shared.u32 	%r907, [%r906];
	// inline asm
	bfe.u32 %r431, %r393, %r12, 8;
	// inline asm
	shl.b32 	%r908, %r431, 2;
	add.s32 	%r909, %r16, %r908;
	xor.b32  	%r910, %r901, %r904;
	xor.b32  	%r911, %r910, %r907;
	ld.shared.u32 	%r912, [%r909];
	xor.b32  	%r913, %r911, %r912;
	xor.b32  	%r916, %r913, 1995889416;
	st.global.v4.u32 	[%rd21+64], {%r919, %r918, %r917, %r916};
	add.s32 	%r915, %r915, 8;
	setp.lt.u32	%p5, %r915, 136;
	add.s64 	%rd21, %rd21, 128;
	@%p5 bra 	BB14_4;

	st.global.v2.u32 	[%rd1], {%r919, %r918};
	st.global.v2.u32 	[%rd1+8], {%r917, %r916};

BB14_6:
	ret;
}

	// .globl	_Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj
.visible .entry _Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj(
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj_param_0,
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj_param_1,
	.param .u32 _Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<1125>;
	.reg .b64 	%rd<26>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvjE1T[8192];

	ld.param.u64 	%rd6, [_Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj_param_0];
	ld.param.u64 	%rd7, [_Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj_param_1];
	ld.param.u32 	%r44, [_Z11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvj_param_2];
	shl.b32 	%r45, %r44, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r1119, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r1119;
	setp.ge.u32	%p1, %r4, %r45;
	@%p1 bra 	BB15_6;

	setp.gt.s32	%p2, %r1119, 2047;
	@%p2 bra 	BB15_3;

BB15_2:
	mul.wide.s32 	%rd8, %r1119, 4;
	mov.u64 	%rd9, AES_TABLE;
	add.s64 	%rd10, %rd9, %rd8;
	ld.const.u32 	%r46, [%rd10];
	shl.b32 	%r47, %r1119, 2;
	mov.u32 	%r48, _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvjE1T;
	add.s32 	%r49, %r48, %r47;
	st.shared.u32 	[%r49], %r46;
	add.s32 	%r1119, %r1119, %r1;
	setp.lt.s32	%p3, %r1119, 2048;
	@%p3 bra 	BB15_2;

BB15_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r51, %r7, 2;
	mul.wide.u32 	%rd11, %r51, 4;
	mov.u64 	%rd12, AES_STATE_HASH;
	add.s64 	%rd13, %rd12, %rd11;
	ld.const.u32 	%r1120, [%rd13];
	ld.const.u32 	%r1121, [%rd13+4];
	ld.const.u32 	%r1122, [%rd13+8];
	ld.const.u32 	%r1123, [%rd13+12];
	and.b32  	%r52, %r4, 1;
	shl.b32 	%r53, %r4, 4;
	and.b32  	%r54, %r53, 16;
	add.s32 	%r12, %r54, 8;
	xor.b32  	%r55, %r54, 16;
	add.s32 	%r13, %r55, 8;
	shr.u32 	%r56, %r4, 2;
	cvt.u64.u32	%rd1, %r56;
	cvt.u64.u32	%rd2, %r7;
	setp.eq.s32	%p4, %r52, 0;
	mov.u32 	%r50, 0;
	mov.u32 	%r57, _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048ELy64EEvPKvPvjE1T;
	add.s32 	%r58, %r57, 4096;
	selp.b32	%r14, %r57, %r58, %p4;
	add.s32 	%r59, %r57, 7168;
	add.s32 	%r60, %r57, 1024;
	selp.b32	%r15, %r60, %r59, %p4;
	add.s32 	%r61, %r57, 6144;
	add.s32 	%r62, %r57, 2048;
	selp.b32	%r16, %r62, %r61, %p4;
	add.s32 	%r63, %r57, 5120;
	add.s32 	%r64, %r57, 3072;
	selp.b32	%r17, %r64, %r63, %p4;
	mul.wide.u32 	%rd14, %r56, 131076;
	cvt.u64.u32	%rd15, %r4;
	and.b64  	%rd16, %rd15, 3;
	or.b64  	%rd17, %rd14, %rd16;
	cvta.to.global.u64 	%rd18, %rd6;
	shl.b64 	%rd19, %rd17, 4;
	add.s64 	%rd25, %rd18, %rd19;
	mov.u32 	%r1124, %r50;
	bra.uni 	BB15_4;

BB15_7:
	ld.global.v4.u32 	{%r607, %r608, %r609, %r610}, [%rd25];
	mov.u32 	%r597, 0;
	ld.shared.u32 	%r615, [%r23];
	xor.b32  	%r616, %r607, %r615;
	ld.shared.u32 	%r617, [%r24];
	xor.b32  	%r618, %r616, %r617;
	ld.shared.u32 	%r619, [%r25];
	xor.b32  	%r620, %r618, %r619;
	ld.shared.u32 	%r621, [%r26];
	xor.b32  	%r311, %r620, %r621;
	ld.shared.u32 	%r622, [%r27];
	xor.b32  	%r623, %r622, %r608;
	ld.shared.u32 	%r624, [%r28];
	xor.b32  	%r625, %r623, %r624;
	ld.shared.u32 	%r626, [%r29];
	xor.b32  	%r627, %r625, %r626;
	ld.shared.u32 	%r628, [%r30];
	xor.b32  	%r314, %r627, %r628;
	ld.shared.u32 	%r629, [%r31];
	xor.b32  	%r630, %r629, %r609;
	ld.shared.u32 	%r631, [%r32];
	xor.b32  	%r632, %r630, %r631;
	ld.shared.u32 	%r633, [%r33];
	xor.b32  	%r634, %r632, %r633;
	ld.shared.u32 	%r635, [%r34];
	xor.b32  	%r317, %r634, %r635;
	ld.shared.u32 	%r636, [%r35];
	xor.b32  	%r637, %r636, %r610;
	ld.shared.u32 	%r638, [%r36];
	xor.b32  	%r639, %r637, %r638;
	ld.shared.u32 	%r640, [%r37];
	xor.b32  	%r641, %r639, %r640;
	ld.shared.u32 	%r642, [%r38];
	xor.b32  	%r308, %r641, %r642;
	ld.global.v4.u32 	{%r643, %r644, %r645, %r646}, [%rd25+64];
	// inline asm
	bfe.u32 %r271, %r311, %r597, 8;
	// inline asm
	shl.b32 	%r651, %r271, 2;
	add.s32 	%r652, %r14, %r651;
	ld.shared.u32 	%r653, [%r652];
	// inline asm
	bfe.u32 %r274, %r314, %r12, 8;
	// inline asm
	shl.b32 	%r654, %r274, 2;
	add.s32 	%r655, %r15, %r654;
	ld.shared.u32 	%r656, [%r655];
	// inline asm
	bfe.u32 %r277, %r317, %r111, 8;
	// inline asm
	shl.b32 	%r657, %r277, 2;
	add.s32 	%r658, %r16, %r657;
	ld.shared.u32 	%r659, [%r658];
	// inline asm
	bfe.u32 %r280, %r308, %r13, 8;
	// inline asm
	shl.b32 	%r660, %r280, 2;
	add.s32 	%r661, %r17, %r660;
	xor.b32  	%r662, %r653, %r643;
	xor.b32  	%r663, %r662, %r656;
	xor.b32  	%r664, %r663, %r659;
	ld.shared.u32 	%r665, [%r661];
	xor.b32  	%r359, %r664, %r665;
	// inline asm
	bfe.u32 %r283, %r314, %r597, 8;
	// inline asm
	shl.b32 	%r666, %r283, 2;
	add.s32 	%r667, %r14, %r666;
	ld.shared.u32 	%r668, [%r667];
	// inline asm
	bfe.u32 %r286, %r317, %r12, 8;
	// inline asm
	shl.b32 	%r669, %r286, 2;
	add.s32 	%r670, %r15, %r669;
	ld.shared.u32 	%r671, [%r670];
	// inline asm
	bfe.u32 %r289, %r308, %r111, 8;
	// inline asm
	shl.b32 	%r672, %r289, 2;
	add.s32 	%r673, %r16, %r672;
	ld.shared.u32 	%r674, [%r673];
	// inline asm
	bfe.u32 %r292, %r311, %r13, 8;
	// inline asm
	shl.b32 	%r675, %r292, 2;
	add.s32 	%r676, %r17, %r675;
	xor.b32  	%r677, %r668, %r644;
	xor.b32  	%r678, %r677, %r671;
	xor.b32  	%r679, %r678, %r674;
	ld.shared.u32 	%r680, [%r676];
	xor.b32  	%r362, %r679, %r680;
	// inline asm
	bfe.u32 %r295, %r317, %r597, 8;
	// inline asm
	shl.b32 	%r681, %r295, 2;
	add.s32 	%r682, %r14, %r681;
	ld.shared.u32 	%r683, [%r682];
	// inline asm
	bfe.u32 %r298, %r308, %r12, 8;
	// inline asm
	shl.b32 	%r684, %r298, 2;
	add.s32 	%r685, %r15, %r684;
	ld.shared.u32 	%r686, [%r685];
	// inline asm
	bfe.u32 %r301, %r311, %r111, 8;
	// inline asm
	shl.b32 	%r687, %r301, 2;
	add.s32 	%r688, %r16, %r687;
	ld.shared.u32 	%r689, [%r688];
	// inline asm
	bfe.u32 %r304, %r314, %r13, 8;
	// inline asm
	shl.b32 	%r690, %r304, 2;
	add.s32 	%r691, %r17, %r690;
	xor.b32  	%r692, %r683, %r645;
	xor.b32  	%r693, %r692, %r686;
	xor.b32  	%r694, %r693, %r689;
	ld.shared.u32 	%r695, [%r691];
	xor.b32  	%r365, %r694, %r695;
	// inline asm
	bfe.u32 %r307, %r308, %r597, 8;
	// inline asm
	shl.b32 	%r696, %r307, 2;
	add.s32 	%r697, %r14, %r696;
	ld.shared.u32 	%r698, [%r697];
	// inline asm
	bfe.u32 %r310, %r311, %r12, 8;
	// inline asm
	shl.b32 	%r699, %r310, 2;
	add.s32 	%r700, %r15, %r699;
	ld.shared.u32 	%r701, [%r700];
	// inline asm
	bfe.u32 %r313, %r314, %r111, 8;
	// inline asm
	shl.b32 	%r702, %r313, 2;
	add.s32 	%r703, %r16, %r702;
	ld.shared.u32 	%r704, [%r703];
	// inline asm
	bfe.u32 %r316, %r317, %r13, 8;
	// inline asm
	shl.b32 	%r705, %r316, 2;
	add.s32 	%r706, %r17, %r705;
	xor.b32  	%r707, %r698, %r646;
	xor.b32  	%r708, %r707, %r701;
	xor.b32  	%r709, %r708, %r704;
	ld.shared.u32 	%r710, [%r706];
	xor.b32  	%r356, %r709, %r710;
	ld.global.v4.u32 	{%r711, %r712, %r713, %r714}, [%rd25+128];
	// inline asm
	bfe.u32 %r319, %r359, %r597, 8;
	// inline asm
	shl.b32 	%r719, %r319, 2;
	add.s32 	%r720, %r14, %r719;
	ld.shared.u32 	%r721, [%r720];
	// inline asm
	bfe.u32 %r322, %r362, %r12, 8;
	// inline asm
	shl.b32 	%r722, %r322, 2;
	add.s32 	%r723, %r15, %r722;
	ld.shared.u32 	%r724, [%r723];
	// inline asm
	bfe.u32 %r325, %r365, %r111, 8;
	// inline asm
	shl.b32 	%r725, %r325, 2;
	add.s32 	%r726, %r16, %r725;
	ld.shared.u32 	%r727, [%r726];
	// inline asm
	bfe.u32 %r328, %r356, %r13, 8;
	// inline asm
	shl.b32 	%r728, %r328, 2;
	add.s32 	%r729, %r17, %r728;
	xor.b32  	%r730, %r721, %r711;
	xor.b32  	%r731, %r730, %r724;
	xor.b32  	%r732, %r731, %r727;
	ld.shared.u32 	%r733, [%r729];
	xor.b32  	%r407, %r732, %r733;
	// inline asm
	bfe.u32 %r331, %r362, %r597, 8;
	// inline asm
	shl.b32 	%r734, %r331, 2;
	add.s32 	%r735, %r14, %r734;
	ld.shared.u32 	%r736, [%r735];
	// inline asm
	bfe.u32 %r334, %r365, %r12, 8;
	// inline asm
	shl.b32 	%r737, %r334, 2;
	add.s32 	%r738, %r15, %r737;
	ld.shared.u32 	%r739, [%r738];
	// inline asm
	bfe.u32 %r337, %r356, %r111, 8;
	// inline asm
	shl.b32 	%r740, %r337, 2;
	add.s32 	%r741, %r16, %r740;
	ld.shared.u32 	%r742, [%r741];
	// inline asm
	bfe.u32 %r340, %r359, %r13, 8;
	// inline asm
	shl.b32 	%r743, %r340, 2;
	add.s32 	%r744, %r17, %r743;
	xor.b32  	%r745, %r736, %r712;
	xor.b32  	%r746, %r745, %r739;
	xor.b32  	%r747, %r746, %r742;
	ld.shared.u32 	%r748, [%r744];
	xor.b32  	%r410, %r747, %r748;
	// inline asm
	bfe.u32 %r343, %r365, %r597, 8;
	// inline asm
	shl.b32 	%r749, %r343, 2;
	add.s32 	%r750, %r14, %r749;
	ld.shared.u32 	%r751, [%r750];
	// inline asm
	bfe.u32 %r346, %r356, %r12, 8;
	// inline asm
	shl.b32 	%r752, %r346, 2;
	add.s32 	%r753, %r15, %r752;
	ld.shared.u32 	%r754, [%r753];
	// inline asm
	bfe.u32 %r349, %r359, %r111, 8;
	// inline asm
	shl.b32 	%r755, %r349, 2;
	add.s32 	%r756, %r16, %r755;
	ld.shared.u32 	%r757, [%r756];
	// inline asm
	bfe.u32 %r352, %r362, %r13, 8;
	// inline asm
	shl.b32 	%r758, %r352, 2;
	add.s32 	%r759, %r17, %r758;
	xor.b32  	%r760, %r751, %r713;
	xor.b32  	%r761, %r760, %r754;
	xor.b32  	%r762, %r761, %r757;
	ld.shared.u32 	%r763, [%r759];
	xor.b32  	%r413, %r762, %r763;
	// inline asm
	bfe.u32 %r355, %r356, %r597, 8;
	// inline asm
	shl.b32 	%r764, %r355, 2;
	add.s32 	%r765, %r14, %r764;
	ld.shared.u32 	%r766, [%r765];
	// inline asm
	bfe.u32 %r358, %r359, %r12, 8;
	// inline asm
	shl.b32 	%r767, %r358, 2;
	add.s32 	%r768, %r15, %r767;
	ld.shared.u32 	%r769, [%r768];
	// inline asm
	bfe.u32 %r361, %r362, %r111, 8;
	// inline asm
	shl.b32 	%r770, %r361, 2;
	add.s32 	%r771, %r16, %r770;
	ld.shared.u32 	%r772, [%r771];
	// inline asm
	bfe.u32 %r364, %r365, %r13, 8;
	// inline asm
	shl.b32 	%r773, %r364, 2;
	add.s32 	%r774, %r17, %r773;
	xor.b32  	%r775, %r766, %r714;
	xor.b32  	%r776, %r775, %r769;
	xor.b32  	%r777, %r776, %r772;
	ld.shared.u32 	%r778, [%r774];
	xor.b32  	%r404, %r777, %r778;
	ld.global.v4.u32 	{%r779, %r780, %r781, %r782}, [%rd25+192];
	// inline asm
	bfe.u32 %r367, %r407, %r597, 8;
	// inline asm
	shl.b32 	%r787, %r367, 2;
	add.s32 	%r788, %r14, %r787;
	ld.shared.u32 	%r789, [%r788];
	// inline asm
	bfe.u32 %r370, %r410, %r12, 8;
	// inline asm
	shl.b32 	%r790, %r370, 2;
	add.s32 	%r791, %r15, %r790;
	ld.shared.u32 	%r792, [%r791];
	// inline asm
	bfe.u32 %r373, %r413, %r111, 8;
	// inline asm
	shl.b32 	%r793, %r373, 2;
	add.s32 	%r794, %r16, %r793;
	ld.shared.u32 	%r795, [%r794];
	// inline asm
	bfe.u32 %r376, %r404, %r13, 8;
	// inline asm
	shl.b32 	%r796, %r376, 2;
	add.s32 	%r797, %r17, %r796;
	xor.b32  	%r798, %r789, %r779;
	xor.b32  	%r799, %r798, %r792;
	xor.b32  	%r800, %r799, %r795;
	ld.shared.u32 	%r801, [%r797];
	xor.b32  	%r455, %r800, %r801;
	// inline asm
	bfe.u32 %r379, %r410, %r597, 8;
	// inline asm
	shl.b32 	%r802, %r379, 2;
	add.s32 	%r803, %r14, %r802;
	ld.shared.u32 	%r804, [%r803];
	// inline asm
	bfe.u32 %r382, %r413, %r12, 8;
	// inline asm
	shl.b32 	%r805, %r382, 2;
	add.s32 	%r806, %r15, %r805;
	ld.shared.u32 	%r807, [%r806];
	// inline asm
	bfe.u32 %r385, %r404, %r111, 8;
	// inline asm
	shl.b32 	%r808, %r385, 2;
	add.s32 	%r809, %r16, %r808;
	ld.shared.u32 	%r810, [%r809];
	// inline asm
	bfe.u32 %r388, %r407, %r13, 8;
	// inline asm
	shl.b32 	%r811, %r388, 2;
	add.s32 	%r812, %r17, %r811;
	xor.b32  	%r813, %r804, %r780;
	xor.b32  	%r814, %r813, %r807;
	xor.b32  	%r815, %r814, %r810;
	ld.shared.u32 	%r816, [%r812];
	xor.b32  	%r458, %r815, %r816;
	// inline asm
	bfe.u32 %r391, %r413, %r597, 8;
	// inline asm
	shl.b32 	%r817, %r391, 2;
	add.s32 	%r818, %r14, %r817;
	ld.shared.u32 	%r819, [%r818];
	// inline asm
	bfe.u32 %r394, %r404, %r12, 8;
	// inline asm
	shl.b32 	%r820, %r394, 2;
	add.s32 	%r821, %r15, %r820;
	ld.shared.u32 	%r822, [%r821];
	// inline asm
	bfe.u32 %r397, %r407, %r111, 8;
	// inline asm
	shl.b32 	%r823, %r397, 2;
	add.s32 	%r824, %r16, %r823;
	ld.shared.u32 	%r825, [%r824];
	// inline asm
	bfe.u32 %r400, %r410, %r13, 8;
	// inline asm
	shl.b32 	%r826, %r400, 2;
	add.s32 	%r827, %r17, %r826;
	xor.b32  	%r828, %r819, %r781;
	xor.b32  	%r829, %r828, %r822;
	xor.b32  	%r830, %r829, %r825;
	ld.shared.u32 	%r831, [%r827];
	xor.b32  	%r461, %r830, %r831;
	// inline asm
	bfe.u32 %r403, %r404, %r597, 8;
	// inline asm
	shl.b32 	%r832, %r403, 2;
	add.s32 	%r833, %r14, %r832;
	ld.shared.u32 	%r834, [%r833];
	// inline asm
	bfe.u32 %r406, %r407, %r12, 8;
	// inline asm
	shl.b32 	%r835, %r406, 2;
	add.s32 	%r836, %r15, %r835;
	ld.shared.u32 	%r837, [%r836];
	// inline asm
	bfe.u32 %r409, %r410, %r111, 8;
	// inline asm
	shl.b32 	%r838, %r409, 2;
	add.s32 	%r839, %r16, %r838;
	ld.shared.u32 	%r840, [%r839];
	// inline asm
	bfe.u32 %r412, %r413, %r13, 8;
	// inline asm
	shl.b32 	%r841, %r412, 2;
	add.s32 	%r842, %r17, %r841;
	xor.b32  	%r843, %r834, %r782;
	xor.b32  	%r844, %r843, %r837;
	xor.b32  	%r845, %r844, %r840;
	ld.shared.u32 	%r846, [%r842];
	xor.b32  	%r452, %r845, %r846;
	ld.global.v4.u32 	{%r847, %r848, %r849, %r850}, [%rd25+256];
	// inline asm
	bfe.u32 %r415, %r455, %r597, 8;
	// inline asm
	shl.b32 	%r855, %r415, 2;
	add.s32 	%r856, %r14, %r855;
	ld.shared.u32 	%r857, [%r856];
	// inline asm
	bfe.u32 %r418, %r458, %r12, 8;
	// inline asm
	shl.b32 	%r858, %r418, 2;
	add.s32 	%r859, %r15, %r858;
	ld.shared.u32 	%r860, [%r859];
	// inline asm
	bfe.u32 %r421, %r461, %r111, 8;
	// inline asm
	shl.b32 	%r861, %r421, 2;
	add.s32 	%r862, %r16, %r861;
	ld.shared.u32 	%r863, [%r862];
	// inline asm
	bfe.u32 %r424, %r452, %r13, 8;
	// inline asm
	shl.b32 	%r864, %r424, 2;
	add.s32 	%r865, %r17, %r864;
	xor.b32  	%r866, %r857, %r847;
	xor.b32  	%r867, %r866, %r860;
	xor.b32  	%r868, %r867, %r863;
	ld.shared.u32 	%r869, [%r865];
	xor.b32  	%r503, %r868, %r869;
	// inline asm
	bfe.u32 %r427, %r458, %r597, 8;
	// inline asm
	shl.b32 	%r870, %r427, 2;
	add.s32 	%r871, %r14, %r870;
	ld.shared.u32 	%r872, [%r871];
	// inline asm
	bfe.u32 %r430, %r461, %r12, 8;
	// inline asm
	shl.b32 	%r873, %r430, 2;
	add.s32 	%r874, %r15, %r873;
	ld.shared.u32 	%r875, [%r874];
	// inline asm
	bfe.u32 %r433, %r452, %r111, 8;
	// inline asm
	shl.b32 	%r876, %r433, 2;
	add.s32 	%r877, %r16, %r876;
	ld.shared.u32 	%r878, [%r877];
	// inline asm
	bfe.u32 %r436, %r455, %r13, 8;
	// inline asm
	shl.b32 	%r879, %r436, 2;
	add.s32 	%r880, %r17, %r879;
	xor.b32  	%r881, %r872, %r848;
	xor.b32  	%r882, %r881, %r875;
	xor.b32  	%r883, %r882, %r878;
	ld.shared.u32 	%r884, [%r880];
	xor.b32  	%r506, %r883, %r884;
	// inline asm
	bfe.u32 %r439, %r461, %r597, 8;
	// inline asm
	shl.b32 	%r885, %r439, 2;
	add.s32 	%r886, %r14, %r885;
	ld.shared.u32 	%r887, [%r886];
	// inline asm
	bfe.u32 %r442, %r452, %r12, 8;
	// inline asm
	shl.b32 	%r888, %r442, 2;
	add.s32 	%r889, %r15, %r888;
	ld.shared.u32 	%r890, [%r889];
	// inline asm
	bfe.u32 %r445, %r455, %r111, 8;
	// inline asm
	shl.b32 	%r891, %r445, 2;
	add.s32 	%r892, %r16, %r891;
	ld.shared.u32 	%r893, [%r892];
	// inline asm
	bfe.u32 %r448, %r458, %r13, 8;
	// inline asm
	shl.b32 	%r894, %r448, 2;
	add.s32 	%r895, %r17, %r894;
	xor.b32  	%r896, %r887, %r849;
	xor.b32  	%r897, %r896, %r890;
	xor.b32  	%r898, %r897, %r893;
	ld.shared.u32 	%r899, [%r895];
	xor.b32  	%r509, %r898, %r899;
	// inline asm
	bfe.u32 %r451, %r452, %r597, 8;
	// inline asm
	shl.b32 	%r900, %r451, 2;
	add.s32 	%r901, %r14, %r900;
	ld.shared.u32 	%r902, [%r901];
	// inline asm
	bfe.u32 %r454, %r455, %r12, 8;
	// inline asm
	shl.b32 	%r903, %r454, 2;
	add.s32 	%r904, %r15, %r903;
	ld.shared.u32 	%r905, [%r904];
	// inline asm
	bfe.u32 %r457, %r458, %r111, 8;
	// inline asm
	shl.b32 	%r906, %r457, 2;
	add.s32 	%r907, %r16, %r906;
	ld.shared.u32 	%r908, [%r907];
	// inline asm
	bfe.u32 %r460, %r461, %r13, 8;
	// inline asm
	shl.b32 	%r909, %r460, 2;
	add.s32 	%r910, %r17, %r909;
	xor.b32  	%r911, %r902, %r850;
	xor.b32  	%r912, %r911, %r905;
	xor.b32  	%r913, %r912, %r908;
	ld.shared.u32 	%r914, [%r910];
	xor.b32  	%r500, %r913, %r914;
	ld.global.v4.u32 	{%r915, %r916, %r917, %r918}, [%rd25+320];
	// inline asm
	bfe.u32 %r463, %r503, %r597, 8;
	// inline asm
	shl.b32 	%r923, %r463, 2;
	add.s32 	%r924, %r14, %r923;
	ld.shared.u32 	%r925, [%r924];
	// inline asm
	bfe.u32 %r466, %r506, %r12, 8;
	// inline asm
	shl.b32 	%r926, %r466, 2;
	add.s32 	%r927, %r15, %r926;
	ld.shared.u32 	%r928, [%r927];
	// inline asm
	bfe.u32 %r469, %r509, %r111, 8;
	// inline asm
	shl.b32 	%r929, %r469, 2;
	add.s32 	%r930, %r16, %r929;
	ld.shared.u32 	%r931, [%r930];
	// inline asm
	bfe.u32 %r472, %r500, %r13, 8;
	// inline asm
	shl.b32 	%r932, %r472, 2;
	add.s32 	%r933, %r17, %r932;
	xor.b32  	%r934, %r925, %r915;
	xor.b32  	%r935, %r934, %r928;
	xor.b32  	%r936, %r935, %r931;
	ld.shared.u32 	%r937, [%r933];
	xor.b32  	%r551, %r936, %r937;
	// inline asm
	bfe.u32 %r475, %r506, %r597, 8;
	// inline asm
	shl.b32 	%r938, %r475, 2;
	add.s32 	%r939, %r14, %r938;
	ld.shared.u32 	%r940, [%r939];
	// inline asm
	bfe.u32 %r478, %r509, %r12, 8;
	// inline asm
	shl.b32 	%r941, %r478, 2;
	add.s32 	%r942, %r15, %r941;
	ld.shared.u32 	%r943, [%r942];
	// inline asm
	bfe.u32 %r481, %r500, %r111, 8;
	// inline asm
	shl.b32 	%r944, %r481, 2;
	add.s32 	%r945, %r16, %r944;
	ld.shared.u32 	%r946, [%r945];
	// inline asm
	bfe.u32 %r484, %r503, %r13, 8;
	// inline asm
	shl.b32 	%r947, %r484, 2;
	add.s32 	%r948, %r17, %r947;
	xor.b32  	%r949, %r940, %r916;
	xor.b32  	%r950, %r949, %r943;
	xor.b32  	%r951, %r950, %r946;
	ld.shared.u32 	%r952, [%r948];
	xor.b32  	%r554, %r951, %r952;
	// inline asm
	bfe.u32 %r487, %r509, %r597, 8;
	// inline asm
	shl.b32 	%r953, %r487, 2;
	add.s32 	%r954, %r14, %r953;
	ld.shared.u32 	%r955, [%r954];
	// inline asm
	bfe.u32 %r490, %r500, %r12, 8;
	// inline asm
	shl.b32 	%r956, %r490, 2;
	add.s32 	%r957, %r15, %r956;
	ld.shared.u32 	%r958, [%r957];
	// inline asm
	bfe.u32 %r493, %r503, %r111, 8;
	// inline asm
	shl.b32 	%r959, %r493, 2;
	add.s32 	%r960, %r16, %r959;
	ld.shared.u32 	%r961, [%r960];
	// inline asm
	bfe.u32 %r496, %r506, %r13, 8;
	// inline asm
	shl.b32 	%r962, %r496, 2;
	add.s32 	%r963, %r17, %r962;
	xor.b32  	%r964, %r955, %r917;
	xor.b32  	%r965, %r964, %r958;
	xor.b32  	%r966, %r965, %r961;
	ld.shared.u32 	%r967, [%r963];
	xor.b32  	%r557, %r966, %r967;
	// inline asm
	bfe.u32 %r499, %r500, %r597, 8;
	// inline asm
	shl.b32 	%r968, %r499, 2;
	add.s32 	%r969, %r14, %r968;
	ld.shared.u32 	%r970, [%r969];
	// inline asm
	bfe.u32 %r502, %r503, %r12, 8;
	// inline asm
	shl.b32 	%r971, %r502, 2;
	add.s32 	%r972, %r15, %r971;
	ld.shared.u32 	%r973, [%r972];
	// inline asm
	bfe.u32 %r505, %r506, %r111, 8;
	// inline asm
	shl.b32 	%r974, %r505, 2;
	add.s32 	%r975, %r16, %r974;
	ld.shared.u32 	%r976, [%r975];
	// inline asm
	bfe.u32 %r508, %r509, %r13, 8;
	// inline asm
	shl.b32 	%r977, %r508, 2;
	add.s32 	%r978, %r17, %r977;
	xor.b32  	%r979, %r970, %r918;
	xor.b32  	%r980, %r979, %r973;
	xor.b32  	%r981, %r980, %r976;
	ld.shared.u32 	%r982, [%r978];
	xor.b32  	%r548, %r981, %r982;
	ld.global.v4.u32 	{%r983, %r984, %r985, %r986}, [%rd25+384];
	// inline asm
	bfe.u32 %r511, %r551, %r597, 8;
	// inline asm
	shl.b32 	%r991, %r511, 2;
	add.s32 	%r992, %r14, %r991;
	ld.shared.u32 	%r993, [%r992];
	// inline asm
	bfe.u32 %r514, %r554, %r12, 8;
	// inline asm
	shl.b32 	%r994, %r514, 2;
	add.s32 	%r995, %r15, %r994;
	ld.shared.u32 	%r996, [%r995];
	// inline asm
	bfe.u32 %r517, %r557, %r111, 8;
	// inline asm
	shl.b32 	%r997, %r517, 2;
	add.s32 	%r998, %r16, %r997;
	ld.shared.u32 	%r999, [%r998];
	// inline asm
	bfe.u32 %r520, %r548, %r13, 8;
	// inline asm
	shl.b32 	%r1000, %r520, 2;
	add.s32 	%r1001, %r17, %r1000;
	xor.b32  	%r1002, %r993, %r983;
	xor.b32  	%r1003, %r1002, %r996;
	xor.b32  	%r1004, %r1003, %r999;
	ld.shared.u32 	%r1005, [%r1001];
	xor.b32  	%r599, %r1004, %r1005;
	// inline asm
	bfe.u32 %r523, %r554, %r597, 8;
	// inline asm
	shl.b32 	%r1006, %r523, 2;
	add.s32 	%r1007, %r14, %r1006;
	ld.shared.u32 	%r1008, [%r1007];
	// inline asm
	bfe.u32 %r526, %r557, %r12, 8;
	// inline asm
	shl.b32 	%r1009, %r526, 2;
	add.s32 	%r1010, %r15, %r1009;
	ld.shared.u32 	%r1011, [%r1010];
	// inline asm
	bfe.u32 %r529, %r548, %r111, 8;
	// inline asm
	shl.b32 	%r1012, %r529, 2;
	add.s32 	%r1013, %r16, %r1012;
	ld.shared.u32 	%r1014, [%r1013];
	// inline asm
	bfe.u32 %r532, %r551, %r13, 8;
	// inline asm
	shl.b32 	%r1015, %r532, 2;
	add.s32 	%r1016, %r17, %r1015;
	xor.b32  	%r1017, %r1008, %r984;
	xor.b32  	%r1018, %r1017, %r1011;
	xor.b32  	%r1019, %r1018, %r1014;
	ld.shared.u32 	%r1020, [%r1016];
	xor.b32  	%r602, %r1019, %r1020;
	// inline asm
	bfe.u32 %r535, %r557, %r597, 8;
	// inline asm
	shl.b32 	%r1021, %r535, 2;
	add.s32 	%r1022, %r14, %r1021;
	ld.shared.u32 	%r1023, [%r1022];
	// inline asm
	bfe.u32 %r538, %r548, %r12, 8;
	// inline asm
	shl.b32 	%r1024, %r538, 2;
	add.s32 	%r1025, %r15, %r1024;
	ld.shared.u32 	%r1026, [%r1025];
	// inline asm
	bfe.u32 %r541, %r551, %r111, 8;
	// inline asm
	shl.b32 	%r1027, %r541, 2;
	add.s32 	%r1028, %r16, %r1027;
	ld.shared.u32 	%r1029, [%r1028];
	// inline asm
	bfe.u32 %r544, %r554, %r13, 8;
	// inline asm
	shl.b32 	%r1030, %r544, 2;
	add.s32 	%r1031, %r17, %r1030;
	xor.b32  	%r1032, %r1023, %r985;
	xor.b32  	%r1033, %r1032, %r1026;
	xor.b32  	%r1034, %r1033, %r1029;
	ld.shared.u32 	%r1035, [%r1031];
	xor.b32  	%r605, %r1034, %r1035;
	// inline asm
	bfe.u32 %r547, %r548, %r597, 8;
	// inline asm
	shl.b32 	%r1036, %r547, 2;
	add.s32 	%r1037, %r14, %r1036;
	ld.shared.u32 	%r1038, [%r1037];
	// inline asm
	bfe.u32 %r550, %r551, %r12, 8;
	// inline asm
	shl.b32 	%r1039, %r550, 2;
	add.s32 	%r1040, %r15, %r1039;
	ld.shared.u32 	%r1041, [%r1040];
	// inline asm
	bfe.u32 %r553, %r554, %r111, 8;
	// inline asm
	shl.b32 	%r1042, %r553, 2;
	add.s32 	%r1043, %r16, %r1042;
	ld.shared.u32 	%r1044, [%r1043];
	// inline asm
	bfe.u32 %r556, %r557, %r13, 8;
	// inline asm
	shl.b32 	%r1045, %r556, 2;
	add.s32 	%r1046, %r17, %r1045;
	xor.b32  	%r1047, %r1038, %r986;
	xor.b32  	%r1048, %r1047, %r1041;
	xor.b32  	%r1049, %r1048, %r1044;
	ld.shared.u32 	%r1050, [%r1046];
	xor.b32  	%r596, %r1049, %r1050;
	ld.global.v4.u32 	{%r1051, %r1052, %r1053, %r1054}, [%rd25+448];
	// inline asm
	bfe.u32 %r559, %r599, %r597, 8;
	// inline asm
	shl.b32 	%r1059, %r559, 2;
	add.s32 	%r1060, %r14, %r1059;
	ld.shared.u32 	%r1061, [%r1060];
	// inline asm
	bfe.u32 %r562, %r602, %r12, 8;
	// inline asm
	shl.b32 	%r1062, %r562, 2;
	add.s32 	%r1063, %r15, %r1062;
	ld.shared.u32 	%r1064, [%r1063];
	// inline asm
	bfe.u32 %r565, %r605, %r111, 8;
	// inline asm
	shl.b32 	%r1065, %r565, 2;
	add.s32 	%r1066, %r16, %r1065;
	ld.shared.u32 	%r1067, [%r1066];
	// inline asm
	bfe.u32 %r568, %r596, %r13, 8;
	// inline asm
	shl.b32 	%r1068, %r568, 2;
	add.s32 	%r1069, %r17, %r1068;
	xor.b32  	%r1070, %r1061, %r1051;
	xor.b32  	%r1071, %r1070, %r1064;
	xor.b32  	%r1072, %r1071, %r1067;
	ld.shared.u32 	%r1073, [%r1069];
	xor.b32  	%r1120, %r1072, %r1073;
	// inline asm
	bfe.u32 %r571, %r602, %r597, 8;
	// inline asm
	shl.b32 	%r1074, %r571, 2;
	add.s32 	%r1075, %r14, %r1074;
	ld.shared.u32 	%r1076, [%r1075];
	// inline asm
	bfe.u32 %r574, %r605, %r12, 8;
	// inline asm
	shl.b32 	%r1077, %r574, 2;
	add.s32 	%r1078, %r15, %r1077;
	ld.shared.u32 	%r1079, [%r1078];
	// inline asm
	bfe.u32 %r577, %r596, %r111, 8;
	// inline asm
	shl.b32 	%r1080, %r577, 2;
	add.s32 	%r1081, %r16, %r1080;
	ld.shared.u32 	%r1082, [%r1081];
	// inline asm
	bfe.u32 %r580, %r599, %r13, 8;
	// inline asm
	shl.b32 	%r1083, %r580, 2;
	add.s32 	%r1084, %r17, %r1083;
	xor.b32  	%r1085, %r1076, %r1052;
	xor.b32  	%r1086, %r1085, %r1079;
	xor.b32  	%r1087, %r1086, %r1082;
	ld.shared.u32 	%r1088, [%r1084];
	xor.b32  	%r1121, %r1087, %r1088;
	// inline asm
	bfe.u32 %r583, %r605, %r597, 8;
	// inline asm
	shl.b32 	%r1089, %r583, 2;
	add.s32 	%r1090, %r14, %r1089;
	ld.shared.u32 	%r1091, [%r1090];
	// inline asm
	bfe.u32 %r586, %r596, %r12, 8;
	// inline asm
	shl.b32 	%r1092, %r586, 2;
	add.s32 	%r1093, %r15, %r1092;
	ld.shared.u32 	%r1094, [%r1093];
	// inline asm
	bfe.u32 %r589, %r599, %r111, 8;
	// inline asm
	shl.b32 	%r1095, %r589, 2;
	add.s32 	%r1096, %r16, %r1095;
	ld.shared.u32 	%r1097, [%r1096];
	// inline asm
	bfe.u32 %r592, %r602, %r13, 8;
	// inline asm
	shl.b32 	%r1098, %r592, 2;
	add.s32 	%r1099, %r17, %r1098;
	xor.b32  	%r1100, %r1091, %r1053;
	xor.b32  	%r1101, %r1100, %r1094;
	xor.b32  	%r1102, %r1101, %r1097;
	ld.shared.u32 	%r1103, [%r1099];
	xor.b32  	%r1122, %r1102, %r1103;
	// inline asm
	bfe.u32 %r595, %r596, %r597, 8;
	// inline asm
	shl.b32 	%r1104, %r595, 2;
	add.s32 	%r1105, %r14, %r1104;
	ld.shared.u32 	%r1106, [%r1105];
	// inline asm
	bfe.u32 %r598, %r599, %r12, 8;
	// inline asm
	shl.b32 	%r1107, %r598, 2;
	add.s32 	%r1108, %r15, %r1107;
	ld.shared.u32 	%r1109, [%r1108];
	// inline asm
	bfe.u32 %r601, %r602, %r111, 8;
	// inline asm
	shl.b32 	%r1110, %r601, 2;
	add.s32 	%r1111, %r16, %r1110;
	ld.shared.u32 	%r1112, [%r1111];
	// inline asm
	bfe.u32 %r604, %r605, %r13, 8;
	// inline asm
	shl.b32 	%r1113, %r604, 2;
	add.s32 	%r1114, %r17, %r1113;
	xor.b32  	%r1115, %r1106, %r1054;
	xor.b32  	%r1116, %r1115, %r1109;
	xor.b32  	%r1117, %r1116, %r1112;
	ld.shared.u32 	%r1118, [%r1114];
	xor.b32  	%r1123, %r1117, %r1118;
	add.s32 	%r1124, %r1124, 32;
	add.s64 	%rd25, %rd25, 512;

BB15_4:
	// inline asm
	bfe.u32 %r67, %r1120, %r50, 8;
	// inline asm
	shl.b32 	%r115, %r67, 2;
	add.s32 	%r23, %r14, %r115;
	// inline asm
	bfe.u32 %r70, %r1121, %r12, 8;
	// inline asm
	shl.b32 	%r116, %r70, 2;
	add.s32 	%r24, %r15, %r116;
	mov.u32 	%r111, 16;
	// inline asm
	bfe.u32 %r73, %r1122, %r111, 8;
	// inline asm
	shl.b32 	%r117, %r73, 2;
	add.s32 	%r25, %r16, %r117;
	// inline asm
	bfe.u32 %r76, %r1123, %r13, 8;
	// inline asm
	shl.b32 	%r118, %r76, 2;
	add.s32 	%r26, %r17, %r118;
	// inline asm
	bfe.u32 %r79, %r1121, %r50, 8;
	// inline asm
	shl.b32 	%r119, %r79, 2;
	add.s32 	%r27, %r14, %r119;
	// inline asm
	bfe.u32 %r82, %r1122, %r12, 8;
	// inline asm
	shl.b32 	%r120, %r82, 2;
	add.s32 	%r28, %r15, %r120;
	// inline asm
	bfe.u32 %r85, %r1123, %r111, 8;
	// inline asm
	shl.b32 	%r121, %r85, 2;
	add.s32 	%r29, %r16, %r121;
	// inline asm
	bfe.u32 %r88, %r1120, %r13, 8;
	// inline asm
	shl.b32 	%r122, %r88, 2;
	add.s32 	%r30, %r17, %r122;
	// inline asm
	bfe.u32 %r91, %r1122, %r50, 8;
	// inline asm
	shl.b32 	%r123, %r91, 2;
	add.s32 	%r31, %r14, %r123;
	// inline asm
	bfe.u32 %r94, %r1123, %r12, 8;
	// inline asm
	shl.b32 	%r124, %r94, 2;
	add.s32 	%r32, %r15, %r124;
	// inline asm
	bfe.u32 %r97, %r1120, %r111, 8;
	// inline asm
	shl.b32 	%r125, %r97, 2;
	add.s32 	%r33, %r16, %r125;
	// inline asm
	bfe.u32 %r100, %r1121, %r13, 8;
	// inline asm
	shl.b32 	%r126, %r100, 2;
	add.s32 	%r34, %r17, %r126;
	// inline asm
	bfe.u32 %r103, %r1123, %r50, 8;
	// inline asm
	shl.b32 	%r127, %r103, 2;
	add.s32 	%r35, %r14, %r127;
	// inline asm
	bfe.u32 %r106, %r1120, %r12, 8;
	// inline asm
	shl.b32 	%r128, %r106, 2;
	add.s32 	%r36, %r15, %r128;
	// inline asm
	bfe.u32 %r109, %r1121, %r111, 8;
	// inline asm
	shl.b32 	%r129, %r109, 2;
	add.s32 	%r37, %r16, %r129;
	// inline asm
	bfe.u32 %r112, %r1122, %r13, 8;
	// inline asm
	shl.b32 	%r130, %r112, 2;
	add.s32 	%r38, %r17, %r130;
	setp.lt.u32	%p5, %r1124, 131072;
	@%p5 bra 	BB15_7;

	ld.shared.u32 	%r179, [%r23];
	ld.shared.u32 	%r180, [%r24];
	xor.b32  	%r181, %r179, %r180;
	ld.shared.u32 	%r182, [%r25];
	xor.b32  	%r183, %r181, %r182;
	ld.shared.u32 	%r184, [%r26];
	xor.b32  	%r185, %r183, %r184;
	xor.b32  	%r132, %r185, -151354487;
	ld.shared.u32 	%r186, [%r27];
	ld.shared.u32 	%r187, [%r28];
	xor.b32  	%r188, %r186, %r187;
	ld.shared.u32 	%r189, [%r29];
	xor.b32  	%r190, %r188, %r189;
	ld.shared.u32 	%r191, [%r30];
	xor.b32  	%r192, %r190, %r191;
	xor.b32  	%r135, %r192, -1960536929;
	ld.shared.u32 	%r193, [%r31];
	ld.shared.u32 	%r194, [%r32];
	xor.b32  	%r195, %r193, %r194;
	ld.shared.u32 	%r196, [%r33];
	xor.b32  	%r197, %r195, %r196;
	ld.shared.u32 	%r198, [%r34];
	xor.b32  	%r199, %r197, %r198;
	xor.b32  	%r138, %r199, -1864608065;
	ld.shared.u32 	%r200, [%r35];
	ld.shared.u32 	%r201, [%r36];
	xor.b32  	%r202, %r200, %r201;
	ld.shared.u32 	%r203, [%r37];
	xor.b32  	%r204, %r202, %r203;
	ld.shared.u32 	%r205, [%r38];
	xor.b32  	%r206, %r204, %r205;
	xor.b32  	%r141, %r206, 109642241;
	mov.u32 	%r169, 0;
	// inline asm
	bfe.u32 %r131, %r132, %r169, 8;
	// inline asm
	shl.b32 	%r207, %r131, 2;
	add.s32 	%r208, %r14, %r207;
	ld.shared.u32 	%r209, [%r208];
	// inline asm
	bfe.u32 %r134, %r135, %r12, 8;
	// inline asm
	shl.b32 	%r210, %r134, 2;
	add.s32 	%r211, %r15, %r210;
	ld.shared.u32 	%r212, [%r211];
	// inline asm
	bfe.u32 %r137, %r138, %r111, 8;
	// inline asm
	shl.b32 	%r213, %r137, 2;
	add.s32 	%r214, %r16, %r213;
	ld.shared.u32 	%r215, [%r214];
	// inline asm
	bfe.u32 %r140, %r141, %r13, 8;
	// inline asm
	shl.b32 	%r216, %r140, 2;
	add.s32 	%r217, %r17, %r216;
	xor.b32  	%r218, %r209, %r212;
	xor.b32  	%r219, %r218, %r215;
	ld.shared.u32 	%r220, [%r217];
	xor.b32  	%r221, %r219, %r220;
	// inline asm
	bfe.u32 %r143, %r135, %r169, 8;
	// inline asm
	shl.b32 	%r222, %r143, 2;
	add.s32 	%r223, %r14, %r222;
	ld.shared.u32 	%r224, [%r223];
	// inline asm
	bfe.u32 %r146, %r138, %r12, 8;
	// inline asm
	shl.b32 	%r225, %r146, 2;
	add.s32 	%r226, %r15, %r225;
	ld.shared.u32 	%r227, [%r226];
	// inline asm
	bfe.u32 %r149, %r141, %r111, 8;
	// inline asm
	shl.b32 	%r228, %r149, 2;
	add.s32 	%r229, %r16, %r228;
	ld.shared.u32 	%r230, [%r229];
	// inline asm
	bfe.u32 %r152, %r132, %r13, 8;
	// inline asm
	shl.b32 	%r231, %r152, 2;
	add.s32 	%r232, %r17, %r231;
	xor.b32  	%r233, %r224, %r227;
	xor.b32  	%r234, %r233, %r230;
	ld.shared.u32 	%r235, [%r232];
	xor.b32  	%r236, %r234, %r235;
	// inline asm
	bfe.u32 %r155, %r138, %r169, 8;
	// inline asm
	shl.b32 	%r237, %r155, 2;
	add.s32 	%r238, %r14, %r237;
	ld.shared.u32 	%r239, [%r238];
	// inline asm
	bfe.u32 %r158, %r141, %r12, 8;
	// inline asm
	shl.b32 	%r240, %r158, 2;
	add.s32 	%r241, %r15, %r240;
	ld.shared.u32 	%r242, [%r241];
	// inline asm
	bfe.u32 %r161, %r132, %r111, 8;
	// inline asm
	shl.b32 	%r243, %r161, 2;
	add.s32 	%r244, %r16, %r243;
	ld.shared.u32 	%r245, [%r244];
	// inline asm
	bfe.u32 %r164, %r135, %r13, 8;
	// inline asm
	shl.b32 	%r246, %r164, 2;
	add.s32 	%r247, %r17, %r246;
	xor.b32  	%r248, %r239, %r242;
	xor.b32  	%r249, %r248, %r245;
	ld.shared.u32 	%r250, [%r247];
	xor.b32  	%r251, %r249, %r250;
	// inline asm
	bfe.u32 %r167, %r141, %r169, 8;
	// inline asm
	shl.b32 	%r252, %r167, 2;
	add.s32 	%r253, %r14, %r252;
	ld.shared.u32 	%r254, [%r253];
	// inline asm
	bfe.u32 %r170, %r132, %r12, 8;
	// inline asm
	shl.b32 	%r255, %r170, 2;
	add.s32 	%r256, %r15, %r255;
	ld.shared.u32 	%r257, [%r256];
	// inline asm
	bfe.u32 %r173, %r135, %r111, 8;
	// inline asm
	shl.b32 	%r258, %r173, 2;
	add.s32 	%r259, %r16, %r258;
	ld.shared.u32 	%r260, [%r259];
	// inline asm
	bfe.u32 %r176, %r138, %r13, 8;
	// inline asm
	shl.b32 	%r261, %r176, 2;
	add.s32 	%r262, %r17, %r261;
	xor.b32  	%r263, %r254, %r257;
	xor.b32  	%r264, %r263, %r260;
	ld.shared.u32 	%r265, [%r262];
	xor.b32  	%r266, %r264, %r265;
	shl.b64 	%rd20, %rd1, 7;
	add.s64 	%rd21, %rd2, %rd20;
	cvta.to.global.u64 	%rd22, %rd7;
	shl.b64 	%rd23, %rd21, 4;
	add.s64 	%rd24, %rd22, %rd23;
	xor.b32  	%r267, %r266, -317130341;
	xor.b32  	%r268, %r251, -300923962;
	xor.b32  	%r269, %r236, 1375002684;
	xor.b32  	%r270, %r221, 1639080913;
	st.global.v4.u32 	[%rd24+192], {%r270, %r269, %r268, %r267};

BB15_6:
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2745>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 2048;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 32;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249095;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301160;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	add.s64 	%rd2698, %rd2651, %rd1381;
	add.s64 	%rd2699, %rd2698, %rd2670;
	xor.b64  	%rd2700, %rd2699, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2700;
	mov.b64	%rd2701, {%r3436, %r3435};
	add.s64 	%rd2702, %rd2701, %rd2682;
	xor.b64  	%rd2703, %rd2702, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2703;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2704, {%r3440, %r3439};
	add.s64 	%rd2705, %rd2699, %rd1383;
	add.s64 	%rd2706, %rd2705, %rd2704;
	xor.b64  	%rd2707, %rd2706, %rd2701;
	mov.b64	{%r3441, %r3442}, %rd2707;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2708, {%r3444, %r3443};
	add.s64 	%rd2709, %rd2708, %rd2702;
	xor.b64  	%rd2710, %rd2709, %rd2704;
	mov.b64	{%r1518, %r1519}, %rd2710;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	add.s64 	%rd2711, %rd2665, %rd1392;
	add.s64 	%rd2712, %rd2711, %rd2684;
	xor.b64  	%rd2713, %rd2712, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2713;
	mov.b64	%rd2714, {%r3446, %r3445};
	add.s64 	%rd2715, %rd2714, %rd2640;
	xor.b64  	%rd2716, %rd2715, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2716;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2717, {%r3450, %r3449};
	add.s64 	%rd2718, %rd2712, %rd1388;
	add.s64 	%rd2719, %rd2718, %rd2717;
	xor.b64  	%rd2720, %rd2719, %rd2714;
	mov.b64	{%r3451, %r3452}, %rd2720;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2721, {%r3454, %r3453};
	add.s64 	%rd2722, %rd2721, %rd2715;
	xor.b64  	%rd2723, %rd2722, %rd2717;
	mov.b64	{%r1526, %r1527}, %rd2723;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	add.s64 	%rd2724, %rd2642, %rd1386;
	add.s64 	%rd2725, %rd2724, %rd2679;
	xor.b64  	%rd2726, %rd2725, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2726;
	mov.b64	%rd2727, {%r3456, %r3455};
	add.s64 	%rd2728, %rd2727, %rd2654;
	xor.b64  	%rd2729, %rd2728, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2729;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2730, {%r3460, %r3459};
	add.s64 	%rd2731, %rd2725, %rd1384;
	add.s64 	%rd2732, %rd2731, %rd2730;
	xor.b64  	%rd2733, %rd2732, %rd2727;
	mov.b64	{%r3461, %r3462}, %rd2733;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2734, {%r3464, %r3463};
	add.s64 	%rd2735, %rd2734, %rd2728;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r1534, %r1535}, %rd2736;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	xor.b64  	%rd2737, %rd2693, %rd1366;
	xor.b64  	%rd2738, %rd2737, %rd2722;
	st.global.u64 	[%rd8], %rd2738;
	xor.b64  	%rd2739, %rd2706, %rd1368;
	xor.b64  	%rd2740, %rd2739, %rd2735;
	st.global.u64 	[%rd8+8], %rd2740;
	xor.b64  	%rd2741, %rd2696, %rd1370;
	xor.b64  	%rd2742, %rd2741, %rd2719;
	st.global.u64 	[%rd8+16], %rd2742;
	xor.b64  	%rd2743, %rd2709, %rd1372;
	xor.b64  	%rd2744, %rd2743, %rd2732;
	st.global.u64 	[%rd8+24], %rd2744;
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2757>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 2048;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 64;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249063;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301192;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	mov.b64	%rd2698, {%r1505, %r1509};
	add.s64 	%rd2699, %rd2651, %rd1381;
	add.s64 	%rd2700, %rd2699, %rd2670;
	xor.b64  	%rd2701, %rd2700, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2701;
	mov.b64	%rd2702, {%r3436, %r3435};
	add.s64 	%rd2703, %rd2702, %rd2682;
	xor.b64  	%rd2704, %rd2703, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2704;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2705, {%r3440, %r3439};
	add.s64 	%rd2706, %rd2700, %rd1383;
	add.s64 	%rd2707, %rd2706, %rd2705;
	xor.b64  	%rd2708, %rd2707, %rd2702;
	mov.b64	{%r3441, %r3442}, %rd2708;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2709, {%r3444, %r3443};
	add.s64 	%rd2710, %rd2709, %rd2703;
	xor.b64  	%rd2711, %rd2710, %rd2705;
	mov.b64	{%r1518, %r1519}, %rd2711;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	mov.b64	%rd2712, {%r1513, %r1517};
	add.s64 	%rd2713, %rd2665, %rd1392;
	add.s64 	%rd2714, %rd2713, %rd2684;
	xor.b64  	%rd2715, %rd2714, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2715;
	mov.b64	%rd2716, {%r3446, %r3445};
	add.s64 	%rd2717, %rd2716, %rd2640;
	xor.b64  	%rd2718, %rd2717, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2718;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2719, {%r3450, %r3449};
	add.s64 	%rd2720, %rd2714, %rd1388;
	add.s64 	%rd2721, %rd2720, %rd2719;
	xor.b64  	%rd2722, %rd2721, %rd2716;
	mov.b64	{%r3451, %r3452}, %rd2722;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2723, {%r3454, %r3453};
	add.s64 	%rd2724, %rd2723, %rd2717;
	xor.b64  	%rd2725, %rd2724, %rd2719;
	mov.b64	{%r1526, %r1527}, %rd2725;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	mov.b64	%rd2726, {%r1521, %r1525};
	add.s64 	%rd2727, %rd2642, %rd1386;
	add.s64 	%rd2728, %rd2727, %rd2679;
	xor.b64  	%rd2729, %rd2728, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2729;
	mov.b64	%rd2730, {%r3456, %r3455};
	add.s64 	%rd2731, %rd2730, %rd2654;
	xor.b64  	%rd2732, %rd2731, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2732;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2733, {%r3460, %r3459};
	add.s64 	%rd2734, %rd2728, %rd1384;
	add.s64 	%rd2735, %rd2734, %rd2733;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r3461, %r3462}, %rd2736;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2737, {%r3464, %r3463};
	add.s64 	%rd2738, %rd2737, %rd2731;
	xor.b64  	%rd2739, %rd2738, %rd2733;
	mov.b64	{%r1534, %r1535}, %rd2739;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	mov.b64	%rd2740, {%r1529, %r1533};
	xor.b64  	%rd2741, %rd2693, %rd1366;
	xor.b64  	%rd2742, %rd2741, %rd2724;
	st.global.u64 	[%rd8], %rd2742;
	xor.b64  	%rd2743, %rd2707, %rd1368;
	xor.b64  	%rd2744, %rd2743, %rd2738;
	st.global.u64 	[%rd8+8], %rd2744;
	xor.b64  	%rd2745, %rd2696, %rd1370;
	xor.b64  	%rd2746, %rd2745, %rd2721;
	st.global.u64 	[%rd8+16], %rd2746;
	xor.b64  	%rd2747, %rd2710, %rd1372;
	xor.b64  	%rd2748, %rd2747, %rd2735;
	st.global.u64 	[%rd8+24], %rd2748;
	xor.b64  	%rd2749, %rd2709, %rd1374;
	xor.b64  	%rd2750, %rd2749, %rd2740;
	st.global.u64 	[%rd8+32], %rd2750;
	xor.b64  	%rd2751, %rd2698, %rd1376;
	xor.b64  	%rd2752, %rd2751, %rd2723;
	st.global.u64 	[%rd8+40], %rd2752;
	xor.b64  	%rd2753, %rd2712, %rd1378;
	xor.b64  	%rd2754, %rd2753, %rd2737;
	st.global.u64 	[%rd8+48], %rd2754;
	xor.b64  	%rd2755, %rd2695, %rd1380;
	xor.b64  	%rd2756, %rd2755, %rd2726;
	st.global.u64 	[%rd8+56], %rd2756;
	ret;
}

	// .globl	_Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj
.visible .entry _Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj(
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj_param_0,
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj_param_1,
	.param .u32 _Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<1125>;
	.reg .b64 	%rd<26>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvjE1T[8192];

	ld.param.u64 	%rd6, [_Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj_param_0];
	ld.param.u64 	%rd7, [_Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj_param_1];
	ld.param.u32 	%r44, [_Z11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvj_param_2];
	shl.b32 	%r45, %r44, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r1119, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r1119;
	setp.ge.u32	%p1, %r4, %r45;
	@%p1 bra 	BB18_6;

	setp.gt.s32	%p2, %r1119, 2047;
	@%p2 bra 	BB18_3;

BB18_2:
	mul.wide.s32 	%rd8, %r1119, 4;
	mov.u64 	%rd9, AES_TABLE;
	add.s64 	%rd10, %rd9, %rd8;
	ld.const.u32 	%r46, [%rd10];
	shl.b32 	%r47, %r1119, 2;
	mov.u32 	%r48, _ZZ11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvjE1T;
	add.s32 	%r49, %r48, %r47;
	st.shared.u32 	[%r49], %r46;
	add.s32 	%r1119, %r1119, %r1;
	setp.lt.s32	%p3, %r1119, 2048;
	@%p3 bra 	BB18_2;

BB18_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r51, %r7, 2;
	mul.wide.u32 	%rd11, %r51, 4;
	mov.u64 	%rd12, AES_STATE_HASH;
	add.s64 	%rd13, %rd12, %rd11;
	ld.const.u32 	%r1120, [%rd13];
	ld.const.u32 	%r1121, [%rd13+4];
	ld.const.u32 	%r1122, [%rd13+8];
	ld.const.u32 	%r1123, [%rd13+12];
	and.b32  	%r52, %r4, 1;
	shl.b32 	%r53, %r4, 4;
	and.b32  	%r54, %r53, 16;
	add.s32 	%r12, %r54, 8;
	xor.b32  	%r55, %r54, 16;
	add.s32 	%r13, %r55, 8;
	shr.u32 	%r56, %r4, 2;
	cvt.u64.u32	%rd1, %r56;
	cvt.u64.u32	%rd2, %r7;
	setp.eq.s32	%p4, %r52, 0;
	mov.u32 	%r50, 0;
	mov.u32 	%r57, _ZZ11hashAes1Rx4ILy2097152ELj192ELj256ELy64EEvPKvPvjE1T;
	add.s32 	%r58, %r57, 4096;
	selp.b32	%r14, %r57, %r58, %p4;
	add.s32 	%r59, %r57, 7168;
	add.s32 	%r60, %r57, 1024;
	selp.b32	%r15, %r60, %r59, %p4;
	add.s32 	%r61, %r57, 6144;
	add.s32 	%r62, %r57, 2048;
	selp.b32	%r16, %r62, %r61, %p4;
	add.s32 	%r63, %r57, 5120;
	add.s32 	%r64, %r57, 3072;
	selp.b32	%r17, %r64, %r63, %p4;
	mul.wide.u32 	%rd14, %r56, 131076;
	cvt.u64.u32	%rd15, %r4;
	and.b64  	%rd16, %rd15, 3;
	or.b64  	%rd17, %rd14, %rd16;
	cvta.to.global.u64 	%rd18, %rd6;
	shl.b64 	%rd19, %rd17, 4;
	add.s64 	%rd25, %rd18, %rd19;
	mov.u32 	%r1124, %r50;
	bra.uni 	BB18_4;

BB18_7:
	ld.global.v4.u32 	{%r607, %r608, %r609, %r610}, [%rd25];
	mov.u32 	%r597, 0;
	ld.shared.u32 	%r615, [%r23];
	xor.b32  	%r616, %r607, %r615;
	ld.shared.u32 	%r617, [%r24];
	xor.b32  	%r618, %r616, %r617;
	ld.shared.u32 	%r619, [%r25];
	xor.b32  	%r620, %r618, %r619;
	ld.shared.u32 	%r621, [%r26];
	xor.b32  	%r311, %r620, %r621;
	ld.shared.u32 	%r622, [%r27];
	xor.b32  	%r623, %r622, %r608;
	ld.shared.u32 	%r624, [%r28];
	xor.b32  	%r625, %r623, %r624;
	ld.shared.u32 	%r626, [%r29];
	xor.b32  	%r627, %r625, %r626;
	ld.shared.u32 	%r628, [%r30];
	xor.b32  	%r314, %r627, %r628;
	ld.shared.u32 	%r629, [%r31];
	xor.b32  	%r630, %r629, %r609;
	ld.shared.u32 	%r631, [%r32];
	xor.b32  	%r632, %r630, %r631;
	ld.shared.u32 	%r633, [%r33];
	xor.b32  	%r634, %r632, %r633;
	ld.shared.u32 	%r635, [%r34];
	xor.b32  	%r317, %r634, %r635;
	ld.shared.u32 	%r636, [%r35];
	xor.b32  	%r637, %r636, %r610;
	ld.shared.u32 	%r638, [%r36];
	xor.b32  	%r639, %r637, %r638;
	ld.shared.u32 	%r640, [%r37];
	xor.b32  	%r641, %r639, %r640;
	ld.shared.u32 	%r642, [%r38];
	xor.b32  	%r308, %r641, %r642;
	ld.global.v4.u32 	{%r643, %r644, %r645, %r646}, [%rd25+64];
	// inline asm
	bfe.u32 %r271, %r311, %r597, 8;
	// inline asm
	shl.b32 	%r651, %r271, 2;
	add.s32 	%r652, %r14, %r651;
	ld.shared.u32 	%r653, [%r652];
	// inline asm
	bfe.u32 %r274, %r314, %r12, 8;
	// inline asm
	shl.b32 	%r654, %r274, 2;
	add.s32 	%r655, %r15, %r654;
	ld.shared.u32 	%r656, [%r655];
	// inline asm
	bfe.u32 %r277, %r317, %r111, 8;
	// inline asm
	shl.b32 	%r657, %r277, 2;
	add.s32 	%r658, %r16, %r657;
	ld.shared.u32 	%r659, [%r658];
	// inline asm
	bfe.u32 %r280, %r308, %r13, 8;
	// inline asm
	shl.b32 	%r660, %r280, 2;
	add.s32 	%r661, %r17, %r660;
	xor.b32  	%r662, %r653, %r643;
	xor.b32  	%r663, %r662, %r656;
	xor.b32  	%r664, %r663, %r659;
	ld.shared.u32 	%r665, [%r661];
	xor.b32  	%r359, %r664, %r665;
	// inline asm
	bfe.u32 %r283, %r314, %r597, 8;
	// inline asm
	shl.b32 	%r666, %r283, 2;
	add.s32 	%r667, %r14, %r666;
	ld.shared.u32 	%r668, [%r667];
	// inline asm
	bfe.u32 %r286, %r317, %r12, 8;
	// inline asm
	shl.b32 	%r669, %r286, 2;
	add.s32 	%r670, %r15, %r669;
	ld.shared.u32 	%r671, [%r670];
	// inline asm
	bfe.u32 %r289, %r308, %r111, 8;
	// inline asm
	shl.b32 	%r672, %r289, 2;
	add.s32 	%r673, %r16, %r672;
	ld.shared.u32 	%r674, [%r673];
	// inline asm
	bfe.u32 %r292, %r311, %r13, 8;
	// inline asm
	shl.b32 	%r675, %r292, 2;
	add.s32 	%r676, %r17, %r675;
	xor.b32  	%r677, %r668, %r644;
	xor.b32  	%r678, %r677, %r671;
	xor.b32  	%r679, %r678, %r674;
	ld.shared.u32 	%r680, [%r676];
	xor.b32  	%r362, %r679, %r680;
	// inline asm
	bfe.u32 %r295, %r317, %r597, 8;
	// inline asm
	shl.b32 	%r681, %r295, 2;
	add.s32 	%r682, %r14, %r681;
	ld.shared.u32 	%r683, [%r682];
	// inline asm
	bfe.u32 %r298, %r308, %r12, 8;
	// inline asm
	shl.b32 	%r684, %r298, 2;
	add.s32 	%r685, %r15, %r684;
	ld.shared.u32 	%r686, [%r685];
	// inline asm
	bfe.u32 %r301, %r311, %r111, 8;
	// inline asm
	shl.b32 	%r687, %r301, 2;
	add.s32 	%r688, %r16, %r687;
	ld.shared.u32 	%r689, [%r688];
	// inline asm
	bfe.u32 %r304, %r314, %r13, 8;
	// inline asm
	shl.b32 	%r690, %r304, 2;
	add.s32 	%r691, %r17, %r690;
	xor.b32  	%r692, %r683, %r645;
	xor.b32  	%r693, %r692, %r686;
	xor.b32  	%r694, %r693, %r689;
	ld.shared.u32 	%r695, [%r691];
	xor.b32  	%r365, %r694, %r695;
	// inline asm
	bfe.u32 %r307, %r308, %r597, 8;
	// inline asm
	shl.b32 	%r696, %r307, 2;
	add.s32 	%r697, %r14, %r696;
	ld.shared.u32 	%r698, [%r697];
	// inline asm
	bfe.u32 %r310, %r311, %r12, 8;
	// inline asm
	shl.b32 	%r699, %r310, 2;
	add.s32 	%r700, %r15, %r699;
	ld.shared.u32 	%r701, [%r700];
	// inline asm
	bfe.u32 %r313, %r314, %r111, 8;
	// inline asm
	shl.b32 	%r702, %r313, 2;
	add.s32 	%r703, %r16, %r702;
	ld.shared.u32 	%r704, [%r703];
	// inline asm
	bfe.u32 %r316, %r317, %r13, 8;
	// inline asm
	shl.b32 	%r705, %r316, 2;
	add.s32 	%r706, %r17, %r705;
	xor.b32  	%r707, %r698, %r646;
	xor.b32  	%r708, %r707, %r701;
	xor.b32  	%r709, %r708, %r704;
	ld.shared.u32 	%r710, [%r706];
	xor.b32  	%r356, %r709, %r710;
	ld.global.v4.u32 	{%r711, %r712, %r713, %r714}, [%rd25+128];
	// inline asm
	bfe.u32 %r319, %r359, %r597, 8;
	// inline asm
	shl.b32 	%r719, %r319, 2;
	add.s32 	%r720, %r14, %r719;
	ld.shared.u32 	%r721, [%r720];
	// inline asm
	bfe.u32 %r322, %r362, %r12, 8;
	// inline asm
	shl.b32 	%r722, %r322, 2;
	add.s32 	%r723, %r15, %r722;
	ld.shared.u32 	%r724, [%r723];
	// inline asm
	bfe.u32 %r325, %r365, %r111, 8;
	// inline asm
	shl.b32 	%r725, %r325, 2;
	add.s32 	%r726, %r16, %r725;
	ld.shared.u32 	%r727, [%r726];
	// inline asm
	bfe.u32 %r328, %r356, %r13, 8;
	// inline asm
	shl.b32 	%r728, %r328, 2;
	add.s32 	%r729, %r17, %r728;
	xor.b32  	%r730, %r721, %r711;
	xor.b32  	%r731, %r730, %r724;
	xor.b32  	%r732, %r731, %r727;
	ld.shared.u32 	%r733, [%r729];
	xor.b32  	%r407, %r732, %r733;
	// inline asm
	bfe.u32 %r331, %r362, %r597, 8;
	// inline asm
	shl.b32 	%r734, %r331, 2;
	add.s32 	%r735, %r14, %r734;
	ld.shared.u32 	%r736, [%r735];
	// inline asm
	bfe.u32 %r334, %r365, %r12, 8;
	// inline asm
	shl.b32 	%r737, %r334, 2;
	add.s32 	%r738, %r15, %r737;
	ld.shared.u32 	%r739, [%r738];
	// inline asm
	bfe.u32 %r337, %r356, %r111, 8;
	// inline asm
	shl.b32 	%r740, %r337, 2;
	add.s32 	%r741, %r16, %r740;
	ld.shared.u32 	%r742, [%r741];
	// inline asm
	bfe.u32 %r340, %r359, %r13, 8;
	// inline asm
	shl.b32 	%r743, %r340, 2;
	add.s32 	%r744, %r17, %r743;
	xor.b32  	%r745, %r736, %r712;
	xor.b32  	%r746, %r745, %r739;
	xor.b32  	%r747, %r746, %r742;
	ld.shared.u32 	%r748, [%r744];
	xor.b32  	%r410, %r747, %r748;
	// inline asm
	bfe.u32 %r343, %r365, %r597, 8;
	// inline asm
	shl.b32 	%r749, %r343, 2;
	add.s32 	%r750, %r14, %r749;
	ld.shared.u32 	%r751, [%r750];
	// inline asm
	bfe.u32 %r346, %r356, %r12, 8;
	// inline asm
	shl.b32 	%r752, %r346, 2;
	add.s32 	%r753, %r15, %r752;
	ld.shared.u32 	%r754, [%r753];
	// inline asm
	bfe.u32 %r349, %r359, %r111, 8;
	// inline asm
	shl.b32 	%r755, %r349, 2;
	add.s32 	%r756, %r16, %r755;
	ld.shared.u32 	%r757, [%r756];
	// inline asm
	bfe.u32 %r352, %r362, %r13, 8;
	// inline asm
	shl.b32 	%r758, %r352, 2;
	add.s32 	%r759, %r17, %r758;
	xor.b32  	%r760, %r751, %r713;
	xor.b32  	%r761, %r760, %r754;
	xor.b32  	%r762, %r761, %r757;
	ld.shared.u32 	%r763, [%r759];
	xor.b32  	%r413, %r762, %r763;
	// inline asm
	bfe.u32 %r355, %r356, %r597, 8;
	// inline asm
	shl.b32 	%r764, %r355, 2;
	add.s32 	%r765, %r14, %r764;
	ld.shared.u32 	%r766, [%r765];
	// inline asm
	bfe.u32 %r358, %r359, %r12, 8;
	// inline asm
	shl.b32 	%r767, %r358, 2;
	add.s32 	%r768, %r15, %r767;
	ld.shared.u32 	%r769, [%r768];
	// inline asm
	bfe.u32 %r361, %r362, %r111, 8;
	// inline asm
	shl.b32 	%r770, %r361, 2;
	add.s32 	%r771, %r16, %r770;
	ld.shared.u32 	%r772, [%r771];
	// inline asm
	bfe.u32 %r364, %r365, %r13, 8;
	// inline asm
	shl.b32 	%r773, %r364, 2;
	add.s32 	%r774, %r17, %r773;
	xor.b32  	%r775, %r766, %r714;
	xor.b32  	%r776, %r775, %r769;
	xor.b32  	%r777, %r776, %r772;
	ld.shared.u32 	%r778, [%r774];
	xor.b32  	%r404, %r777, %r778;
	ld.global.v4.u32 	{%r779, %r780, %r781, %r782}, [%rd25+192];
	// inline asm
	bfe.u32 %r367, %r407, %r597, 8;
	// inline asm
	shl.b32 	%r787, %r367, 2;
	add.s32 	%r788, %r14, %r787;
	ld.shared.u32 	%r789, [%r788];
	// inline asm
	bfe.u32 %r370, %r410, %r12, 8;
	// inline asm
	shl.b32 	%r790, %r370, 2;
	add.s32 	%r791, %r15, %r790;
	ld.shared.u32 	%r792, [%r791];
	// inline asm
	bfe.u32 %r373, %r413, %r111, 8;
	// inline asm
	shl.b32 	%r793, %r373, 2;
	add.s32 	%r794, %r16, %r793;
	ld.shared.u32 	%r795, [%r794];
	// inline asm
	bfe.u32 %r376, %r404, %r13, 8;
	// inline asm
	shl.b32 	%r796, %r376, 2;
	add.s32 	%r797, %r17, %r796;
	xor.b32  	%r798, %r789, %r779;
	xor.b32  	%r799, %r798, %r792;
	xor.b32  	%r800, %r799, %r795;
	ld.shared.u32 	%r801, [%r797];
	xor.b32  	%r455, %r800, %r801;
	// inline asm
	bfe.u32 %r379, %r410, %r597, 8;
	// inline asm
	shl.b32 	%r802, %r379, 2;
	add.s32 	%r803, %r14, %r802;
	ld.shared.u32 	%r804, [%r803];
	// inline asm
	bfe.u32 %r382, %r413, %r12, 8;
	// inline asm
	shl.b32 	%r805, %r382, 2;
	add.s32 	%r806, %r15, %r805;
	ld.shared.u32 	%r807, [%r806];
	// inline asm
	bfe.u32 %r385, %r404, %r111, 8;
	// inline asm
	shl.b32 	%r808, %r385, 2;
	add.s32 	%r809, %r16, %r808;
	ld.shared.u32 	%r810, [%r809];
	// inline asm
	bfe.u32 %r388, %r407, %r13, 8;
	// inline asm
	shl.b32 	%r811, %r388, 2;
	add.s32 	%r812, %r17, %r811;
	xor.b32  	%r813, %r804, %r780;
	xor.b32  	%r814, %r813, %r807;
	xor.b32  	%r815, %r814, %r810;
	ld.shared.u32 	%r816, [%r812];
	xor.b32  	%r458, %r815, %r816;
	// inline asm
	bfe.u32 %r391, %r413, %r597, 8;
	// inline asm
	shl.b32 	%r817, %r391, 2;
	add.s32 	%r818, %r14, %r817;
	ld.shared.u32 	%r819, [%r818];
	// inline asm
	bfe.u32 %r394, %r404, %r12, 8;
	// inline asm
	shl.b32 	%r820, %r394, 2;
	add.s32 	%r821, %r15, %r820;
	ld.shared.u32 	%r822, [%r821];
	// inline asm
	bfe.u32 %r397, %r407, %r111, 8;
	// inline asm
	shl.b32 	%r823, %r397, 2;
	add.s32 	%r824, %r16, %r823;
	ld.shared.u32 	%r825, [%r824];
	// inline asm
	bfe.u32 %r400, %r410, %r13, 8;
	// inline asm
	shl.b32 	%r826, %r400, 2;
	add.s32 	%r827, %r17, %r826;
	xor.b32  	%r828, %r819, %r781;
	xor.b32  	%r829, %r828, %r822;
	xor.b32  	%r830, %r829, %r825;
	ld.shared.u32 	%r831, [%r827];
	xor.b32  	%r461, %r830, %r831;
	// inline asm
	bfe.u32 %r403, %r404, %r597, 8;
	// inline asm
	shl.b32 	%r832, %r403, 2;
	add.s32 	%r833, %r14, %r832;
	ld.shared.u32 	%r834, [%r833];
	// inline asm
	bfe.u32 %r406, %r407, %r12, 8;
	// inline asm
	shl.b32 	%r835, %r406, 2;
	add.s32 	%r836, %r15, %r835;
	ld.shared.u32 	%r837, [%r836];
	// inline asm
	bfe.u32 %r409, %r410, %r111, 8;
	// inline asm
	shl.b32 	%r838, %r409, 2;
	add.s32 	%r839, %r16, %r838;
	ld.shared.u32 	%r840, [%r839];
	// inline asm
	bfe.u32 %r412, %r413, %r13, 8;
	// inline asm
	shl.b32 	%r841, %r412, 2;
	add.s32 	%r842, %r17, %r841;
	xor.b32  	%r843, %r834, %r782;
	xor.b32  	%r844, %r843, %r837;
	xor.b32  	%r845, %r844, %r840;
	ld.shared.u32 	%r846, [%r842];
	xor.b32  	%r452, %r845, %r846;
	ld.global.v4.u32 	{%r847, %r848, %r849, %r850}, [%rd25+256];
	// inline asm
	bfe.u32 %r415, %r455, %r597, 8;
	// inline asm
	shl.b32 	%r855, %r415, 2;
	add.s32 	%r856, %r14, %r855;
	ld.shared.u32 	%r857, [%r856];
	// inline asm
	bfe.u32 %r418, %r458, %r12, 8;
	// inline asm
	shl.b32 	%r858, %r418, 2;
	add.s32 	%r859, %r15, %r858;
	ld.shared.u32 	%r860, [%r859];
	// inline asm
	bfe.u32 %r421, %r461, %r111, 8;
	// inline asm
	shl.b32 	%r861, %r421, 2;
	add.s32 	%r862, %r16, %r861;
	ld.shared.u32 	%r863, [%r862];
	// inline asm
	bfe.u32 %r424, %r452, %r13, 8;
	// inline asm
	shl.b32 	%r864, %r424, 2;
	add.s32 	%r865, %r17, %r864;
	xor.b32  	%r866, %r857, %r847;
	xor.b32  	%r867, %r866, %r860;
	xor.b32  	%r868, %r867, %r863;
	ld.shared.u32 	%r869, [%r865];
	xor.b32  	%r503, %r868, %r869;
	// inline asm
	bfe.u32 %r427, %r458, %r597, 8;
	// inline asm
	shl.b32 	%r870, %r427, 2;
	add.s32 	%r871, %r14, %r870;
	ld.shared.u32 	%r872, [%r871];
	// inline asm
	bfe.u32 %r430, %r461, %r12, 8;
	// inline asm
	shl.b32 	%r873, %r430, 2;
	add.s32 	%r874, %r15, %r873;
	ld.shared.u32 	%r875, [%r874];
	// inline asm
	bfe.u32 %r433, %r452, %r111, 8;
	// inline asm
	shl.b32 	%r876, %r433, 2;
	add.s32 	%r877, %r16, %r876;
	ld.shared.u32 	%r878, [%r877];
	// inline asm
	bfe.u32 %r436, %r455, %r13, 8;
	// inline asm
	shl.b32 	%r879, %r436, 2;
	add.s32 	%r880, %r17, %r879;
	xor.b32  	%r881, %r872, %r848;
	xor.b32  	%r882, %r881, %r875;
	xor.b32  	%r883, %r882, %r878;
	ld.shared.u32 	%r884, [%r880];
	xor.b32  	%r506, %r883, %r884;
	// inline asm
	bfe.u32 %r439, %r461, %r597, 8;
	// inline asm
	shl.b32 	%r885, %r439, 2;
	add.s32 	%r886, %r14, %r885;
	ld.shared.u32 	%r887, [%r886];
	// inline asm
	bfe.u32 %r442, %r452, %r12, 8;
	// inline asm
	shl.b32 	%r888, %r442, 2;
	add.s32 	%r889, %r15, %r888;
	ld.shared.u32 	%r890, [%r889];
	// inline asm
	bfe.u32 %r445, %r455, %r111, 8;
	// inline asm
	shl.b32 	%r891, %r445, 2;
	add.s32 	%r892, %r16, %r891;
	ld.shared.u32 	%r893, [%r892];
	// inline asm
	bfe.u32 %r448, %r458, %r13, 8;
	// inline asm
	shl.b32 	%r894, %r448, 2;
	add.s32 	%r895, %r17, %r894;
	xor.b32  	%r896, %r887, %r849;
	xor.b32  	%r897, %r896, %r890;
	xor.b32  	%r898, %r897, %r893;
	ld.shared.u32 	%r899, [%r895];
	xor.b32  	%r509, %r898, %r899;
	// inline asm
	bfe.u32 %r451, %r452, %r597, 8;
	// inline asm
	shl.b32 	%r900, %r451, 2;
	add.s32 	%r901, %r14, %r900;
	ld.shared.u32 	%r902, [%r901];
	// inline asm
	bfe.u32 %r454, %r455, %r12, 8;
	// inline asm
	shl.b32 	%r903, %r454, 2;
	add.s32 	%r904, %r15, %r903;
	ld.shared.u32 	%r905, [%r904];
	// inline asm
	bfe.u32 %r457, %r458, %r111, 8;
	// inline asm
	shl.b32 	%r906, %r457, 2;
	add.s32 	%r907, %r16, %r906;
	ld.shared.u32 	%r908, [%r907];
	// inline asm
	bfe.u32 %r460, %r461, %r13, 8;
	// inline asm
	shl.b32 	%r909, %r460, 2;
	add.s32 	%r910, %r17, %r909;
	xor.b32  	%r911, %r902, %r850;
	xor.b32  	%r912, %r911, %r905;
	xor.b32  	%r913, %r912, %r908;
	ld.shared.u32 	%r914, [%r910];
	xor.b32  	%r500, %r913, %r914;
	ld.global.v4.u32 	{%r915, %r916, %r917, %r918}, [%rd25+320];
	// inline asm
	bfe.u32 %r463, %r503, %r597, 8;
	// inline asm
	shl.b32 	%r923, %r463, 2;
	add.s32 	%r924, %r14, %r923;
	ld.shared.u32 	%r925, [%r924];
	// inline asm
	bfe.u32 %r466, %r506, %r12, 8;
	// inline asm
	shl.b32 	%r926, %r466, 2;
	add.s32 	%r927, %r15, %r926;
	ld.shared.u32 	%r928, [%r927];
	// inline asm
	bfe.u32 %r469, %r509, %r111, 8;
	// inline asm
	shl.b32 	%r929, %r469, 2;
	add.s32 	%r930, %r16, %r929;
	ld.shared.u32 	%r931, [%r930];
	// inline asm
	bfe.u32 %r472, %r500, %r13, 8;
	// inline asm
	shl.b32 	%r932, %r472, 2;
	add.s32 	%r933, %r17, %r932;
	xor.b32  	%r934, %r925, %r915;
	xor.b32  	%r935, %r934, %r928;
	xor.b32  	%r936, %r935, %r931;
	ld.shared.u32 	%r937, [%r933];
	xor.b32  	%r551, %r936, %r937;
	// inline asm
	bfe.u32 %r475, %r506, %r597, 8;
	// inline asm
	shl.b32 	%r938, %r475, 2;
	add.s32 	%r939, %r14, %r938;
	ld.shared.u32 	%r940, [%r939];
	// inline asm
	bfe.u32 %r478, %r509, %r12, 8;
	// inline asm
	shl.b32 	%r941, %r478, 2;
	add.s32 	%r942, %r15, %r941;
	ld.shared.u32 	%r943, [%r942];
	// inline asm
	bfe.u32 %r481, %r500, %r111, 8;
	// inline asm
	shl.b32 	%r944, %r481, 2;
	add.s32 	%r945, %r16, %r944;
	ld.shared.u32 	%r946, [%r945];
	// inline asm
	bfe.u32 %r484, %r503, %r13, 8;
	// inline asm
	shl.b32 	%r947, %r484, 2;
	add.s32 	%r948, %r17, %r947;
	xor.b32  	%r949, %r940, %r916;
	xor.b32  	%r950, %r949, %r943;
	xor.b32  	%r951, %r950, %r946;
	ld.shared.u32 	%r952, [%r948];
	xor.b32  	%r554, %r951, %r952;
	// inline asm
	bfe.u32 %r487, %r509, %r597, 8;
	// inline asm
	shl.b32 	%r953, %r487, 2;
	add.s32 	%r954, %r14, %r953;
	ld.shared.u32 	%r955, [%r954];
	// inline asm
	bfe.u32 %r490, %r500, %r12, 8;
	// inline asm
	shl.b32 	%r956, %r490, 2;
	add.s32 	%r957, %r15, %r956;
	ld.shared.u32 	%r958, [%r957];
	// inline asm
	bfe.u32 %r493, %r503, %r111, 8;
	// inline asm
	shl.b32 	%r959, %r493, 2;
	add.s32 	%r960, %r16, %r959;
	ld.shared.u32 	%r961, [%r960];
	// inline asm
	bfe.u32 %r496, %r506, %r13, 8;
	// inline asm
	shl.b32 	%r962, %r496, 2;
	add.s32 	%r963, %r17, %r962;
	xor.b32  	%r964, %r955, %r917;
	xor.b32  	%r965, %r964, %r958;
	xor.b32  	%r966, %r965, %r961;
	ld.shared.u32 	%r967, [%r963];
	xor.b32  	%r557, %r966, %r967;
	// inline asm
	bfe.u32 %r499, %r500, %r597, 8;
	// inline asm
	shl.b32 	%r968, %r499, 2;
	add.s32 	%r969, %r14, %r968;
	ld.shared.u32 	%r970, [%r969];
	// inline asm
	bfe.u32 %r502, %r503, %r12, 8;
	// inline asm
	shl.b32 	%r971, %r502, 2;
	add.s32 	%r972, %r15, %r971;
	ld.shared.u32 	%r973, [%r972];
	// inline asm
	bfe.u32 %r505, %r506, %r111, 8;
	// inline asm
	shl.b32 	%r974, %r505, 2;
	add.s32 	%r975, %r16, %r974;
	ld.shared.u32 	%r976, [%r975];
	// inline asm
	bfe.u32 %r508, %r509, %r13, 8;
	// inline asm
	shl.b32 	%r977, %r508, 2;
	add.s32 	%r978, %r17, %r977;
	xor.b32  	%r979, %r970, %r918;
	xor.b32  	%r980, %r979, %r973;
	xor.b32  	%r981, %r980, %r976;
	ld.shared.u32 	%r982, [%r978];
	xor.b32  	%r548, %r981, %r982;
	ld.global.v4.u32 	{%r983, %r984, %r985, %r986}, [%rd25+384];
	// inline asm
	bfe.u32 %r511, %r551, %r597, 8;
	// inline asm
	shl.b32 	%r991, %r511, 2;
	add.s32 	%r992, %r14, %r991;
	ld.shared.u32 	%r993, [%r992];
	// inline asm
	bfe.u32 %r514, %r554, %r12, 8;
	// inline asm
	shl.b32 	%r994, %r514, 2;
	add.s32 	%r995, %r15, %r994;
	ld.shared.u32 	%r996, [%r995];
	// inline asm
	bfe.u32 %r517, %r557, %r111, 8;
	// inline asm
	shl.b32 	%r997, %r517, 2;
	add.s32 	%r998, %r16, %r997;
	ld.shared.u32 	%r999, [%r998];
	// inline asm
	bfe.u32 %r520, %r548, %r13, 8;
	// inline asm
	shl.b32 	%r1000, %r520, 2;
	add.s32 	%r1001, %r17, %r1000;
	xor.b32  	%r1002, %r993, %r983;
	xor.b32  	%r1003, %r1002, %r996;
	xor.b32  	%r1004, %r1003, %r999;
	ld.shared.u32 	%r1005, [%r1001];
	xor.b32  	%r599, %r1004, %r1005;
	// inline asm
	bfe.u32 %r523, %r554, %r597, 8;
	// inline asm
	shl.b32 	%r1006, %r523, 2;
	add.s32 	%r1007, %r14, %r1006;
	ld.shared.u32 	%r1008, [%r1007];
	// inline asm
	bfe.u32 %r526, %r557, %r12, 8;
	// inline asm
	shl.b32 	%r1009, %r526, 2;
	add.s32 	%r1010, %r15, %r1009;
	ld.shared.u32 	%r1011, [%r1010];
	// inline asm
	bfe.u32 %r529, %r548, %r111, 8;
	// inline asm
	shl.b32 	%r1012, %r529, 2;
	add.s32 	%r1013, %r16, %r1012;
	ld.shared.u32 	%r1014, [%r1013];
	// inline asm
	bfe.u32 %r532, %r551, %r13, 8;
	// inline asm
	shl.b32 	%r1015, %r532, 2;
	add.s32 	%r1016, %r17, %r1015;
	xor.b32  	%r1017, %r1008, %r984;
	xor.b32  	%r1018, %r1017, %r1011;
	xor.b32  	%r1019, %r1018, %r1014;
	ld.shared.u32 	%r1020, [%r1016];
	xor.b32  	%r602, %r1019, %r1020;
	// inline asm
	bfe.u32 %r535, %r557, %r597, 8;
	// inline asm
	shl.b32 	%r1021, %r535, 2;
	add.s32 	%r1022, %r14, %r1021;
	ld.shared.u32 	%r1023, [%r1022];
	// inline asm
	bfe.u32 %r538, %r548, %r12, 8;
	// inline asm
	shl.b32 	%r1024, %r538, 2;
	add.s32 	%r1025, %r15, %r1024;
	ld.shared.u32 	%r1026, [%r1025];
	// inline asm
	bfe.u32 %r541, %r551, %r111, 8;
	// inline asm
	shl.b32 	%r1027, %r541, 2;
	add.s32 	%r1028, %r16, %r1027;
	ld.shared.u32 	%r1029, [%r1028];
	// inline asm
	bfe.u32 %r544, %r554, %r13, 8;
	// inline asm
	shl.b32 	%r1030, %r544, 2;
	add.s32 	%r1031, %r17, %r1030;
	xor.b32  	%r1032, %r1023, %r985;
	xor.b32  	%r1033, %r1032, %r1026;
	xor.b32  	%r1034, %r1033, %r1029;
	ld.shared.u32 	%r1035, [%r1031];
	xor.b32  	%r605, %r1034, %r1035;
	// inline asm
	bfe.u32 %r547, %r548, %r597, 8;
	// inline asm
	shl.b32 	%r1036, %r547, 2;
	add.s32 	%r1037, %r14, %r1036;
	ld.shared.u32 	%r1038, [%r1037];
	// inline asm
	bfe.u32 %r550, %r551, %r12, 8;
	// inline asm
	shl.b32 	%r1039, %r550, 2;
	add.s32 	%r1040, %r15, %r1039;
	ld.shared.u32 	%r1041, [%r1040];
	// inline asm
	bfe.u32 %r553, %r554, %r111, 8;
	// inline asm
	shl.b32 	%r1042, %r553, 2;
	add.s32 	%r1043, %r16, %r1042;
	ld.shared.u32 	%r1044, [%r1043];
	// inline asm
	bfe.u32 %r556, %r557, %r13, 8;
	// inline asm
	shl.b32 	%r1045, %r556, 2;
	add.s32 	%r1046, %r17, %r1045;
	xor.b32  	%r1047, %r1038, %r986;
	xor.b32  	%r1048, %r1047, %r1041;
	xor.b32  	%r1049, %r1048, %r1044;
	ld.shared.u32 	%r1050, [%r1046];
	xor.b32  	%r596, %r1049, %r1050;
	ld.global.v4.u32 	{%r1051, %r1052, %r1053, %r1054}, [%rd25+448];
	// inline asm
	bfe.u32 %r559, %r599, %r597, 8;
	// inline asm
	shl.b32 	%r1059, %r559, 2;
	add.s32 	%r1060, %r14, %r1059;
	ld.shared.u32 	%r1061, [%r1060];
	// inline asm
	bfe.u32 %r562, %r602, %r12, 8;
	// inline asm
	shl.b32 	%r1062, %r562, 2;
	add.s32 	%r1063, %r15, %r1062;
	ld.shared.u32 	%r1064, [%r1063];
	// inline asm
	bfe.u32 %r565, %r605, %r111, 8;
	// inline asm
	shl.b32 	%r1065, %r565, 2;
	add.s32 	%r1066, %r16, %r1065;
	ld.shared.u32 	%r1067, [%r1066];
	// inline asm
	bfe.u32 %r568, %r596, %r13, 8;
	// inline asm
	shl.b32 	%r1068, %r568, 2;
	add.s32 	%r1069, %r17, %r1068;
	xor.b32  	%r1070, %r1061, %r1051;
	xor.b32  	%r1071, %r1070, %r1064;
	xor.b32  	%r1072, %r1071, %r1067;
	ld.shared.u32 	%r1073, [%r1069];
	xor.b32  	%r1120, %r1072, %r1073;
	// inline asm
	bfe.u32 %r571, %r602, %r597, 8;
	// inline asm
	shl.b32 	%r1074, %r571, 2;
	add.s32 	%r1075, %r14, %r1074;
	ld.shared.u32 	%r1076, [%r1075];
	// inline asm
	bfe.u32 %r574, %r605, %r12, 8;
	// inline asm
	shl.b32 	%r1077, %r574, 2;
	add.s32 	%r1078, %r15, %r1077;
	ld.shared.u32 	%r1079, [%r1078];
	// inline asm
	bfe.u32 %r577, %r596, %r111, 8;
	// inline asm
	shl.b32 	%r1080, %r577, 2;
	add.s32 	%r1081, %r16, %r1080;
	ld.shared.u32 	%r1082, [%r1081];
	// inline asm
	bfe.u32 %r580, %r599, %r13, 8;
	// inline asm
	shl.b32 	%r1083, %r580, 2;
	add.s32 	%r1084, %r17, %r1083;
	xor.b32  	%r1085, %r1076, %r1052;
	xor.b32  	%r1086, %r1085, %r1079;
	xor.b32  	%r1087, %r1086, %r1082;
	ld.shared.u32 	%r1088, [%r1084];
	xor.b32  	%r1121, %r1087, %r1088;
	// inline asm
	bfe.u32 %r583, %r605, %r597, 8;
	// inline asm
	shl.b32 	%r1089, %r583, 2;
	add.s32 	%r1090, %r14, %r1089;
	ld.shared.u32 	%r1091, [%r1090];
	// inline asm
	bfe.u32 %r586, %r596, %r12, 8;
	// inline asm
	shl.b32 	%r1092, %r586, 2;
	add.s32 	%r1093, %r15, %r1092;
	ld.shared.u32 	%r1094, [%r1093];
	// inline asm
	bfe.u32 %r589, %r599, %r111, 8;
	// inline asm
	shl.b32 	%r1095, %r589, 2;
	add.s32 	%r1096, %r16, %r1095;
	ld.shared.u32 	%r1097, [%r1096];
	// inline asm
	bfe.u32 %r592, %r602, %r13, 8;
	// inline asm
	shl.b32 	%r1098, %r592, 2;
	add.s32 	%r1099, %r17, %r1098;
	xor.b32  	%r1100, %r1091, %r1053;
	xor.b32  	%r1101, %r1100, %r1094;
	xor.b32  	%r1102, %r1101, %r1097;
	ld.shared.u32 	%r1103, [%r1099];
	xor.b32  	%r1122, %r1102, %r1103;
	// inline asm
	bfe.u32 %r595, %r596, %r597, 8;
	// inline asm
	shl.b32 	%r1104, %r595, 2;
	add.s32 	%r1105, %r14, %r1104;
	ld.shared.u32 	%r1106, [%r1105];
	// inline asm
	bfe.u32 %r598, %r599, %r12, 8;
	// inline asm
	shl.b32 	%r1107, %r598, 2;
	add.s32 	%r1108, %r15, %r1107;
	ld.shared.u32 	%r1109, [%r1108];
	// inline asm
	bfe.u32 %r601, %r602, %r111, 8;
	// inline asm
	shl.b32 	%r1110, %r601, 2;
	add.s32 	%r1111, %r16, %r1110;
	ld.shared.u32 	%r1112, [%r1111];
	// inline asm
	bfe.u32 %r604, %r605, %r13, 8;
	// inline asm
	shl.b32 	%r1113, %r604, 2;
	add.s32 	%r1114, %r17, %r1113;
	xor.b32  	%r1115, %r1106, %r1054;
	xor.b32  	%r1116, %r1115, %r1109;
	xor.b32  	%r1117, %r1116, %r1112;
	ld.shared.u32 	%r1118, [%r1114];
	xor.b32  	%r1123, %r1117, %r1118;
	add.s32 	%r1124, %r1124, 32;
	add.s64 	%rd25, %rd25, 512;

BB18_4:
	// inline asm
	bfe.u32 %r67, %r1120, %r50, 8;
	// inline asm
	shl.b32 	%r115, %r67, 2;
	add.s32 	%r23, %r14, %r115;
	// inline asm
	bfe.u32 %r70, %r1121, %r12, 8;
	// inline asm
	shl.b32 	%r116, %r70, 2;
	add.s32 	%r24, %r15, %r116;
	mov.u32 	%r111, 16;
	// inline asm
	bfe.u32 %r73, %r1122, %r111, 8;
	// inline asm
	shl.b32 	%r117, %r73, 2;
	add.s32 	%r25, %r16, %r117;
	// inline asm
	bfe.u32 %r76, %r1123, %r13, 8;
	// inline asm
	shl.b32 	%r118, %r76, 2;
	add.s32 	%r26, %r17, %r118;
	// inline asm
	bfe.u32 %r79, %r1121, %r50, 8;
	// inline asm
	shl.b32 	%r119, %r79, 2;
	add.s32 	%r27, %r14, %r119;
	// inline asm
	bfe.u32 %r82, %r1122, %r12, 8;
	// inline asm
	shl.b32 	%r120, %r82, 2;
	add.s32 	%r28, %r15, %r120;
	// inline asm
	bfe.u32 %r85, %r1123, %r111, 8;
	// inline asm
	shl.b32 	%r121, %r85, 2;
	add.s32 	%r29, %r16, %r121;
	// inline asm
	bfe.u32 %r88, %r1120, %r13, 8;
	// inline asm
	shl.b32 	%r122, %r88, 2;
	add.s32 	%r30, %r17, %r122;
	// inline asm
	bfe.u32 %r91, %r1122, %r50, 8;
	// inline asm
	shl.b32 	%r123, %r91, 2;
	add.s32 	%r31, %r14, %r123;
	// inline asm
	bfe.u32 %r94, %r1123, %r12, 8;
	// inline asm
	shl.b32 	%r124, %r94, 2;
	add.s32 	%r32, %r15, %r124;
	// inline asm
	bfe.u32 %r97, %r1120, %r111, 8;
	// inline asm
	shl.b32 	%r125, %r97, 2;
	add.s32 	%r33, %r16, %r125;
	// inline asm
	bfe.u32 %r100, %r1121, %r13, 8;
	// inline asm
	shl.b32 	%r126, %r100, 2;
	add.s32 	%r34, %r17, %r126;
	// inline asm
	bfe.u32 %r103, %r1123, %r50, 8;
	// inline asm
	shl.b32 	%r127, %r103, 2;
	add.s32 	%r35, %r14, %r127;
	// inline asm
	bfe.u32 %r106, %r1120, %r12, 8;
	// inline asm
	shl.b32 	%r128, %r106, 2;
	add.s32 	%r36, %r15, %r128;
	// inline asm
	bfe.u32 %r109, %r1121, %r111, 8;
	// inline asm
	shl.b32 	%r129, %r109, 2;
	add.s32 	%r37, %r16, %r129;
	// inline asm
	bfe.u32 %r112, %r1122, %r13, 8;
	// inline asm
	shl.b32 	%r130, %r112, 2;
	add.s32 	%r38, %r17, %r130;
	setp.lt.u32	%p5, %r1124, 131072;
	@%p5 bra 	BB18_7;

	ld.shared.u32 	%r179, [%r23];
	ld.shared.u32 	%r180, [%r24];
	xor.b32  	%r181, %r179, %r180;
	ld.shared.u32 	%r182, [%r25];
	xor.b32  	%r183, %r181, %r182;
	ld.shared.u32 	%r184, [%r26];
	xor.b32  	%r185, %r183, %r184;
	xor.b32  	%r132, %r185, -151354487;
	ld.shared.u32 	%r186, [%r27];
	ld.shared.u32 	%r187, [%r28];
	xor.b32  	%r188, %r186, %r187;
	ld.shared.u32 	%r189, [%r29];
	xor.b32  	%r190, %r188, %r189;
	ld.shared.u32 	%r191, [%r30];
	xor.b32  	%r192, %r190, %r191;
	xor.b32  	%r135, %r192, -1960536929;
	ld.shared.u32 	%r193, [%r31];
	ld.shared.u32 	%r194, [%r32];
	xor.b32  	%r195, %r193, %r194;
	ld.shared.u32 	%r196, [%r33];
	xor.b32  	%r197, %r195, %r196;
	ld.shared.u32 	%r198, [%r34];
	xor.b32  	%r199, %r197, %r198;
	xor.b32  	%r138, %r199, -1864608065;
	ld.shared.u32 	%r200, [%r35];
	ld.shared.u32 	%r201, [%r36];
	xor.b32  	%r202, %r200, %r201;
	ld.shared.u32 	%r203, [%r37];
	xor.b32  	%r204, %r202, %r203;
	ld.shared.u32 	%r205, [%r38];
	xor.b32  	%r206, %r204, %r205;
	xor.b32  	%r141, %r206, 109642241;
	mov.u32 	%r169, 0;
	// inline asm
	bfe.u32 %r131, %r132, %r169, 8;
	// inline asm
	shl.b32 	%r207, %r131, 2;
	add.s32 	%r208, %r14, %r207;
	ld.shared.u32 	%r209, [%r208];
	// inline asm
	bfe.u32 %r134, %r135, %r12, 8;
	// inline asm
	shl.b32 	%r210, %r134, 2;
	add.s32 	%r211, %r15, %r210;
	ld.shared.u32 	%r212, [%r211];
	// inline asm
	bfe.u32 %r137, %r138, %r111, 8;
	// inline asm
	shl.b32 	%r213, %r137, 2;
	add.s32 	%r214, %r16, %r213;
	ld.shared.u32 	%r215, [%r214];
	// inline asm
	bfe.u32 %r140, %r141, %r13, 8;
	// inline asm
	shl.b32 	%r216, %r140, 2;
	add.s32 	%r217, %r17, %r216;
	xor.b32  	%r218, %r209, %r212;
	xor.b32  	%r219, %r218, %r215;
	ld.shared.u32 	%r220, [%r217];
	xor.b32  	%r221, %r219, %r220;
	// inline asm
	bfe.u32 %r143, %r135, %r169, 8;
	// inline asm
	shl.b32 	%r222, %r143, 2;
	add.s32 	%r223, %r14, %r222;
	ld.shared.u32 	%r224, [%r223];
	// inline asm
	bfe.u32 %r146, %r138, %r12, 8;
	// inline asm
	shl.b32 	%r225, %r146, 2;
	add.s32 	%r226, %r15, %r225;
	ld.shared.u32 	%r227, [%r226];
	// inline asm
	bfe.u32 %r149, %r141, %r111, 8;
	// inline asm
	shl.b32 	%r228, %r149, 2;
	add.s32 	%r229, %r16, %r228;
	ld.shared.u32 	%r230, [%r229];
	// inline asm
	bfe.u32 %r152, %r132, %r13, 8;
	// inline asm
	shl.b32 	%r231, %r152, 2;
	add.s32 	%r232, %r17, %r231;
	xor.b32  	%r233, %r224, %r227;
	xor.b32  	%r234, %r233, %r230;
	ld.shared.u32 	%r235, [%r232];
	xor.b32  	%r236, %r234, %r235;
	// inline asm
	bfe.u32 %r155, %r138, %r169, 8;
	// inline asm
	shl.b32 	%r237, %r155, 2;
	add.s32 	%r238, %r14, %r237;
	ld.shared.u32 	%r239, [%r238];
	// inline asm
	bfe.u32 %r158, %r141, %r12, 8;
	// inline asm
	shl.b32 	%r240, %r158, 2;
	add.s32 	%r241, %r15, %r240;
	ld.shared.u32 	%r242, [%r241];
	// inline asm
	bfe.u32 %r161, %r132, %r111, 8;
	// inline asm
	shl.b32 	%r243, %r161, 2;
	add.s32 	%r244, %r16, %r243;
	ld.shared.u32 	%r245, [%r244];
	// inline asm
	bfe.u32 %r164, %r135, %r13, 8;
	// inline asm
	shl.b32 	%r246, %r164, 2;
	add.s32 	%r247, %r17, %r246;
	xor.b32  	%r248, %r239, %r242;
	xor.b32  	%r249, %r248, %r245;
	ld.shared.u32 	%r250, [%r247];
	xor.b32  	%r251, %r249, %r250;
	// inline asm
	bfe.u32 %r167, %r141, %r169, 8;
	// inline asm
	shl.b32 	%r252, %r167, 2;
	add.s32 	%r253, %r14, %r252;
	ld.shared.u32 	%r254, [%r253];
	// inline asm
	bfe.u32 %r170, %r132, %r12, 8;
	// inline asm
	shl.b32 	%r255, %r170, 2;
	add.s32 	%r256, %r15, %r255;
	ld.shared.u32 	%r257, [%r256];
	// inline asm
	bfe.u32 %r173, %r135, %r111, 8;
	// inline asm
	shl.b32 	%r258, %r173, 2;
	add.s32 	%r259, %r16, %r258;
	ld.shared.u32 	%r260, [%r259];
	// inline asm
	bfe.u32 %r176, %r138, %r13, 8;
	// inline asm
	shl.b32 	%r261, %r176, 2;
	add.s32 	%r262, %r17, %r261;
	xor.b32  	%r263, %r254, %r257;
	xor.b32  	%r264, %r263, %r260;
	ld.shared.u32 	%r265, [%r262];
	xor.b32  	%r266, %r264, %r265;
	shl.b64 	%rd20, %rd1, 4;
	add.s64 	%rd21, %rd2, %rd20;
	cvta.to.global.u64 	%rd22, %rd7;
	shl.b64 	%rd23, %rd21, 4;
	add.s64 	%rd24, %rd22, %rd23;
	xor.b32  	%r267, %r266, -317130341;
	xor.b32  	%r268, %r251, -300923962;
	xor.b32  	%r269, %r236, 1375002684;
	xor.b32  	%r270, %r221, 1639080913;
	st.global.v4.u32 	[%rd24+192], {%r270, %r269, %r268, %r267};

BB18_6:
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2745>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 256;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 32;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249095;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301160;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	add.s64 	%rd2698, %rd2651, %rd1381;
	add.s64 	%rd2699, %rd2698, %rd2670;
	xor.b64  	%rd2700, %rd2699, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2700;
	mov.b64	%rd2701, {%r3436, %r3435};
	add.s64 	%rd2702, %rd2701, %rd2682;
	xor.b64  	%rd2703, %rd2702, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2703;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2704, {%r3440, %r3439};
	add.s64 	%rd2705, %rd2699, %rd1383;
	add.s64 	%rd2706, %rd2705, %rd2704;
	xor.b64  	%rd2707, %rd2706, %rd2701;
	mov.b64	{%r3441, %r3442}, %rd2707;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2708, {%r3444, %r3443};
	add.s64 	%rd2709, %rd2708, %rd2702;
	xor.b64  	%rd2710, %rd2709, %rd2704;
	mov.b64	{%r1518, %r1519}, %rd2710;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	add.s64 	%rd2711, %rd2665, %rd1392;
	add.s64 	%rd2712, %rd2711, %rd2684;
	xor.b64  	%rd2713, %rd2712, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2713;
	mov.b64	%rd2714, {%r3446, %r3445};
	add.s64 	%rd2715, %rd2714, %rd2640;
	xor.b64  	%rd2716, %rd2715, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2716;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2717, {%r3450, %r3449};
	add.s64 	%rd2718, %rd2712, %rd1388;
	add.s64 	%rd2719, %rd2718, %rd2717;
	xor.b64  	%rd2720, %rd2719, %rd2714;
	mov.b64	{%r3451, %r3452}, %rd2720;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2721, {%r3454, %r3453};
	add.s64 	%rd2722, %rd2721, %rd2715;
	xor.b64  	%rd2723, %rd2722, %rd2717;
	mov.b64	{%r1526, %r1527}, %rd2723;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	add.s64 	%rd2724, %rd2642, %rd1386;
	add.s64 	%rd2725, %rd2724, %rd2679;
	xor.b64  	%rd2726, %rd2725, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2726;
	mov.b64	%rd2727, {%r3456, %r3455};
	add.s64 	%rd2728, %rd2727, %rd2654;
	xor.b64  	%rd2729, %rd2728, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2729;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2730, {%r3460, %r3459};
	add.s64 	%rd2731, %rd2725, %rd1384;
	add.s64 	%rd2732, %rd2731, %rd2730;
	xor.b64  	%rd2733, %rd2732, %rd2727;
	mov.b64	{%r3461, %r3462}, %rd2733;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2734, {%r3464, %r3463};
	add.s64 	%rd2735, %rd2734, %rd2728;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r1534, %r1535}, %rd2736;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	xor.b64  	%rd2737, %rd2693, %rd1366;
	xor.b64  	%rd2738, %rd2737, %rd2722;
	st.global.u64 	[%rd8], %rd2738;
	xor.b64  	%rd2739, %rd2706, %rd1368;
	xor.b64  	%rd2740, %rd2739, %rd2735;
	st.global.u64 	[%rd8+8], %rd2740;
	xor.b64  	%rd2741, %rd2696, %rd1370;
	xor.b64  	%rd2742, %rd2741, %rd2719;
	st.global.u64 	[%rd8+16], %rd2742;
	xor.b64  	%rd2743, %rd2709, %rd1372;
	xor.b64  	%rd2744, %rd2743, %rd2732;
	st.global.u64 	[%rd8+24], %rd2744;
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2757>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 256;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 64;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249063;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301192;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	mov.b64	%rd2698, {%r1505, %r1509};
	add.s64 	%rd2699, %rd2651, %rd1381;
	add.s64 	%rd2700, %rd2699, %rd2670;
	xor.b64  	%rd2701, %rd2700, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2701;
	mov.b64	%rd2702, {%r3436, %r3435};
	add.s64 	%rd2703, %rd2702, %rd2682;
	xor.b64  	%rd2704, %rd2703, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2704;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2705, {%r3440, %r3439};
	add.s64 	%rd2706, %rd2700, %rd1383;
	add.s64 	%rd2707, %rd2706, %rd2705;
	xor.b64  	%rd2708, %rd2707, %rd2702;
	mov.b64	{%r3441, %r3442}, %rd2708;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2709, {%r3444, %r3443};
	add.s64 	%rd2710, %rd2709, %rd2703;
	xor.b64  	%rd2711, %rd2710, %rd2705;
	mov.b64	{%r1518, %r1519}, %rd2711;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	mov.b64	%rd2712, {%r1513, %r1517};
	add.s64 	%rd2713, %rd2665, %rd1392;
	add.s64 	%rd2714, %rd2713, %rd2684;
	xor.b64  	%rd2715, %rd2714, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2715;
	mov.b64	%rd2716, {%r3446, %r3445};
	add.s64 	%rd2717, %rd2716, %rd2640;
	xor.b64  	%rd2718, %rd2717, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2718;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2719, {%r3450, %r3449};
	add.s64 	%rd2720, %rd2714, %rd1388;
	add.s64 	%rd2721, %rd2720, %rd2719;
	xor.b64  	%rd2722, %rd2721, %rd2716;
	mov.b64	{%r3451, %r3452}, %rd2722;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2723, {%r3454, %r3453};
	add.s64 	%rd2724, %rd2723, %rd2717;
	xor.b64  	%rd2725, %rd2724, %rd2719;
	mov.b64	{%r1526, %r1527}, %rd2725;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	mov.b64	%rd2726, {%r1521, %r1525};
	add.s64 	%rd2727, %rd2642, %rd1386;
	add.s64 	%rd2728, %rd2727, %rd2679;
	xor.b64  	%rd2729, %rd2728, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2729;
	mov.b64	%rd2730, {%r3456, %r3455};
	add.s64 	%rd2731, %rd2730, %rd2654;
	xor.b64  	%rd2732, %rd2731, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2732;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2733, {%r3460, %r3459};
	add.s64 	%rd2734, %rd2728, %rd1384;
	add.s64 	%rd2735, %rd2734, %rd2733;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r3461, %r3462}, %rd2736;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2737, {%r3464, %r3463};
	add.s64 	%rd2738, %rd2737, %rd2731;
	xor.b64  	%rd2739, %rd2738, %rd2733;
	mov.b64	{%r1534, %r1535}, %rd2739;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	mov.b64	%rd2740, {%r1529, %r1533};
	xor.b64  	%rd2741, %rd2693, %rd1366;
	xor.b64  	%rd2742, %rd2741, %rd2724;
	st.global.u64 	[%rd8], %rd2742;
	xor.b64  	%rd2743, %rd2707, %rd1368;
	xor.b64  	%rd2744, %rd2743, %rd2738;
	st.global.u64 	[%rd8+8], %rd2744;
	xor.b64  	%rd2745, %rd2696, %rd1370;
	xor.b64  	%rd2746, %rd2745, %rd2721;
	st.global.u64 	[%rd8+16], %rd2746;
	xor.b64  	%rd2747, %rd2710, %rd1372;
	xor.b64  	%rd2748, %rd2747, %rd2735;
	st.global.u64 	[%rd8+24], %rd2748;
	xor.b64  	%rd2749, %rd2709, %rd1374;
	xor.b64  	%rd2750, %rd2749, %rd2740;
	st.global.u64 	[%rd8+32], %rd2750;
	xor.b64  	%rd2751, %rd2698, %rd1376;
	xor.b64  	%rd2752, %rd2751, %rd2723;
	st.global.u64 	[%rd8+40], %rd2752;
	xor.b64  	%rd2753, %rd2712, %rd1378;
	xor.b64  	%rd2754, %rd2753, %rd2737;
	st.global.u64 	[%rd8+48], %rd2754;
	xor.b64  	%rd2755, %rd2695, %rd1380;
	xor.b64  	%rd2756, %rd2755, %rd2726;
	st.global.u64 	[%rd8+56], %rd2756;
	ret;
}

	// .globl	_Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj
.visible .entry _Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj(
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj_param_0,
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj_param_1,
	.param .u32 _Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<1125>;
	.reg .b64 	%rd<26>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvjE1T[8192];

	ld.param.u64 	%rd6, [_Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj_param_0];
	ld.param.u64 	%rd7, [_Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj_param_1];
	ld.param.u32 	%r44, [_Z11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvj_param_2];
	shl.b32 	%r45, %r44, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r1119, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r1119;
	setp.ge.u32	%p1, %r4, %r45;
	@%p1 bra 	BB21_6;

	setp.gt.s32	%p2, %r1119, 2047;
	@%p2 bra 	BB21_3;

BB21_2:
	mul.wide.s32 	%rd8, %r1119, 4;
	mov.u64 	%rd9, AES_TABLE;
	add.s64 	%rd10, %rd9, %rd8;
	ld.const.u32 	%r46, [%rd10];
	shl.b32 	%r47, %r1119, 2;
	mov.u32 	%r48, _ZZ11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvjE1T;
	add.s32 	%r49, %r48, %r47;
	st.shared.u32 	[%r49], %r46;
	add.s32 	%r1119, %r1119, %r1;
	setp.lt.s32	%p3, %r1119, 2048;
	@%p3 bra 	BB21_2;

BB21_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r51, %r7, 2;
	mul.wide.u32 	%rd11, %r51, 4;
	mov.u64 	%rd12, AES_STATE_HASH;
	add.s64 	%rd13, %rd12, %rd11;
	ld.const.u32 	%r1120, [%rd13];
	ld.const.u32 	%r1121, [%rd13+4];
	ld.const.u32 	%r1122, [%rd13+8];
	ld.const.u32 	%r1123, [%rd13+12];
	and.b32  	%r52, %r4, 1;
	shl.b32 	%r53, %r4, 4;
	and.b32  	%r54, %r53, 16;
	add.s32 	%r12, %r54, 8;
	xor.b32  	%r55, %r54, 16;
	add.s32 	%r13, %r55, 8;
	shr.u32 	%r56, %r4, 2;
	cvt.u64.u32	%rd1, %r56;
	cvt.u64.u32	%rd2, %r7;
	setp.eq.s32	%p4, %r52, 0;
	mov.u32 	%r50, 0;
	mov.u32 	%r57, _ZZ11hashAes1Rx4ILy2097152ELj0ELj64ELy64EEvPKvPvjE1T;
	add.s32 	%r58, %r57, 4096;
	selp.b32	%r14, %r57, %r58, %p4;
	add.s32 	%r59, %r57, 7168;
	add.s32 	%r60, %r57, 1024;
	selp.b32	%r15, %r60, %r59, %p4;
	add.s32 	%r61, %r57, 6144;
	add.s32 	%r62, %r57, 2048;
	selp.b32	%r16, %r62, %r61, %p4;
	add.s32 	%r63, %r57, 5120;
	add.s32 	%r64, %r57, 3072;
	selp.b32	%r17, %r64, %r63, %p4;
	mul.wide.u32 	%rd14, %r56, 131076;
	cvt.u64.u32	%rd15, %r4;
	and.b64  	%rd16, %rd15, 3;
	or.b64  	%rd17, %rd14, %rd16;
	cvta.to.global.u64 	%rd18, %rd6;
	shl.b64 	%rd19, %rd17, 4;
	add.s64 	%rd25, %rd18, %rd19;
	mov.u32 	%r1124, %r50;
	bra.uni 	BB21_4;

BB21_7:
	ld.global.v4.u32 	{%r607, %r608, %r609, %r610}, [%rd25];
	mov.u32 	%r597, 0;
	ld.shared.u32 	%r615, [%r23];
	xor.b32  	%r616, %r607, %r615;
	ld.shared.u32 	%r617, [%r24];
	xor.b32  	%r618, %r616, %r617;
	ld.shared.u32 	%r619, [%r25];
	xor.b32  	%r620, %r618, %r619;
	ld.shared.u32 	%r621, [%r26];
	xor.b32  	%r311, %r620, %r621;
	ld.shared.u32 	%r622, [%r27];
	xor.b32  	%r623, %r622, %r608;
	ld.shared.u32 	%r624, [%r28];
	xor.b32  	%r625, %r623, %r624;
	ld.shared.u32 	%r626, [%r29];
	xor.b32  	%r627, %r625, %r626;
	ld.shared.u32 	%r628, [%r30];
	xor.b32  	%r314, %r627, %r628;
	ld.shared.u32 	%r629, [%r31];
	xor.b32  	%r630, %r629, %r609;
	ld.shared.u32 	%r631, [%r32];
	xor.b32  	%r632, %r630, %r631;
	ld.shared.u32 	%r633, [%r33];
	xor.b32  	%r634, %r632, %r633;
	ld.shared.u32 	%r635, [%r34];
	xor.b32  	%r317, %r634, %r635;
	ld.shared.u32 	%r636, [%r35];
	xor.b32  	%r637, %r636, %r610;
	ld.shared.u32 	%r638, [%r36];
	xor.b32  	%r639, %r637, %r638;
	ld.shared.u32 	%r640, [%r37];
	xor.b32  	%r641, %r639, %r640;
	ld.shared.u32 	%r642, [%r38];
	xor.b32  	%r308, %r641, %r642;
	ld.global.v4.u32 	{%r643, %r644, %r645, %r646}, [%rd25+64];
	// inline asm
	bfe.u32 %r271, %r311, %r597, 8;
	// inline asm
	shl.b32 	%r651, %r271, 2;
	add.s32 	%r652, %r14, %r651;
	ld.shared.u32 	%r653, [%r652];
	// inline asm
	bfe.u32 %r274, %r314, %r12, 8;
	// inline asm
	shl.b32 	%r654, %r274, 2;
	add.s32 	%r655, %r15, %r654;
	ld.shared.u32 	%r656, [%r655];
	// inline asm
	bfe.u32 %r277, %r317, %r111, 8;
	// inline asm
	shl.b32 	%r657, %r277, 2;
	add.s32 	%r658, %r16, %r657;
	ld.shared.u32 	%r659, [%r658];
	// inline asm
	bfe.u32 %r280, %r308, %r13, 8;
	// inline asm
	shl.b32 	%r660, %r280, 2;
	add.s32 	%r661, %r17, %r660;
	xor.b32  	%r662, %r653, %r643;
	xor.b32  	%r663, %r662, %r656;
	xor.b32  	%r664, %r663, %r659;
	ld.shared.u32 	%r665, [%r661];
	xor.b32  	%r359, %r664, %r665;
	// inline asm
	bfe.u32 %r283, %r314, %r597, 8;
	// inline asm
	shl.b32 	%r666, %r283, 2;
	add.s32 	%r667, %r14, %r666;
	ld.shared.u32 	%r668, [%r667];
	// inline asm
	bfe.u32 %r286, %r317, %r12, 8;
	// inline asm
	shl.b32 	%r669, %r286, 2;
	add.s32 	%r670, %r15, %r669;
	ld.shared.u32 	%r671, [%r670];
	// inline asm
	bfe.u32 %r289, %r308, %r111, 8;
	// inline asm
	shl.b32 	%r672, %r289, 2;
	add.s32 	%r673, %r16, %r672;
	ld.shared.u32 	%r674, [%r673];
	// inline asm
	bfe.u32 %r292, %r311, %r13, 8;
	// inline asm
	shl.b32 	%r675, %r292, 2;
	add.s32 	%r676, %r17, %r675;
	xor.b32  	%r677, %r668, %r644;
	xor.b32  	%r678, %r677, %r671;
	xor.b32  	%r679, %r678, %r674;
	ld.shared.u32 	%r680, [%r676];
	xor.b32  	%r362, %r679, %r680;
	// inline asm
	bfe.u32 %r295, %r317, %r597, 8;
	// inline asm
	shl.b32 	%r681, %r295, 2;
	add.s32 	%r682, %r14, %r681;
	ld.shared.u32 	%r683, [%r682];
	// inline asm
	bfe.u32 %r298, %r308, %r12, 8;
	// inline asm
	shl.b32 	%r684, %r298, 2;
	add.s32 	%r685, %r15, %r684;
	ld.shared.u32 	%r686, [%r685];
	// inline asm
	bfe.u32 %r301, %r311, %r111, 8;
	// inline asm
	shl.b32 	%r687, %r301, 2;
	add.s32 	%r688, %r16, %r687;
	ld.shared.u32 	%r689, [%r688];
	// inline asm
	bfe.u32 %r304, %r314, %r13, 8;
	// inline asm
	shl.b32 	%r690, %r304, 2;
	add.s32 	%r691, %r17, %r690;
	xor.b32  	%r692, %r683, %r645;
	xor.b32  	%r693, %r692, %r686;
	xor.b32  	%r694, %r693, %r689;
	ld.shared.u32 	%r695, [%r691];
	xor.b32  	%r365, %r694, %r695;
	// inline asm
	bfe.u32 %r307, %r308, %r597, 8;
	// inline asm
	shl.b32 	%r696, %r307, 2;
	add.s32 	%r697, %r14, %r696;
	ld.shared.u32 	%r698, [%r697];
	// inline asm
	bfe.u32 %r310, %r311, %r12, 8;
	// inline asm
	shl.b32 	%r699, %r310, 2;
	add.s32 	%r700, %r15, %r699;
	ld.shared.u32 	%r701, [%r700];
	// inline asm
	bfe.u32 %r313, %r314, %r111, 8;
	// inline asm
	shl.b32 	%r702, %r313, 2;
	add.s32 	%r703, %r16, %r702;
	ld.shared.u32 	%r704, [%r703];
	// inline asm
	bfe.u32 %r316, %r317, %r13, 8;
	// inline asm
	shl.b32 	%r705, %r316, 2;
	add.s32 	%r706, %r17, %r705;
	xor.b32  	%r707, %r698, %r646;
	xor.b32  	%r708, %r707, %r701;
	xor.b32  	%r709, %r708, %r704;
	ld.shared.u32 	%r710, [%r706];
	xor.b32  	%r356, %r709, %r710;
	ld.global.v4.u32 	{%r711, %r712, %r713, %r714}, [%rd25+128];
	// inline asm
	bfe.u32 %r319, %r359, %r597, 8;
	// inline asm
	shl.b32 	%r719, %r319, 2;
	add.s32 	%r720, %r14, %r719;
	ld.shared.u32 	%r721, [%r720];
	// inline asm
	bfe.u32 %r322, %r362, %r12, 8;
	// inline asm
	shl.b32 	%r722, %r322, 2;
	add.s32 	%r723, %r15, %r722;
	ld.shared.u32 	%r724, [%r723];
	// inline asm
	bfe.u32 %r325, %r365, %r111, 8;
	// inline asm
	shl.b32 	%r725, %r325, 2;
	add.s32 	%r726, %r16, %r725;
	ld.shared.u32 	%r727, [%r726];
	// inline asm
	bfe.u32 %r328, %r356, %r13, 8;
	// inline asm
	shl.b32 	%r728, %r328, 2;
	add.s32 	%r729, %r17, %r728;
	xor.b32  	%r730, %r721, %r711;
	xor.b32  	%r731, %r730, %r724;
	xor.b32  	%r732, %r731, %r727;
	ld.shared.u32 	%r733, [%r729];
	xor.b32  	%r407, %r732, %r733;
	// inline asm
	bfe.u32 %r331, %r362, %r597, 8;
	// inline asm
	shl.b32 	%r734, %r331, 2;
	add.s32 	%r735, %r14, %r734;
	ld.shared.u32 	%r736, [%r735];
	// inline asm
	bfe.u32 %r334, %r365, %r12, 8;
	// inline asm
	shl.b32 	%r737, %r334, 2;
	add.s32 	%r738, %r15, %r737;
	ld.shared.u32 	%r739, [%r738];
	// inline asm
	bfe.u32 %r337, %r356, %r111, 8;
	// inline asm
	shl.b32 	%r740, %r337, 2;
	add.s32 	%r741, %r16, %r740;
	ld.shared.u32 	%r742, [%r741];
	// inline asm
	bfe.u32 %r340, %r359, %r13, 8;
	// inline asm
	shl.b32 	%r743, %r340, 2;
	add.s32 	%r744, %r17, %r743;
	xor.b32  	%r745, %r736, %r712;
	xor.b32  	%r746, %r745, %r739;
	xor.b32  	%r747, %r746, %r742;
	ld.shared.u32 	%r748, [%r744];
	xor.b32  	%r410, %r747, %r748;
	// inline asm
	bfe.u32 %r343, %r365, %r597, 8;
	// inline asm
	shl.b32 	%r749, %r343, 2;
	add.s32 	%r750, %r14, %r749;
	ld.shared.u32 	%r751, [%r750];
	// inline asm
	bfe.u32 %r346, %r356, %r12, 8;
	// inline asm
	shl.b32 	%r752, %r346, 2;
	add.s32 	%r753, %r15, %r752;
	ld.shared.u32 	%r754, [%r753];
	// inline asm
	bfe.u32 %r349, %r359, %r111, 8;
	// inline asm
	shl.b32 	%r755, %r349, 2;
	add.s32 	%r756, %r16, %r755;
	ld.shared.u32 	%r757, [%r756];
	// inline asm
	bfe.u32 %r352, %r362, %r13, 8;
	// inline asm
	shl.b32 	%r758, %r352, 2;
	add.s32 	%r759, %r17, %r758;
	xor.b32  	%r760, %r751, %r713;
	xor.b32  	%r761, %r760, %r754;
	xor.b32  	%r762, %r761, %r757;
	ld.shared.u32 	%r763, [%r759];
	xor.b32  	%r413, %r762, %r763;
	// inline asm
	bfe.u32 %r355, %r356, %r597, 8;
	// inline asm
	shl.b32 	%r764, %r355, 2;
	add.s32 	%r765, %r14, %r764;
	ld.shared.u32 	%r766, [%r765];
	// inline asm
	bfe.u32 %r358, %r359, %r12, 8;
	// inline asm
	shl.b32 	%r767, %r358, 2;
	add.s32 	%r768, %r15, %r767;
	ld.shared.u32 	%r769, [%r768];
	// inline asm
	bfe.u32 %r361, %r362, %r111, 8;
	// inline asm
	shl.b32 	%r770, %r361, 2;
	add.s32 	%r771, %r16, %r770;
	ld.shared.u32 	%r772, [%r771];
	// inline asm
	bfe.u32 %r364, %r365, %r13, 8;
	// inline asm
	shl.b32 	%r773, %r364, 2;
	add.s32 	%r774, %r17, %r773;
	xor.b32  	%r775, %r766, %r714;
	xor.b32  	%r776, %r775, %r769;
	xor.b32  	%r777, %r776, %r772;
	ld.shared.u32 	%r778, [%r774];
	xor.b32  	%r404, %r777, %r778;
	ld.global.v4.u32 	{%r779, %r780, %r781, %r782}, [%rd25+192];
	// inline asm
	bfe.u32 %r367, %r407, %r597, 8;
	// inline asm
	shl.b32 	%r787, %r367, 2;
	add.s32 	%r788, %r14, %r787;
	ld.shared.u32 	%r789, [%r788];
	// inline asm
	bfe.u32 %r370, %r410, %r12, 8;
	// inline asm
	shl.b32 	%r790, %r370, 2;
	add.s32 	%r791, %r15, %r790;
	ld.shared.u32 	%r792, [%r791];
	// inline asm
	bfe.u32 %r373, %r413, %r111, 8;
	// inline asm
	shl.b32 	%r793, %r373, 2;
	add.s32 	%r794, %r16, %r793;
	ld.shared.u32 	%r795, [%r794];
	// inline asm
	bfe.u32 %r376, %r404, %r13, 8;
	// inline asm
	shl.b32 	%r796, %r376, 2;
	add.s32 	%r797, %r17, %r796;
	xor.b32  	%r798, %r789, %r779;
	xor.b32  	%r799, %r798, %r792;
	xor.b32  	%r800, %r799, %r795;
	ld.shared.u32 	%r801, [%r797];
	xor.b32  	%r455, %r800, %r801;
	// inline asm
	bfe.u32 %r379, %r410, %r597, 8;
	// inline asm
	shl.b32 	%r802, %r379, 2;
	add.s32 	%r803, %r14, %r802;
	ld.shared.u32 	%r804, [%r803];
	// inline asm
	bfe.u32 %r382, %r413, %r12, 8;
	// inline asm
	shl.b32 	%r805, %r382, 2;
	add.s32 	%r806, %r15, %r805;
	ld.shared.u32 	%r807, [%r806];
	// inline asm
	bfe.u32 %r385, %r404, %r111, 8;
	// inline asm
	shl.b32 	%r808, %r385, 2;
	add.s32 	%r809, %r16, %r808;
	ld.shared.u32 	%r810, [%r809];
	// inline asm
	bfe.u32 %r388, %r407, %r13, 8;
	// inline asm
	shl.b32 	%r811, %r388, 2;
	add.s32 	%r812, %r17, %r811;
	xor.b32  	%r813, %r804, %r780;
	xor.b32  	%r814, %r813, %r807;
	xor.b32  	%r815, %r814, %r810;
	ld.shared.u32 	%r816, [%r812];
	xor.b32  	%r458, %r815, %r816;
	// inline asm
	bfe.u32 %r391, %r413, %r597, 8;
	// inline asm
	shl.b32 	%r817, %r391, 2;
	add.s32 	%r818, %r14, %r817;
	ld.shared.u32 	%r819, [%r818];
	// inline asm
	bfe.u32 %r394, %r404, %r12, 8;
	// inline asm
	shl.b32 	%r820, %r394, 2;
	add.s32 	%r821, %r15, %r820;
	ld.shared.u32 	%r822, [%r821];
	// inline asm
	bfe.u32 %r397, %r407, %r111, 8;
	// inline asm
	shl.b32 	%r823, %r397, 2;
	add.s32 	%r824, %r16, %r823;
	ld.shared.u32 	%r825, [%r824];
	// inline asm
	bfe.u32 %r400, %r410, %r13, 8;
	// inline asm
	shl.b32 	%r826, %r400, 2;
	add.s32 	%r827, %r17, %r826;
	xor.b32  	%r828, %r819, %r781;
	xor.b32  	%r829, %r828, %r822;
	xor.b32  	%r830, %r829, %r825;
	ld.shared.u32 	%r831, [%r827];
	xor.b32  	%r461, %r830, %r831;
	// inline asm
	bfe.u32 %r403, %r404, %r597, 8;
	// inline asm
	shl.b32 	%r832, %r403, 2;
	add.s32 	%r833, %r14, %r832;
	ld.shared.u32 	%r834, [%r833];
	// inline asm
	bfe.u32 %r406, %r407, %r12, 8;
	// inline asm
	shl.b32 	%r835, %r406, 2;
	add.s32 	%r836, %r15, %r835;
	ld.shared.u32 	%r837, [%r836];
	// inline asm
	bfe.u32 %r409, %r410, %r111, 8;
	// inline asm
	shl.b32 	%r838, %r409, 2;
	add.s32 	%r839, %r16, %r838;
	ld.shared.u32 	%r840, [%r839];
	// inline asm
	bfe.u32 %r412, %r413, %r13, 8;
	// inline asm
	shl.b32 	%r841, %r412, 2;
	add.s32 	%r842, %r17, %r841;
	xor.b32  	%r843, %r834, %r782;
	xor.b32  	%r844, %r843, %r837;
	xor.b32  	%r845, %r844, %r840;
	ld.shared.u32 	%r846, [%r842];
	xor.b32  	%r452, %r845, %r846;
	ld.global.v4.u32 	{%r847, %r848, %r849, %r850}, [%rd25+256];
	// inline asm
	bfe.u32 %r415, %r455, %r597, 8;
	// inline asm
	shl.b32 	%r855, %r415, 2;
	add.s32 	%r856, %r14, %r855;
	ld.shared.u32 	%r857, [%r856];
	// inline asm
	bfe.u32 %r418, %r458, %r12, 8;
	// inline asm
	shl.b32 	%r858, %r418, 2;
	add.s32 	%r859, %r15, %r858;
	ld.shared.u32 	%r860, [%r859];
	// inline asm
	bfe.u32 %r421, %r461, %r111, 8;
	// inline asm
	shl.b32 	%r861, %r421, 2;
	add.s32 	%r862, %r16, %r861;
	ld.shared.u32 	%r863, [%r862];
	// inline asm
	bfe.u32 %r424, %r452, %r13, 8;
	// inline asm
	shl.b32 	%r864, %r424, 2;
	add.s32 	%r865, %r17, %r864;
	xor.b32  	%r866, %r857, %r847;
	xor.b32  	%r867, %r866, %r860;
	xor.b32  	%r868, %r867, %r863;
	ld.shared.u32 	%r869, [%r865];
	xor.b32  	%r503, %r868, %r869;
	// inline asm
	bfe.u32 %r427, %r458, %r597, 8;
	// inline asm
	shl.b32 	%r870, %r427, 2;
	add.s32 	%r871, %r14, %r870;
	ld.shared.u32 	%r872, [%r871];
	// inline asm
	bfe.u32 %r430, %r461, %r12, 8;
	// inline asm
	shl.b32 	%r873, %r430, 2;
	add.s32 	%r874, %r15, %r873;
	ld.shared.u32 	%r875, [%r874];
	// inline asm
	bfe.u32 %r433, %r452, %r111, 8;
	// inline asm
	shl.b32 	%r876, %r433, 2;
	add.s32 	%r877, %r16, %r876;
	ld.shared.u32 	%r878, [%r877];
	// inline asm
	bfe.u32 %r436, %r455, %r13, 8;
	// inline asm
	shl.b32 	%r879, %r436, 2;
	add.s32 	%r880, %r17, %r879;
	xor.b32  	%r881, %r872, %r848;
	xor.b32  	%r882, %r881, %r875;
	xor.b32  	%r883, %r882, %r878;
	ld.shared.u32 	%r884, [%r880];
	xor.b32  	%r506, %r883, %r884;
	// inline asm
	bfe.u32 %r439, %r461, %r597, 8;
	// inline asm
	shl.b32 	%r885, %r439, 2;
	add.s32 	%r886, %r14, %r885;
	ld.shared.u32 	%r887, [%r886];
	// inline asm
	bfe.u32 %r442, %r452, %r12, 8;
	// inline asm
	shl.b32 	%r888, %r442, 2;
	add.s32 	%r889, %r15, %r888;
	ld.shared.u32 	%r890, [%r889];
	// inline asm
	bfe.u32 %r445, %r455, %r111, 8;
	// inline asm
	shl.b32 	%r891, %r445, 2;
	add.s32 	%r892, %r16, %r891;
	ld.shared.u32 	%r893, [%r892];
	// inline asm
	bfe.u32 %r448, %r458, %r13, 8;
	// inline asm
	shl.b32 	%r894, %r448, 2;
	add.s32 	%r895, %r17, %r894;
	xor.b32  	%r896, %r887, %r849;
	xor.b32  	%r897, %r896, %r890;
	xor.b32  	%r898, %r897, %r893;
	ld.shared.u32 	%r899, [%r895];
	xor.b32  	%r509, %r898, %r899;
	// inline asm
	bfe.u32 %r451, %r452, %r597, 8;
	// inline asm
	shl.b32 	%r900, %r451, 2;
	add.s32 	%r901, %r14, %r900;
	ld.shared.u32 	%r902, [%r901];
	// inline asm
	bfe.u32 %r454, %r455, %r12, 8;
	// inline asm
	shl.b32 	%r903, %r454, 2;
	add.s32 	%r904, %r15, %r903;
	ld.shared.u32 	%r905, [%r904];
	// inline asm
	bfe.u32 %r457, %r458, %r111, 8;
	// inline asm
	shl.b32 	%r906, %r457, 2;
	add.s32 	%r907, %r16, %r906;
	ld.shared.u32 	%r908, [%r907];
	// inline asm
	bfe.u32 %r460, %r461, %r13, 8;
	// inline asm
	shl.b32 	%r909, %r460, 2;
	add.s32 	%r910, %r17, %r909;
	xor.b32  	%r911, %r902, %r850;
	xor.b32  	%r912, %r911, %r905;
	xor.b32  	%r913, %r912, %r908;
	ld.shared.u32 	%r914, [%r910];
	xor.b32  	%r500, %r913, %r914;
	ld.global.v4.u32 	{%r915, %r916, %r917, %r918}, [%rd25+320];
	// inline asm
	bfe.u32 %r463, %r503, %r597, 8;
	// inline asm
	shl.b32 	%r923, %r463, 2;
	add.s32 	%r924, %r14, %r923;
	ld.shared.u32 	%r925, [%r924];
	// inline asm
	bfe.u32 %r466, %r506, %r12, 8;
	// inline asm
	shl.b32 	%r926, %r466, 2;
	add.s32 	%r927, %r15, %r926;
	ld.shared.u32 	%r928, [%r927];
	// inline asm
	bfe.u32 %r469, %r509, %r111, 8;
	// inline asm
	shl.b32 	%r929, %r469, 2;
	add.s32 	%r930, %r16, %r929;
	ld.shared.u32 	%r931, [%r930];
	// inline asm
	bfe.u32 %r472, %r500, %r13, 8;
	// inline asm
	shl.b32 	%r932, %r472, 2;
	add.s32 	%r933, %r17, %r932;
	xor.b32  	%r934, %r925, %r915;
	xor.b32  	%r935, %r934, %r928;
	xor.b32  	%r936, %r935, %r931;
	ld.shared.u32 	%r937, [%r933];
	xor.b32  	%r551, %r936, %r937;
	// inline asm
	bfe.u32 %r475, %r506, %r597, 8;
	// inline asm
	shl.b32 	%r938, %r475, 2;
	add.s32 	%r939, %r14, %r938;
	ld.shared.u32 	%r940, [%r939];
	// inline asm
	bfe.u32 %r478, %r509, %r12, 8;
	// inline asm
	shl.b32 	%r941, %r478, 2;
	add.s32 	%r942, %r15, %r941;
	ld.shared.u32 	%r943, [%r942];
	// inline asm
	bfe.u32 %r481, %r500, %r111, 8;
	// inline asm
	shl.b32 	%r944, %r481, 2;
	add.s32 	%r945, %r16, %r944;
	ld.shared.u32 	%r946, [%r945];
	// inline asm
	bfe.u32 %r484, %r503, %r13, 8;
	// inline asm
	shl.b32 	%r947, %r484, 2;
	add.s32 	%r948, %r17, %r947;
	xor.b32  	%r949, %r940, %r916;
	xor.b32  	%r950, %r949, %r943;
	xor.b32  	%r951, %r950, %r946;
	ld.shared.u32 	%r952, [%r948];
	xor.b32  	%r554, %r951, %r952;
	// inline asm
	bfe.u32 %r487, %r509, %r597, 8;
	// inline asm
	shl.b32 	%r953, %r487, 2;
	add.s32 	%r954, %r14, %r953;
	ld.shared.u32 	%r955, [%r954];
	// inline asm
	bfe.u32 %r490, %r500, %r12, 8;
	// inline asm
	shl.b32 	%r956, %r490, 2;
	add.s32 	%r957, %r15, %r956;
	ld.shared.u32 	%r958, [%r957];
	// inline asm
	bfe.u32 %r493, %r503, %r111, 8;
	// inline asm
	shl.b32 	%r959, %r493, 2;
	add.s32 	%r960, %r16, %r959;
	ld.shared.u32 	%r961, [%r960];
	// inline asm
	bfe.u32 %r496, %r506, %r13, 8;
	// inline asm
	shl.b32 	%r962, %r496, 2;
	add.s32 	%r963, %r17, %r962;
	xor.b32  	%r964, %r955, %r917;
	xor.b32  	%r965, %r964, %r958;
	xor.b32  	%r966, %r965, %r961;
	ld.shared.u32 	%r967, [%r963];
	xor.b32  	%r557, %r966, %r967;
	// inline asm
	bfe.u32 %r499, %r500, %r597, 8;
	// inline asm
	shl.b32 	%r968, %r499, 2;
	add.s32 	%r969, %r14, %r968;
	ld.shared.u32 	%r970, [%r969];
	// inline asm
	bfe.u32 %r502, %r503, %r12, 8;
	// inline asm
	shl.b32 	%r971, %r502, 2;
	add.s32 	%r972, %r15, %r971;
	ld.shared.u32 	%r973, [%r972];
	// inline asm
	bfe.u32 %r505, %r506, %r111, 8;
	// inline asm
	shl.b32 	%r974, %r505, 2;
	add.s32 	%r975, %r16, %r974;
	ld.shared.u32 	%r976, [%r975];
	// inline asm
	bfe.u32 %r508, %r509, %r13, 8;
	// inline asm
	shl.b32 	%r977, %r508, 2;
	add.s32 	%r978, %r17, %r977;
	xor.b32  	%r979, %r970, %r918;
	xor.b32  	%r980, %r979, %r973;
	xor.b32  	%r981, %r980, %r976;
	ld.shared.u32 	%r982, [%r978];
	xor.b32  	%r548, %r981, %r982;
	ld.global.v4.u32 	{%r983, %r984, %r985, %r986}, [%rd25+384];
	// inline asm
	bfe.u32 %r511, %r551, %r597, 8;
	// inline asm
	shl.b32 	%r991, %r511, 2;
	add.s32 	%r992, %r14, %r991;
	ld.shared.u32 	%r993, [%r992];
	// inline asm
	bfe.u32 %r514, %r554, %r12, 8;
	// inline asm
	shl.b32 	%r994, %r514, 2;
	add.s32 	%r995, %r15, %r994;
	ld.shared.u32 	%r996, [%r995];
	// inline asm
	bfe.u32 %r517, %r557, %r111, 8;
	// inline asm
	shl.b32 	%r997, %r517, 2;
	add.s32 	%r998, %r16, %r997;
	ld.shared.u32 	%r999, [%r998];
	// inline asm
	bfe.u32 %r520, %r548, %r13, 8;
	// inline asm
	shl.b32 	%r1000, %r520, 2;
	add.s32 	%r1001, %r17, %r1000;
	xor.b32  	%r1002, %r993, %r983;
	xor.b32  	%r1003, %r1002, %r996;
	xor.b32  	%r1004, %r1003, %r999;
	ld.shared.u32 	%r1005, [%r1001];
	xor.b32  	%r599, %r1004, %r1005;
	// inline asm
	bfe.u32 %r523, %r554, %r597, 8;
	// inline asm
	shl.b32 	%r1006, %r523, 2;
	add.s32 	%r1007, %r14, %r1006;
	ld.shared.u32 	%r1008, [%r1007];
	// inline asm
	bfe.u32 %r526, %r557, %r12, 8;
	// inline asm
	shl.b32 	%r1009, %r526, 2;
	add.s32 	%r1010, %r15, %r1009;
	ld.shared.u32 	%r1011, [%r1010];
	// inline asm
	bfe.u32 %r529, %r548, %r111, 8;
	// inline asm
	shl.b32 	%r1012, %r529, 2;
	add.s32 	%r1013, %r16, %r1012;
	ld.shared.u32 	%r1014, [%r1013];
	// inline asm
	bfe.u32 %r532, %r551, %r13, 8;
	// inline asm
	shl.b32 	%r1015, %r532, 2;
	add.s32 	%r1016, %r17, %r1015;
	xor.b32  	%r1017, %r1008, %r984;
	xor.b32  	%r1018, %r1017, %r1011;
	xor.b32  	%r1019, %r1018, %r1014;
	ld.shared.u32 	%r1020, [%r1016];
	xor.b32  	%r602, %r1019, %r1020;
	// inline asm
	bfe.u32 %r535, %r557, %r597, 8;
	// inline asm
	shl.b32 	%r1021, %r535, 2;
	add.s32 	%r1022, %r14, %r1021;
	ld.shared.u32 	%r1023, [%r1022];
	// inline asm
	bfe.u32 %r538, %r548, %r12, 8;
	// inline asm
	shl.b32 	%r1024, %r538, 2;
	add.s32 	%r1025, %r15, %r1024;
	ld.shared.u32 	%r1026, [%r1025];
	// inline asm
	bfe.u32 %r541, %r551, %r111, 8;
	// inline asm
	shl.b32 	%r1027, %r541, 2;
	add.s32 	%r1028, %r16, %r1027;
	ld.shared.u32 	%r1029, [%r1028];
	// inline asm
	bfe.u32 %r544, %r554, %r13, 8;
	// inline asm
	shl.b32 	%r1030, %r544, 2;
	add.s32 	%r1031, %r17, %r1030;
	xor.b32  	%r1032, %r1023, %r985;
	xor.b32  	%r1033, %r1032, %r1026;
	xor.b32  	%r1034, %r1033, %r1029;
	ld.shared.u32 	%r1035, [%r1031];
	xor.b32  	%r605, %r1034, %r1035;
	// inline asm
	bfe.u32 %r547, %r548, %r597, 8;
	// inline asm
	shl.b32 	%r1036, %r547, 2;
	add.s32 	%r1037, %r14, %r1036;
	ld.shared.u32 	%r1038, [%r1037];
	// inline asm
	bfe.u32 %r550, %r551, %r12, 8;
	// inline asm
	shl.b32 	%r1039, %r550, 2;
	add.s32 	%r1040, %r15, %r1039;
	ld.shared.u32 	%r1041, [%r1040];
	// inline asm
	bfe.u32 %r553, %r554, %r111, 8;
	// inline asm
	shl.b32 	%r1042, %r553, 2;
	add.s32 	%r1043, %r16, %r1042;
	ld.shared.u32 	%r1044, [%r1043];
	// inline asm
	bfe.u32 %r556, %r557, %r13, 8;
	// inline asm
	shl.b32 	%r1045, %r556, 2;
	add.s32 	%r1046, %r17, %r1045;
	xor.b32  	%r1047, %r1038, %r986;
	xor.b32  	%r1048, %r1047, %r1041;
	xor.b32  	%r1049, %r1048, %r1044;
	ld.shared.u32 	%r1050, [%r1046];
	xor.b32  	%r596, %r1049, %r1050;
	ld.global.v4.u32 	{%r1051, %r1052, %r1053, %r1054}, [%rd25+448];
	// inline asm
	bfe.u32 %r559, %r599, %r597, 8;
	// inline asm
	shl.b32 	%r1059, %r559, 2;
	add.s32 	%r1060, %r14, %r1059;
	ld.shared.u32 	%r1061, [%r1060];
	// inline asm
	bfe.u32 %r562, %r602, %r12, 8;
	// inline asm
	shl.b32 	%r1062, %r562, 2;
	add.s32 	%r1063, %r15, %r1062;
	ld.shared.u32 	%r1064, [%r1063];
	// inline asm
	bfe.u32 %r565, %r605, %r111, 8;
	// inline asm
	shl.b32 	%r1065, %r565, 2;
	add.s32 	%r1066, %r16, %r1065;
	ld.shared.u32 	%r1067, [%r1066];
	// inline asm
	bfe.u32 %r568, %r596, %r13, 8;
	// inline asm
	shl.b32 	%r1068, %r568, 2;
	add.s32 	%r1069, %r17, %r1068;
	xor.b32  	%r1070, %r1061, %r1051;
	xor.b32  	%r1071, %r1070, %r1064;
	xor.b32  	%r1072, %r1071, %r1067;
	ld.shared.u32 	%r1073, [%r1069];
	xor.b32  	%r1120, %r1072, %r1073;
	// inline asm
	bfe.u32 %r571, %r602, %r597, 8;
	// inline asm
	shl.b32 	%r1074, %r571, 2;
	add.s32 	%r1075, %r14, %r1074;
	ld.shared.u32 	%r1076, [%r1075];
	// inline asm
	bfe.u32 %r574, %r605, %r12, 8;
	// inline asm
	shl.b32 	%r1077, %r574, 2;
	add.s32 	%r1078, %r15, %r1077;
	ld.shared.u32 	%r1079, [%r1078];
	// inline asm
	bfe.u32 %r577, %r596, %r111, 8;
	// inline asm
	shl.b32 	%r1080, %r577, 2;
	add.s32 	%r1081, %r16, %r1080;
	ld.shared.u32 	%r1082, [%r1081];
	// inline asm
	bfe.u32 %r580, %r599, %r13, 8;
	// inline asm
	shl.b32 	%r1083, %r580, 2;
	add.s32 	%r1084, %r17, %r1083;
	xor.b32  	%r1085, %r1076, %r1052;
	xor.b32  	%r1086, %r1085, %r1079;
	xor.b32  	%r1087, %r1086, %r1082;
	ld.shared.u32 	%r1088, [%r1084];
	xor.b32  	%r1121, %r1087, %r1088;
	// inline asm
	bfe.u32 %r583, %r605, %r597, 8;
	// inline asm
	shl.b32 	%r1089, %r583, 2;
	add.s32 	%r1090, %r14, %r1089;
	ld.shared.u32 	%r1091, [%r1090];
	// inline asm
	bfe.u32 %r586, %r596, %r12, 8;
	// inline asm
	shl.b32 	%r1092, %r586, 2;
	add.s32 	%r1093, %r15, %r1092;
	ld.shared.u32 	%r1094, [%r1093];
	// inline asm
	bfe.u32 %r589, %r599, %r111, 8;
	// inline asm
	shl.b32 	%r1095, %r589, 2;
	add.s32 	%r1096, %r16, %r1095;
	ld.shared.u32 	%r1097, [%r1096];
	// inline asm
	bfe.u32 %r592, %r602, %r13, 8;
	// inline asm
	shl.b32 	%r1098, %r592, 2;
	add.s32 	%r1099, %r17, %r1098;
	xor.b32  	%r1100, %r1091, %r1053;
	xor.b32  	%r1101, %r1100, %r1094;
	xor.b32  	%r1102, %r1101, %r1097;
	ld.shared.u32 	%r1103, [%r1099];
	xor.b32  	%r1122, %r1102, %r1103;
	// inline asm
	bfe.u32 %r595, %r596, %r597, 8;
	// inline asm
	shl.b32 	%r1104, %r595, 2;
	add.s32 	%r1105, %r14, %r1104;
	ld.shared.u32 	%r1106, [%r1105];
	// inline asm
	bfe.u32 %r598, %r599, %r12, 8;
	// inline asm
	shl.b32 	%r1107, %r598, 2;
	add.s32 	%r1108, %r15, %r1107;
	ld.shared.u32 	%r1109, [%r1108];
	// inline asm
	bfe.u32 %r601, %r602, %r111, 8;
	// inline asm
	shl.b32 	%r1110, %r601, 2;
	add.s32 	%r1111, %r16, %r1110;
	ld.shared.u32 	%r1112, [%r1111];
	// inline asm
	bfe.u32 %r604, %r605, %r13, 8;
	// inline asm
	shl.b32 	%r1113, %r604, 2;
	add.s32 	%r1114, %r17, %r1113;
	xor.b32  	%r1115, %r1106, %r1054;
	xor.b32  	%r1116, %r1115, %r1109;
	xor.b32  	%r1117, %r1116, %r1112;
	ld.shared.u32 	%r1118, [%r1114];
	xor.b32  	%r1123, %r1117, %r1118;
	add.s32 	%r1124, %r1124, 32;
	add.s64 	%rd25, %rd25, 512;

BB21_4:
	// inline asm
	bfe.u32 %r67, %r1120, %r50, 8;
	// inline asm
	shl.b32 	%r115, %r67, 2;
	add.s32 	%r23, %r14, %r115;
	// inline asm
	bfe.u32 %r70, %r1121, %r12, 8;
	// inline asm
	shl.b32 	%r116, %r70, 2;
	add.s32 	%r24, %r15, %r116;
	mov.u32 	%r111, 16;
	// inline asm
	bfe.u32 %r73, %r1122, %r111, 8;
	// inline asm
	shl.b32 	%r117, %r73, 2;
	add.s32 	%r25, %r16, %r117;
	// inline asm
	bfe.u32 %r76, %r1123, %r13, 8;
	// inline asm
	shl.b32 	%r118, %r76, 2;
	add.s32 	%r26, %r17, %r118;
	// inline asm
	bfe.u32 %r79, %r1121, %r50, 8;
	// inline asm
	shl.b32 	%r119, %r79, 2;
	add.s32 	%r27, %r14, %r119;
	// inline asm
	bfe.u32 %r82, %r1122, %r12, 8;
	// inline asm
	shl.b32 	%r120, %r82, 2;
	add.s32 	%r28, %r15, %r120;
	// inline asm
	bfe.u32 %r85, %r1123, %r111, 8;
	// inline asm
	shl.b32 	%r121, %r85, 2;
	add.s32 	%r29, %r16, %r121;
	// inline asm
	bfe.u32 %r88, %r1120, %r13, 8;
	// inline asm
	shl.b32 	%r122, %r88, 2;
	add.s32 	%r30, %r17, %r122;
	// inline asm
	bfe.u32 %r91, %r1122, %r50, 8;
	// inline asm
	shl.b32 	%r123, %r91, 2;
	add.s32 	%r31, %r14, %r123;
	// inline asm
	bfe.u32 %r94, %r1123, %r12, 8;
	// inline asm
	shl.b32 	%r124, %r94, 2;
	add.s32 	%r32, %r15, %r124;
	// inline asm
	bfe.u32 %r97, %r1120, %r111, 8;
	// inline asm
	shl.b32 	%r125, %r97, 2;
	add.s32 	%r33, %r16, %r125;
	// inline asm
	bfe.u32 %r100, %r1121, %r13, 8;
	// inline asm
	shl.b32 	%r126, %r100, 2;
	add.s32 	%r34, %r17, %r126;
	// inline asm
	bfe.u32 %r103, %r1123, %r50, 8;
	// inline asm
	shl.b32 	%r127, %r103, 2;
	add.s32 	%r35, %r14, %r127;
	// inline asm
	bfe.u32 %r106, %r1120, %r12, 8;
	// inline asm
	shl.b32 	%r128, %r106, 2;
	add.s32 	%r36, %r15, %r128;
	// inline asm
	bfe.u32 %r109, %r1121, %r111, 8;
	// inline asm
	shl.b32 	%r129, %r109, 2;
	add.s32 	%r37, %r16, %r129;
	// inline asm
	bfe.u32 %r112, %r1122, %r13, 8;
	// inline asm
	shl.b32 	%r130, %r112, 2;
	add.s32 	%r38, %r17, %r130;
	setp.lt.u32	%p5, %r1124, 131072;
	@%p5 bra 	BB21_7;

	ld.shared.u32 	%r179, [%r23];
	ld.shared.u32 	%r180, [%r24];
	xor.b32  	%r181, %r179, %r180;
	ld.shared.u32 	%r182, [%r25];
	xor.b32  	%r183, %r181, %r182;
	ld.shared.u32 	%r184, [%r26];
	xor.b32  	%r185, %r183, %r184;
	xor.b32  	%r132, %r185, -151354487;
	ld.shared.u32 	%r186, [%r27];
	ld.shared.u32 	%r187, [%r28];
	xor.b32  	%r188, %r186, %r187;
	ld.shared.u32 	%r189, [%r29];
	xor.b32  	%r190, %r188, %r189;
	ld.shared.u32 	%r191, [%r30];
	xor.b32  	%r192, %r190, %r191;
	xor.b32  	%r135, %r192, -1960536929;
	ld.shared.u32 	%r193, [%r31];
	ld.shared.u32 	%r194, [%r32];
	xor.b32  	%r195, %r193, %r194;
	ld.shared.u32 	%r196, [%r33];
	xor.b32  	%r197, %r195, %r196;
	ld.shared.u32 	%r198, [%r34];
	xor.b32  	%r199, %r197, %r198;
	xor.b32  	%r138, %r199, -1864608065;
	ld.shared.u32 	%r200, [%r35];
	ld.shared.u32 	%r201, [%r36];
	xor.b32  	%r202, %r200, %r201;
	ld.shared.u32 	%r203, [%r37];
	xor.b32  	%r204, %r202, %r203;
	ld.shared.u32 	%r205, [%r38];
	xor.b32  	%r206, %r204, %r205;
	xor.b32  	%r141, %r206, 109642241;
	mov.u32 	%r169, 0;
	// inline asm
	bfe.u32 %r131, %r132, %r169, 8;
	// inline asm
	shl.b32 	%r207, %r131, 2;
	add.s32 	%r208, %r14, %r207;
	ld.shared.u32 	%r209, [%r208];
	// inline asm
	bfe.u32 %r134, %r135, %r12, 8;
	// inline asm
	shl.b32 	%r210, %r134, 2;
	add.s32 	%r211, %r15, %r210;
	ld.shared.u32 	%r212, [%r211];
	// inline asm
	bfe.u32 %r137, %r138, %r111, 8;
	// inline asm
	shl.b32 	%r213, %r137, 2;
	add.s32 	%r214, %r16, %r213;
	ld.shared.u32 	%r215, [%r214];
	// inline asm
	bfe.u32 %r140, %r141, %r13, 8;
	// inline asm
	shl.b32 	%r216, %r140, 2;
	add.s32 	%r217, %r17, %r216;
	xor.b32  	%r218, %r209, %r212;
	xor.b32  	%r219, %r218, %r215;
	ld.shared.u32 	%r220, [%r217];
	xor.b32  	%r221, %r219, %r220;
	// inline asm
	bfe.u32 %r143, %r135, %r169, 8;
	// inline asm
	shl.b32 	%r222, %r143, 2;
	add.s32 	%r223, %r14, %r222;
	ld.shared.u32 	%r224, [%r223];
	// inline asm
	bfe.u32 %r146, %r138, %r12, 8;
	// inline asm
	shl.b32 	%r225, %r146, 2;
	add.s32 	%r226, %r15, %r225;
	ld.shared.u32 	%r227, [%r226];
	// inline asm
	bfe.u32 %r149, %r141, %r111, 8;
	// inline asm
	shl.b32 	%r228, %r149, 2;
	add.s32 	%r229, %r16, %r228;
	ld.shared.u32 	%r230, [%r229];
	// inline asm
	bfe.u32 %r152, %r132, %r13, 8;
	// inline asm
	shl.b32 	%r231, %r152, 2;
	add.s32 	%r232, %r17, %r231;
	xor.b32  	%r233, %r224, %r227;
	xor.b32  	%r234, %r233, %r230;
	ld.shared.u32 	%r235, [%r232];
	xor.b32  	%r236, %r234, %r235;
	// inline asm
	bfe.u32 %r155, %r138, %r169, 8;
	// inline asm
	shl.b32 	%r237, %r155, 2;
	add.s32 	%r238, %r14, %r237;
	ld.shared.u32 	%r239, [%r238];
	// inline asm
	bfe.u32 %r158, %r141, %r12, 8;
	// inline asm
	shl.b32 	%r240, %r158, 2;
	add.s32 	%r241, %r15, %r240;
	ld.shared.u32 	%r242, [%r241];
	// inline asm
	bfe.u32 %r161, %r132, %r111, 8;
	// inline asm
	shl.b32 	%r243, %r161, 2;
	add.s32 	%r244, %r16, %r243;
	ld.shared.u32 	%r245, [%r244];
	// inline asm
	bfe.u32 %r164, %r135, %r13, 8;
	// inline asm
	shl.b32 	%r246, %r164, 2;
	add.s32 	%r247, %r17, %r246;
	xor.b32  	%r248, %r239, %r242;
	xor.b32  	%r249, %r248, %r245;
	ld.shared.u32 	%r250, [%r247];
	xor.b32  	%r251, %r249, %r250;
	// inline asm
	bfe.u32 %r167, %r141, %r169, 8;
	// inline asm
	shl.b32 	%r252, %r167, 2;
	add.s32 	%r253, %r14, %r252;
	ld.shared.u32 	%r254, [%r253];
	// inline asm
	bfe.u32 %r170, %r132, %r12, 8;
	// inline asm
	shl.b32 	%r255, %r170, 2;
	add.s32 	%r256, %r15, %r255;
	ld.shared.u32 	%r257, [%r256];
	// inline asm
	bfe.u32 %r173, %r135, %r111, 8;
	// inline asm
	shl.b32 	%r258, %r173, 2;
	add.s32 	%r259, %r16, %r258;
	ld.shared.u32 	%r260, [%r259];
	// inline asm
	bfe.u32 %r176, %r138, %r13, 8;
	// inline asm
	shl.b32 	%r261, %r176, 2;
	add.s32 	%r262, %r17, %r261;
	xor.b32  	%r263, %r254, %r257;
	xor.b32  	%r264, %r263, %r260;
	ld.shared.u32 	%r265, [%r262];
	xor.b32  	%r266, %r264, %r265;
	shl.b64 	%rd20, %rd1, 2;
	add.s64 	%rd21, %rd20, %rd2;
	cvta.to.global.u64 	%rd22, %rd7;
	shl.b64 	%rd23, %rd21, 4;
	add.s64 	%rd24, %rd22, %rd23;
	xor.b32  	%r267, %r266, -317130341;
	xor.b32  	%r268, %r251, -300923962;
	xor.b32  	%r269, %r236, 1375002684;
	xor.b32  	%r270, %r221, 1639080913;
	st.global.v4.u32 	[%rd24], {%r270, %r269, %r268, %r267};

BB21_6:
	ret;
}

	// .globl	_Z30blake2b_512_single_block_benchILj76EEvPyPKvy
.visible .entry _Z30blake2b_512_single_block_benchILj76EEvPyPKvy(
	.param .u64 _Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_0,
	.param .u64 _Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_1,
	.param .u64 _Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<1739>;
	.reg .b64 	%rd<1287>;


	ld.param.u64 	%rd3, [_Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_1];
	ld.param.u64 	%rd4, [_Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_2];
	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r769, %ntid.x;
	mov.u32 	%r770, %ctaid.x;
	mul.lo.s32 	%r771, %r769, %r770;
	cvt.u64.u32	%rd6, %r771;
	add.s64 	%rd7, %rd6, %rd4;
	mov.u32 	%r772, %tid.x;
	cvt.u64.u32	%rd8, %r772;
	add.s64 	%rd1, %rd7, %rd8;
	ld.global.u64 	%rd9, [%rd5+16];
	ld.global.u64 	%rd10, [%rd5+24];
	ld.global.u64 	%rd11, [%rd5+32];
	ld.global.u64 	%rd12, [%rd5+40];
	ld.global.u64 	%rd13, [%rd5+48];
	ld.global.u64 	%rd14, [%rd5+56];
	ld.global.u64 	%rd15, [%rd5+64];
	ld.global.u32 	%rd16, [%rd5+72];
	add.s64 	%rd17, %rd1, -4965156021692249063;
	xor.b64  	%rd18, %rd17, 5840696475078001309;
	mov.b64	{%r773, %r774}, %rd18;
	mov.b64	%rd19, {%r774, %r773};
	add.s64 	%rd20, %rd19, 7640891576956012808;
	xor.b64  	%rd21, %rd20, 5840696475078001361;
	mov.b64	{%r775, %r776}, %rd21;
	mov.u32 	%r768, 1;
	mov.u32 	%r777, 25923;
	mov.u32 	%r778, 8455;
	prmt.b32 	%r779, %r775, %r776, %r778;
	prmt.b32 	%r780, %r775, %r776, %r777;
	mov.b64	%rd22, {%r780, %r779};
	ld.global.u64 	%rd23, [%rd5+8];
	add.s64 	%rd24, %rd17, %rd23;
	add.s64 	%rd25, %rd24, %rd22;
	xor.b64  	%rd26, %rd25, %rd19;
	mov.b64	{%r781, %r782}, %rd26;
	mov.u32 	%r783, 21554;
	mov.u32 	%r784, 4214;
	prmt.b32 	%r785, %r781, %r782, %r784;
	prmt.b32 	%r786, %r781, %r782, %r783;
	mov.b64	%rd27, {%r786, %r785};
	add.s64 	%rd28, %rd27, %rd20;
	xor.b64  	%rd29, %rd28, %rd22;
	mov.b64	{%r6, %r7}, %rd29;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r768;
	// inline asm
	mov.b64	%rd30, {%r1, %r5};
	add.s64 	%rd31, %rd9, 6227659224458531674;
	xor.b64  	%rd32, %rd31, -7276294671716946913;
	mov.b64	{%r787, %r788}, %rd32;
	mov.b64	%rd33, {%r788, %r787};
	add.s64 	%rd34, %rd33, -4942790177534073029;
	xor.b64  	%rd35, %rd34, -7276294671716946913;
	mov.b64	{%r789, %r790}, %rd35;
	prmt.b32 	%r791, %r789, %r790, %r778;
	prmt.b32 	%r792, %r789, %r790, %r777;
	mov.b64	%rd36, {%r792, %r791};
	add.s64 	%rd37, %rd10, %rd31;
	add.s64 	%rd38, %rd37, %rd36;
	xor.b64  	%rd39, %rd38, %rd33;
	mov.b64	{%r793, %r794}, %rd39;
	prmt.b32 	%r795, %r793, %r794, %r784;
	prmt.b32 	%r796, %r793, %r794, %r783;
	mov.b64	%rd40, {%r796, %r795};
	add.s64 	%rd41, %rd40, %rd34;
	xor.b64  	%rd42, %rd41, %rd36;
	mov.b64	{%r14, %r15}, %rd42;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r768;
	// inline asm
	mov.b64	%rd43, {%r9, %r13};
	add.s64 	%rd44, %rd11, 6625583534739731862;
	xor.b64  	%rd45, %rd44, -2270897969802886508;
	mov.b64	{%r797, %r798}, %rd45;
	mov.b64	%rd46, {%r798, %r797};
	add.s64 	%rd47, %rd46, 4354685564936845355;
	xor.b64  	%rd48, %rd47, 2270897969802886507;
	mov.b64	{%r799, %r800}, %rd48;
	prmt.b32 	%r801, %r799, %r800, %r778;
	prmt.b32 	%r802, %r799, %r800, %r777;
	mov.b64	%rd49, {%r802, %r801};
	add.s64 	%rd50, %rd12, %rd44;
	add.s64 	%rd51, %rd50, %rd49;
	xor.b64  	%rd52, %rd51, %rd46;
	mov.b64	{%r803, %r804}, %rd52;
	prmt.b32 	%r805, %r803, %r804, %r784;
	prmt.b32 	%r806, %r803, %r804, %r783;
	mov.b64	%rd53, {%r806, %r805};
	add.s64 	%rd54, %rd53, %rd47;
	xor.b64  	%rd55, %rd54, %rd49;
	mov.b64	{%r22, %r23}, %rd55;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r768;
	// inline asm
	mov.b64	%rd56, {%r17, %r21};
	add.s64 	%rd57, %rd13, 85782056580896874;
	xor.b64  	%rd58, %rd57, 6620516959819538809;
	mov.b64	{%r807, %r808}, %rd58;
	mov.b64	%rd59, {%r808, %r807};
	add.s64 	%rd60, %rd59, -6534734903238641935;
	xor.b64  	%rd61, %rd60, 6620516959819538809;
	mov.b64	{%r809, %r810}, %rd61;
	prmt.b32 	%r811, %r809, %r810, %r778;
	prmt.b32 	%r812, %r809, %r810, %r777;
	mov.b64	%rd62, {%r812, %r811};
	add.s64 	%rd63, %rd14, %rd57;
	add.s64 	%rd64, %rd63, %rd62;
	xor.b64  	%rd65, %rd64, %rd59;
	mov.b64	{%r813, %r814}, %rd65;
	prmt.b32 	%r815, %r813, %r814, %r784;
	prmt.b32 	%r816, %r813, %r814, %r783;
	mov.b64	%rd66, {%r816, %r815};
	add.s64 	%rd67, %rd66, %rd60;
	xor.b64  	%rd68, %rd67, %rd62;
	mov.b64	{%r30, %r31}, %rd68;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r768;
	// inline asm
	mov.b64	%rd69, {%r25, %r29};
	add.s64 	%rd70, %rd25, %rd15;
	add.s64 	%rd71, %rd70, %rd43;
	xor.b64  	%rd72, %rd66, %rd71;
	mov.b64	{%r817, %r818}, %rd72;
	mov.b64	%rd73, {%r818, %r817};
	add.s64 	%rd74, %rd73, %rd54;
	xor.b64  	%rd75, %rd74, %rd43;
	mov.b64	{%r819, %r820}, %rd75;
	prmt.b32 	%r821, %r819, %r820, %r778;
	prmt.b32 	%r822, %r819, %r820, %r777;
	mov.b64	%rd76, {%r822, %r821};
	add.s64 	%rd77, %rd71, %rd16;
	add.s64 	%rd78, %rd77, %rd76;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r823, %r824}, %rd79;
	prmt.b32 	%r825, %r823, %r824, %r784;
	prmt.b32 	%r826, %r823, %r824, %r783;
	mov.b64	%rd80, {%r826, %r825};
	add.s64 	%rd81, %rd74, %rd80;
	xor.b64  	%rd82, %rd81, %rd76;
	mov.b64	{%r38, %r39}, %rd82;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r768;
	// inline asm
	mov.b64	%rd83, {%r33, %r37};
	add.s64 	%rd84, %rd56, %rd38;
	xor.b64  	%rd85, %rd84, %rd27;
	mov.b64	{%r827, %r828}, %rd85;
	mov.b64	%rd86, {%r828, %r827};
	add.s64 	%rd87, %rd86, %rd67;
	xor.b64  	%rd88, %rd87, %rd56;
	mov.b64	{%r829, %r830}, %rd88;
	prmt.b32 	%r831, %r829, %r830, %r778;
	prmt.b32 	%r832, %r829, %r830, %r777;
	mov.b64	%rd89, {%r832, %r831};
	add.s64 	%rd90, %rd89, %rd84;
	xor.b64  	%rd91, %rd90, %rd86;
	mov.b64	{%r833, %r834}, %rd91;
	prmt.b32 	%r835, %r833, %r834, %r784;
	prmt.b32 	%r836, %r833, %r834, %r783;
	mov.b64	%rd92, {%r836, %r835};
	add.s64 	%rd93, %rd92, %rd87;
	xor.b64  	%rd94, %rd93, %rd89;
	mov.b64	{%r46, %r47}, %rd94;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r768;
	// inline asm
	mov.b64	%rd95, {%r41, %r45};
	add.s64 	%rd96, %rd69, %rd51;
	xor.b64  	%rd97, %rd96, %rd40;
	mov.b64	{%r837, %r838}, %rd97;
	mov.b64	%rd98, {%r838, %r837};
	add.s64 	%rd99, %rd98, %rd28;
	xor.b64  	%rd100, %rd99, %rd69;
	mov.b64	{%r839, %r840}, %rd100;
	prmt.b32 	%r841, %r839, %r840, %r778;
	prmt.b32 	%r842, %r839, %r840, %r777;
	mov.b64	%rd101, {%r842, %r841};
	add.s64 	%rd102, %rd101, %rd96;
	xor.b64  	%rd103, %rd102, %rd98;
	mov.b64	{%r843, %r844}, %rd103;
	prmt.b32 	%r845, %r843, %r844, %r784;
	prmt.b32 	%r846, %r843, %r844, %r783;
	mov.b64	%rd104, {%r846, %r845};
	add.s64 	%rd105, %rd104, %rd99;
	xor.b64  	%rd106, %rd105, %rd101;
	mov.b64	{%r54, %r55}, %rd106;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r768;
	// inline asm
	mov.b64	%rd107, {%r49, %r53};
	add.s64 	%rd108, %rd64, %rd30;
	xor.b64  	%rd109, %rd108, %rd53;
	mov.b64	{%r847, %r848}, %rd109;
	mov.b64	%rd110, {%r848, %r847};
	add.s64 	%rd111, %rd110, %rd41;
	xor.b64  	%rd112, %rd111, %rd30;
	mov.b64	{%r849, %r850}, %rd112;
	prmt.b32 	%r851, %r849, %r850, %r778;
	prmt.b32 	%r852, %r849, %r850, %r777;
	mov.b64	%rd113, {%r852, %r851};
	add.s64 	%rd114, %rd113, %rd108;
	xor.b64  	%rd115, %rd114, %rd110;
	mov.b64	{%r853, %r854}, %rd115;
	prmt.b32 	%r855, %r853, %r854, %r784;
	prmt.b32 	%r856, %r853, %r854, %r783;
	mov.b64	%rd116, {%r856, %r855};
	add.s64 	%rd117, %rd116, %rd111;
	xor.b64  	%rd118, %rd117, %rd113;
	mov.b64	{%r62, %r63}, %rd118;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r768;
	// inline asm
	mov.b64	%rd119, {%r57, %r61};
	add.s64 	%rd120, %rd119, %rd78;
	xor.b64  	%rd121, %rd120, %rd92;
	mov.b64	{%r857, %r858}, %rd121;
	mov.b64	%rd122, {%r858, %r857};
	add.s64 	%rd123, %rd122, %rd105;
	xor.b64  	%rd124, %rd123, %rd119;
	mov.b64	{%r859, %r860}, %rd124;
	prmt.b32 	%r861, %r859, %r860, %r778;
	prmt.b32 	%r862, %r859, %r860, %r777;
	mov.b64	%rd125, {%r862, %r861};
	add.s64 	%rd126, %rd125, %rd120;
	xor.b64  	%rd127, %rd122, %rd126;
	mov.b64	{%r863, %r864}, %rd127;
	prmt.b32 	%r865, %r863, %r864, %r784;
	prmt.b32 	%r866, %r863, %r864, %r783;
	mov.b64	%rd128, {%r866, %r865};
	add.s64 	%rd129, %rd123, %rd128;
	xor.b64  	%rd130, %rd129, %rd125;
	mov.b64	{%r70, %r71}, %rd130;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r768;
	// inline asm
	mov.b64	%rd131, {%r65, %r69};
	add.s64 	%rd132, %rd83, %rd11;
	add.s64 	%rd133, %rd132, %rd90;
	xor.b64  	%rd134, %rd104, %rd133;
	mov.b64	{%r867, %r868}, %rd134;
	mov.b64	%rd135, {%r868, %r867};
	add.s64 	%rd136, %rd117, %rd135;
	xor.b64  	%rd137, %rd136, %rd83;
	mov.b64	{%r869, %r870}, %rd137;
	prmt.b32 	%r871, %r869, %r870, %r778;
	prmt.b32 	%r872, %r869, %r870, %r777;
	mov.b64	%rd138, {%r872, %r871};
	add.s64 	%rd139, %rd133, %rd15;
	add.s64 	%rd140, %rd139, %rd138;
	xor.b64  	%rd141, %rd140, %rd135;
	mov.b64	{%r873, %r874}, %rd141;
	prmt.b32 	%r875, %r873, %r874, %r784;
	prmt.b32 	%r876, %r873, %r874, %r783;
	mov.b64	%rd142, {%r876, %r875};
	add.s64 	%rd143, %rd142, %rd136;
	xor.b64  	%rd144, %rd143, %rd138;
	mov.b64	{%r78, %r79}, %rd144;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r768;
	// inline asm
	mov.b64	%rd145, {%r73, %r77};
	add.s64 	%rd146, %rd95, %rd16;
	add.s64 	%rd147, %rd146, %rd102;
	xor.b64  	%rd148, %rd116, %rd147;
	mov.b64	{%r877, %r878}, %rd148;
	mov.b64	%rd149, {%r878, %r877};
	add.s64 	%rd150, %rd149, %rd81;
	xor.b64  	%rd151, %rd150, %rd95;
	mov.b64	{%r879, %r880}, %rd151;
	prmt.b32 	%r881, %r879, %r880, %r778;
	prmt.b32 	%r882, %r879, %r880, %r777;
	mov.b64	%rd152, {%r882, %r881};
	add.s64 	%rd153, %rd152, %rd147;
	xor.b64  	%rd154, %rd153, %rd149;
	mov.b64	{%r883, %r884}, %rd154;
	prmt.b32 	%r885, %r883, %r884, %r784;
	prmt.b32 	%r886, %r883, %r884, %r783;
	mov.b64	%rd155, {%r886, %r885};
	add.s64 	%rd156, %rd155, %rd150;
	xor.b64  	%rd157, %rd156, %rd152;
	mov.b64	{%r86, %r87}, %rd157;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r768;
	// inline asm
	mov.b64	%rd158, {%r81, %r85};
	add.s64 	%rd159, %rd114, %rd107;
	xor.b64  	%rd160, %rd159, %rd80;
	mov.b64	{%r887, %r888}, %rd160;
	mov.b64	%rd161, {%r888, %r887};
	add.s64 	%rd162, %rd161, %rd93;
	xor.b64  	%rd163, %rd162, %rd107;
	mov.b64	{%r889, %r890}, %rd163;
	prmt.b32 	%r891, %r889, %r890, %r778;
	prmt.b32 	%r892, %r889, %r890, %r777;
	mov.b64	%rd164, {%r892, %r891};
	add.s64 	%rd165, %rd159, %rd13;
	add.s64 	%rd166, %rd165, %rd164;
	xor.b64  	%rd167, %rd166, %rd161;
	mov.b64	{%r893, %r894}, %rd167;
	prmt.b32 	%r895, %r893, %r894, %r784;
	prmt.b32 	%r896, %r893, %r894, %r783;
	mov.b64	%rd168, {%r896, %r895};
	add.s64 	%rd169, %rd168, %rd162;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r94, %r95}, %rd170;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r768;
	// inline asm
	mov.b64	%rd171, {%r89, %r93};
	add.s64 	%rd172, %rd126, %rd23;
	add.s64 	%rd173, %rd172, %rd145;
	xor.b64  	%rd174, %rd168, %rd173;
	mov.b64	{%r897, %r898}, %rd174;
	mov.b64	%rd175, {%r898, %r897};
	add.s64 	%rd176, %rd175, %rd156;
	xor.b64  	%rd177, %rd176, %rd145;
	mov.b64	{%r899, %r900}, %rd177;
	prmt.b32 	%r901, %r899, %r900, %r778;
	prmt.b32 	%r902, %r899, %r900, %r777;
	mov.b64	%rd178, {%r902, %r901};
	add.s64 	%rd179, %rd178, %rd173;
	xor.b64  	%rd180, %rd175, %rd179;
	mov.b64	{%r903, %r904}, %rd180;
	prmt.b32 	%r905, %r903, %r904, %r784;
	prmt.b32 	%r906, %r903, %r904, %r783;
	mov.b64	%rd181, {%r906, %r905};
	add.s64 	%rd182, %rd176, %rd181;
	xor.b64  	%rd183, %rd182, %rd178;
	mov.b64	{%r102, %r103}, %rd183;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r768;
	// inline asm
	mov.b64	%rd184, {%r97, %r101};
	add.s64 	%rd185, %rd140, %rd1;
	add.s64 	%rd186, %rd185, %rd158;
	xor.b64  	%rd187, %rd186, %rd128;
	mov.b64	{%r907, %r908}, %rd187;
	mov.b64	%rd188, {%r908, %r907};
	add.s64 	%rd189, %rd188, %rd169;
	xor.b64  	%rd190, %rd189, %rd158;
	mov.b64	{%r909, %r910}, %rd190;
	prmt.b32 	%r911, %r909, %r910, %r778;
	prmt.b32 	%r912, %r909, %r910, %r777;
	mov.b64	%rd191, {%r912, %r911};
	add.s64 	%rd192, %rd186, %rd9;
	add.s64 	%rd193, %rd192, %rd191;
	xor.b64  	%rd194, %rd193, %rd188;
	mov.b64	{%r913, %r914}, %rd194;
	prmt.b32 	%r915, %r913, %r914, %r784;
	prmt.b32 	%r916, %r913, %r914, %r783;
	mov.b64	%rd195, {%r916, %r915};
	add.s64 	%rd196, %rd195, %rd189;
	xor.b64  	%rd197, %rd196, %rd191;
	mov.b64	{%r110, %r111}, %rd197;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r768;
	// inline asm
	mov.b64	%rd198, {%r105, %r109};
	add.s64 	%rd199, %rd171, %rd153;
	xor.b64  	%rd200, %rd199, %rd142;
	mov.b64	{%r917, %r918}, %rd200;
	mov.b64	%rd201, {%r918, %r917};
	add.s64 	%rd202, %rd201, %rd129;
	xor.b64  	%rd203, %rd202, %rd171;
	mov.b64	{%r919, %r920}, %rd203;
	prmt.b32 	%r921, %r919, %r920, %r778;
	prmt.b32 	%r922, %r919, %r920, %r777;
	mov.b64	%rd204, {%r922, %r921};
	add.s64 	%rd205, %rd199, %rd14;
	add.s64 	%rd206, %rd205, %rd204;
	xor.b64  	%rd207, %rd206, %rd201;
	mov.b64	{%r923, %r924}, %rd207;
	prmt.b32 	%r925, %r923, %r924, %r784;
	prmt.b32 	%r926, %r923, %r924, %r783;
	mov.b64	%rd208, {%r926, %r925};
	add.s64 	%rd209, %rd208, %rd202;
	xor.b64  	%rd210, %rd209, %rd204;
	mov.b64	{%r118, %r119}, %rd210;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r768;
	// inline asm
	mov.b64	%rd211, {%r113, %r117};
	add.s64 	%rd212, %rd131, %rd12;
	add.s64 	%rd213, %rd212, %rd166;
	xor.b64  	%rd214, %rd213, %rd155;
	mov.b64	{%r927, %r928}, %rd214;
	mov.b64	%rd215, {%r928, %r927};
	add.s64 	%rd216, %rd215, %rd143;
	xor.b64  	%rd217, %rd216, %rd131;
	mov.b64	{%r929, %r930}, %rd217;
	prmt.b32 	%r931, %r929, %r930, %r778;
	prmt.b32 	%r932, %r929, %r930, %r777;
	mov.b64	%rd218, {%r932, %r931};
	add.s64 	%rd219, %rd213, %rd10;
	add.s64 	%rd220, %rd219, %rd218;
	xor.b64  	%rd221, %rd220, %rd215;
	mov.b64	{%r933, %r934}, %rd221;
	prmt.b32 	%r935, %r933, %r934, %r784;
	prmt.b32 	%r936, %r933, %r934, %r783;
	mov.b64	%rd222, {%r936, %r935};
	add.s64 	%rd223, %rd222, %rd216;
	xor.b64  	%rd224, %rd223, %rd218;
	mov.b64	{%r126, %r127}, %rd224;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r768;
	// inline asm
	mov.b64	%rd225, {%r121, %r125};
	add.s64 	%rd226, %rd225, %rd179;
	xor.b64  	%rd227, %rd226, %rd195;
	mov.b64	{%r937, %r938}, %rd227;
	mov.b64	%rd228, {%r938, %r937};
	add.s64 	%rd229, %rd228, %rd209;
	xor.b64  	%rd230, %rd229, %rd225;
	mov.b64	{%r939, %r940}, %rd230;
	prmt.b32 	%r941, %r939, %r940, %r778;
	prmt.b32 	%r942, %r939, %r940, %r777;
	mov.b64	%rd231, {%r942, %r941};
	add.s64 	%rd232, %rd226, %rd15;
	add.s64 	%rd233, %rd232, %rd231;
	xor.b64  	%rd234, %rd228, %rd233;
	mov.b64	{%r943, %r944}, %rd234;
	prmt.b32 	%r945, %r943, %r944, %r784;
	prmt.b32 	%r946, %r943, %r944, %r783;
	mov.b64	%rd235, {%r946, %r945};
	add.s64 	%rd236, %rd229, %rd235;
	xor.b64  	%rd237, %rd236, %rd231;
	mov.b64	{%r134, %r135}, %rd237;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r768;
	// inline asm
	mov.b64	%rd238, {%r129, %r133};
	add.s64 	%rd239, %rd193, %rd184;
	xor.b64  	%rd240, %rd208, %rd239;
	mov.b64	{%r947, %r948}, %rd240;
	mov.b64	%rd241, {%r948, %r947};
	add.s64 	%rd242, %rd223, %rd241;
	xor.b64  	%rd243, %rd242, %rd184;
	mov.b64	{%r949, %r950}, %rd243;
	prmt.b32 	%r951, %r949, %r950, %r778;
	prmt.b32 	%r952, %r949, %r950, %r777;
	mov.b64	%rd244, {%r952, %r951};
	add.s64 	%rd245, %rd239, %rd1;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd241;
	mov.b64	{%r953, %r954}, %rd247;
	prmt.b32 	%r955, %r953, %r954, %r784;
	prmt.b32 	%r956, %r953, %r954, %r783;
	mov.b64	%rd248, {%r956, %r955};
	add.s64 	%rd249, %rd248, %rd242;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r142, %r143}, %rd250;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r768;
	// inline asm
	mov.b64	%rd251, {%r137, %r141};
	add.s64 	%rd252, %rd198, %rd12;
	add.s64 	%rd253, %rd252, %rd206;
	xor.b64  	%rd254, %rd222, %rd253;
	mov.b64	{%r957, %r958}, %rd254;
	mov.b64	%rd255, {%r958, %r957};
	add.s64 	%rd256, %rd255, %rd182;
	xor.b64  	%rd257, %rd256, %rd198;
	mov.b64	{%r959, %r960}, %rd257;
	prmt.b32 	%r961, %r959, %r960, %r778;
	prmt.b32 	%r962, %r959, %r960, %r777;
	mov.b64	%rd258, {%r962, %r961};
	add.s64 	%rd259, %rd253, %rd9;
	add.s64 	%rd260, %rd259, %rd258;
	xor.b64  	%rd261, %rd260, %rd255;
	mov.b64	{%r963, %r964}, %rd261;
	prmt.b32 	%r965, %r963, %r964, %r784;
	prmt.b32 	%r966, %r963, %r964, %r783;
	mov.b64	%rd262, {%r966, %r965};
	add.s64 	%rd263, %rd262, %rd256;
	xor.b64  	%rd264, %rd263, %rd258;
	mov.b64	{%r150, %r151}, %rd264;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r768;
	// inline asm
	mov.b64	%rd265, {%r145, %r149};
	add.s64 	%rd266, %rd220, %rd211;
	xor.b64  	%rd267, %rd266, %rd181;
	mov.b64	{%r967, %r968}, %rd267;
	mov.b64	%rd268, {%r968, %r967};
	add.s64 	%rd269, %rd268, %rd196;
	xor.b64  	%rd270, %rd269, %rd211;
	mov.b64	{%r969, %r970}, %rd270;
	prmt.b32 	%r971, %r969, %r970, %r778;
	prmt.b32 	%r972, %r969, %r970, %r777;
	mov.b64	%rd271, {%r972, %r971};
	add.s64 	%rd272, %rd271, %rd266;
	xor.b64  	%rd273, %rd272, %rd268;
	mov.b64	{%r973, %r974}, %rd273;
	prmt.b32 	%r975, %r973, %r974, %r784;
	prmt.b32 	%r976, %r973, %r974, %r783;
	mov.b64	%rd274, {%r976, %r975};
	add.s64 	%rd275, %rd274, %rd269;
	xor.b64  	%rd276, %rd275, %rd271;
	mov.b64	{%r158, %r159}, %rd276;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r768;
	// inline asm
	mov.b64	%rd277, {%r153, %r157};
	add.s64 	%rd278, %rd251, %rd233;
	xor.b64  	%rd279, %rd274, %rd278;
	mov.b64	{%r977, %r978}, %rd279;
	mov.b64	%rd280, {%r978, %r977};
	add.s64 	%rd281, %rd280, %rd263;
	xor.b64  	%rd282, %rd281, %rd251;
	mov.b64	{%r979, %r980}, %rd282;
	prmt.b32 	%r981, %r979, %r980, %r778;
	prmt.b32 	%r982, %r979, %r980, %r777;
	mov.b64	%rd283, {%r982, %r981};
	add.s64 	%rd284, %rd283, %rd278;
	xor.b64  	%rd285, %rd280, %rd284;
	mov.b64	{%r983, %r984}, %rd285;
	prmt.b32 	%r985, %r983, %r984, %r784;
	prmt.b32 	%r986, %r983, %r984, %r783;
	mov.b64	%rd286, {%r986, %r985};
	add.s64 	%rd287, %rd281, %rd286;
	xor.b64  	%rd288, %rd287, %rd283;
	mov.b64	{%r166, %r167}, %rd288;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r768;
	// inline asm
	mov.b64	%rd289, {%r161, %r165};
	add.s64 	%rd290, %rd246, %rd10;
	add.s64 	%rd291, %rd290, %rd265;
	xor.b64  	%rd292, %rd291, %rd235;
	mov.b64	{%r987, %r988}, %rd292;
	mov.b64	%rd293, {%r988, %r987};
	add.s64 	%rd294, %rd293, %rd275;
	xor.b64  	%rd295, %rd294, %rd265;
	mov.b64	{%r989, %r990}, %rd295;
	prmt.b32 	%r991, %r989, %r990, %r778;
	prmt.b32 	%r992, %r989, %r990, %r777;
	mov.b64	%rd296, {%r992, %r991};
	add.s64 	%rd297, %rd291, %rd13;
	add.s64 	%rd298, %rd297, %rd296;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r993, %r994}, %rd299;
	prmt.b32 	%r995, %r993, %r994, %r784;
	prmt.b32 	%r996, %r993, %r994, %r783;
	mov.b64	%rd300, {%r996, %r995};
	add.s64 	%rd301, %rd300, %rd294;
	xor.b64  	%rd302, %rd301, %rd296;
	mov.b64	{%r174, %r175}, %rd302;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r768;
	// inline asm
	mov.b64	%rd303, {%r169, %r173};
	add.s64 	%rd304, %rd260, %rd14;
	add.s64 	%rd305, %rd304, %rd277;
	xor.b64  	%rd306, %rd305, %rd248;
	mov.b64	{%r997, %r998}, %rd306;
	mov.b64	%rd307, {%r998, %r997};
	add.s64 	%rd308, %rd307, %rd236;
	xor.b64  	%rd309, %rd308, %rd277;
	mov.b64	{%r999, %r1000}, %rd309;
	prmt.b32 	%r1001, %r999, %r1000, %r778;
	prmt.b32 	%r1002, %r999, %r1000, %r777;
	mov.b64	%rd310, {%r1002, %r1001};
	add.s64 	%rd311, %rd305, %rd23;
	add.s64 	%rd312, %rd311, %rd310;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r1003, %r1004}, %rd313;
	prmt.b32 	%r1005, %r1003, %r1004, %r784;
	prmt.b32 	%r1006, %r1003, %r1004, %r783;
	mov.b64	%rd314, {%r1006, %r1005};
	add.s64 	%rd315, %rd314, %rd308;
	xor.b64  	%rd316, %rd315, %rd310;
	mov.b64	{%r182, %r183}, %rd316;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r768;
	// inline asm
	mov.b64	%rd317, {%r177, %r181};
	add.s64 	%rd318, %rd238, %rd16;
	add.s64 	%rd319, %rd318, %rd272;
	xor.b64  	%rd320, %rd319, %rd262;
	mov.b64	{%r1007, %r1008}, %rd320;
	mov.b64	%rd321, {%r1008, %r1007};
	add.s64 	%rd322, %rd321, %rd249;
	xor.b64  	%rd323, %rd322, %rd238;
	mov.b64	{%r1009, %r1010}, %rd323;
	prmt.b32 	%r1011, %r1009, %r1010, %r778;
	prmt.b32 	%r1012, %r1009, %r1010, %r777;
	mov.b64	%rd324, {%r1012, %r1011};
	add.s64 	%rd325, %rd319, %rd11;
	add.s64 	%rd326, %rd325, %rd324;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r1013, %r1014}, %rd327;
	prmt.b32 	%r1015, %r1013, %r1014, %r784;
	prmt.b32 	%r1016, %r1013, %r1014, %r783;
	mov.b64	%rd328, {%r1016, %r1015};
	add.s64 	%rd329, %rd328, %rd322;
	xor.b64  	%rd330, %rd329, %rd324;
	mov.b64	{%r190, %r191}, %rd330;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r768;
	// inline asm
	mov.b64	%rd331, {%r185, %r189};
	add.s64 	%rd332, %rd284, %rd14;
	add.s64 	%rd333, %rd332, %rd331;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1017, %r1018}, %rd334;
	mov.b64	%rd335, {%r1018, %r1017};
	add.s64 	%rd336, %rd335, %rd315;
	xor.b64  	%rd337, %rd336, %rd331;
	mov.b64	{%r1019, %r1020}, %rd337;
	prmt.b32 	%r1021, %r1019, %r1020, %r778;
	prmt.b32 	%r1022, %r1019, %r1020, %r777;
	mov.b64	%rd338, {%r1022, %r1021};
	add.s64 	%rd339, %rd333, %rd16;
	add.s64 	%rd340, %rd339, %rd338;
	xor.b64  	%rd341, %rd335, %rd340;
	mov.b64	{%r1023, %r1024}, %rd341;
	prmt.b32 	%r1025, %r1023, %r1024, %r784;
	prmt.b32 	%r1026, %r1023, %r1024, %r783;
	mov.b64	%rd342, {%r1026, %r1025};
	add.s64 	%rd343, %rd336, %rd342;
	xor.b64  	%rd344, %rd343, %rd338;
	mov.b64	{%r198, %r199}, %rd344;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r768;
	// inline asm
	mov.b64	%rd345, {%r193, %r197};
	add.s64 	%rd346, %rd289, %rd10;
	add.s64 	%rd347, %rd346, %rd298;
	xor.b64  	%rd348, %rd314, %rd347;
	mov.b64	{%r1027, %r1028}, %rd348;
	mov.b64	%rd349, {%r1028, %r1027};
	add.s64 	%rd350, %rd329, %rd349;
	xor.b64  	%rd351, %rd350, %rd289;
	mov.b64	{%r1029, %r1030}, %rd351;
	prmt.b32 	%r1031, %r1029, %r1030, %r778;
	prmt.b32 	%r1032, %r1029, %r1030, %r777;
	mov.b64	%rd352, {%r1032, %r1031};
	add.s64 	%rd353, %rd347, %rd23;
	add.s64 	%rd354, %rd353, %rd352;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r1033, %r1034}, %rd355;
	prmt.b32 	%r1035, %r1033, %r1034, %r784;
	prmt.b32 	%r1036, %r1033, %r1034, %r783;
	mov.b64	%rd356, {%r1036, %r1035};
	add.s64 	%rd357, %rd356, %rd350;
	xor.b64  	%rd358, %rd357, %rd352;
	mov.b64	{%r206, %r207}, %rd358;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r768;
	// inline asm
	mov.b64	%rd359, {%r201, %r205};
	add.s64 	%rd360, %rd312, %rd303;
	xor.b64  	%rd361, %rd328, %rd360;
	mov.b64	{%r1037, %r1038}, %rd361;
	mov.b64	%rd362, {%r1038, %r1037};
	add.s64 	%rd363, %rd362, %rd287;
	xor.b64  	%rd364, %rd363, %rd303;
	mov.b64	{%r1039, %r1040}, %rd364;
	prmt.b32 	%r1041, %r1039, %r1040, %r778;
	prmt.b32 	%r1042, %r1039, %r1040, %r777;
	mov.b64	%rd365, {%r1042, %r1041};
	add.s64 	%rd366, %rd365, %rd360;
	xor.b64  	%rd367, %rd366, %rd362;
	mov.b64	{%r1043, %r1044}, %rd367;
	prmt.b32 	%r1045, %r1043, %r1044, %r784;
	prmt.b32 	%r1046, %r1043, %r1044, %r783;
	mov.b64	%rd368, {%r1046, %r1045};
	add.s64 	%rd369, %rd368, %rd363;
	xor.b64  	%rd370, %rd369, %rd365;
	mov.b64	{%r214, %r215}, %rd370;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r768;
	// inline asm
	mov.b64	%rd371, {%r209, %r213};
	add.s64 	%rd372, %rd326, %rd317;
	xor.b64  	%rd373, %rd372, %rd286;
	mov.b64	{%r1047, %r1048}, %rd373;
	mov.b64	%rd374, {%r1048, %r1047};
	add.s64 	%rd375, %rd374, %rd301;
	xor.b64  	%rd376, %rd375, %rd317;
	mov.b64	{%r1049, %r1050}, %rd376;
	prmt.b32 	%r1051, %r1049, %r1050, %r778;
	prmt.b32 	%r1052, %r1049, %r1050, %r777;
	mov.b64	%rd377, {%r1052, %r1051};
	add.s64 	%rd378, %rd377, %rd372;
	xor.b64  	%rd379, %rd378, %rd374;
	mov.b64	{%r1053, %r1054}, %rd379;
	prmt.b32 	%r1055, %r1053, %r1054, %r784;
	prmt.b32 	%r1056, %r1053, %r1054, %r783;
	mov.b64	%rd380, {%r1056, %r1055};
	add.s64 	%rd381, %rd380, %rd375;
	xor.b64  	%rd382, %rd381, %rd377;
	mov.b64	{%r222, %r223}, %rd382;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r768;
	// inline asm
	mov.b64	%rd383, {%r217, %r221};
	add.s64 	%rd384, %rd340, %rd9;
	add.s64 	%rd385, %rd384, %rd359;
	xor.b64  	%rd386, %rd380, %rd385;
	mov.b64	{%r1057, %r1058}, %rd386;
	mov.b64	%rd387, {%r1058, %r1057};
	add.s64 	%rd388, %rd387, %rd369;
	xor.b64  	%rd389, %rd388, %rd359;
	mov.b64	{%r1059, %r1060}, %rd389;
	prmt.b32 	%r1061, %r1059, %r1060, %r778;
	prmt.b32 	%r1062, %r1059, %r1060, %r777;
	mov.b64	%rd390, {%r1062, %r1061};
	add.s64 	%rd391, %rd385, %rd13;
	add.s64 	%rd392, %rd391, %rd390;
	xor.b64  	%rd393, %rd387, %rd392;
	mov.b64	{%r1063, %r1064}, %rd393;
	prmt.b32 	%r1065, %r1063, %r1064, %r784;
	prmt.b32 	%r1066, %r1063, %r1064, %r783;
	mov.b64	%rd394, {%r1066, %r1065};
	add.s64 	%rd395, %rd388, %rd394;
	xor.b64  	%rd396, %rd395, %rd390;
	mov.b64	{%r230, %r231}, %rd396;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r768;
	// inline asm
	mov.b64	%rd397, {%r225, %r229};
	add.s64 	%rd398, %rd354, %rd12;
	add.s64 	%rd399, %rd398, %rd371;
	xor.b64  	%rd400, %rd399, %rd342;
	mov.b64	{%r1067, %r1068}, %rd400;
	mov.b64	%rd401, {%r1068, %r1067};
	add.s64 	%rd402, %rd401, %rd381;
	xor.b64  	%rd403, %rd402, %rd371;
	mov.b64	{%r1069, %r1070}, %rd403;
	prmt.b32 	%r1071, %r1069, %r1070, %r778;
	prmt.b32 	%r1072, %r1069, %r1070, %r777;
	mov.b64	%rd404, {%r1072, %r1071};
	add.s64 	%rd405, %rd404, %rd399;
	xor.b64  	%rd406, %rd405, %rd401;
	mov.b64	{%r1073, %r1074}, %rd406;
	prmt.b32 	%r1075, %r1073, %r1074, %r784;
	prmt.b32 	%r1076, %r1073, %r1074, %r783;
	mov.b64	%rd407, {%r1076, %r1075};
	add.s64 	%rd408, %rd407, %rd402;
	xor.b64  	%rd409, %rd408, %rd404;
	mov.b64	{%r238, %r239}, %rd409;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r768;
	// inline asm
	mov.b64	%rd410, {%r233, %r237};
	add.s64 	%rd411, %rd366, %rd11;
	add.s64 	%rd412, %rd411, %rd383;
	xor.b64  	%rd413, %rd412, %rd356;
	mov.b64	{%r1077, %r1078}, %rd413;
	mov.b64	%rd414, {%r1078, %r1077};
	add.s64 	%rd415, %rd414, %rd343;
	xor.b64  	%rd416, %rd415, %rd383;
	mov.b64	{%r1079, %r1080}, %rd416;
	prmt.b32 	%r1081, %r1079, %r1080, %r778;
	prmt.b32 	%r1082, %r1079, %r1080, %r777;
	mov.b64	%rd417, {%r1082, %r1081};
	add.s64 	%rd418, %rd412, %rd1;
	add.s64 	%rd419, %rd418, %rd417;
	xor.b64  	%rd420, %rd419, %rd414;
	mov.b64	{%r1083, %r1084}, %rd420;
	prmt.b32 	%r1085, %r1083, %r1084, %r784;
	prmt.b32 	%r1086, %r1083, %r1084, %r783;
	mov.b64	%rd421, {%r1086, %r1085};
	add.s64 	%rd422, %rd421, %rd415;
	xor.b64  	%rd423, %rd422, %rd417;
	mov.b64	{%r246, %r247}, %rd423;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r768;
	// inline asm
	mov.b64	%rd424, {%r241, %r245};
	add.s64 	%rd425, %rd378, %rd345;
	xor.b64  	%rd426, %rd425, %rd368;
	mov.b64	{%r1087, %r1088}, %rd426;
	mov.b64	%rd427, {%r1088, %r1087};
	add.s64 	%rd428, %rd427, %rd357;
	xor.b64  	%rd429, %rd428, %rd345;
	mov.b64	{%r1089, %r1090}, %rd429;
	prmt.b32 	%r1091, %r1089, %r1090, %r778;
	prmt.b32 	%r1092, %r1089, %r1090, %r777;
	mov.b64	%rd430, {%r1092, %r1091};
	add.s64 	%rd431, %rd425, %rd15;
	add.s64 	%rd432, %rd431, %rd430;
	xor.b64  	%rd433, %rd432, %rd427;
	mov.b64	{%r1093, %r1094}, %rd433;
	prmt.b32 	%r1095, %r1093, %r1094, %r784;
	prmt.b32 	%r1096, %r1093, %r1094, %r783;
	mov.b64	%rd434, {%r1096, %r1095};
	add.s64 	%rd435, %rd434, %rd428;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r254, %r255}, %rd436;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r768;
	// inline asm
	mov.b64	%rd437, {%r249, %r253};
	add.s64 	%rd438, %rd392, %rd16;
	add.s64 	%rd439, %rd438, %rd437;
	xor.b64  	%rd440, %rd439, %rd407;
	mov.b64	{%r1097, %r1098}, %rd440;
	mov.b64	%rd441, {%r1098, %r1097};
	add.s64 	%rd442, %rd441, %rd422;
	xor.b64  	%rd443, %rd442, %rd437;
	mov.b64	{%r1099, %r1100}, %rd443;
	prmt.b32 	%r1101, %r1099, %r1100, %r778;
	prmt.b32 	%r1102, %r1099, %r1100, %r777;
	mov.b64	%rd444, {%r1102, %r1101};
	add.s64 	%rd445, %rd439, %rd1;
	add.s64 	%rd446, %rd445, %rd444;
	xor.b64  	%rd447, %rd441, %rd446;
	mov.b64	{%r1103, %r1104}, %rd447;
	prmt.b32 	%r1105, %r1103, %r1104, %r784;
	prmt.b32 	%r1106, %r1103, %r1104, %r783;
	mov.b64	%rd448, {%r1106, %r1105};
	add.s64 	%rd449, %rd442, %rd448;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r262, %r263}, %rd450;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r768;
	// inline asm
	mov.b64	%rd451, {%r257, %r261};
	add.s64 	%rd452, %rd397, %rd12;
	add.s64 	%rd453, %rd452, %rd405;
	xor.b64  	%rd454, %rd421, %rd453;
	mov.b64	{%r1107, %r1108}, %rd454;
	mov.b64	%rd455, {%r1108, %r1107};
	add.s64 	%rd456, %rd435, %rd455;
	xor.b64  	%rd457, %rd456, %rd397;
	mov.b64	{%r1109, %r1110}, %rd457;
	prmt.b32 	%r1111, %r1109, %r1110, %r778;
	prmt.b32 	%r1112, %r1109, %r1110, %r777;
	mov.b64	%rd458, {%r1112, %r1111};
	add.s64 	%rd459, %rd453, %rd14;
	add.s64 	%rd460, %rd459, %rd458;
	xor.b64  	%rd461, %rd460, %rd455;
	mov.b64	{%r1113, %r1114}, %rd461;
	prmt.b32 	%r1115, %r1113, %r1114, %r784;
	prmt.b32 	%r1116, %r1113, %r1114, %r783;
	mov.b64	%rd462, {%r1116, %r1115};
	add.s64 	%rd463, %rd462, %rd456;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r270, %r271}, %rd464;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r768;
	// inline asm
	mov.b64	%rd465, {%r265, %r269};
	add.s64 	%rd466, %rd410, %rd9;
	add.s64 	%rd467, %rd466, %rd419;
	xor.b64  	%rd468, %rd434, %rd467;
	mov.b64	{%r1117, %r1118}, %rd468;
	mov.b64	%rd469, {%r1118, %r1117};
	add.s64 	%rd470, %rd469, %rd395;
	xor.b64  	%rd471, %rd470, %rd410;
	mov.b64	{%r1119, %r1120}, %rd471;
	prmt.b32 	%r1121, %r1119, %r1120, %r778;
	prmt.b32 	%r1122, %r1119, %r1120, %r777;
	mov.b64	%rd472, {%r1122, %r1121};
	add.s64 	%rd473, %rd467, %rd11;
	add.s64 	%rd474, %rd473, %rd472;
	xor.b64  	%rd475, %rd474, %rd469;
	mov.b64	{%r1123, %r1124}, %rd475;
	prmt.b32 	%r1125, %r1123, %r1124, %r784;
	prmt.b32 	%r1126, %r1123, %r1124, %r783;
	mov.b64	%rd476, {%r1126, %r1125};
	add.s64 	%rd477, %rd476, %rd470;
	xor.b64  	%rd478, %rd477, %rd472;
	mov.b64	{%r278, %r279}, %rd478;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r768;
	// inline asm
	mov.b64	%rd479, {%r273, %r277};
	add.s64 	%rd480, %rd432, %rd424;
	xor.b64  	%rd481, %rd480, %rd394;
	mov.b64	{%r1127, %r1128}, %rd481;
	mov.b64	%rd482, {%r1128, %r1127};
	add.s64 	%rd483, %rd482, %rd408;
	xor.b64  	%rd484, %rd483, %rd424;
	mov.b64	{%r1129, %r1130}, %rd484;
	prmt.b32 	%r1131, %r1129, %r1130, %r778;
	prmt.b32 	%r1132, %r1129, %r1130, %r777;
	mov.b64	%rd485, {%r1132, %r1131};
	add.s64 	%rd486, %rd485, %rd480;
	xor.b64  	%rd487, %rd486, %rd482;
	mov.b64	{%r1133, %r1134}, %rd487;
	prmt.b32 	%r1135, %r1133, %r1134, %r784;
	prmt.b32 	%r1136, %r1133, %r1134, %r783;
	mov.b64	%rd488, {%r1136, %r1135};
	add.s64 	%rd489, %rd488, %rd483;
	xor.b64  	%rd490, %rd489, %rd485;
	mov.b64	{%r286, %r287}, %rd490;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r768;
	// inline asm
	mov.b64	%rd491, {%r281, %r285};
	add.s64 	%rd492, %rd465, %rd446;
	xor.b64  	%rd493, %rd488, %rd492;
	mov.b64	{%r1137, %r1138}, %rd493;
	mov.b64	%rd494, {%r1138, %r1137};
	add.s64 	%rd495, %rd494, %rd477;
	xor.b64  	%rd496, %rd495, %rd465;
	mov.b64	{%r1139, %r1140}, %rd496;
	prmt.b32 	%r1141, %r1139, %r1140, %r778;
	prmt.b32 	%r1142, %r1139, %r1140, %r777;
	mov.b64	%rd497, {%r1142, %r1141};
	add.s64 	%rd498, %rd492, %rd23;
	add.s64 	%rd499, %rd498, %rd497;
	xor.b64  	%rd500, %rd494, %rd499;
	mov.b64	{%r1143, %r1144}, %rd500;
	prmt.b32 	%r1145, %r1143, %r1144, %r784;
	prmt.b32 	%r1146, %r1143, %r1144, %r783;
	mov.b64	%rd501, {%r1146, %r1145};
	add.s64 	%rd502, %rd495, %rd501;
	xor.b64  	%rd503, %rd502, %rd497;
	mov.b64	{%r294, %r295}, %rd503;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r768;
	// inline asm
	mov.b64	%rd504, {%r289, %r293};
	add.s64 	%rd505, %rd479, %rd460;
	xor.b64  	%rd506, %rd505, %rd448;
	mov.b64	{%r1147, %r1148}, %rd506;
	mov.b64	%rd507, {%r1148, %r1147};
	add.s64 	%rd508, %rd507, %rd489;
	xor.b64  	%rd509, %rd508, %rd479;
	mov.b64	{%r1149, %r1150}, %rd509;
	prmt.b32 	%r1151, %r1149, %r1150, %r778;
	prmt.b32 	%r1152, %r1149, %r1150, %r777;
	mov.b64	%rd510, {%r1152, %r1151};
	add.s64 	%rd511, %rd510, %rd505;
	xor.b64  	%rd512, %rd511, %rd507;
	mov.b64	{%r1153, %r1154}, %rd512;
	prmt.b32 	%r1155, %r1153, %r1154, %r784;
	prmt.b32 	%r1156, %r1153, %r1154, %r783;
	mov.b64	%rd513, {%r1156, %r1155};
	add.s64 	%rd514, %rd513, %rd508;
	xor.b64  	%rd515, %rd514, %rd510;
	mov.b64	{%r302, %r303}, %rd515;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r768;
	// inline asm
	mov.b64	%rd516, {%r297, %r301};
	add.s64 	%rd517, %rd474, %rd13;
	add.s64 	%rd518, %rd517, %rd491;
	xor.b64  	%rd519, %rd518, %rd462;
	mov.b64	{%r1157, %r1158}, %rd519;
	mov.b64	%rd520, {%r1158, %r1157};
	add.s64 	%rd521, %rd520, %rd449;
	xor.b64  	%rd522, %rd521, %rd491;
	mov.b64	{%r1159, %r1160}, %rd522;
	prmt.b32 	%r1161, %r1159, %r1160, %r778;
	prmt.b32 	%r1162, %r1159, %r1160, %r777;
	mov.b64	%rd523, {%r1162, %r1161};
	add.s64 	%rd524, %rd518, %rd15;
	add.s64 	%rd525, %rd524, %rd523;
	xor.b64  	%rd526, %rd525, %rd520;
	mov.b64	{%r1163, %r1164}, %rd526;
	prmt.b32 	%r1165, %r1163, %r1164, %r784;
	prmt.b32 	%r1166, %r1163, %r1164, %r783;
	mov.b64	%rd527, {%r1166, %r1165};
	add.s64 	%rd528, %rd527, %rd521;
	xor.b64  	%rd529, %rd528, %rd523;
	mov.b64	{%r310, %r311}, %rd529;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r768;
	// inline asm
	mov.b64	%rd530, {%r305, %r309};
	add.s64 	%rd531, %rd451, %rd10;
	add.s64 	%rd532, %rd531, %rd486;
	xor.b64  	%rd533, %rd532, %rd476;
	mov.b64	{%r1167, %r1168}, %rd533;
	mov.b64	%rd534, {%r1168, %r1167};
	add.s64 	%rd535, %rd534, %rd463;
	xor.b64  	%rd536, %rd535, %rd451;
	mov.b64	{%r1169, %r1170}, %rd536;
	prmt.b32 	%r1171, %r1169, %r1170, %r778;
	prmt.b32 	%r1172, %r1169, %r1170, %r777;
	mov.b64	%rd537, {%r1172, %r1171};
	add.s64 	%rd538, %rd537, %rd532;
	xor.b64  	%rd539, %rd538, %rd534;
	mov.b64	{%r1173, %r1174}, %rd539;
	prmt.b32 	%r1175, %r1173, %r1174, %r784;
	prmt.b32 	%r1176, %r1173, %r1174, %r783;
	mov.b64	%rd540, {%r1176, %r1175};
	add.s64 	%rd541, %rd540, %rd535;
	xor.b64  	%rd542, %rd541, %rd537;
	mov.b64	{%r318, %r319}, %rd542;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r768;
	// inline asm
	mov.b64	%rd543, {%r313, %r317};
	add.s64 	%rd544, %rd499, %rd9;
	add.s64 	%rd545, %rd544, %rd543;
	xor.b64  	%rd546, %rd545, %rd513;
	mov.b64	{%r1177, %r1178}, %rd546;
	mov.b64	%rd547, {%r1178, %r1177};
	add.s64 	%rd548, %rd547, %rd528;
	xor.b64  	%rd549, %rd548, %rd543;
	mov.b64	{%r1179, %r1180}, %rd549;
	prmt.b32 	%r1181, %r1179, %r1180, %r778;
	prmt.b32 	%r1182, %r1179, %r1180, %r777;
	mov.b64	%rd550, {%r1182, %r1181};
	add.s64 	%rd551, %rd550, %rd545;
	xor.b64  	%rd552, %rd547, %rd551;
	mov.b64	{%r1183, %r1184}, %rd552;
	prmt.b32 	%r1185, %r1183, %r1184, %r784;
	prmt.b32 	%r1186, %r1183, %r1184, %r783;
	mov.b64	%rd553, {%r1186, %r1185};
	add.s64 	%rd554, %rd548, %rd553;
	xor.b64  	%rd555, %rd554, %rd550;
	mov.b64	{%r326, %r327}, %rd555;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r768;
	// inline asm
	mov.b64	%rd556, {%r321, %r325};
	add.s64 	%rd557, %rd504, %rd13;
	add.s64 	%rd558, %rd557, %rd511;
	xor.b64  	%rd559, %rd527, %rd558;
	mov.b64	{%r1187, %r1188}, %rd559;
	mov.b64	%rd560, {%r1188, %r1187};
	add.s64 	%rd561, %rd541, %rd560;
	xor.b64  	%rd562, %rd561, %rd504;
	mov.b64	{%r1189, %r1190}, %rd562;
	prmt.b32 	%r1191, %r1189, %r1190, %r778;
	prmt.b32 	%r1192, %r1189, %r1190, %r777;
	mov.b64	%rd563, {%r1192, %r1191};
	add.s64 	%rd564, %rd563, %rd558;
	xor.b64  	%rd565, %rd564, %rd560;
	mov.b64	{%r1193, %r1194}, %rd565;
	prmt.b32 	%r1195, %r1193, %r1194, %r784;
	prmt.b32 	%r1196, %r1193, %r1194, %r783;
	mov.b64	%rd566, {%r1196, %r1195};
	add.s64 	%rd567, %rd566, %rd561;
	xor.b64  	%rd568, %rd567, %rd563;
	mov.b64	{%r334, %r335}, %rd568;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r768;
	// inline asm
	mov.b64	%rd569, {%r329, %r333};
	add.s64 	%rd570, %rd516, %rd1;
	add.s64 	%rd571, %rd570, %rd525;
	xor.b64  	%rd572, %rd540, %rd571;
	mov.b64	{%r1197, %r1198}, %rd572;
	mov.b64	%rd573, {%r1198, %r1197};
	add.s64 	%rd574, %rd573, %rd502;
	xor.b64  	%rd575, %rd574, %rd516;
	mov.b64	{%r1199, %r1200}, %rd575;
	prmt.b32 	%r1201, %r1199, %r1200, %r778;
	prmt.b32 	%r1202, %r1199, %r1200, %r777;
	mov.b64	%rd576, {%r1202, %r1201};
	add.s64 	%rd577, %rd576, %rd571;
	xor.b64  	%rd578, %rd577, %rd573;
	mov.b64	{%r1203, %r1204}, %rd578;
	prmt.b32 	%r1205, %r1203, %r1204, %r784;
	prmt.b32 	%r1206, %r1203, %r1204, %r783;
	mov.b64	%rd579, {%r1206, %r1205};
	add.s64 	%rd580, %rd579, %rd574;
	xor.b64  	%rd581, %rd580, %rd576;
	mov.b64	{%r342, %r343}, %rd581;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r768;
	// inline asm
	mov.b64	%rd582, {%r337, %r341};
	add.s64 	%rd583, %rd530, %rd15;
	add.s64 	%rd584, %rd583, %rd538;
	xor.b64  	%rd585, %rd584, %rd501;
	mov.b64	{%r1207, %r1208}, %rd585;
	mov.b64	%rd586, {%r1208, %r1207};
	add.s64 	%rd587, %rd586, %rd514;
	xor.b64  	%rd588, %rd587, %rd530;
	mov.b64	{%r1209, %r1210}, %rd588;
	prmt.b32 	%r1211, %r1209, %r1210, %r778;
	prmt.b32 	%r1212, %r1209, %r1210, %r777;
	mov.b64	%rd589, {%r1212, %r1211};
	add.s64 	%rd590, %rd584, %rd10;
	add.s64 	%rd591, %rd590, %rd589;
	xor.b64  	%rd592, %rd591, %rd586;
	mov.b64	{%r1213, %r1214}, %rd592;
	prmt.b32 	%r1215, %r1213, %r1214, %r784;
	prmt.b32 	%r1216, %r1213, %r1214, %r783;
	mov.b64	%rd593, {%r1216, %r1215};
	add.s64 	%rd594, %rd593, %rd587;
	xor.b64  	%rd595, %rd594, %rd589;
	mov.b64	{%r350, %r351}, %rd595;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r768;
	// inline asm
	mov.b64	%rd596, {%r345, %r349};
	add.s64 	%rd597, %rd551, %rd11;
	add.s64 	%rd598, %rd597, %rd569;
	xor.b64  	%rd599, %rd593, %rd598;
	mov.b64	{%r1217, %r1218}, %rd599;
	mov.b64	%rd600, {%r1218, %r1217};
	add.s64 	%rd601, %rd600, %rd580;
	xor.b64  	%rd602, %rd601, %rd569;
	mov.b64	{%r1219, %r1220}, %rd602;
	prmt.b32 	%r1221, %r1219, %r1220, %r778;
	prmt.b32 	%r1222, %r1219, %r1220, %r777;
	mov.b64	%rd603, {%r1222, %r1221};
	add.s64 	%rd604, %rd603, %rd598;
	xor.b64  	%rd605, %rd600, %rd604;
	mov.b64	{%r1223, %r1224}, %rd605;
	prmt.b32 	%r1225, %r1223, %r1224, %r784;
	prmt.b32 	%r1226, %r1223, %r1224, %r783;
	mov.b64	%rd606, {%r1226, %r1225};
	add.s64 	%rd607, %rd601, %rd606;
	xor.b64  	%rd608, %rd607, %rd603;
	mov.b64	{%r358, %r359}, %rd608;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r768;
	// inline asm
	mov.b64	%rd609, {%r353, %r357};
	add.s64 	%rd610, %rd564, %rd14;
	add.s64 	%rd611, %rd610, %rd582;
	xor.b64  	%rd612, %rd611, %rd553;
	mov.b64	{%r1227, %r1228}, %rd612;
	mov.b64	%rd613, {%r1228, %r1227};
	add.s64 	%rd614, %rd613, %rd594;
	xor.b64  	%rd615, %rd614, %rd582;
	mov.b64	{%r1229, %r1230}, %rd615;
	prmt.b32 	%r1231, %r1229, %r1230, %r778;
	prmt.b32 	%r1232, %r1229, %r1230, %r777;
	mov.b64	%rd616, {%r1232, %r1231};
	add.s64 	%rd617, %rd611, %rd12;
	add.s64 	%rd618, %rd617, %rd616;
	xor.b64  	%rd619, %rd618, %rd613;
	mov.b64	{%r1233, %r1234}, %rd619;
	prmt.b32 	%r1235, %r1233, %r1234, %r784;
	prmt.b32 	%r1236, %r1233, %r1234, %r783;
	mov.b64	%rd620, {%r1236, %r1235};
	add.s64 	%rd621, %rd620, %rd614;
	xor.b64  	%rd622, %rd621, %rd616;
	mov.b64	{%r366, %r367}, %rd622;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r768;
	// inline asm
	mov.b64	%rd623, {%r361, %r365};
	add.s64 	%rd624, %rd596, %rd577;
	xor.b64  	%rd625, %rd624, %rd566;
	mov.b64	{%r1237, %r1238}, %rd625;
	mov.b64	%rd626, {%r1238, %r1237};
	add.s64 	%rd627, %rd626, %rd554;
	xor.b64  	%rd628, %rd627, %rd596;
	mov.b64	{%r1239, %r1240}, %rd628;
	prmt.b32 	%r1241, %r1239, %r1240, %r778;
	prmt.b32 	%r1242, %r1239, %r1240, %r777;
	mov.b64	%rd629, {%r1242, %r1241};
	add.s64 	%rd630, %rd629, %rd624;
	xor.b64  	%rd631, %rd630, %rd626;
	mov.b64	{%r1243, %r1244}, %rd631;
	prmt.b32 	%r1245, %r1243, %r1244, %r784;
	prmt.b32 	%r1246, %r1243, %r1244, %r783;
	mov.b64	%rd632, {%r1246, %r1245};
	add.s64 	%rd633, %rd632, %rd627;
	xor.b64  	%rd634, %rd633, %rd629;
	mov.b64	{%r374, %r375}, %rd634;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r768;
	// inline asm
	mov.b64	%rd635, {%r369, %r373};
	add.s64 	%rd636, %rd556, %rd23;
	add.s64 	%rd637, %rd636, %rd591;
	xor.b64  	%rd638, %rd637, %rd579;
	mov.b64	{%r1247, %r1248}, %rd638;
	mov.b64	%rd639, {%r1248, %r1247};
	add.s64 	%rd640, %rd639, %rd567;
	xor.b64  	%rd641, %rd640, %rd556;
	mov.b64	{%r1249, %r1250}, %rd641;
	prmt.b32 	%r1251, %r1249, %r1250, %r778;
	prmt.b32 	%r1252, %r1249, %r1250, %r777;
	mov.b64	%rd642, {%r1252, %r1251};
	add.s64 	%rd643, %rd637, %rd16;
	add.s64 	%rd644, %rd643, %rd642;
	xor.b64  	%rd645, %rd644, %rd639;
	mov.b64	{%r1253, %r1254}, %rd645;
	prmt.b32 	%r1255, %r1253, %r1254, %r784;
	prmt.b32 	%r1256, %r1253, %r1254, %r783;
	mov.b64	%rd646, {%r1256, %r1255};
	add.s64 	%rd647, %rd646, %rd640;
	xor.b64  	%rd648, %rd647, %rd642;
	mov.b64	{%r382, %r383}, %rd648;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r768;
	// inline asm
	mov.b64	%rd649, {%r377, %r381};
	add.s64 	%rd650, %rd649, %rd604;
	xor.b64  	%rd651, %rd650, %rd620;
	mov.b64	{%r1257, %r1258}, %rd651;
	mov.b64	%rd652, {%r1258, %r1257};
	add.s64 	%rd653, %rd652, %rd633;
	xor.b64  	%rd654, %rd653, %rd649;
	mov.b64	{%r1259, %r1260}, %rd654;
	prmt.b32 	%r1261, %r1259, %r1260, %r778;
	prmt.b32 	%r1262, %r1259, %r1260, %r777;
	mov.b64	%rd655, {%r1262, %r1261};
	add.s64 	%rd656, %rd650, %rd12;
	add.s64 	%rd657, %rd656, %rd655;
	xor.b64  	%rd658, %rd652, %rd657;
	mov.b64	{%r1263, %r1264}, %rd658;
	prmt.b32 	%r1265, %r1263, %r1264, %r784;
	prmt.b32 	%r1266, %r1263, %r1264, %r783;
	mov.b64	%rd659, {%r1266, %r1265};
	add.s64 	%rd660, %rd653, %rd659;
	xor.b64  	%rd661, %rd660, %rd655;
	mov.b64	{%r390, %r391}, %rd661;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r768;
	// inline asm
	mov.b64	%rd662, {%r385, %r389};
	add.s64 	%rd663, %rd609, %rd23;
	add.s64 	%rd664, %rd663, %rd618;
	xor.b64  	%rd665, %rd632, %rd664;
	mov.b64	{%r1267, %r1268}, %rd665;
	mov.b64	%rd666, {%r1268, %r1267};
	add.s64 	%rd667, %rd647, %rd666;
	xor.b64  	%rd668, %rd667, %rd609;
	mov.b64	{%r1269, %r1270}, %rd668;
	prmt.b32 	%r1271, %r1269, %r1270, %r778;
	prmt.b32 	%r1272, %r1269, %r1270, %r777;
	mov.b64	%rd669, {%r1272, %r1271};
	add.s64 	%rd670, %rd669, %rd664;
	xor.b64  	%rd671, %rd670, %rd666;
	mov.b64	{%r1273, %r1274}, %rd671;
	prmt.b32 	%r1275, %r1273, %r1274, %r784;
	prmt.b32 	%r1276, %r1273, %r1274, %r783;
	mov.b64	%rd672, {%r1276, %r1275};
	add.s64 	%rd673, %rd672, %rd667;
	xor.b64  	%rd674, %rd673, %rd669;
	mov.b64	{%r398, %r399}, %rd674;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r768;
	// inline asm
	mov.b64	%rd675, {%r393, %r397};
	add.s64 	%rd676, %rd630, %rd623;
	xor.b64  	%rd677, %rd646, %rd676;
	mov.b64	{%r1277, %r1278}, %rd677;
	mov.b64	%rd678, {%r1278, %r1277};
	add.s64 	%rd679, %rd678, %rd607;
	xor.b64  	%rd680, %rd679, %rd623;
	mov.b64	{%r1279, %r1280}, %rd680;
	prmt.b32 	%r1281, %r1279, %r1280, %r778;
	prmt.b32 	%r1282, %r1279, %r1280, %r777;
	mov.b64	%rd681, {%r1282, %r1281};
	add.s64 	%rd682, %rd681, %rd676;
	xor.b64  	%rd683, %rd682, %rd678;
	mov.b64	{%r1283, %r1284}, %rd683;
	prmt.b32 	%r1285, %r1283, %r1284, %r784;
	prmt.b32 	%r1286, %r1283, %r1284, %r783;
	mov.b64	%rd684, {%r1286, %r1285};
	add.s64 	%rd685, %rd684, %rd679;
	xor.b64  	%rd686, %rd685, %rd681;
	mov.b64	{%r406, %r407}, %rd686;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r768;
	// inline asm
	mov.b64	%rd687, {%r401, %r405};
	add.s64 	%rd688, %rd635, %rd11;
	add.s64 	%rd689, %rd688, %rd644;
	xor.b64  	%rd690, %rd689, %rd606;
	mov.b64	{%r1287, %r1288}, %rd690;
	mov.b64	%rd691, {%r1288, %r1287};
	add.s64 	%rd692, %rd691, %rd621;
	xor.b64  	%rd693, %rd692, %rd635;
	mov.b64	{%r1289, %r1290}, %rd693;
	prmt.b32 	%r1291, %r1289, %r1290, %r778;
	prmt.b32 	%r1292, %r1289, %r1290, %r777;
	mov.b64	%rd694, {%r1292, %r1291};
	add.s64 	%rd695, %rd694, %rd689;
	xor.b64  	%rd696, %rd695, %rd691;
	mov.b64	{%r1293, %r1294}, %rd696;
	prmt.b32 	%r1295, %r1293, %r1294, %r784;
	prmt.b32 	%r1296, %r1293, %r1294, %r783;
	mov.b64	%rd697, {%r1296, %r1295};
	add.s64 	%rd698, %rd697, %rd692;
	xor.b64  	%rd699, %rd698, %rd694;
	mov.b64	{%r414, %r415}, %rd699;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r768;
	// inline asm
	mov.b64	%rd700, {%r409, %r413};
	add.s64 	%rd701, %rd657, %rd1;
	add.s64 	%rd702, %rd701, %rd675;
	xor.b64  	%rd703, %rd697, %rd702;
	mov.b64	{%r1297, %r1298}, %rd703;
	mov.b64	%rd704, {%r1298, %r1297};
	add.s64 	%rd705, %rd704, %rd685;
	xor.b64  	%rd706, %rd705, %rd675;
	mov.b64	{%r1299, %r1300}, %rd706;
	prmt.b32 	%r1301, %r1299, %r1300, %r778;
	prmt.b32 	%r1302, %r1299, %r1300, %r777;
	mov.b64	%rd707, {%r1302, %r1301};
	add.s64 	%rd708, %rd702, %rd14;
	add.s64 	%rd709, %rd708, %rd707;
	xor.b64  	%rd710, %rd704, %rd709;
	mov.b64	{%r1303, %r1304}, %rd710;
	prmt.b32 	%r1305, %r1303, %r1304, %r784;
	prmt.b32 	%r1306, %r1303, %r1304, %r783;
	mov.b64	%rd711, {%r1306, %r1305};
	add.s64 	%rd712, %rd705, %rd711;
	xor.b64  	%rd713, %rd712, %rd707;
	mov.b64	{%r422, %r423}, %rd713;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r768;
	// inline asm
	mov.b64	%rd714, {%r417, %r421};
	add.s64 	%rd715, %rd670, %rd13;
	add.s64 	%rd716, %rd715, %rd687;
	xor.b64  	%rd717, %rd716, %rd659;
	mov.b64	{%r1307, %r1308}, %rd717;
	mov.b64	%rd718, {%r1308, %r1307};
	add.s64 	%rd719, %rd718, %rd698;
	xor.b64  	%rd720, %rd719, %rd687;
	mov.b64	{%r1309, %r1310}, %rd720;
	prmt.b32 	%r1311, %r1309, %r1310, %r778;
	prmt.b32 	%r1312, %r1309, %r1310, %r777;
	mov.b64	%rd721, {%r1312, %r1311};
	add.s64 	%rd722, %rd716, %rd10;
	add.s64 	%rd723, %rd722, %rd721;
	xor.b64  	%rd724, %rd723, %rd718;
	mov.b64	{%r1313, %r1314}, %rd724;
	prmt.b32 	%r1315, %r1313, %r1314, %r784;
	prmt.b32 	%r1316, %r1313, %r1314, %r783;
	mov.b64	%rd725, {%r1316, %r1315};
	add.s64 	%rd726, %rd725, %rd719;
	xor.b64  	%rd727, %rd726, %rd721;
	mov.b64	{%r430, %r431}, %rd727;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r768;
	// inline asm
	mov.b64	%rd728, {%r425, %r429};
	add.s64 	%rd729, %rd682, %rd16;
	add.s64 	%rd730, %rd729, %rd700;
	xor.b64  	%rd731, %rd730, %rd672;
	mov.b64	{%r1317, %r1318}, %rd731;
	mov.b64	%rd732, {%r1318, %r1317};
	add.s64 	%rd733, %rd732, %rd660;
	xor.b64  	%rd734, %rd733, %rd700;
	mov.b64	{%r1319, %r1320}, %rd734;
	prmt.b32 	%r1321, %r1319, %r1320, %r778;
	prmt.b32 	%r1322, %r1319, %r1320, %r777;
	mov.b64	%rd735, {%r1322, %r1321};
	add.s64 	%rd736, %rd730, %rd9;
	add.s64 	%rd737, %rd736, %rd735;
	xor.b64  	%rd738, %rd737, %rd732;
	mov.b64	{%r1323, %r1324}, %rd738;
	prmt.b32 	%r1325, %r1323, %r1324, %r784;
	prmt.b32 	%r1326, %r1323, %r1324, %r783;
	mov.b64	%rd739, {%r1326, %r1325};
	add.s64 	%rd740, %rd739, %rd733;
	xor.b64  	%rd741, %rd740, %rd735;
	mov.b64	{%r438, %r439}, %rd741;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r768;
	// inline asm
	mov.b64	%rd742, {%r433, %r437};
	add.s64 	%rd743, %rd662, %rd15;
	add.s64 	%rd744, %rd743, %rd695;
	xor.b64  	%rd745, %rd744, %rd684;
	mov.b64	{%r1327, %r1328}, %rd745;
	mov.b64	%rd746, {%r1328, %r1327};
	add.s64 	%rd747, %rd746, %rd673;
	xor.b64  	%rd748, %rd747, %rd662;
	mov.b64	{%r1329, %r1330}, %rd748;
	prmt.b32 	%r1331, %r1329, %r1330, %r778;
	prmt.b32 	%r1332, %r1329, %r1330, %r777;
	mov.b64	%rd749, {%r1332, %r1331};
	add.s64 	%rd750, %rd749, %rd744;
	xor.b64  	%rd751, %rd750, %rd746;
	mov.b64	{%r1333, %r1334}, %rd751;
	prmt.b32 	%r1335, %r1333, %r1334, %r784;
	prmt.b32 	%r1336, %r1333, %r1334, %r783;
	mov.b64	%rd752, {%r1336, %r1335};
	add.s64 	%rd753, %rd752, %rd747;
	xor.b64  	%rd754, %rd753, %rd749;
	mov.b64	{%r446, %r447}, %rd754;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r768;
	// inline asm
	mov.b64	%rd755, {%r441, %r445};
	add.s64 	%rd756, %rd755, %rd709;
	xor.b64  	%rd757, %rd756, %rd725;
	mov.b64	{%r1337, %r1338}, %rd757;
	mov.b64	%rd758, {%r1338, %r1337};
	add.s64 	%rd759, %rd758, %rd740;
	xor.b64  	%rd760, %rd759, %rd755;
	mov.b64	{%r1339, %r1340}, %rd760;
	prmt.b32 	%r1341, %r1339, %r1340, %r778;
	prmt.b32 	%r1342, %r1339, %r1340, %r777;
	mov.b64	%rd761, {%r1342, %r1341};
	add.s64 	%rd762, %rd761, %rd756;
	xor.b64  	%rd763, %rd758, %rd762;
	mov.b64	{%r1343, %r1344}, %rd763;
	prmt.b32 	%r1345, %r1343, %r1344, %r784;
	prmt.b32 	%r1346, %r1343, %r1344, %r783;
	mov.b64	%rd764, {%r1346, %r1345};
	add.s64 	%rd765, %rd759, %rd764;
	xor.b64  	%rd766, %rd765, %rd761;
	mov.b64	{%r454, %r455}, %rd766;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r768;
	// inline asm
	mov.b64	%rd767, {%r449, %r453};
	add.s64 	%rd768, %rd714, %rd14;
	add.s64 	%rd769, %rd768, %rd723;
	xor.b64  	%rd770, %rd739, %rd769;
	mov.b64	{%r1347, %r1348}, %rd770;
	mov.b64	%rd771, {%r1348, %r1347};
	add.s64 	%rd772, %rd753, %rd771;
	xor.b64  	%rd773, %rd772, %rd714;
	mov.b64	{%r1349, %r1350}, %rd773;
	prmt.b32 	%r1351, %r1349, %r1350, %r778;
	prmt.b32 	%r1352, %r1349, %r1350, %r777;
	mov.b64	%rd774, {%r1352, %r1351};
	add.s64 	%rd775, %rd774, %rd769;
	xor.b64  	%rd776, %rd775, %rd771;
	mov.b64	{%r1353, %r1354}, %rd776;
	prmt.b32 	%r1355, %r1353, %r1354, %r784;
	prmt.b32 	%r1356, %r1353, %r1354, %r783;
	mov.b64	%rd777, {%r1356, %r1355};
	add.s64 	%rd778, %rd777, %rd772;
	xor.b64  	%rd779, %rd778, %rd774;
	mov.b64	{%r462, %r463}, %rd779;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r768;
	// inline asm
	mov.b64	%rd780, {%r457, %r461};
	add.s64 	%rd781, %rd737, %rd728;
	xor.b64  	%rd782, %rd752, %rd781;
	mov.b64	{%r1357, %r1358}, %rd782;
	mov.b64	%rd783, {%r1358, %r1357};
	add.s64 	%rd784, %rd783, %rd712;
	xor.b64  	%rd785, %rd784, %rd728;
	mov.b64	{%r1359, %r1360}, %rd785;
	prmt.b32 	%r1361, %r1359, %r1360, %r778;
	prmt.b32 	%r1362, %r1359, %r1360, %r777;
	mov.b64	%rd786, {%r1362, %r1361};
	add.s64 	%rd787, %rd781, %rd23;
	add.s64 	%rd788, %rd787, %rd786;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r1363, %r1364}, %rd789;
	prmt.b32 	%r1365, %r1363, %r1364, %r784;
	prmt.b32 	%r1366, %r1363, %r1364, %r783;
	mov.b64	%rd790, {%r1366, %r1365};
	add.s64 	%rd791, %rd790, %rd784;
	xor.b64  	%rd792, %rd791, %rd786;
	mov.b64	{%r470, %r471}, %rd792;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r768;
	// inline asm
	mov.b64	%rd793, {%r465, %r469};
	add.s64 	%rd794, %rd742, %rd10;
	add.s64 	%rd795, %rd794, %rd750;
	xor.b64  	%rd796, %rd795, %rd711;
	mov.b64	{%r1367, %r1368}, %rd796;
	mov.b64	%rd797, {%r1368, %r1367};
	add.s64 	%rd798, %rd797, %rd726;
	xor.b64  	%rd799, %rd798, %rd742;
	mov.b64	{%r1369, %r1370}, %rd799;
	prmt.b32 	%r1371, %r1369, %r1370, %r778;
	prmt.b32 	%r1372, %r1369, %r1370, %r777;
	mov.b64	%rd800, {%r1372, %r1371};
	add.s64 	%rd801, %rd795, %rd16;
	add.s64 	%rd802, %rd801, %rd800;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r1373, %r1374}, %rd803;
	prmt.b32 	%r1375, %r1373, %r1374, %r784;
	prmt.b32 	%r1376, %r1373, %r1374, %r783;
	mov.b64	%rd804, {%r1376, %r1375};
	add.s64 	%rd805, %rd804, %rd798;
	xor.b64  	%rd806, %rd805, %rd800;
	mov.b64	{%r478, %r479}, %rd806;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r768;
	// inline asm
	mov.b64	%rd807, {%r473, %r477};
	add.s64 	%rd808, %rd762, %rd12;
	add.s64 	%rd809, %rd808, %rd780;
	xor.b64  	%rd810, %rd804, %rd809;
	mov.b64	{%r1377, %r1378}, %rd810;
	mov.b64	%rd811, {%r1378, %r1377};
	add.s64 	%rd812, %rd811, %rd791;
	xor.b64  	%rd813, %rd812, %rd780;
	mov.b64	{%r1379, %r1380}, %rd813;
	prmt.b32 	%r1381, %r1379, %r1380, %r778;
	prmt.b32 	%r1382, %r1379, %r1380, %r777;
	mov.b64	%rd814, {%r1382, %r1381};
	add.s64 	%rd815, %rd809, %rd1;
	add.s64 	%rd816, %rd815, %rd814;
	xor.b64  	%rd817, %rd811, %rd816;
	mov.b64	{%r1383, %r1384}, %rd817;
	prmt.b32 	%r1385, %r1383, %r1384, %r784;
	prmt.b32 	%r1386, %r1383, %r1384, %r783;
	mov.b64	%rd818, {%r1386, %r1385};
	add.s64 	%rd819, %rd812, %rd818;
	xor.b64  	%rd820, %rd819, %rd814;
	mov.b64	{%r486, %r487}, %rd820;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r768;
	// inline asm
	mov.b64	%rd821, {%r481, %r485};
	add.s64 	%rd822, %rd793, %rd775;
	xor.b64  	%rd823, %rd822, %rd764;
	mov.b64	{%r1387, %r1388}, %rd823;
	mov.b64	%rd824, {%r1388, %r1387};
	add.s64 	%rd825, %rd824, %rd805;
	xor.b64  	%rd826, %rd825, %rd793;
	mov.b64	{%r1389, %r1390}, %rd826;
	prmt.b32 	%r1391, %r1389, %r1390, %r778;
	prmt.b32 	%r1392, %r1389, %r1390, %r777;
	mov.b64	%rd827, {%r1392, %r1391};
	add.s64 	%rd828, %rd822, %rd11;
	add.s64 	%rd829, %rd828, %rd827;
	xor.b64  	%rd830, %rd829, %rd824;
	mov.b64	{%r1393, %r1394}, %rd830;
	prmt.b32 	%r1395, %r1393, %r1394, %r784;
	prmt.b32 	%r1396, %r1393, %r1394, %r783;
	mov.b64	%rd831, {%r1396, %r1395};
	add.s64 	%rd832, %rd831, %rd825;
	xor.b64  	%rd833, %rd832, %rd827;
	mov.b64	{%r494, %r495}, %rd833;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r768;
	// inline asm
	mov.b64	%rd834, {%r489, %r493};
	add.s64 	%rd835, %rd788, %rd15;
	add.s64 	%rd836, %rd835, %rd807;
	xor.b64  	%rd837, %rd836, %rd777;
	mov.b64	{%r1397, %r1398}, %rd837;
	mov.b64	%rd838, {%r1398, %r1397};
	add.s64 	%rd839, %rd838, %rd765;
	xor.b64  	%rd840, %rd839, %rd807;
	mov.b64	{%r1399, %r1400}, %rd840;
	prmt.b32 	%r1401, %r1399, %r1400, %r778;
	prmt.b32 	%r1402, %r1399, %r1400, %r777;
	mov.b64	%rd841, {%r1402, %r1401};
	add.s64 	%rd842, %rd836, %rd13;
	add.s64 	%rd843, %rd842, %rd841;
	xor.b64  	%rd844, %rd843, %rd838;
	mov.b64	{%r1403, %r1404}, %rd844;
	prmt.b32 	%r1405, %r1403, %r1404, %r784;
	prmt.b32 	%r1406, %r1403, %r1404, %r783;
	mov.b64	%rd845, {%r1406, %r1405};
	add.s64 	%rd846, %rd845, %rd839;
	xor.b64  	%rd847, %rd846, %rd841;
	mov.b64	{%r502, %r503}, %rd847;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r768;
	// inline asm
	mov.b64	%rd848, {%r497, %r501};
	add.s64 	%rd849, %rd767, %rd9;
	add.s64 	%rd850, %rd849, %rd802;
	xor.b64  	%rd851, %rd850, %rd790;
	mov.b64	{%r1407, %r1408}, %rd851;
	mov.b64	%rd852, {%r1408, %r1407};
	add.s64 	%rd853, %rd852, %rd778;
	xor.b64  	%rd854, %rd853, %rd767;
	mov.b64	{%r1409, %r1410}, %rd854;
	prmt.b32 	%r1411, %r1409, %r1410, %r778;
	prmt.b32 	%r1412, %r1409, %r1410, %r777;
	mov.b64	%rd855, {%r1412, %r1411};
	add.s64 	%rd856, %rd855, %rd850;
	xor.b64  	%rd857, %rd856, %rd852;
	mov.b64	{%r1413, %r1414}, %rd857;
	prmt.b32 	%r1415, %r1413, %r1414, %r784;
	prmt.b32 	%r1416, %r1413, %r1414, %r783;
	mov.b64	%rd858, {%r1416, %r1415};
	add.s64 	%rd859, %rd858, %rd853;
	xor.b64  	%rd860, %rd859, %rd855;
	mov.b64	{%r510, %r511}, %rd860;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r768;
	// inline asm
	mov.b64	%rd861, {%r505, %r509};
	add.s64 	%rd862, %rd816, %rd13;
	add.s64 	%rd863, %rd862, %rd861;
	xor.b64  	%rd864, %rd863, %rd831;
	mov.b64	{%r1417, %r1418}, %rd864;
	mov.b64	%rd865, {%r1418, %r1417};
	add.s64 	%rd866, %rd865, %rd846;
	xor.b64  	%rd867, %rd866, %rd861;
	mov.b64	{%r1419, %r1420}, %rd867;
	prmt.b32 	%r1421, %r1419, %r1420, %r778;
	prmt.b32 	%r1422, %r1419, %r1420, %r777;
	mov.b64	%rd868, {%r1422, %r1421};
	add.s64 	%rd869, %rd868, %rd863;
	xor.b64  	%rd870, %rd865, %rd869;
	mov.b64	{%r1423, %r1424}, %rd870;
	prmt.b32 	%r1425, %r1423, %r1424, %r784;
	prmt.b32 	%r1426, %r1423, %r1424, %r783;
	mov.b64	%rd871, {%r1426, %r1425};
	add.s64 	%rd872, %rd866, %rd871;
	xor.b64  	%rd873, %rd872, %rd868;
	mov.b64	{%r518, %r519}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r768;
	// inline asm
	mov.b64	%rd874, {%r513, %r517};
	add.s64 	%rd875, %rd829, %rd821;
	xor.b64  	%rd876, %rd845, %rd875;
	mov.b64	{%r1427, %r1428}, %rd876;
	mov.b64	%rd877, {%r1428, %r1427};
	add.s64 	%rd878, %rd859, %rd877;
	xor.b64  	%rd879, %rd878, %rd821;
	mov.b64	{%r1429, %r1430}, %rd879;
	prmt.b32 	%r1431, %r1429, %r1430, %r778;
	prmt.b32 	%r1432, %r1429, %r1430, %r777;
	mov.b64	%rd880, {%r1432, %r1431};
	add.s64 	%rd881, %rd875, %rd16;
	add.s64 	%rd882, %rd881, %rd880;
	xor.b64  	%rd883, %rd882, %rd877;
	mov.b64	{%r1433, %r1434}, %rd883;
	prmt.b32 	%r1435, %r1433, %r1434, %r784;
	prmt.b32 	%r1436, %r1433, %r1434, %r783;
	mov.b64	%rd884, {%r1436, %r1435};
	add.s64 	%rd885, %rd884, %rd878;
	xor.b64  	%rd886, %rd885, %rd880;
	mov.b64	{%r526, %r527}, %rd886;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r768;
	// inline asm
	mov.b64	%rd887, {%r521, %r525};
	add.s64 	%rd888, %rd843, %rd834;
	xor.b64  	%rd889, %rd858, %rd888;
	mov.b64	{%r1437, %r1438}, %rd889;
	mov.b64	%rd890, {%r1438, %r1437};
	add.s64 	%rd891, %rd890, %rd819;
	xor.b64  	%rd892, %rd891, %rd834;
	mov.b64	{%r1439, %r1440}, %rd892;
	prmt.b32 	%r1441, %r1439, %r1440, %r778;
	prmt.b32 	%r1442, %r1439, %r1440, %r777;
	mov.b64	%rd893, {%r1442, %r1441};
	add.s64 	%rd894, %rd888, %rd10;
	add.s64 	%rd895, %rd894, %rd893;
	xor.b64  	%rd896, %rd895, %rd890;
	mov.b64	{%r1443, %r1444}, %rd896;
	prmt.b32 	%r1445, %r1443, %r1444, %r784;
	prmt.b32 	%r1446, %r1443, %r1444, %r783;
	mov.b64	%rd897, {%r1446, %r1445};
	add.s64 	%rd898, %rd897, %rd891;
	xor.b64  	%rd899, %rd898, %rd893;
	mov.b64	{%r534, %r535}, %rd899;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r768;
	// inline asm
	mov.b64	%rd900, {%r529, %r533};
	add.s64 	%rd901, %rd848, %rd1;
	add.s64 	%rd902, %rd901, %rd856;
	xor.b64  	%rd903, %rd902, %rd818;
	mov.b64	{%r1447, %r1448}, %rd903;
	mov.b64	%rd904, {%r1448, %r1447};
	add.s64 	%rd905, %rd904, %rd832;
	xor.b64  	%rd906, %rd905, %rd848;
	mov.b64	{%r1449, %r1450}, %rd906;
	prmt.b32 	%r1451, %r1449, %r1450, %r778;
	prmt.b32 	%r1452, %r1449, %r1450, %r777;
	mov.b64	%rd907, {%r1452, %r1451};
	add.s64 	%rd908, %rd902, %rd15;
	add.s64 	%rd909, %rd908, %rd907;
	xor.b64  	%rd910, %rd909, %rd904;
	mov.b64	{%r1453, %r1454}, %rd910;
	prmt.b32 	%r1455, %r1453, %r1454, %r784;
	prmt.b32 	%r1456, %r1453, %r1454, %r783;
	mov.b64	%rd911, {%r1456, %r1455};
	add.s64 	%rd912, %rd911, %rd905;
	xor.b64  	%rd913, %rd912, %rd907;
	mov.b64	{%r542, %r543}, %rd913;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r768;
	// inline asm
	mov.b64	%rd914, {%r537, %r541};
	add.s64 	%rd915, %rd887, %rd869;
	xor.b64  	%rd916, %rd911, %rd915;
	mov.b64	{%r1457, %r1458}, %rd916;
	mov.b64	%rd917, {%r1458, %r1457};
	add.s64 	%rd918, %rd917, %rd898;
	xor.b64  	%rd919, %rd918, %rd887;
	mov.b64	{%r1459, %r1460}, %rd919;
	prmt.b32 	%r1461, %r1459, %r1460, %r778;
	prmt.b32 	%r1462, %r1459, %r1460, %r777;
	mov.b64	%rd920, {%r1462, %r1461};
	add.s64 	%rd921, %rd915, %rd9;
	add.s64 	%rd922, %rd921, %rd920;
	xor.b64  	%rd923, %rd917, %rd922;
	mov.b64	{%r1463, %r1464}, %rd923;
	prmt.b32 	%r1465, %r1463, %r1464, %r784;
	prmt.b32 	%r1466, %r1463, %r1464, %r783;
	mov.b64	%rd924, {%r1466, %r1465};
	add.s64 	%rd925, %rd918, %rd924;
	xor.b64  	%rd926, %rd925, %rd920;
	mov.b64	{%r550, %r551}, %rd926;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r768;
	// inline asm
	mov.b64	%rd927, {%r545, %r549};
	add.s64 	%rd928, %rd900, %rd882;
	xor.b64  	%rd929, %rd928, %rd871;
	mov.b64	{%r1467, %r1468}, %rd929;
	mov.b64	%rd930, {%r1468, %r1467};
	add.s64 	%rd931, %rd930, %rd912;
	xor.b64  	%rd932, %rd931, %rd900;
	mov.b64	{%r1469, %r1470}, %rd932;
	prmt.b32 	%r1471, %r1469, %r1470, %r778;
	prmt.b32 	%r1472, %r1469, %r1470, %r777;
	mov.b64	%rd933, {%r1472, %r1471};
	add.s64 	%rd934, %rd928, %rd14;
	add.s64 	%rd935, %rd934, %rd933;
	xor.b64  	%rd936, %rd935, %rd930;
	mov.b64	{%r1473, %r1474}, %rd936;
	prmt.b32 	%r1475, %r1473, %r1474, %r784;
	prmt.b32 	%r1476, %r1473, %r1474, %r783;
	mov.b64	%rd937, {%r1476, %r1475};
	add.s64 	%rd938, %rd937, %rd931;
	xor.b64  	%rd939, %rd938, %rd933;
	mov.b64	{%r558, %r559}, %rd939;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r768;
	// inline asm
	mov.b64	%rd940, {%r553, %r557};
	add.s64 	%rd941, %rd895, %rd23;
	add.s64 	%rd942, %rd941, %rd914;
	xor.b64  	%rd943, %rd942, %rd884;
	mov.b64	{%r1477, %r1478}, %rd943;
	mov.b64	%rd944, {%r1478, %r1477};
	add.s64 	%rd945, %rd944, %rd872;
	xor.b64  	%rd946, %rd945, %rd914;
	mov.b64	{%r1479, %r1480}, %rd946;
	prmt.b32 	%r1481, %r1479, %r1480, %r778;
	prmt.b32 	%r1482, %r1479, %r1480, %r777;
	mov.b64	%rd947, {%r1482, %r1481};
	add.s64 	%rd948, %rd942, %rd11;
	add.s64 	%rd949, %rd948, %rd947;
	xor.b64  	%rd950, %rd949, %rd944;
	mov.b64	{%r1483, %r1484}, %rd950;
	prmt.b32 	%r1485, %r1483, %r1484, %r784;
	prmt.b32 	%r1486, %r1483, %r1484, %r783;
	mov.b64	%rd951, {%r1486, %r1485};
	add.s64 	%rd952, %rd951, %rd945;
	xor.b64  	%rd953, %rd952, %rd947;
	mov.b64	{%r566, %r567}, %rd953;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r768;
	// inline asm
	mov.b64	%rd954, {%r561, %r565};
	add.s64 	%rd955, %rd909, %rd874;
	xor.b64  	%rd956, %rd955, %rd897;
	mov.b64	{%r1487, %r1488}, %rd956;
	mov.b64	%rd957, {%r1488, %r1487};
	add.s64 	%rd958, %rd957, %rd885;
	xor.b64  	%rd959, %rd958, %rd874;
	mov.b64	{%r1489, %r1490}, %rd959;
	prmt.b32 	%r1491, %r1489, %r1490, %r778;
	prmt.b32 	%r1492, %r1489, %r1490, %r777;
	mov.b64	%rd960, {%r1492, %r1491};
	add.s64 	%rd961, %rd955, %rd12;
	add.s64 	%rd962, %rd961, %rd960;
	xor.b64  	%rd963, %rd962, %rd957;
	mov.b64	{%r1493, %r1494}, %rd963;
	prmt.b32 	%r1495, %r1493, %r1494, %r784;
	prmt.b32 	%r1496, %r1493, %r1494, %r783;
	mov.b64	%rd964, {%r1496, %r1495};
	add.s64 	%rd965, %rd964, %rd958;
	xor.b64  	%rd966, %rd965, %rd960;
	mov.b64	{%r574, %r575}, %rd966;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r768;
	// inline asm
	mov.b64	%rd967, {%r569, %r573};
	add.s64 	%rd968, %rd967, %rd922;
	xor.b64  	%rd969, %rd968, %rd937;
	mov.b64	{%r1497, %r1498}, %rd969;
	mov.b64	%rd970, {%r1498, %r1497};
	add.s64 	%rd971, %rd970, %rd952;
	xor.b64  	%rd972, %rd971, %rd967;
	mov.b64	{%r1499, %r1500}, %rd972;
	prmt.b32 	%r1501, %r1499, %r1500, %r778;
	prmt.b32 	%r1502, %r1499, %r1500, %r777;
	mov.b64	%rd973, {%r1502, %r1501};
	add.s64 	%rd974, %rd968, %rd9;
	add.s64 	%rd975, %rd974, %rd973;
	xor.b64  	%rd976, %rd970, %rd975;
	mov.b64	{%r1503, %r1504}, %rd976;
	prmt.b32 	%r1505, %r1503, %r1504, %r784;
	prmt.b32 	%r1506, %r1503, %r1504, %r783;
	mov.b64	%rd977, {%r1506, %r1505};
	add.s64 	%rd978, %rd971, %rd977;
	xor.b64  	%rd979, %rd978, %rd973;
	mov.b64	{%r582, %r583}, %rd979;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r768;
	// inline asm
	mov.b64	%rd980, {%r577, %r581};
	add.s64 	%rd981, %rd927, %rd15;
	add.s64 	%rd982, %rd981, %rd935;
	xor.b64  	%rd983, %rd951, %rd982;
	mov.b64	{%r1507, %r1508}, %rd983;
	mov.b64	%rd984, {%r1508, %r1507};
	add.s64 	%rd985, %rd965, %rd984;
	xor.b64  	%rd986, %rd985, %rd927;
	mov.b64	{%r1509, %r1510}, %rd986;
	prmt.b32 	%r1511, %r1509, %r1510, %r778;
	prmt.b32 	%r1512, %r1509, %r1510, %r777;
	mov.b64	%rd987, {%r1512, %r1511};
	add.s64 	%rd988, %rd982, %rd11;
	add.s64 	%rd989, %rd988, %rd987;
	xor.b64  	%rd990, %rd989, %rd984;
	mov.b64	{%r1513, %r1514}, %rd990;
	prmt.b32 	%r1515, %r1513, %r1514, %r784;
	prmt.b32 	%r1516, %r1513, %r1514, %r783;
	mov.b64	%rd991, {%r1516, %r1515};
	add.s64 	%rd992, %rd991, %rd985;
	xor.b64  	%rd993, %rd992, %rd987;
	mov.b64	{%r590, %r591}, %rd993;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r768;
	// inline asm
	mov.b64	%rd994, {%r585, %r589};
	add.s64 	%rd995, %rd940, %rd14;
	add.s64 	%rd996, %rd995, %rd949;
	xor.b64  	%rd997, %rd964, %rd996;
	mov.b64	{%r1517, %r1518}, %rd997;
	mov.b64	%rd998, {%r1518, %r1517};
	add.s64 	%rd999, %rd998, %rd925;
	xor.b64  	%rd1000, %rd999, %rd940;
	mov.b64	{%r1519, %r1520}, %rd1000;
	prmt.b32 	%r1521, %r1519, %r1520, %r778;
	prmt.b32 	%r1522, %r1519, %r1520, %r777;
	mov.b64	%rd1001, {%r1522, %r1521};
	add.s64 	%rd1002, %rd996, %rd13;
	add.s64 	%rd1003, %rd1002, %rd1001;
	xor.b64  	%rd1004, %rd1003, %rd998;
	mov.b64	{%r1523, %r1524}, %rd1004;
	prmt.b32 	%r1525, %r1523, %r1524, %r784;
	prmt.b32 	%r1526, %r1523, %r1524, %r783;
	mov.b64	%rd1005, {%r1526, %r1525};
	add.s64 	%rd1006, %rd1005, %rd999;
	xor.b64  	%rd1007, %rd1006, %rd1001;
	mov.b64	{%r598, %r599}, %rd1007;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r768;
	// inline asm
	mov.b64	%rd1008, {%r593, %r597};
	add.s64 	%rd1009, %rd954, %rd23;
	add.s64 	%rd1010, %rd1009, %rd962;
	xor.b64  	%rd1011, %rd1010, %rd924;
	mov.b64	{%r1527, %r1528}, %rd1011;
	mov.b64	%rd1012, {%r1528, %r1527};
	add.s64 	%rd1013, %rd1012, %rd938;
	xor.b64  	%rd1014, %rd1013, %rd954;
	mov.b64	{%r1529, %r1530}, %rd1014;
	prmt.b32 	%r1531, %r1529, %r1530, %r778;
	prmt.b32 	%r1532, %r1529, %r1530, %r777;
	mov.b64	%rd1015, {%r1532, %r1531};
	add.s64 	%rd1016, %rd1010, %rd12;
	add.s64 	%rd1017, %rd1016, %rd1015;
	xor.b64  	%rd1018, %rd1017, %rd1012;
	mov.b64	{%r1533, %r1534}, %rd1018;
	prmt.b32 	%r1535, %r1533, %r1534, %r784;
	prmt.b32 	%r1536, %r1533, %r1534, %r783;
	mov.b64	%rd1019, {%r1536, %r1535};
	add.s64 	%rd1020, %rd1019, %rd1013;
	xor.b64  	%rd1021, %rd1020, %rd1015;
	mov.b64	{%r606, %r607}, %rd1021;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r768;
	// inline asm
	mov.b64	%rd1022, {%r601, %r605};
	add.s64 	%rd1023, %rd994, %rd975;
	xor.b64  	%rd1024, %rd1019, %rd1023;
	mov.b64	{%r1537, %r1538}, %rd1024;
	mov.b64	%rd1025, {%r1538, %r1537};
	add.s64 	%rd1026, %rd1025, %rd1006;
	xor.b64  	%rd1027, %rd1026, %rd994;
	mov.b64	{%r1539, %r1540}, %rd1027;
	prmt.b32 	%r1541, %r1539, %r1540, %r778;
	prmt.b32 	%r1542, %r1539, %r1540, %r777;
	mov.b64	%rd1028, {%r1542, %r1541};
	add.s64 	%rd1029, %rd1028, %rd1023;
	xor.b64  	%rd1030, %rd1025, %rd1029;
	mov.b64	{%r1543, %r1544}, %rd1030;
	prmt.b32 	%r1545, %r1543, %r1544, %r784;
	prmt.b32 	%r1546, %r1543, %r1544, %r783;
	mov.b64	%rd1031, {%r1546, %r1545};
	add.s64 	%rd1032, %rd1026, %rd1031;
	xor.b64  	%rd1033, %rd1032, %rd1028;
	mov.b64	{%r614, %r615}, %rd1033;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r768;
	// inline asm
	mov.b64	%rd1034, {%r609, %r613};
	add.s64 	%rd1035, %rd989, %rd16;
	add.s64 	%rd1036, %rd1035, %rd1008;
	xor.b64  	%rd1037, %rd1036, %rd977;
	mov.b64	{%r1547, %r1548}, %rd1037;
	mov.b64	%rd1038, {%r1548, %r1547};
	add.s64 	%rd1039, %rd1038, %rd1020;
	xor.b64  	%rd1040, %rd1039, %rd1008;
	mov.b64	{%r1549, %r1550}, %rd1040;
	prmt.b32 	%r1551, %r1549, %r1550, %r778;
	prmt.b32 	%r1552, %r1549, %r1550, %r777;
	mov.b64	%rd1041, {%r1552, %r1551};
	add.s64 	%rd1042, %rd1041, %rd1036;
	xor.b64  	%rd1043, %rd1042, %rd1038;
	mov.b64	{%r1553, %r1554}, %rd1043;
	prmt.b32 	%r1555, %r1553, %r1554, %r784;
	prmt.b32 	%r1556, %r1553, %r1554, %r783;
	mov.b64	%rd1044, {%r1556, %r1555};
	add.s64 	%rd1045, %rd1044, %rd1039;
	xor.b64  	%rd1046, %rd1045, %rd1041;
	mov.b64	{%r622, %r623}, %rd1046;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r768;
	// inline asm
	mov.b64	%rd1047, {%r617, %r621};
	add.s64 	%rd1048, %rd1003, %rd10;
	add.s64 	%rd1049, %rd1048, %rd1022;
	xor.b64  	%rd1050, %rd1049, %rd991;
	mov.b64	{%r1557, %r1558}, %rd1050;
	mov.b64	%rd1051, {%r1558, %r1557};
	add.s64 	%rd1052, %rd1051, %rd978;
	xor.b64  	%rd1053, %rd1052, %rd1022;
	mov.b64	{%r1559, %r1560}, %rd1053;
	prmt.b32 	%r1561, %r1559, %r1560, %r778;
	prmt.b32 	%r1562, %r1559, %r1560, %r777;
	mov.b64	%rd1054, {%r1562, %r1561};
	add.s64 	%rd1055, %rd1054, %rd1049;
	xor.b64  	%rd1056, %rd1055, %rd1051;
	mov.b64	{%r1563, %r1564}, %rd1056;
	prmt.b32 	%r1565, %r1563, %r1564, %r784;
	prmt.b32 	%r1566, %r1563, %r1564, %r783;
	mov.b64	%rd1057, {%r1566, %r1565};
	add.s64 	%rd1058, %rd1057, %rd1052;
	xor.b64  	%rd1059, %rd1058, %rd1054;
	mov.b64	{%r630, %r631}, %rd1059;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r768;
	// inline asm
	mov.b64	%rd1060, {%r625, %r629};
	add.s64 	%rd1061, %rd1017, %rd980;
	xor.b64  	%rd1062, %rd1061, %rd1005;
	mov.b64	{%r1567, %r1568}, %rd1062;
	mov.b64	%rd1063, {%r1568, %r1567};
	add.s64 	%rd1064, %rd1063, %rd992;
	xor.b64  	%rd1065, %rd1064, %rd980;
	mov.b64	{%r1569, %r1570}, %rd1065;
	prmt.b32 	%r1571, %r1569, %r1570, %r778;
	prmt.b32 	%r1572, %r1569, %r1570, %r777;
	mov.b64	%rd1066, {%r1572, %r1571};
	add.s64 	%rd1067, %rd1061, %rd1;
	add.s64 	%rd1068, %rd1067, %rd1066;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r1573, %r1574}, %rd1069;
	prmt.b32 	%r1575, %r1573, %r1574, %r784;
	prmt.b32 	%r1576, %r1573, %r1574, %r783;
	mov.b64	%rd1070, {%r1576, %r1575};
	add.s64 	%rd1071, %rd1070, %rd1064;
	xor.b64  	%rd1072, %rd1071, %rd1066;
	mov.b64	{%r638, %r639}, %rd1072;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r768;
	// inline asm
	mov.b64	%rd1073, {%r633, %r637};
	add.s64 	%rd1074, %rd1029, %rd1;
	add.s64 	%rd1075, %rd1074, %rd1073;
	xor.b64  	%rd1076, %rd1075, %rd1044;
	mov.b64	{%r1577, %r1578}, %rd1076;
	mov.b64	%rd1077, {%r1578, %r1577};
	add.s64 	%rd1078, %rd1077, %rd1058;
	xor.b64  	%rd1079, %rd1078, %rd1073;
	mov.b64	{%r1579, %r1580}, %rd1079;
	prmt.b32 	%r1581, %r1579, %r1580, %r778;
	prmt.b32 	%r1582, %r1579, %r1580, %r777;
	mov.b64	%rd1080, {%r1582, %r1581};
	add.s64 	%rd1081, %rd1075, %rd23;
	add.s64 	%rd1082, %rd1081, %rd1080;
	xor.b64  	%rd1083, %rd1077, %rd1082;
	mov.b64	{%r1583, %r1584}, %rd1083;
	prmt.b32 	%r1585, %r1583, %r1584, %r784;
	prmt.b32 	%r1586, %r1583, %r1584, %r783;
	mov.b64	%rd1084, {%r1586, %r1585};
	add.s64 	%rd1085, %rd1078, %rd1084;
	xor.b64  	%rd1086, %rd1085, %rd1080;
	mov.b64	{%r646, %r647}, %rd1086;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r768;
	// inline asm
	mov.b64	%rd1087, {%r641, %r645};
	add.s64 	%rd1088, %rd1034, %rd9;
	add.s64 	%rd1089, %rd1088, %rd1042;
	xor.b64  	%rd1090, %rd1057, %rd1089;
	mov.b64	{%r1587, %r1588}, %rd1090;
	mov.b64	%rd1091, {%r1588, %r1587};
	add.s64 	%rd1092, %rd1071, %rd1091;
	xor.b64  	%rd1093, %rd1092, %rd1034;
	mov.b64	{%r1589, %r1590}, %rd1093;
	prmt.b32 	%r1591, %r1589, %r1590, %r778;
	prmt.b32 	%r1592, %r1589, %r1590, %r777;
	mov.b64	%rd1094, {%r1592, %r1591};
	add.s64 	%rd1095, %rd1089, %rd10;
	add.s64 	%rd1096, %rd1095, %rd1094;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r1593, %r1594}, %rd1097;
	prmt.b32 	%r1595, %r1593, %r1594, %r784;
	prmt.b32 	%r1596, %r1593, %r1594, %r783;
	mov.b64	%rd1098, {%r1596, %r1595};
	add.s64 	%rd1099, %rd1098, %rd1092;
	xor.b64  	%rd1100, %rd1099, %rd1094;
	mov.b64	{%r654, %r655}, %rd1100;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r768;
	// inline asm
	mov.b64	%rd1101, {%r649, %r653};
	add.s64 	%rd1102, %rd1047, %rd11;
	add.s64 	%rd1103, %rd1102, %rd1055;
	xor.b64  	%rd1104, %rd1070, %rd1103;
	mov.b64	{%r1597, %r1598}, %rd1104;
	mov.b64	%rd1105, {%r1598, %r1597};
	add.s64 	%rd1106, %rd1105, %rd1032;
	xor.b64  	%rd1107, %rd1106, %rd1047;
	mov.b64	{%r1599, %r1600}, %rd1107;
	prmt.b32 	%r1601, %r1599, %r1600, %r778;
	prmt.b32 	%r1602, %r1599, %r1600, %r777;
	mov.b64	%rd1108, {%r1602, %r1601};
	add.s64 	%rd1109, %rd1103, %rd12;
	add.s64 	%rd1110, %rd1109, %rd1108;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r1603, %r1604}, %rd1111;
	prmt.b32 	%r1605, %r1603, %r1604, %r784;
	prmt.b32 	%r1606, %r1603, %r1604, %r783;
	mov.b64	%rd1112, {%r1606, %r1605};
	add.s64 	%rd1113, %rd1112, %rd1106;
	xor.b64  	%rd1114, %rd1113, %rd1108;
	mov.b64	{%r662, %r663}, %rd1114;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r768;
	// inline asm
	mov.b64	%rd1115, {%r657, %r661};
	add.s64 	%rd1116, %rd1060, %rd13;
	add.s64 	%rd1117, %rd1116, %rd1068;
	xor.b64  	%rd1118, %rd1117, %rd1031;
	mov.b64	{%r1607, %r1608}, %rd1118;
	mov.b64	%rd1119, {%r1608, %r1607};
	add.s64 	%rd1120, %rd1119, %rd1045;
	xor.b64  	%rd1121, %rd1120, %rd1060;
	mov.b64	{%r1609, %r1610}, %rd1121;
	prmt.b32 	%r1611, %r1609, %r1610, %r778;
	prmt.b32 	%r1612, %r1609, %r1610, %r777;
	mov.b64	%rd1122, {%r1612, %r1611};
	add.s64 	%rd1123, %rd1117, %rd14;
	add.s64 	%rd1124, %rd1123, %rd1122;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r1613, %r1614}, %rd1125;
	prmt.b32 	%r1615, %r1613, %r1614, %r784;
	prmt.b32 	%r1616, %r1613, %r1614, %r783;
	mov.b64	%rd1126, {%r1616, %r1615};
	add.s64 	%rd1127, %rd1126, %rd1120;
	xor.b64  	%rd1128, %rd1127, %rd1122;
	mov.b64	{%r670, %r671}, %rd1128;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r768;
	// inline asm
	mov.b64	%rd1129, {%r665, %r669};
	add.s64 	%rd1130, %rd1082, %rd15;
	add.s64 	%rd1131, %rd1130, %rd1101;
	xor.b64  	%rd1132, %rd1126, %rd1131;
	mov.b64	{%r1617, %r1618}, %rd1132;
	mov.b64	%rd1133, {%r1618, %r1617};
	add.s64 	%rd1134, %rd1133, %rd1113;
	xor.b64  	%rd1135, %rd1134, %rd1101;
	mov.b64	{%r1619, %r1620}, %rd1135;
	prmt.b32 	%r1621, %r1619, %r1620, %r778;
	prmt.b32 	%r1622, %r1619, %r1620, %r777;
	mov.b64	%rd1136, {%r1622, %r1621};
	add.s64 	%rd1137, %rd1131, %rd16;
	add.s64 	%rd1138, %rd1137, %rd1136;
	xor.b64  	%rd1139, %rd1133, %rd1138;
	mov.b64	{%r1623, %r1624}, %rd1139;
	prmt.b32 	%r1625, %r1623, %r1624, %r784;
	prmt.b32 	%r1626, %r1623, %r1624, %r783;
	mov.b64	%rd1140, {%r1626, %r1625};
	add.s64 	%rd1141, %rd1134, %rd1140;
	xor.b64  	%rd1142, %rd1141, %rd1136;
	mov.b64	{%r678, %r679}, %rd1142;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r768;
	// inline asm
	mov.b64	%rd1143, {%r673, %r677};
	add.s64 	%rd1144, %rd1115, %rd1096;
	xor.b64  	%rd1145, %rd1144, %rd1084;
	mov.b64	{%r1627, %r1628}, %rd1145;
	mov.b64	%rd1146, {%r1628, %r1627};
	add.s64 	%rd1147, %rd1146, %rd1127;
	xor.b64  	%rd1148, %rd1147, %rd1115;
	mov.b64	{%r1629, %r1630}, %rd1148;
	prmt.b32 	%r1631, %r1629, %r1630, %r778;
	prmt.b32 	%r1632, %r1629, %r1630, %r777;
	mov.b64	%rd1149, {%r1632, %r1631};
	add.s64 	%rd1150, %rd1149, %rd1144;
	xor.b64  	%rd1151, %rd1150, %rd1146;
	mov.b64	{%r1633, %r1634}, %rd1151;
	prmt.b32 	%r1635, %r1633, %r1634, %r784;
	prmt.b32 	%r1636, %r1633, %r1634, %r783;
	mov.b64	%rd1152, {%r1636, %r1635};
	add.s64 	%rd1153, %rd1152, %rd1147;
	xor.b64  	%rd1154, %rd1153, %rd1149;
	mov.b64	{%r686, %r687}, %rd1154;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r768;
	// inline asm
	mov.b64	%rd1155, {%r681, %r685};
	add.s64 	%rd1156, %rd1129, %rd1110;
	xor.b64  	%rd1157, %rd1156, %rd1098;
	mov.b64	{%r1637, %r1638}, %rd1157;
	mov.b64	%rd1158, {%r1638, %r1637};
	add.s64 	%rd1159, %rd1158, %rd1085;
	xor.b64  	%rd1160, %rd1159, %rd1129;
	mov.b64	{%r1639, %r1640}, %rd1160;
	prmt.b32 	%r1641, %r1639, %r1640, %r778;
	prmt.b32 	%r1642, %r1639, %r1640, %r777;
	mov.b64	%rd1161, {%r1642, %r1641};
	add.s64 	%rd1162, %rd1161, %rd1156;
	xor.b64  	%rd1163, %rd1162, %rd1158;
	mov.b64	{%r1643, %r1644}, %rd1163;
	prmt.b32 	%r1645, %r1643, %r1644, %r784;
	prmt.b32 	%r1646, %r1643, %r1644, %r783;
	mov.b64	%rd1164, {%r1646, %r1645};
	add.s64 	%rd1165, %rd1164, %rd1159;
	xor.b64  	%rd1166, %rd1165, %rd1161;
	mov.b64	{%r694, %r695}, %rd1166;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r768;
	// inline asm
	mov.b64	%rd1167, {%r689, %r693};
	add.s64 	%rd1168, %rd1124, %rd1087;
	xor.b64  	%rd1169, %rd1168, %rd1112;
	mov.b64	{%r1647, %r1648}, %rd1169;
	mov.b64	%rd1170, {%r1648, %r1647};
	add.s64 	%rd1171, %rd1170, %rd1099;
	xor.b64  	%rd1172, %rd1171, %rd1087;
	mov.b64	{%r1649, %r1650}, %rd1172;
	prmt.b32 	%r1651, %r1649, %r1650, %r778;
	prmt.b32 	%r1652, %r1649, %r1650, %r777;
	mov.b64	%rd1173, {%r1652, %r1651};
	add.s64 	%rd1174, %rd1173, %rd1168;
	xor.b64  	%rd1175, %rd1174, %rd1170;
	mov.b64	{%r1653, %r1654}, %rd1175;
	prmt.b32 	%r1655, %r1653, %r1654, %r784;
	prmt.b32 	%r1656, %r1653, %r1654, %r783;
	mov.b64	%rd1176, {%r1656, %r1655};
	add.s64 	%rd1177, %rd1176, %rd1171;
	xor.b64  	%rd1178, %rd1177, %rd1173;
	mov.b64	{%r702, %r703}, %rd1178;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r768;
	// inline asm
	mov.b64	%rd1179, {%r697, %r701};
	add.s64 	%rd1180, %rd1179, %rd1138;
	xor.b64  	%rd1181, %rd1180, %rd1152;
	mov.b64	{%r1657, %r1658}, %rd1181;
	mov.b64	%rd1182, {%r1658, %r1657};
	add.s64 	%rd1183, %rd1182, %rd1165;
	xor.b64  	%rd1184, %rd1183, %rd1179;
	mov.b64	{%r1659, %r1660}, %rd1184;
	prmt.b32 	%r1661, %r1659, %r1660, %r778;
	prmt.b32 	%r1662, %r1659, %r1660, %r777;
	mov.b64	%rd1185, {%r1662, %r1661};
	add.s64 	%rd1186, %rd1185, %rd1180;
	xor.b64  	%rd1187, %rd1182, %rd1186;
	mov.b64	{%r1663, %r1664}, %rd1187;
	prmt.b32 	%r1665, %r1663, %r1664, %r784;
	prmt.b32 	%r1666, %r1663, %r1664, %r783;
	mov.b64	%rd1188, {%r1666, %r1665};
	add.s64 	%rd1189, %rd1183, %rd1188;
	xor.b64  	%rd1190, %rd1189, %rd1185;
	mov.b64	{%r710, %r711}, %rd1190;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r768;
	// inline asm
	mov.b64	%rd1191, {%r705, %r709};
	add.s64 	%rd1192, %rd1143, %rd11;
	add.s64 	%rd1193, %rd1192, %rd1150;
	xor.b64  	%rd1194, %rd1164, %rd1193;
	mov.b64	{%r1667, %r1668}, %rd1194;
	mov.b64	%rd1195, {%r1668, %r1667};
	add.s64 	%rd1196, %rd1177, %rd1195;
	xor.b64  	%rd1197, %rd1196, %rd1143;
	mov.b64	{%r1669, %r1670}, %rd1197;
	prmt.b32 	%r1671, %r1669, %r1670, %r778;
	prmt.b32 	%r1672, %r1669, %r1670, %r777;
	mov.b64	%rd1198, {%r1672, %r1671};
	add.s64 	%rd1199, %rd1193, %rd15;
	add.s64 	%rd1200, %rd1199, %rd1198;
	xor.b64  	%rd1201, %rd1200, %rd1195;
	mov.b64	{%r1673, %r1674}, %rd1201;
	prmt.b32 	%r1675, %r1673, %r1674, %r784;
	prmt.b32 	%r1676, %r1673, %r1674, %r783;
	mov.b64	%rd1202, {%r1676, %r1675};
	add.s64 	%rd1203, %rd1202, %rd1196;
	xor.b64  	%rd1204, %rd1203, %rd1198;
	mov.b64	{%r718, %r719}, %rd1204;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r768;
	// inline asm
	mov.b64	%rd1205, {%r713, %r717};
	add.s64 	%rd1206, %rd1155, %rd16;
	add.s64 	%rd1207, %rd1206, %rd1162;
	xor.b64  	%rd1208, %rd1176, %rd1207;
	mov.b64	{%r1677, %r1678}, %rd1208;
	mov.b64	%rd1209, {%r1678, %r1677};
	add.s64 	%rd1210, %rd1209, %rd1141;
	xor.b64  	%rd1211, %rd1210, %rd1155;
	mov.b64	{%r1679, %r1680}, %rd1211;
	prmt.b32 	%r1681, %r1679, %r1680, %r778;
	prmt.b32 	%r1682, %r1679, %r1680, %r777;
	mov.b64	%rd1212, {%r1682, %r1681};
	add.s64 	%rd1213, %rd1212, %rd1207;
	xor.b64  	%rd1214, %rd1213, %rd1209;
	mov.b64	{%r1683, %r1684}, %rd1214;
	prmt.b32 	%r1685, %r1683, %r1684, %r784;
	prmt.b32 	%r1686, %r1683, %r1684, %r783;
	mov.b64	%rd1215, {%r1686, %r1685};
	add.s64 	%rd1216, %rd1215, %rd1210;
	xor.b64  	%rd1217, %rd1216, %rd1212;
	mov.b64	{%r726, %r727}, %rd1217;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r768;
	// inline asm
	mov.b64	%rd1218, {%r721, %r725};
	add.s64 	%rd1219, %rd1174, %rd1167;
	xor.b64  	%rd1220, %rd1219, %rd1140;
	mov.b64	{%r1687, %r1688}, %rd1220;
	mov.b64	%rd1221, {%r1688, %r1687};
	add.s64 	%rd1222, %rd1221, %rd1153;
	xor.b64  	%rd1223, %rd1222, %rd1167;
	mov.b64	{%r1689, %r1690}, %rd1223;
	prmt.b32 	%r1691, %r1689, %r1690, %r778;
	prmt.b32 	%r1692, %r1689, %r1690, %r777;
	mov.b64	%rd1224, {%r1692, %r1691};
	add.s64 	%rd1225, %rd1219, %rd13;
	add.s64 	%rd1226, %rd1225, %rd1224;
	xor.b64  	%rd1227, %rd1226, %rd1221;
	mov.b64	{%r1693, %r1694}, %rd1227;
	prmt.b32 	%r1695, %r1693, %r1694, %r784;
	prmt.b32 	%r1696, %r1693, %r1694, %r783;
	mov.b64	%rd1228, {%r1696, %r1695};
	add.s64 	%rd1229, %rd1228, %rd1222;
	xor.b64  	%rd1230, %rd1229, %rd1224;
	mov.b64	{%r734, %r735}, %rd1230;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r768;
	// inline asm
	mov.b64	%rd1231, {%r729, %r733};
	add.s64 	%rd1232, %rd1186, %rd23;
	add.s64 	%rd1233, %rd1232, %rd1205;
	xor.b64  	%rd1234, %rd1228, %rd1233;
	mov.b64	{%r1697, %r1698}, %rd1234;
	mov.b64	%rd1235, {%r1698, %r1697};
	add.s64 	%rd1236, %rd1235, %rd1216;
	xor.b64  	%rd1237, %rd1236, %rd1205;
	mov.b64	{%r1699, %r1700}, %rd1237;
	prmt.b32 	%r1701, %r1699, %r1700, %r778;
	prmt.b32 	%r1702, %r1699, %r1700, %r777;
	mov.b64	%rd1238, {%r1702, %r1701};
	add.s64 	%rd1239, %rd1238, %rd1233;
	xor.b64  	%rd1240, %rd1235, %rd1239;
	mov.b64	{%r1703, %r1704}, %rd1240;
	prmt.b32 	%r1705, %r1703, %r1704, %r784;
	prmt.b32 	%r1706, %r1703, %r1704, %r783;
	mov.b64	%rd1241, {%r1706, %r1705};
	add.s64 	%rd1242, %rd1236, %rd1241;
	xor.b64  	%rd1243, %rd1242, %rd1238;
	mov.b64	{%r742, %r743}, %rd1243;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r768;
	// inline asm
	add.s64 	%rd1244, %rd1200, %rd1;
	add.s64 	%rd1245, %rd1244, %rd1218;
	xor.b64  	%rd1246, %rd1245, %rd1188;
	mov.b64	{%r1707, %r1708}, %rd1246;
	mov.b64	%rd1247, {%r1708, %r1707};
	add.s64 	%rd1248, %rd1247, %rd1229;
	xor.b64  	%rd1249, %rd1248, %rd1218;
	mov.b64	{%r1709, %r1710}, %rd1249;
	prmt.b32 	%r1711, %r1709, %r1710, %r778;
	prmt.b32 	%r1712, %r1709, %r1710, %r777;
	mov.b64	%rd1250, {%r1712, %r1711};
	add.s64 	%rd1251, %rd1245, %rd9;
	add.s64 	%rd1252, %rd1251, %rd1250;
	xor.b64  	%rd1253, %rd1252, %rd1247;
	mov.b64	{%r1713, %r1714}, %rd1253;
	prmt.b32 	%r1715, %r1713, %r1714, %r784;
	prmt.b32 	%r1716, %r1713, %r1714, %r783;
	mov.b64	%rd1254, {%r1716, %r1715};
	add.s64 	%rd1255, %rd1254, %rd1248;
	xor.b64  	%rd1256, %rd1255, %rd1250;
	mov.b64	{%r750, %r751}, %rd1256;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r768;
	// inline asm
	add.s64 	%rd1257, %rd1231, %rd1213;
	xor.b64  	%rd1258, %rd1257, %rd1202;
	mov.b64	{%r1717, %r1718}, %rd1258;
	mov.b64	%rd1259, {%r1718, %r1717};
	add.s64 	%rd1260, %rd1259, %rd1189;
	xor.b64  	%rd1261, %rd1260, %rd1231;
	mov.b64	{%r1719, %r1720}, %rd1261;
	prmt.b32 	%r1721, %r1719, %r1720, %r778;
	prmt.b32 	%r1722, %r1719, %r1720, %r777;
	mov.b64	%rd1262, {%r1722, %r1721};
	add.s64 	%rd1263, %rd1257, %rd14;
	add.s64 	%rd1264, %rd1263, %rd1262;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r1723, %r1724}, %rd1265;
	prmt.b32 	%r1725, %r1723, %r1724, %r784;
	prmt.b32 	%r1726, %r1723, %r1724, %r783;
	mov.b64	%rd1266, {%r1726, %r1725};
	add.s64 	%rd1267, %rd1266, %rd1260;
	xor.b64  	%rd1268, %rd1267, %rd1262;
	mov.b64	{%r758, %r759}, %rd1268;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r768;
	// inline asm
	mov.b64	%rd1269, {%r753, %r757};
	add.s64 	%rd1270, %rd1191, %rd12;
	add.s64 	%rd1271, %rd1270, %rd1226;
	xor.b64  	%rd1272, %rd1271, %rd1215;
	mov.b64	{%r1727, %r1728}, %rd1272;
	mov.b64	%rd1273, {%r1728, %r1727};
	add.s64 	%rd1274, %rd1273, %rd1203;
	xor.b64  	%rd1275, %rd1274, %rd1191;
	mov.b64	{%r1729, %r1730}, %rd1275;
	prmt.b32 	%r1731, %r1729, %r1730, %r778;
	prmt.b32 	%r1732, %r1729, %r1730, %r777;
	mov.b64	%rd1276, {%r1732, %r1731};
	add.s64 	%rd1277, %rd1271, %rd10;
	add.s64 	%rd1278, %rd1277, %rd1276;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r1733, %r1734}, %rd1279;
	prmt.b32 	%r1735, %r1733, %r1734, %r784;
	prmt.b32 	%r1736, %r1733, %r1734, %r783;
	mov.b64	%rd1280, {%r1736, %r1735};
	add.s64 	%rd1281, %rd1280, %rd1274;
	xor.b64  	%rd1282, %rd1281, %rd1276;
	mov.b64	{%r766, %r767}, %rd1282;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r768;
	// inline asm
	xor.b64  	%rd1283, %rd1241, %rd1269;
	xor.b64  	%rd1284, %rd1283, 6620516959819538809;
	mov.b64	{%r1737, %r1738}, %rd1284;
	setp.ne.s32	%p1, %r1738, 0;
	@%p1 bra 	BB22_2;

	ld.param.u64 	%rd1286, [_Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_0];
	cvta.to.global.u64 	%rd1285, %rd1286;
	st.global.u64 	[%rd1285], %rd1;

BB22_2:
	ret;
}

	// .globl	_Z30blake2b_512_double_block_benchILj256EEvPyPKvy
.visible .entry _Z30blake2b_512_double_block_benchILj256EEvPyPKvy(
	.param .u64 _Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_0,
	.param .u64 _Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_1,
	.param .u64 _Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<3467>;
	.reg .b64 	%rd<2741>;


	ld.param.u64 	%rd3, [_Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_1];
	ld.param.u64 	%rd4, [_Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_2];
	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r1537, %ntid.x;
	mov.u32 	%r1538, %ctaid.x;
	mul.lo.s32 	%r1539, %r1537, %r1538;
	cvt.u64.u32	%rd6, %r1539;
	add.s64 	%rd7, %rd6, %rd4;
	mov.u32 	%r1540, %tid.x;
	cvt.u64.u32	%rd8, %r1540;
	add.s64 	%rd1, %rd7, %rd8;
	ld.global.u64 	%rd9, [%rd5+16];
	ld.global.u64 	%rd10, [%rd5+24];
	ld.global.u64 	%rd11, [%rd5+32];
	ld.global.u64 	%rd12, [%rd5+40];
	ld.global.u64 	%rd13, [%rd5+48];
	ld.global.u64 	%rd14, [%rd5+56];
	ld.global.u64 	%rd15, [%rd5+64];
	ld.global.u64 	%rd16, [%rd5+72];
	ld.global.u64 	%rd17, [%rd5+80];
	ld.global.u64 	%rd18, [%rd5+88];
	ld.global.u64 	%rd19, [%rd5+96];
	ld.global.u64 	%rd20, [%rd5+104];
	ld.global.u64 	%rd21, [%rd5+112];
	ld.global.u64 	%rd22, [%rd5+120];
	add.s64 	%rd23, %rd1, -4965156021692249063;
	xor.b64  	%rd24, %rd23, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd24;
	mov.b64	%rd25, {%r1542, %r1541};
	add.s64 	%rd26, %rd25, 7640891576956012808;
	xor.b64  	%rd27, %rd26, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd27;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd28, {%r1548, %r1547};
	ld.global.u64 	%rd29, [%rd5+8];
	add.s64 	%rd30, %rd23, %rd29;
	add.s64 	%rd31, %rd30, %rd28;
	xor.b64  	%rd32, %rd31, %rd25;
	mov.b64	{%r1549, %r1550}, %rd32;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd33, {%r1554, %r1553};
	add.s64 	%rd34, %rd33, %rd26;
	xor.b64  	%rd35, %rd34, %rd28;
	mov.b64	{%r6, %r7}, %rd35;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd36, {%r1, %r5};
	add.s64 	%rd37, %rd9, 6227659224458531674;
	xor.b64  	%rd38, %rd37, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd38;
	mov.b64	%rd39, {%r1556, %r1555};
	add.s64 	%rd40, %rd39, -4942790177534073029;
	xor.b64  	%rd41, %rd40, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd41;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd42, {%r1560, %r1559};
	add.s64 	%rd43, %rd10, %rd37;
	add.s64 	%rd44, %rd43, %rd42;
	xor.b64  	%rd45, %rd44, %rd39;
	mov.b64	{%r1561, %r1562}, %rd45;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd46, {%r1564, %r1563};
	add.s64 	%rd47, %rd46, %rd40;
	xor.b64  	%rd48, %rd47, %rd42;
	mov.b64	{%r14, %r15}, %rd48;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd49, {%r9, %r13};
	add.s64 	%rd50, %rd11, 6625583534739731862;
	xor.b64  	%rd51, %rd50, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd51;
	mov.b64	%rd52, {%r1566, %r1565};
	add.s64 	%rd53, %rd52, 4354685564936845355;
	xor.b64  	%rd54, %rd53, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd54;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd55, {%r1570, %r1569};
	add.s64 	%rd56, %rd12, %rd50;
	add.s64 	%rd57, %rd56, %rd55;
	xor.b64  	%rd58, %rd57, %rd52;
	mov.b64	{%r1571, %r1572}, %rd58;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd59, {%r1574, %r1573};
	add.s64 	%rd60, %rd59, %rd53;
	xor.b64  	%rd61, %rd60, %rd55;
	mov.b64	{%r22, %r23}, %rd61;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd62, {%r17, %r21};
	add.s64 	%rd63, %rd13, 85782056580896874;
	xor.b64  	%rd64, %rd63, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd64;
	mov.b64	%rd65, {%r1576, %r1575};
	add.s64 	%rd66, %rd65, -6534734903238641935;
	xor.b64  	%rd67, %rd66, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd67;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd68, {%r1580, %r1579};
	add.s64 	%rd69, %rd14, %rd63;
	add.s64 	%rd70, %rd69, %rd68;
	xor.b64  	%rd71, %rd70, %rd65;
	mov.b64	{%r1581, %r1582}, %rd71;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd72, {%r1584, %r1583};
	add.s64 	%rd73, %rd72, %rd66;
	xor.b64  	%rd74, %rd73, %rd68;
	mov.b64	{%r30, %r31}, %rd74;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd75, {%r25, %r29};
	add.s64 	%rd76, %rd31, %rd15;
	add.s64 	%rd77, %rd76, %rd49;
	xor.b64  	%rd78, %rd72, %rd77;
	mov.b64	{%r1585, %r1586}, %rd78;
	mov.b64	%rd79, {%r1586, %r1585};
	add.s64 	%rd80, %rd79, %rd60;
	xor.b64  	%rd81, %rd80, %rd49;
	mov.b64	{%r1587, %r1588}, %rd81;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd82, {%r1590, %r1589};
	add.s64 	%rd83, %rd77, %rd16;
	add.s64 	%rd84, %rd83, %rd82;
	xor.b64  	%rd85, %rd79, %rd84;
	mov.b64	{%r1591, %r1592}, %rd85;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd86, {%r1594, %r1593};
	add.s64 	%rd87, %rd86, %rd80;
	xor.b64  	%rd88, %rd87, %rd82;
	mov.b64	{%r38, %r39}, %rd88;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd89, {%r33, %r37};
	add.s64 	%rd90, %rd44, %rd17;
	add.s64 	%rd91, %rd90, %rd62;
	xor.b64  	%rd92, %rd91, %rd33;
	mov.b64	{%r1595, %r1596}, %rd92;
	mov.b64	%rd93, {%r1596, %r1595};
	add.s64 	%rd94, %rd93, %rd73;
	xor.b64  	%rd95, %rd94, %rd62;
	mov.b64	{%r1597, %r1598}, %rd95;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd96, {%r1600, %r1599};
	add.s64 	%rd97, %rd91, %rd18;
	add.s64 	%rd98, %rd97, %rd96;
	xor.b64  	%rd99, %rd98, %rd93;
	mov.b64	{%r1601, %r1602}, %rd99;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd100, {%r1604, %r1603};
	add.s64 	%rd101, %rd100, %rd94;
	xor.b64  	%rd102, %rd101, %rd96;
	mov.b64	{%r46, %r47}, %rd102;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd103, {%r41, %r45};
	add.s64 	%rd104, %rd57, %rd19;
	add.s64 	%rd105, %rd104, %rd75;
	xor.b64  	%rd106, %rd105, %rd46;
	mov.b64	{%r1605, %r1606}, %rd106;
	mov.b64	%rd107, {%r1606, %r1605};
	add.s64 	%rd108, %rd107, %rd34;
	xor.b64  	%rd109, %rd108, %rd75;
	mov.b64	{%r1607, %r1608}, %rd109;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd110, {%r1610, %r1609};
	add.s64 	%rd111, %rd105, %rd20;
	add.s64 	%rd112, %rd111, %rd110;
	xor.b64  	%rd113, %rd112, %rd107;
	mov.b64	{%r1611, %r1612}, %rd113;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd114, {%r1614, %r1613};
	add.s64 	%rd115, %rd114, %rd108;
	xor.b64  	%rd116, %rd115, %rd110;
	mov.b64	{%r54, %r55}, %rd116;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd117, {%r49, %r53};
	add.s64 	%rd118, %rd36, %rd21;
	add.s64 	%rd119, %rd118, %rd70;
	xor.b64  	%rd120, %rd119, %rd59;
	mov.b64	{%r1615, %r1616}, %rd120;
	mov.b64	%rd121, {%r1616, %r1615};
	add.s64 	%rd122, %rd121, %rd47;
	xor.b64  	%rd123, %rd122, %rd36;
	mov.b64	{%r1617, %r1618}, %rd123;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd124, {%r1620, %r1619};
	add.s64 	%rd125, %rd119, %rd22;
	add.s64 	%rd126, %rd125, %rd124;
	xor.b64  	%rd127, %rd126, %rd121;
	mov.b64	{%r1621, %r1622}, %rd127;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd128, {%r1624, %r1623};
	add.s64 	%rd129, %rd128, %rd122;
	xor.b64  	%rd130, %rd129, %rd124;
	mov.b64	{%r62, %r63}, %rd130;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd131, {%r57, %r61};
	add.s64 	%rd132, %rd84, %rd21;
	add.s64 	%rd133, %rd132, %rd131;
	xor.b64  	%rd134, %rd133, %rd100;
	mov.b64	{%r1625, %r1626}, %rd134;
	mov.b64	%rd135, {%r1626, %r1625};
	add.s64 	%rd136, %rd135, %rd115;
	xor.b64  	%rd137, %rd136, %rd131;
	mov.b64	{%r1627, %r1628}, %rd137;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd138, {%r1630, %r1629};
	add.s64 	%rd139, %rd133, %rd17;
	add.s64 	%rd140, %rd139, %rd138;
	xor.b64  	%rd141, %rd135, %rd140;
	mov.b64	{%r1631, %r1632}, %rd141;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd142, {%r1634, %r1633};
	add.s64 	%rd143, %rd136, %rd142;
	xor.b64  	%rd144, %rd143, %rd138;
	mov.b64	{%r70, %r71}, %rd144;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd145, {%r65, %r69};
	add.s64 	%rd146, %rd89, %rd11;
	add.s64 	%rd147, %rd146, %rd98;
	xor.b64  	%rd148, %rd114, %rd147;
	mov.b64	{%r1635, %r1636}, %rd148;
	mov.b64	%rd149, {%r1636, %r1635};
	add.s64 	%rd150, %rd129, %rd149;
	xor.b64  	%rd151, %rd150, %rd89;
	mov.b64	{%r1637, %r1638}, %rd151;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd152, {%r1640, %r1639};
	add.s64 	%rd153, %rd147, %rd15;
	add.s64 	%rd154, %rd153, %rd152;
	xor.b64  	%rd155, %rd154, %rd149;
	mov.b64	{%r1641, %r1642}, %rd155;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd156, {%r1644, %r1643};
	add.s64 	%rd157, %rd156, %rd150;
	xor.b64  	%rd158, %rd157, %rd152;
	mov.b64	{%r78, %r79}, %rd158;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd159, {%r73, %r77};
	add.s64 	%rd160, %rd103, %rd16;
	add.s64 	%rd161, %rd160, %rd112;
	xor.b64  	%rd162, %rd128, %rd161;
	mov.b64	{%r1645, %r1646}, %rd162;
	mov.b64	%rd163, {%r1646, %r1645};
	add.s64 	%rd164, %rd163, %rd87;
	xor.b64  	%rd165, %rd164, %rd103;
	mov.b64	{%r1647, %r1648}, %rd165;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd166, {%r1650, %r1649};
	add.s64 	%rd167, %rd161, %rd22;
	add.s64 	%rd168, %rd167, %rd166;
	xor.b64  	%rd169, %rd168, %rd163;
	mov.b64	{%r1651, %r1652}, %rd169;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd170, {%r1654, %r1653};
	add.s64 	%rd171, %rd170, %rd164;
	xor.b64  	%rd172, %rd171, %rd166;
	mov.b64	{%r86, %r87}, %rd172;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd173, {%r81, %r85};
	add.s64 	%rd174, %rd117, %rd20;
	add.s64 	%rd175, %rd174, %rd126;
	xor.b64  	%rd176, %rd175, %rd86;
	mov.b64	{%r1655, %r1656}, %rd176;
	mov.b64	%rd177, {%r1656, %r1655};
	add.s64 	%rd178, %rd177, %rd101;
	xor.b64  	%rd179, %rd178, %rd117;
	mov.b64	{%r1657, %r1658}, %rd179;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd180, {%r1660, %r1659};
	add.s64 	%rd181, %rd175, %rd13;
	add.s64 	%rd182, %rd181, %rd180;
	xor.b64  	%rd183, %rd182, %rd177;
	mov.b64	{%r1661, %r1662}, %rd183;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd184, {%r1664, %r1663};
	add.s64 	%rd185, %rd184, %rd178;
	xor.b64  	%rd186, %rd185, %rd180;
	mov.b64	{%r94, %r95}, %rd186;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd187, {%r89, %r93};
	add.s64 	%rd188, %rd140, %rd29;
	add.s64 	%rd189, %rd188, %rd159;
	xor.b64  	%rd190, %rd184, %rd189;
	mov.b64	{%r1665, %r1666}, %rd190;
	mov.b64	%rd191, {%r1666, %r1665};
	add.s64 	%rd192, %rd191, %rd171;
	xor.b64  	%rd193, %rd192, %rd159;
	mov.b64	{%r1667, %r1668}, %rd193;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd194, {%r1670, %r1669};
	add.s64 	%rd195, %rd189, %rd19;
	add.s64 	%rd196, %rd195, %rd194;
	xor.b64  	%rd197, %rd191, %rd196;
	mov.b64	{%r1671, %r1672}, %rd197;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd198, {%r1674, %r1673};
	add.s64 	%rd199, %rd198, %rd192;
	xor.b64  	%rd200, %rd199, %rd194;
	mov.b64	{%r102, %r103}, %rd200;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd201, {%r97, %r101};
	add.s64 	%rd202, %rd154, %rd1;
	add.s64 	%rd203, %rd202, %rd173;
	xor.b64  	%rd204, %rd203, %rd142;
	mov.b64	{%r1675, %r1676}, %rd204;
	mov.b64	%rd205, {%r1676, %r1675};
	add.s64 	%rd206, %rd205, %rd185;
	xor.b64  	%rd207, %rd206, %rd173;
	mov.b64	{%r1677, %r1678}, %rd207;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd208, {%r1680, %r1679};
	add.s64 	%rd209, %rd203, %rd9;
	add.s64 	%rd210, %rd209, %rd208;
	xor.b64  	%rd211, %rd210, %rd205;
	mov.b64	{%r1681, %r1682}, %rd211;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd212, {%r1684, %r1683};
	add.s64 	%rd213, %rd212, %rd206;
	xor.b64  	%rd214, %rd213, %rd208;
	mov.b64	{%r110, %r111}, %rd214;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd215, {%r105, %r109};
	add.s64 	%rd216, %rd168, %rd18;
	add.s64 	%rd217, %rd216, %rd187;
	xor.b64  	%rd218, %rd217, %rd156;
	mov.b64	{%r1685, %r1686}, %rd218;
	mov.b64	%rd219, {%r1686, %r1685};
	add.s64 	%rd220, %rd219, %rd143;
	xor.b64  	%rd221, %rd220, %rd187;
	mov.b64	{%r1687, %r1688}, %rd221;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd222, {%r1690, %r1689};
	add.s64 	%rd223, %rd217, %rd14;
	add.s64 	%rd224, %rd223, %rd222;
	xor.b64  	%rd225, %rd224, %rd219;
	mov.b64	{%r1691, %r1692}, %rd225;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd226, {%r1694, %r1693};
	add.s64 	%rd227, %rd226, %rd220;
	xor.b64  	%rd228, %rd227, %rd222;
	mov.b64	{%r118, %r119}, %rd228;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd229, {%r113, %r117};
	add.s64 	%rd230, %rd145, %rd12;
	add.s64 	%rd231, %rd230, %rd182;
	xor.b64  	%rd232, %rd231, %rd170;
	mov.b64	{%r1695, %r1696}, %rd232;
	mov.b64	%rd233, {%r1696, %r1695};
	add.s64 	%rd234, %rd233, %rd157;
	xor.b64  	%rd235, %rd234, %rd145;
	mov.b64	{%r1697, %r1698}, %rd235;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd236, {%r1700, %r1699};
	add.s64 	%rd237, %rd231, %rd10;
	add.s64 	%rd238, %rd237, %rd236;
	xor.b64  	%rd239, %rd238, %rd233;
	mov.b64	{%r1701, %r1702}, %rd239;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd240, {%r1704, %r1703};
	add.s64 	%rd241, %rd240, %rd234;
	xor.b64  	%rd242, %rd241, %rd236;
	mov.b64	{%r126, %r127}, %rd242;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd243, {%r121, %r125};
	add.s64 	%rd244, %rd196, %rd18;
	add.s64 	%rd245, %rd244, %rd243;
	xor.b64  	%rd246, %rd245, %rd212;
	mov.b64	{%r1705, %r1706}, %rd246;
	mov.b64	%rd247, {%r1706, %r1705};
	add.s64 	%rd248, %rd247, %rd227;
	xor.b64  	%rd249, %rd248, %rd243;
	mov.b64	{%r1707, %r1708}, %rd249;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd250, {%r1710, %r1709};
	add.s64 	%rd251, %rd245, %rd15;
	add.s64 	%rd252, %rd251, %rd250;
	xor.b64  	%rd253, %rd247, %rd252;
	mov.b64	{%r1711, %r1712}, %rd253;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd254, {%r1714, %r1713};
	add.s64 	%rd255, %rd248, %rd254;
	xor.b64  	%rd256, %rd255, %rd250;
	mov.b64	{%r134, %r135}, %rd256;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd257, {%r129, %r133};
	add.s64 	%rd258, %rd201, %rd19;
	add.s64 	%rd259, %rd258, %rd210;
	xor.b64  	%rd260, %rd226, %rd259;
	mov.b64	{%r1715, %r1716}, %rd260;
	mov.b64	%rd261, {%r1716, %r1715};
	add.s64 	%rd262, %rd241, %rd261;
	xor.b64  	%rd263, %rd262, %rd201;
	mov.b64	{%r1717, %r1718}, %rd263;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd264, {%r1720, %r1719};
	add.s64 	%rd265, %rd259, %rd1;
	add.s64 	%rd266, %rd265, %rd264;
	xor.b64  	%rd267, %rd266, %rd261;
	mov.b64	{%r1721, %r1722}, %rd267;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd268, {%r1724, %r1723};
	add.s64 	%rd269, %rd268, %rd262;
	xor.b64  	%rd270, %rd269, %rd264;
	mov.b64	{%r142, %r143}, %rd270;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd271, {%r137, %r141};
	add.s64 	%rd272, %rd215, %rd12;
	add.s64 	%rd273, %rd272, %rd224;
	xor.b64  	%rd274, %rd240, %rd273;
	mov.b64	{%r1725, %r1726}, %rd274;
	mov.b64	%rd275, {%r1726, %r1725};
	add.s64 	%rd276, %rd275, %rd199;
	xor.b64  	%rd277, %rd276, %rd215;
	mov.b64	{%r1727, %r1728}, %rd277;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd278, {%r1730, %r1729};
	add.s64 	%rd279, %rd273, %rd9;
	add.s64 	%rd280, %rd279, %rd278;
	xor.b64  	%rd281, %rd280, %rd275;
	mov.b64	{%r1731, %r1732}, %rd281;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd282, {%r1734, %r1733};
	add.s64 	%rd283, %rd282, %rd276;
	xor.b64  	%rd284, %rd283, %rd278;
	mov.b64	{%r150, %r151}, %rd284;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd285, {%r145, %r149};
	add.s64 	%rd286, %rd229, %rd22;
	add.s64 	%rd287, %rd286, %rd238;
	xor.b64  	%rd288, %rd287, %rd198;
	mov.b64	{%r1735, %r1736}, %rd288;
	mov.b64	%rd289, {%r1736, %r1735};
	add.s64 	%rd290, %rd289, %rd213;
	xor.b64  	%rd291, %rd290, %rd229;
	mov.b64	{%r1737, %r1738}, %rd291;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd292, {%r1740, %r1739};
	add.s64 	%rd293, %rd287, %rd20;
	add.s64 	%rd294, %rd293, %rd292;
	xor.b64  	%rd295, %rd294, %rd289;
	mov.b64	{%r1741, %r1742}, %rd295;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd296, {%r1744, %r1743};
	add.s64 	%rd297, %rd296, %rd290;
	xor.b64  	%rd298, %rd297, %rd292;
	mov.b64	{%r158, %r159}, %rd298;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd299, {%r153, %r157};
	add.s64 	%rd300, %rd252, %rd17;
	add.s64 	%rd301, %rd300, %rd271;
	xor.b64  	%rd302, %rd296, %rd301;
	mov.b64	{%r1745, %r1746}, %rd302;
	mov.b64	%rd303, {%r1746, %r1745};
	add.s64 	%rd304, %rd303, %rd283;
	xor.b64  	%rd305, %rd304, %rd271;
	mov.b64	{%r1747, %r1748}, %rd305;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd306, {%r1750, %r1749};
	add.s64 	%rd307, %rd301, %rd21;
	add.s64 	%rd308, %rd307, %rd306;
	xor.b64  	%rd309, %rd303, %rd308;
	mov.b64	{%r1751, %r1752}, %rd309;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd310, {%r1754, %r1753};
	add.s64 	%rd311, %rd310, %rd304;
	xor.b64  	%rd312, %rd311, %rd306;
	mov.b64	{%r166, %r167}, %rd312;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd313, {%r161, %r165};
	add.s64 	%rd314, %rd266, %rd10;
	add.s64 	%rd315, %rd314, %rd285;
	xor.b64  	%rd316, %rd315, %rd254;
	mov.b64	{%r1755, %r1756}, %rd316;
	mov.b64	%rd317, {%r1756, %r1755};
	add.s64 	%rd318, %rd317, %rd297;
	xor.b64  	%rd319, %rd318, %rd285;
	mov.b64	{%r1757, %r1758}, %rd319;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd320, {%r1760, %r1759};
	add.s64 	%rd321, %rd315, %rd13;
	add.s64 	%rd322, %rd321, %rd320;
	xor.b64  	%rd323, %rd322, %rd317;
	mov.b64	{%r1761, %r1762}, %rd323;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd324, {%r1764, %r1763};
	add.s64 	%rd325, %rd324, %rd318;
	xor.b64  	%rd326, %rd325, %rd320;
	mov.b64	{%r174, %r175}, %rd326;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd327, {%r169, %r173};
	add.s64 	%rd328, %rd280, %rd14;
	add.s64 	%rd329, %rd328, %rd299;
	xor.b64  	%rd330, %rd329, %rd268;
	mov.b64	{%r1765, %r1766}, %rd330;
	mov.b64	%rd331, {%r1766, %r1765};
	add.s64 	%rd332, %rd331, %rd255;
	xor.b64  	%rd333, %rd332, %rd299;
	mov.b64	{%r1767, %r1768}, %rd333;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd334, {%r1770, %r1769};
	add.s64 	%rd335, %rd329, %rd29;
	add.s64 	%rd336, %rd335, %rd334;
	xor.b64  	%rd337, %rd336, %rd331;
	mov.b64	{%r1771, %r1772}, %rd337;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd338, {%r1774, %r1773};
	add.s64 	%rd339, %rd338, %rd332;
	xor.b64  	%rd340, %rd339, %rd334;
	mov.b64	{%r182, %r183}, %rd340;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd341, {%r177, %r181};
	add.s64 	%rd342, %rd257, %rd16;
	add.s64 	%rd343, %rd342, %rd294;
	xor.b64  	%rd344, %rd343, %rd282;
	mov.b64	{%r1775, %r1776}, %rd344;
	mov.b64	%rd345, {%r1776, %r1775};
	add.s64 	%rd346, %rd345, %rd269;
	xor.b64  	%rd347, %rd346, %rd257;
	mov.b64	{%r1777, %r1778}, %rd347;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd348, {%r1780, %r1779};
	add.s64 	%rd349, %rd343, %rd11;
	add.s64 	%rd350, %rd349, %rd348;
	xor.b64  	%rd351, %rd350, %rd345;
	mov.b64	{%r1781, %r1782}, %rd351;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd352, {%r1784, %r1783};
	add.s64 	%rd353, %rd352, %rd346;
	xor.b64  	%rd354, %rd353, %rd348;
	mov.b64	{%r190, %r191}, %rd354;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd355, {%r185, %r189};
	add.s64 	%rd356, %rd308, %rd14;
	add.s64 	%rd357, %rd356, %rd355;
	xor.b64  	%rd358, %rd357, %rd324;
	mov.b64	{%r1785, %r1786}, %rd358;
	mov.b64	%rd359, {%r1786, %r1785};
	add.s64 	%rd360, %rd359, %rd339;
	xor.b64  	%rd361, %rd360, %rd355;
	mov.b64	{%r1787, %r1788}, %rd361;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd362, {%r1790, %r1789};
	add.s64 	%rd363, %rd357, %rd16;
	add.s64 	%rd364, %rd363, %rd362;
	xor.b64  	%rd365, %rd359, %rd364;
	mov.b64	{%r1791, %r1792}, %rd365;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd366, {%r1794, %r1793};
	add.s64 	%rd367, %rd360, %rd366;
	xor.b64  	%rd368, %rd367, %rd362;
	mov.b64	{%r198, %r199}, %rd368;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd369, {%r193, %r197};
	add.s64 	%rd370, %rd313, %rd10;
	add.s64 	%rd371, %rd370, %rd322;
	xor.b64  	%rd372, %rd338, %rd371;
	mov.b64	{%r1795, %r1796}, %rd372;
	mov.b64	%rd373, {%r1796, %r1795};
	add.s64 	%rd374, %rd353, %rd373;
	xor.b64  	%rd375, %rd374, %rd313;
	mov.b64	{%r1797, %r1798}, %rd375;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd376, {%r1800, %r1799};
	add.s64 	%rd377, %rd371, %rd29;
	add.s64 	%rd378, %rd377, %rd376;
	xor.b64  	%rd379, %rd378, %rd373;
	mov.b64	{%r1801, %r1802}, %rd379;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd380, {%r1804, %r1803};
	add.s64 	%rd381, %rd380, %rd374;
	xor.b64  	%rd382, %rd381, %rd376;
	mov.b64	{%r206, %r207}, %rd382;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd383, {%r201, %r205};
	add.s64 	%rd384, %rd327, %rd20;
	add.s64 	%rd385, %rd384, %rd336;
	xor.b64  	%rd386, %rd352, %rd385;
	mov.b64	{%r1805, %r1806}, %rd386;
	mov.b64	%rd387, {%r1806, %r1805};
	add.s64 	%rd388, %rd387, %rd311;
	xor.b64  	%rd389, %rd388, %rd327;
	mov.b64	{%r1807, %r1808}, %rd389;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd390, {%r1810, %r1809};
	add.s64 	%rd391, %rd385, %rd19;
	add.s64 	%rd392, %rd391, %rd390;
	xor.b64  	%rd393, %rd392, %rd387;
	mov.b64	{%r1811, %r1812}, %rd393;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd394, {%r1814, %r1813};
	add.s64 	%rd395, %rd394, %rd388;
	xor.b64  	%rd396, %rd395, %rd390;
	mov.b64	{%r214, %r215}, %rd396;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd397, {%r209, %r213};
	add.s64 	%rd398, %rd341, %rd18;
	add.s64 	%rd399, %rd398, %rd350;
	xor.b64  	%rd400, %rd399, %rd310;
	mov.b64	{%r1815, %r1816}, %rd400;
	mov.b64	%rd401, {%r1816, %r1815};
	add.s64 	%rd402, %rd401, %rd325;
	xor.b64  	%rd403, %rd402, %rd341;
	mov.b64	{%r1817, %r1818}, %rd403;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd404, {%r1820, %r1819};
	add.s64 	%rd405, %rd399, %rd21;
	add.s64 	%rd406, %rd405, %rd404;
	xor.b64  	%rd407, %rd406, %rd401;
	mov.b64	{%r1821, %r1822}, %rd407;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd408, {%r1824, %r1823};
	add.s64 	%rd409, %rd408, %rd402;
	xor.b64  	%rd410, %rd409, %rd404;
	mov.b64	{%r222, %r223}, %rd410;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd411, {%r217, %r221};
	add.s64 	%rd412, %rd364, %rd9;
	add.s64 	%rd413, %rd412, %rd383;
	xor.b64  	%rd414, %rd408, %rd413;
	mov.b64	{%r1825, %r1826}, %rd414;
	mov.b64	%rd415, {%r1826, %r1825};
	add.s64 	%rd416, %rd415, %rd395;
	xor.b64  	%rd417, %rd416, %rd383;
	mov.b64	{%r1827, %r1828}, %rd417;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd418, {%r1830, %r1829};
	add.s64 	%rd419, %rd413, %rd13;
	add.s64 	%rd420, %rd419, %rd418;
	xor.b64  	%rd421, %rd415, %rd420;
	mov.b64	{%r1831, %r1832}, %rd421;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd422, {%r1834, %r1833};
	add.s64 	%rd423, %rd422, %rd416;
	xor.b64  	%rd424, %rd423, %rd418;
	mov.b64	{%r230, %r231}, %rd424;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd425, {%r225, %r229};
	add.s64 	%rd426, %rd378, %rd12;
	add.s64 	%rd427, %rd426, %rd397;
	xor.b64  	%rd428, %rd427, %rd366;
	mov.b64	{%r1835, %r1836}, %rd428;
	mov.b64	%rd429, {%r1836, %r1835};
	add.s64 	%rd430, %rd429, %rd409;
	xor.b64  	%rd431, %rd430, %rd397;
	mov.b64	{%r1837, %r1838}, %rd431;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd432, {%r1840, %r1839};
	add.s64 	%rd433, %rd427, %rd17;
	add.s64 	%rd434, %rd433, %rd432;
	xor.b64  	%rd435, %rd434, %rd429;
	mov.b64	{%r1841, %r1842}, %rd435;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd436, {%r1844, %r1843};
	add.s64 	%rd437, %rd436, %rd430;
	xor.b64  	%rd438, %rd437, %rd432;
	mov.b64	{%r238, %r239}, %rd438;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd439, {%r233, %r237};
	add.s64 	%rd440, %rd392, %rd11;
	add.s64 	%rd441, %rd440, %rd411;
	xor.b64  	%rd442, %rd441, %rd380;
	mov.b64	{%r1845, %r1846}, %rd442;
	mov.b64	%rd443, {%r1846, %r1845};
	add.s64 	%rd444, %rd443, %rd367;
	xor.b64  	%rd445, %rd444, %rd411;
	mov.b64	{%r1847, %r1848}, %rd445;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd446, {%r1850, %r1849};
	add.s64 	%rd447, %rd441, %rd1;
	add.s64 	%rd448, %rd447, %rd446;
	xor.b64  	%rd449, %rd448, %rd443;
	mov.b64	{%r1851, %r1852}, %rd449;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd450, {%r1854, %r1853};
	add.s64 	%rd451, %rd450, %rd444;
	xor.b64  	%rd452, %rd451, %rd446;
	mov.b64	{%r246, %r247}, %rd452;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd453, {%r241, %r245};
	add.s64 	%rd454, %rd369, %rd22;
	add.s64 	%rd455, %rd454, %rd406;
	xor.b64  	%rd456, %rd455, %rd394;
	mov.b64	{%r1855, %r1856}, %rd456;
	mov.b64	%rd457, {%r1856, %r1855};
	add.s64 	%rd458, %rd457, %rd381;
	xor.b64  	%rd459, %rd458, %rd369;
	mov.b64	{%r1857, %r1858}, %rd459;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd460, {%r1860, %r1859};
	add.s64 	%rd461, %rd455, %rd15;
	add.s64 	%rd462, %rd461, %rd460;
	xor.b64  	%rd463, %rd462, %rd457;
	mov.b64	{%r1861, %r1862}, %rd463;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd464, {%r1864, %r1863};
	add.s64 	%rd465, %rd464, %rd458;
	xor.b64  	%rd466, %rd465, %rd460;
	mov.b64	{%r254, %r255}, %rd466;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd467, {%r249, %r253};
	add.s64 	%rd468, %rd420, %rd16;
	add.s64 	%rd469, %rd468, %rd467;
	xor.b64  	%rd470, %rd469, %rd436;
	mov.b64	{%r1865, %r1866}, %rd470;
	mov.b64	%rd471, {%r1866, %r1865};
	add.s64 	%rd472, %rd471, %rd451;
	xor.b64  	%rd473, %rd472, %rd467;
	mov.b64	{%r1867, %r1868}, %rd473;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd474, {%r1870, %r1869};
	add.s64 	%rd475, %rd469, %rd1;
	add.s64 	%rd476, %rd475, %rd474;
	xor.b64  	%rd477, %rd471, %rd476;
	mov.b64	{%r1871, %r1872}, %rd477;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd478, {%r1874, %r1873};
	add.s64 	%rd479, %rd472, %rd478;
	xor.b64  	%rd480, %rd479, %rd474;
	mov.b64	{%r262, %r263}, %rd480;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd481, {%r257, %r261};
	add.s64 	%rd482, %rd425, %rd12;
	add.s64 	%rd483, %rd482, %rd434;
	xor.b64  	%rd484, %rd450, %rd483;
	mov.b64	{%r1875, %r1876}, %rd484;
	mov.b64	%rd485, {%r1876, %r1875};
	add.s64 	%rd486, %rd465, %rd485;
	xor.b64  	%rd487, %rd486, %rd425;
	mov.b64	{%r1877, %r1878}, %rd487;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd488, {%r1880, %r1879};
	add.s64 	%rd489, %rd483, %rd14;
	add.s64 	%rd490, %rd489, %rd488;
	xor.b64  	%rd491, %rd490, %rd485;
	mov.b64	{%r1881, %r1882}, %rd491;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd492, {%r1884, %r1883};
	add.s64 	%rd493, %rd492, %rd486;
	xor.b64  	%rd494, %rd493, %rd488;
	mov.b64	{%r270, %r271}, %rd494;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd495, {%r265, %r269};
	add.s64 	%rd496, %rd439, %rd9;
	add.s64 	%rd497, %rd496, %rd448;
	xor.b64  	%rd498, %rd464, %rd497;
	mov.b64	{%r1885, %r1886}, %rd498;
	mov.b64	%rd499, {%r1886, %r1885};
	add.s64 	%rd500, %rd499, %rd423;
	xor.b64  	%rd501, %rd500, %rd439;
	mov.b64	{%r1887, %r1888}, %rd501;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd502, {%r1890, %r1889};
	add.s64 	%rd503, %rd497, %rd11;
	add.s64 	%rd504, %rd503, %rd502;
	xor.b64  	%rd505, %rd504, %rd499;
	mov.b64	{%r1891, %r1892}, %rd505;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd506, {%r1894, %r1893};
	add.s64 	%rd507, %rd506, %rd500;
	xor.b64  	%rd508, %rd507, %rd502;
	mov.b64	{%r278, %r279}, %rd508;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd509, {%r273, %r277};
	add.s64 	%rd510, %rd453, %rd17;
	add.s64 	%rd511, %rd510, %rd462;
	xor.b64  	%rd512, %rd511, %rd422;
	mov.b64	{%r1895, %r1896}, %rd512;
	mov.b64	%rd513, {%r1896, %r1895};
	add.s64 	%rd514, %rd513, %rd437;
	xor.b64  	%rd515, %rd514, %rd453;
	mov.b64	{%r1897, %r1898}, %rd515;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd516, {%r1900, %r1899};
	add.s64 	%rd517, %rd511, %rd22;
	add.s64 	%rd518, %rd517, %rd516;
	xor.b64  	%rd519, %rd518, %rd513;
	mov.b64	{%r1901, %r1902}, %rd519;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd520, {%r1904, %r1903};
	add.s64 	%rd521, %rd520, %rd514;
	xor.b64  	%rd522, %rd521, %rd516;
	mov.b64	{%r286, %r287}, %rd522;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd523, {%r281, %r285};
	add.s64 	%rd524, %rd476, %rd21;
	add.s64 	%rd525, %rd524, %rd495;
	xor.b64  	%rd526, %rd520, %rd525;
	mov.b64	{%r1905, %r1906}, %rd526;
	mov.b64	%rd527, {%r1906, %r1905};
	add.s64 	%rd528, %rd527, %rd507;
	xor.b64  	%rd529, %rd528, %rd495;
	mov.b64	{%r1907, %r1908}, %rd529;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd530, {%r1910, %r1909};
	add.s64 	%rd531, %rd525, %rd29;
	add.s64 	%rd532, %rd531, %rd530;
	xor.b64  	%rd533, %rd527, %rd532;
	mov.b64	{%r1911, %r1912}, %rd533;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd534, {%r1914, %r1913};
	add.s64 	%rd535, %rd534, %rd528;
	xor.b64  	%rd536, %rd535, %rd530;
	mov.b64	{%r294, %r295}, %rd536;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd537, {%r289, %r293};
	add.s64 	%rd538, %rd490, %rd18;
	add.s64 	%rd539, %rd538, %rd509;
	xor.b64  	%rd540, %rd539, %rd478;
	mov.b64	{%r1915, %r1916}, %rd540;
	mov.b64	%rd541, {%r1916, %r1915};
	add.s64 	%rd542, %rd541, %rd521;
	xor.b64  	%rd543, %rd542, %rd509;
	mov.b64	{%r1917, %r1918}, %rd543;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd544, {%r1920, %r1919};
	add.s64 	%rd545, %rd539, %rd19;
	add.s64 	%rd546, %rd545, %rd544;
	xor.b64  	%rd547, %rd546, %rd541;
	mov.b64	{%r1921, %r1922}, %rd547;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd548, {%r1924, %r1923};
	add.s64 	%rd549, %rd548, %rd542;
	xor.b64  	%rd550, %rd549, %rd544;
	mov.b64	{%r302, %r303}, %rd550;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd551, {%r297, %r301};
	add.s64 	%rd552, %rd504, %rd13;
	add.s64 	%rd553, %rd552, %rd523;
	xor.b64  	%rd554, %rd553, %rd492;
	mov.b64	{%r1925, %r1926}, %rd554;
	mov.b64	%rd555, {%r1926, %r1925};
	add.s64 	%rd556, %rd555, %rd479;
	xor.b64  	%rd557, %rd556, %rd523;
	mov.b64	{%r1927, %r1928}, %rd557;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd558, {%r1930, %r1929};
	add.s64 	%rd559, %rd553, %rd15;
	add.s64 	%rd560, %rd559, %rd558;
	xor.b64  	%rd561, %rd560, %rd555;
	mov.b64	{%r1931, %r1932}, %rd561;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd562, {%r1934, %r1933};
	add.s64 	%rd563, %rd562, %rd556;
	xor.b64  	%rd564, %rd563, %rd558;
	mov.b64	{%r310, %r311}, %rd564;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd565, {%r305, %r309};
	add.s64 	%rd566, %rd481, %rd10;
	add.s64 	%rd567, %rd566, %rd518;
	xor.b64  	%rd568, %rd567, %rd506;
	mov.b64	{%r1935, %r1936}, %rd568;
	mov.b64	%rd569, {%r1936, %r1935};
	add.s64 	%rd570, %rd569, %rd493;
	xor.b64  	%rd571, %rd570, %rd481;
	mov.b64	{%r1937, %r1938}, %rd571;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd572, {%r1940, %r1939};
	add.s64 	%rd573, %rd567, %rd20;
	add.s64 	%rd574, %rd573, %rd572;
	xor.b64  	%rd575, %rd574, %rd569;
	mov.b64	{%r1941, %r1942}, %rd575;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd576, {%r1944, %r1943};
	add.s64 	%rd577, %rd576, %rd570;
	xor.b64  	%rd578, %rd577, %rd572;
	mov.b64	{%r318, %r319}, %rd578;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd579, {%r313, %r317};
	add.s64 	%rd580, %rd532, %rd9;
	add.s64 	%rd581, %rd580, %rd579;
	xor.b64  	%rd582, %rd581, %rd548;
	mov.b64	{%r1945, %r1946}, %rd582;
	mov.b64	%rd583, {%r1946, %r1945};
	add.s64 	%rd584, %rd583, %rd563;
	xor.b64  	%rd585, %rd584, %rd579;
	mov.b64	{%r1947, %r1948}, %rd585;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd586, {%r1950, %r1949};
	add.s64 	%rd587, %rd581, %rd19;
	add.s64 	%rd588, %rd587, %rd586;
	xor.b64  	%rd589, %rd583, %rd588;
	mov.b64	{%r1951, %r1952}, %rd589;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd590, {%r1954, %r1953};
	add.s64 	%rd591, %rd584, %rd590;
	xor.b64  	%rd592, %rd591, %rd586;
	mov.b64	{%r326, %r327}, %rd592;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd593, {%r321, %r325};
	add.s64 	%rd594, %rd537, %rd13;
	add.s64 	%rd595, %rd594, %rd546;
	xor.b64  	%rd596, %rd562, %rd595;
	mov.b64	{%r1955, %r1956}, %rd596;
	mov.b64	%rd597, {%r1956, %r1955};
	add.s64 	%rd598, %rd577, %rd597;
	xor.b64  	%rd599, %rd598, %rd537;
	mov.b64	{%r1957, %r1958}, %rd599;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd600, {%r1960, %r1959};
	add.s64 	%rd601, %rd595, %rd17;
	add.s64 	%rd602, %rd601, %rd600;
	xor.b64  	%rd603, %rd602, %rd597;
	mov.b64	{%r1961, %r1962}, %rd603;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd604, {%r1964, %r1963};
	add.s64 	%rd605, %rd604, %rd598;
	xor.b64  	%rd606, %rd605, %rd600;
	mov.b64	{%r334, %r335}, %rd606;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd607, {%r329, %r333};
	add.s64 	%rd608, %rd551, %rd1;
	add.s64 	%rd609, %rd608, %rd560;
	xor.b64  	%rd610, %rd576, %rd609;
	mov.b64	{%r1965, %r1966}, %rd610;
	mov.b64	%rd611, {%r1966, %r1965};
	add.s64 	%rd612, %rd611, %rd535;
	xor.b64  	%rd613, %rd612, %rd551;
	mov.b64	{%r1967, %r1968}, %rd613;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd614, {%r1970, %r1969};
	add.s64 	%rd615, %rd609, %rd18;
	add.s64 	%rd616, %rd615, %rd614;
	xor.b64  	%rd617, %rd616, %rd611;
	mov.b64	{%r1971, %r1972}, %rd617;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd618, {%r1974, %r1973};
	add.s64 	%rd619, %rd618, %rd612;
	xor.b64  	%rd620, %rd619, %rd614;
	mov.b64	{%r342, %r343}, %rd620;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd621, {%r337, %r341};
	add.s64 	%rd622, %rd565, %rd15;
	add.s64 	%rd623, %rd622, %rd574;
	xor.b64  	%rd624, %rd623, %rd534;
	mov.b64	{%r1975, %r1976}, %rd624;
	mov.b64	%rd625, {%r1976, %r1975};
	add.s64 	%rd626, %rd625, %rd549;
	xor.b64  	%rd627, %rd626, %rd565;
	mov.b64	{%r1977, %r1978}, %rd627;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd628, {%r1980, %r1979};
	add.s64 	%rd629, %rd623, %rd10;
	add.s64 	%rd630, %rd629, %rd628;
	xor.b64  	%rd631, %rd630, %rd625;
	mov.b64	{%r1981, %r1982}, %rd631;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd632, {%r1984, %r1983};
	add.s64 	%rd633, %rd632, %rd626;
	xor.b64  	%rd634, %rd633, %rd628;
	mov.b64	{%r350, %r351}, %rd634;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd635, {%r345, %r349};
	add.s64 	%rd636, %rd588, %rd11;
	add.s64 	%rd637, %rd636, %rd607;
	xor.b64  	%rd638, %rd632, %rd637;
	mov.b64	{%r1985, %r1986}, %rd638;
	mov.b64	%rd639, {%r1986, %r1985};
	add.s64 	%rd640, %rd639, %rd619;
	xor.b64  	%rd641, %rd640, %rd607;
	mov.b64	{%r1987, %r1988}, %rd641;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd642, {%r1990, %r1989};
	add.s64 	%rd643, %rd637, %rd20;
	add.s64 	%rd644, %rd643, %rd642;
	xor.b64  	%rd645, %rd639, %rd644;
	mov.b64	{%r1991, %r1992}, %rd645;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd646, {%r1994, %r1993};
	add.s64 	%rd647, %rd646, %rd640;
	xor.b64  	%rd648, %rd647, %rd642;
	mov.b64	{%r358, %r359}, %rd648;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd649, {%r353, %r357};
	add.s64 	%rd650, %rd602, %rd14;
	add.s64 	%rd651, %rd650, %rd621;
	xor.b64  	%rd652, %rd651, %rd590;
	mov.b64	{%r1995, %r1996}, %rd652;
	mov.b64	%rd653, {%r1996, %r1995};
	add.s64 	%rd654, %rd653, %rd633;
	xor.b64  	%rd655, %rd654, %rd621;
	mov.b64	{%r1997, %r1998}, %rd655;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd656, {%r2000, %r1999};
	add.s64 	%rd657, %rd651, %rd12;
	add.s64 	%rd658, %rd657, %rd656;
	xor.b64  	%rd659, %rd658, %rd653;
	mov.b64	{%r2001, %r2002}, %rd659;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd660, {%r2004, %r2003};
	add.s64 	%rd661, %rd660, %rd654;
	xor.b64  	%rd662, %rd661, %rd656;
	mov.b64	{%r366, %r367}, %rd662;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd663, {%r361, %r365};
	add.s64 	%rd664, %rd616, %rd22;
	add.s64 	%rd665, %rd664, %rd635;
	xor.b64  	%rd666, %rd665, %rd604;
	mov.b64	{%r2005, %r2006}, %rd666;
	mov.b64	%rd667, {%r2006, %r2005};
	add.s64 	%rd668, %rd667, %rd591;
	xor.b64  	%rd669, %rd668, %rd635;
	mov.b64	{%r2007, %r2008}, %rd669;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd670, {%r2010, %r2009};
	add.s64 	%rd671, %rd665, %rd21;
	add.s64 	%rd672, %rd671, %rd670;
	xor.b64  	%rd673, %rd672, %rd667;
	mov.b64	{%r2011, %r2012}, %rd673;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd674, {%r2014, %r2013};
	add.s64 	%rd675, %rd674, %rd668;
	xor.b64  	%rd676, %rd675, %rd670;
	mov.b64	{%r374, %r375}, %rd676;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd677, {%r369, %r373};
	add.s64 	%rd678, %rd593, %rd29;
	add.s64 	%rd679, %rd678, %rd630;
	xor.b64  	%rd680, %rd679, %rd618;
	mov.b64	{%r2015, %r2016}, %rd680;
	mov.b64	%rd681, {%r2016, %r2015};
	add.s64 	%rd682, %rd681, %rd605;
	xor.b64  	%rd683, %rd682, %rd593;
	mov.b64	{%r2017, %r2018}, %rd683;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd684, {%r2020, %r2019};
	add.s64 	%rd685, %rd679, %rd16;
	add.s64 	%rd686, %rd685, %rd684;
	xor.b64  	%rd687, %rd686, %rd681;
	mov.b64	{%r2021, %r2022}, %rd687;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd688, {%r2024, %r2023};
	add.s64 	%rd689, %rd688, %rd682;
	xor.b64  	%rd690, %rd689, %rd684;
	mov.b64	{%r382, %r383}, %rd690;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd691, {%r377, %r381};
	add.s64 	%rd692, %rd644, %rd19;
	add.s64 	%rd693, %rd692, %rd691;
	xor.b64  	%rd694, %rd693, %rd660;
	mov.b64	{%r2025, %r2026}, %rd694;
	mov.b64	%rd695, {%r2026, %r2025};
	add.s64 	%rd696, %rd695, %rd675;
	xor.b64  	%rd697, %rd696, %rd691;
	mov.b64	{%r2027, %r2028}, %rd697;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd698, {%r2030, %r2029};
	add.s64 	%rd699, %rd693, %rd12;
	add.s64 	%rd700, %rd699, %rd698;
	xor.b64  	%rd701, %rd695, %rd700;
	mov.b64	{%r2031, %r2032}, %rd701;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd702, {%r2034, %r2033};
	add.s64 	%rd703, %rd696, %rd702;
	xor.b64  	%rd704, %rd703, %rd698;
	mov.b64	{%r390, %r391}, %rd704;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd705, {%r385, %r389};
	add.s64 	%rd706, %rd649, %rd29;
	add.s64 	%rd707, %rd706, %rd658;
	xor.b64  	%rd708, %rd674, %rd707;
	mov.b64	{%r2035, %r2036}, %rd708;
	mov.b64	%rd709, {%r2036, %r2035};
	add.s64 	%rd710, %rd689, %rd709;
	xor.b64  	%rd711, %rd710, %rd649;
	mov.b64	{%r2037, %r2038}, %rd711;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd712, {%r2040, %r2039};
	add.s64 	%rd713, %rd707, %rd22;
	add.s64 	%rd714, %rd713, %rd712;
	xor.b64  	%rd715, %rd714, %rd709;
	mov.b64	{%r2041, %r2042}, %rd715;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd716, {%r2044, %r2043};
	add.s64 	%rd717, %rd716, %rd710;
	xor.b64  	%rd718, %rd717, %rd712;
	mov.b64	{%r398, %r399}, %rd718;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd719, {%r393, %r397};
	add.s64 	%rd720, %rd663, %rd21;
	add.s64 	%rd721, %rd720, %rd672;
	xor.b64  	%rd722, %rd688, %rd721;
	mov.b64	{%r2045, %r2046}, %rd722;
	mov.b64	%rd723, {%r2046, %r2045};
	add.s64 	%rd724, %rd723, %rd647;
	xor.b64  	%rd725, %rd724, %rd663;
	mov.b64	{%r2047, %r2048}, %rd725;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd726, {%r2050, %r2049};
	add.s64 	%rd727, %rd721, %rd20;
	add.s64 	%rd728, %rd727, %rd726;
	xor.b64  	%rd729, %rd728, %rd723;
	mov.b64	{%r2051, %r2052}, %rd729;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd730, {%r2054, %r2053};
	add.s64 	%rd731, %rd730, %rd724;
	xor.b64  	%rd732, %rd731, %rd726;
	mov.b64	{%r406, %r407}, %rd732;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd733, {%r401, %r405};
	add.s64 	%rd734, %rd677, %rd11;
	add.s64 	%rd735, %rd734, %rd686;
	xor.b64  	%rd736, %rd735, %rd646;
	mov.b64	{%r2055, %r2056}, %rd736;
	mov.b64	%rd737, {%r2056, %r2055};
	add.s64 	%rd738, %rd737, %rd661;
	xor.b64  	%rd739, %rd738, %rd677;
	mov.b64	{%r2057, %r2058}, %rd739;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd740, {%r2060, %r2059};
	add.s64 	%rd741, %rd735, %rd17;
	add.s64 	%rd742, %rd741, %rd740;
	xor.b64  	%rd743, %rd742, %rd737;
	mov.b64	{%r2061, %r2062}, %rd743;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd744, {%r2064, %r2063};
	add.s64 	%rd745, %rd744, %rd738;
	xor.b64  	%rd746, %rd745, %rd740;
	mov.b64	{%r414, %r415}, %rd746;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd747, {%r409, %r413};
	add.s64 	%rd748, %rd700, %rd1;
	add.s64 	%rd749, %rd748, %rd719;
	xor.b64  	%rd750, %rd744, %rd749;
	mov.b64	{%r2065, %r2066}, %rd750;
	mov.b64	%rd751, {%r2066, %r2065};
	add.s64 	%rd752, %rd751, %rd731;
	xor.b64  	%rd753, %rd752, %rd719;
	mov.b64	{%r2067, %r2068}, %rd753;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd754, {%r2070, %r2069};
	add.s64 	%rd755, %rd749, %rd14;
	add.s64 	%rd756, %rd755, %rd754;
	xor.b64  	%rd757, %rd751, %rd756;
	mov.b64	{%r2071, %r2072}, %rd757;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd758, {%r2074, %r2073};
	add.s64 	%rd759, %rd758, %rd752;
	xor.b64  	%rd760, %rd759, %rd754;
	mov.b64	{%r422, %r423}, %rd760;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd761, {%r417, %r421};
	add.s64 	%rd762, %rd714, %rd13;
	add.s64 	%rd763, %rd762, %rd733;
	xor.b64  	%rd764, %rd763, %rd702;
	mov.b64	{%r2075, %r2076}, %rd764;
	mov.b64	%rd765, {%r2076, %r2075};
	add.s64 	%rd766, %rd765, %rd745;
	xor.b64  	%rd767, %rd766, %rd733;
	mov.b64	{%r2077, %r2078}, %rd767;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd768, {%r2080, %r2079};
	add.s64 	%rd769, %rd763, %rd10;
	add.s64 	%rd770, %rd769, %rd768;
	xor.b64  	%rd771, %rd770, %rd765;
	mov.b64	{%r2081, %r2082}, %rd771;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd772, {%r2084, %r2083};
	add.s64 	%rd773, %rd772, %rd766;
	xor.b64  	%rd774, %rd773, %rd768;
	mov.b64	{%r430, %r431}, %rd774;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd775, {%r425, %r429};
	add.s64 	%rd776, %rd728, %rd16;
	add.s64 	%rd777, %rd776, %rd747;
	xor.b64  	%rd778, %rd777, %rd716;
	mov.b64	{%r2085, %r2086}, %rd778;
	mov.b64	%rd779, {%r2086, %r2085};
	add.s64 	%rd780, %rd779, %rd703;
	xor.b64  	%rd781, %rd780, %rd747;
	mov.b64	{%r2087, %r2088}, %rd781;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd782, {%r2090, %r2089};
	add.s64 	%rd783, %rd777, %rd9;
	add.s64 	%rd784, %rd783, %rd782;
	xor.b64  	%rd785, %rd784, %rd779;
	mov.b64	{%r2091, %r2092}, %rd785;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd786, {%r2094, %r2093};
	add.s64 	%rd787, %rd786, %rd780;
	xor.b64  	%rd788, %rd787, %rd782;
	mov.b64	{%r438, %r439}, %rd788;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd789, {%r433, %r437};
	add.s64 	%rd790, %rd705, %rd15;
	add.s64 	%rd791, %rd790, %rd742;
	xor.b64  	%rd792, %rd791, %rd730;
	mov.b64	{%r2095, %r2096}, %rd792;
	mov.b64	%rd793, {%r2096, %r2095};
	add.s64 	%rd794, %rd793, %rd717;
	xor.b64  	%rd795, %rd794, %rd705;
	mov.b64	{%r2097, %r2098}, %rd795;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd796, {%r2100, %r2099};
	add.s64 	%rd797, %rd791, %rd18;
	add.s64 	%rd798, %rd797, %rd796;
	xor.b64  	%rd799, %rd798, %rd793;
	mov.b64	{%r2101, %r2102}, %rd799;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd800, {%r2104, %r2103};
	add.s64 	%rd801, %rd800, %rd794;
	xor.b64  	%rd802, %rd801, %rd796;
	mov.b64	{%r446, %r447}, %rd802;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd803, {%r441, %r445};
	add.s64 	%rd804, %rd756, %rd20;
	add.s64 	%rd805, %rd804, %rd803;
	xor.b64  	%rd806, %rd805, %rd772;
	mov.b64	{%r2105, %r2106}, %rd806;
	mov.b64	%rd807, {%r2106, %r2105};
	add.s64 	%rd808, %rd807, %rd787;
	xor.b64  	%rd809, %rd808, %rd803;
	mov.b64	{%r2107, %r2108}, %rd809;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd810, {%r2110, %r2109};
	add.s64 	%rd811, %rd805, %rd18;
	add.s64 	%rd812, %rd811, %rd810;
	xor.b64  	%rd813, %rd807, %rd812;
	mov.b64	{%r2111, %r2112}, %rd813;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd814, {%r2114, %r2113};
	add.s64 	%rd815, %rd808, %rd814;
	xor.b64  	%rd816, %rd815, %rd810;
	mov.b64	{%r454, %r455}, %rd816;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd817, {%r449, %r453};
	add.s64 	%rd818, %rd761, %rd14;
	add.s64 	%rd819, %rd818, %rd770;
	xor.b64  	%rd820, %rd786, %rd819;
	mov.b64	{%r2115, %r2116}, %rd820;
	mov.b64	%rd821, {%r2116, %r2115};
	add.s64 	%rd822, %rd801, %rd821;
	xor.b64  	%rd823, %rd822, %rd761;
	mov.b64	{%r2117, %r2118}, %rd823;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd824, {%r2120, %r2119};
	add.s64 	%rd825, %rd819, %rd21;
	add.s64 	%rd826, %rd825, %rd824;
	xor.b64  	%rd827, %rd826, %rd821;
	mov.b64	{%r2121, %r2122}, %rd827;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd828, {%r2124, %r2123};
	add.s64 	%rd829, %rd828, %rd822;
	xor.b64  	%rd830, %rd829, %rd824;
	mov.b64	{%r462, %r463}, %rd830;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd831, {%r457, %r461};
	add.s64 	%rd832, %rd775, %rd19;
	add.s64 	%rd833, %rd832, %rd784;
	xor.b64  	%rd834, %rd800, %rd833;
	mov.b64	{%r2125, %r2126}, %rd834;
	mov.b64	%rd835, {%r2126, %r2125};
	add.s64 	%rd836, %rd835, %rd759;
	xor.b64  	%rd837, %rd836, %rd775;
	mov.b64	{%r2127, %r2128}, %rd837;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd838, {%r2130, %r2129};
	add.s64 	%rd839, %rd833, %rd29;
	add.s64 	%rd840, %rd839, %rd838;
	xor.b64  	%rd841, %rd840, %rd835;
	mov.b64	{%r2131, %r2132}, %rd841;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd842, {%r2134, %r2133};
	add.s64 	%rd843, %rd842, %rd836;
	xor.b64  	%rd844, %rd843, %rd838;
	mov.b64	{%r470, %r471}, %rd844;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd845, {%r465, %r469};
	add.s64 	%rd846, %rd789, %rd10;
	add.s64 	%rd847, %rd846, %rd798;
	xor.b64  	%rd848, %rd847, %rd758;
	mov.b64	{%r2135, %r2136}, %rd848;
	mov.b64	%rd849, {%r2136, %r2135};
	add.s64 	%rd850, %rd849, %rd773;
	xor.b64  	%rd851, %rd850, %rd789;
	mov.b64	{%r2137, %r2138}, %rd851;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd852, {%r2140, %r2139};
	add.s64 	%rd853, %rd847, %rd16;
	add.s64 	%rd854, %rd853, %rd852;
	xor.b64  	%rd855, %rd854, %rd849;
	mov.b64	{%r2141, %r2142}, %rd855;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd856, {%r2144, %r2143};
	add.s64 	%rd857, %rd856, %rd850;
	xor.b64  	%rd858, %rd857, %rd852;
	mov.b64	{%r478, %r479}, %rd858;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd859, {%r473, %r477};
	add.s64 	%rd860, %rd812, %rd12;
	add.s64 	%rd861, %rd860, %rd831;
	xor.b64  	%rd862, %rd856, %rd861;
	mov.b64	{%r2145, %r2146}, %rd862;
	mov.b64	%rd863, {%r2146, %r2145};
	add.s64 	%rd864, %rd863, %rd843;
	xor.b64  	%rd865, %rd864, %rd831;
	mov.b64	{%r2147, %r2148}, %rd865;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd866, {%r2150, %r2149};
	add.s64 	%rd867, %rd861, %rd1;
	add.s64 	%rd868, %rd867, %rd866;
	xor.b64  	%rd869, %rd863, %rd868;
	mov.b64	{%r2151, %r2152}, %rd869;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd870, {%r2154, %r2153};
	add.s64 	%rd871, %rd870, %rd864;
	xor.b64  	%rd872, %rd871, %rd866;
	mov.b64	{%r486, %r487}, %rd872;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd873, {%r481, %r485};
	add.s64 	%rd874, %rd826, %rd22;
	add.s64 	%rd875, %rd874, %rd845;
	xor.b64  	%rd876, %rd875, %rd814;
	mov.b64	{%r2155, %r2156}, %rd876;
	mov.b64	%rd877, {%r2156, %r2155};
	add.s64 	%rd878, %rd877, %rd857;
	xor.b64  	%rd879, %rd878, %rd845;
	mov.b64	{%r2157, %r2158}, %rd879;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd880, {%r2160, %r2159};
	add.s64 	%rd881, %rd875, %rd11;
	add.s64 	%rd882, %rd881, %rd880;
	xor.b64  	%rd883, %rd882, %rd877;
	mov.b64	{%r2161, %r2162}, %rd883;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd884, {%r2164, %r2163};
	add.s64 	%rd885, %rd884, %rd878;
	xor.b64  	%rd886, %rd885, %rd880;
	mov.b64	{%r494, %r495}, %rd886;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd887, {%r489, %r493};
	add.s64 	%rd888, %rd840, %rd15;
	add.s64 	%rd889, %rd888, %rd859;
	xor.b64  	%rd890, %rd889, %rd828;
	mov.b64	{%r2165, %r2166}, %rd890;
	mov.b64	%rd891, {%r2166, %r2165};
	add.s64 	%rd892, %rd891, %rd815;
	xor.b64  	%rd893, %rd892, %rd859;
	mov.b64	{%r2167, %r2168}, %rd893;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd894, {%r2170, %r2169};
	add.s64 	%rd895, %rd889, %rd13;
	add.s64 	%rd896, %rd895, %rd894;
	xor.b64  	%rd897, %rd896, %rd891;
	mov.b64	{%r2171, %r2172}, %rd897;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd898, {%r2174, %r2173};
	add.s64 	%rd899, %rd898, %rd892;
	xor.b64  	%rd900, %rd899, %rd894;
	mov.b64	{%r502, %r503}, %rd900;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd901, {%r497, %r501};
	add.s64 	%rd902, %rd817, %rd9;
	add.s64 	%rd903, %rd902, %rd854;
	xor.b64  	%rd904, %rd903, %rd842;
	mov.b64	{%r2175, %r2176}, %rd904;
	mov.b64	%rd905, {%r2176, %r2175};
	add.s64 	%rd906, %rd905, %rd829;
	xor.b64  	%rd907, %rd906, %rd817;
	mov.b64	{%r2177, %r2178}, %rd907;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd908, {%r2180, %r2179};
	add.s64 	%rd909, %rd903, %rd17;
	add.s64 	%rd910, %rd909, %rd908;
	xor.b64  	%rd911, %rd910, %rd905;
	mov.b64	{%r2181, %r2182}, %rd911;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd912, {%r2184, %r2183};
	add.s64 	%rd913, %rd912, %rd906;
	xor.b64  	%rd914, %rd913, %rd908;
	mov.b64	{%r510, %r511}, %rd914;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd915, {%r505, %r509};
	add.s64 	%rd916, %rd868, %rd13;
	add.s64 	%rd917, %rd916, %rd915;
	xor.b64  	%rd918, %rd917, %rd884;
	mov.b64	{%r2185, %r2186}, %rd918;
	mov.b64	%rd919, {%r2186, %r2185};
	add.s64 	%rd920, %rd919, %rd899;
	xor.b64  	%rd921, %rd920, %rd915;
	mov.b64	{%r2187, %r2188}, %rd921;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd922, {%r2190, %r2189};
	add.s64 	%rd923, %rd917, %rd22;
	add.s64 	%rd924, %rd923, %rd922;
	xor.b64  	%rd925, %rd919, %rd924;
	mov.b64	{%r2191, %r2192}, %rd925;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd926, {%r2194, %r2193};
	add.s64 	%rd927, %rd920, %rd926;
	xor.b64  	%rd928, %rd927, %rd922;
	mov.b64	{%r518, %r519}, %rd928;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd929, {%r513, %r517};
	add.s64 	%rd930, %rd873, %rd21;
	add.s64 	%rd931, %rd930, %rd882;
	xor.b64  	%rd932, %rd898, %rd931;
	mov.b64	{%r2195, %r2196}, %rd932;
	mov.b64	%rd933, {%r2196, %r2195};
	add.s64 	%rd934, %rd913, %rd933;
	xor.b64  	%rd935, %rd934, %rd873;
	mov.b64	{%r2197, %r2198}, %rd935;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd936, {%r2200, %r2199};
	add.s64 	%rd937, %rd931, %rd16;
	add.s64 	%rd938, %rd937, %rd936;
	xor.b64  	%rd939, %rd938, %rd933;
	mov.b64	{%r2201, %r2202}, %rd939;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd940, {%r2204, %r2203};
	add.s64 	%rd941, %rd940, %rd934;
	xor.b64  	%rd942, %rd941, %rd936;
	mov.b64	{%r526, %r527}, %rd942;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd943, {%r521, %r525};
	add.s64 	%rd944, %rd887, %rd18;
	add.s64 	%rd945, %rd944, %rd896;
	xor.b64  	%rd946, %rd912, %rd945;
	mov.b64	{%r2205, %r2206}, %rd946;
	mov.b64	%rd947, {%r2206, %r2205};
	add.s64 	%rd948, %rd947, %rd871;
	xor.b64  	%rd949, %rd948, %rd887;
	mov.b64	{%r2207, %r2208}, %rd949;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd950, {%r2210, %r2209};
	add.s64 	%rd951, %rd945, %rd10;
	add.s64 	%rd952, %rd951, %rd950;
	xor.b64  	%rd953, %rd952, %rd947;
	mov.b64	{%r2211, %r2212}, %rd953;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd954, {%r2214, %r2213};
	add.s64 	%rd955, %rd954, %rd948;
	xor.b64  	%rd956, %rd955, %rd950;
	mov.b64	{%r534, %r535}, %rd956;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd957, {%r529, %r533};
	add.s64 	%rd958, %rd901, %rd1;
	add.s64 	%rd959, %rd958, %rd910;
	xor.b64  	%rd960, %rd959, %rd870;
	mov.b64	{%r2215, %r2216}, %rd960;
	mov.b64	%rd961, {%r2216, %r2215};
	add.s64 	%rd962, %rd961, %rd885;
	xor.b64  	%rd963, %rd962, %rd901;
	mov.b64	{%r2217, %r2218}, %rd963;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd964, {%r2220, %r2219};
	add.s64 	%rd965, %rd959, %rd15;
	add.s64 	%rd966, %rd965, %rd964;
	xor.b64  	%rd967, %rd966, %rd961;
	mov.b64	{%r2221, %r2222}, %rd967;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd968, {%r2224, %r2223};
	add.s64 	%rd969, %rd968, %rd962;
	xor.b64  	%rd970, %rd969, %rd964;
	mov.b64	{%r542, %r543}, %rd970;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd971, {%r537, %r541};
	add.s64 	%rd972, %rd924, %rd19;
	add.s64 	%rd973, %rd972, %rd943;
	xor.b64  	%rd974, %rd968, %rd973;
	mov.b64	{%r2225, %r2226}, %rd974;
	mov.b64	%rd975, {%r2226, %r2225};
	add.s64 	%rd976, %rd975, %rd955;
	xor.b64  	%rd977, %rd976, %rd943;
	mov.b64	{%r2227, %r2228}, %rd977;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd978, {%r2230, %r2229};
	add.s64 	%rd979, %rd973, %rd9;
	add.s64 	%rd980, %rd979, %rd978;
	xor.b64  	%rd981, %rd975, %rd980;
	mov.b64	{%r2231, %r2232}, %rd981;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd982, {%r2234, %r2233};
	add.s64 	%rd983, %rd982, %rd976;
	xor.b64  	%rd984, %rd983, %rd978;
	mov.b64	{%r550, %r551}, %rd984;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd985, {%r545, %r549};
	add.s64 	%rd986, %rd938, %rd20;
	add.s64 	%rd987, %rd986, %rd957;
	xor.b64  	%rd988, %rd987, %rd926;
	mov.b64	{%r2235, %r2236}, %rd988;
	mov.b64	%rd989, {%r2236, %r2235};
	add.s64 	%rd990, %rd989, %rd969;
	xor.b64  	%rd991, %rd990, %rd957;
	mov.b64	{%r2237, %r2238}, %rd991;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd992, {%r2240, %r2239};
	add.s64 	%rd993, %rd987, %rd14;
	add.s64 	%rd994, %rd993, %rd992;
	xor.b64  	%rd995, %rd994, %rd989;
	mov.b64	{%r2241, %r2242}, %rd995;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd996, {%r2244, %r2243};
	add.s64 	%rd997, %rd996, %rd990;
	xor.b64  	%rd998, %rd997, %rd992;
	mov.b64	{%r558, %r559}, %rd998;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd999, {%r553, %r557};
	add.s64 	%rd1000, %rd952, %rd29;
	add.s64 	%rd1001, %rd1000, %rd971;
	xor.b64  	%rd1002, %rd1001, %rd940;
	mov.b64	{%r2245, %r2246}, %rd1002;
	mov.b64	%rd1003, {%r2246, %r2245};
	add.s64 	%rd1004, %rd1003, %rd927;
	xor.b64  	%rd1005, %rd1004, %rd971;
	mov.b64	{%r2247, %r2248}, %rd1005;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1006, {%r2250, %r2249};
	add.s64 	%rd1007, %rd1001, %rd11;
	add.s64 	%rd1008, %rd1007, %rd1006;
	xor.b64  	%rd1009, %rd1008, %rd1003;
	mov.b64	{%r2251, %r2252}, %rd1009;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1010, {%r2254, %r2253};
	add.s64 	%rd1011, %rd1010, %rd1004;
	xor.b64  	%rd1012, %rd1011, %rd1006;
	mov.b64	{%r566, %r567}, %rd1012;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1013, {%r561, %r565};
	add.s64 	%rd1014, %rd929, %rd17;
	add.s64 	%rd1015, %rd1014, %rd966;
	xor.b64  	%rd1016, %rd1015, %rd954;
	mov.b64	{%r2255, %r2256}, %rd1016;
	mov.b64	%rd1017, {%r2256, %r2255};
	add.s64 	%rd1018, %rd1017, %rd941;
	xor.b64  	%rd1019, %rd1018, %rd929;
	mov.b64	{%r2257, %r2258}, %rd1019;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1020, {%r2260, %r2259};
	add.s64 	%rd1021, %rd1015, %rd12;
	add.s64 	%rd1022, %rd1021, %rd1020;
	xor.b64  	%rd1023, %rd1022, %rd1017;
	mov.b64	{%r2261, %r2262}, %rd1023;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1024, {%r2264, %r2263};
	add.s64 	%rd1025, %rd1024, %rd1018;
	xor.b64  	%rd1026, %rd1025, %rd1020;
	mov.b64	{%r574, %r575}, %rd1026;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1027, {%r569, %r573};
	add.s64 	%rd1028, %rd980, %rd17;
	add.s64 	%rd1029, %rd1028, %rd1027;
	xor.b64  	%rd1030, %rd1029, %rd996;
	mov.b64	{%r2265, %r2266}, %rd1030;
	mov.b64	%rd1031, {%r2266, %r2265};
	add.s64 	%rd1032, %rd1031, %rd1011;
	xor.b64  	%rd1033, %rd1032, %rd1027;
	mov.b64	{%r2267, %r2268}, %rd1033;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1034, {%r2270, %r2269};
	add.s64 	%rd1035, %rd1029, %rd9;
	add.s64 	%rd1036, %rd1035, %rd1034;
	xor.b64  	%rd1037, %rd1031, %rd1036;
	mov.b64	{%r2271, %r2272}, %rd1037;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1038, {%r2274, %r2273};
	add.s64 	%rd1039, %rd1032, %rd1038;
	xor.b64  	%rd1040, %rd1039, %rd1034;
	mov.b64	{%r582, %r583}, %rd1040;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1041, {%r577, %r581};
	add.s64 	%rd1042, %rd985, %rd15;
	add.s64 	%rd1043, %rd1042, %rd994;
	xor.b64  	%rd1044, %rd1010, %rd1043;
	mov.b64	{%r2275, %r2276}, %rd1044;
	mov.b64	%rd1045, {%r2276, %r2275};
	add.s64 	%rd1046, %rd1025, %rd1045;
	xor.b64  	%rd1047, %rd1046, %rd985;
	mov.b64	{%r2277, %r2278}, %rd1047;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1048, {%r2280, %r2279};
	add.s64 	%rd1049, %rd1043, %rd11;
	add.s64 	%rd1050, %rd1049, %rd1048;
	xor.b64  	%rd1051, %rd1050, %rd1045;
	mov.b64	{%r2281, %r2282}, %rd1051;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1052, {%r2284, %r2283};
	add.s64 	%rd1053, %rd1052, %rd1046;
	xor.b64  	%rd1054, %rd1053, %rd1048;
	mov.b64	{%r590, %r591}, %rd1054;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1055, {%r585, %r589};
	add.s64 	%rd1056, %rd999, %rd14;
	add.s64 	%rd1057, %rd1056, %rd1008;
	xor.b64  	%rd1058, %rd1024, %rd1057;
	mov.b64	{%r2285, %r2286}, %rd1058;
	mov.b64	%rd1059, {%r2286, %r2285};
	add.s64 	%rd1060, %rd1059, %rd983;
	xor.b64  	%rd1061, %rd1060, %rd999;
	mov.b64	{%r2287, %r2288}, %rd1061;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1062, {%r2290, %r2289};
	add.s64 	%rd1063, %rd1057, %rd13;
	add.s64 	%rd1064, %rd1063, %rd1062;
	xor.b64  	%rd1065, %rd1064, %rd1059;
	mov.b64	{%r2291, %r2292}, %rd1065;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1066, {%r2294, %r2293};
	add.s64 	%rd1067, %rd1066, %rd1060;
	xor.b64  	%rd1068, %rd1067, %rd1062;
	mov.b64	{%r598, %r599}, %rd1068;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1069, {%r593, %r597};
	add.s64 	%rd1070, %rd1013, %rd29;
	add.s64 	%rd1071, %rd1070, %rd1022;
	xor.b64  	%rd1072, %rd1071, %rd982;
	mov.b64	{%r2295, %r2296}, %rd1072;
	mov.b64	%rd1073, {%r2296, %r2295};
	add.s64 	%rd1074, %rd1073, %rd997;
	xor.b64  	%rd1075, %rd1074, %rd1013;
	mov.b64	{%r2297, %r2298}, %rd1075;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1076, {%r2300, %r2299};
	add.s64 	%rd1077, %rd1071, %rd12;
	add.s64 	%rd1078, %rd1077, %rd1076;
	xor.b64  	%rd1079, %rd1078, %rd1073;
	mov.b64	{%r2301, %r2302}, %rd1079;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1080, {%r2304, %r2303};
	add.s64 	%rd1081, %rd1080, %rd1074;
	xor.b64  	%rd1082, %rd1081, %rd1076;
	mov.b64	{%r606, %r607}, %rd1082;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1083, {%r601, %r605};
	add.s64 	%rd1084, %rd1036, %rd22;
	add.s64 	%rd1085, %rd1084, %rd1055;
	xor.b64  	%rd1086, %rd1080, %rd1085;
	mov.b64	{%r2305, %r2306}, %rd1086;
	mov.b64	%rd1087, {%r2306, %r2305};
	add.s64 	%rd1088, %rd1087, %rd1067;
	xor.b64  	%rd1089, %rd1088, %rd1055;
	mov.b64	{%r2307, %r2308}, %rd1089;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1090, {%r2310, %r2309};
	add.s64 	%rd1091, %rd1085, %rd18;
	add.s64 	%rd1092, %rd1091, %rd1090;
	xor.b64  	%rd1093, %rd1087, %rd1092;
	mov.b64	{%r2311, %r2312}, %rd1093;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1094, {%r2314, %r2313};
	add.s64 	%rd1095, %rd1094, %rd1088;
	xor.b64  	%rd1096, %rd1095, %rd1090;
	mov.b64	{%r614, %r615}, %rd1096;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1097, {%r609, %r613};
	add.s64 	%rd1098, %rd1050, %rd16;
	add.s64 	%rd1099, %rd1098, %rd1069;
	xor.b64  	%rd1100, %rd1099, %rd1038;
	mov.b64	{%r2315, %r2316}, %rd1100;
	mov.b64	%rd1101, {%r2316, %r2315};
	add.s64 	%rd1102, %rd1101, %rd1081;
	xor.b64  	%rd1103, %rd1102, %rd1069;
	mov.b64	{%r2317, %r2318}, %rd1103;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1104, {%r2320, %r2319};
	add.s64 	%rd1105, %rd1099, %rd21;
	add.s64 	%rd1106, %rd1105, %rd1104;
	xor.b64  	%rd1107, %rd1106, %rd1101;
	mov.b64	{%r2321, %r2322}, %rd1107;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1108, {%r2324, %r2323};
	add.s64 	%rd1109, %rd1108, %rd1102;
	xor.b64  	%rd1110, %rd1109, %rd1104;
	mov.b64	{%r622, %r623}, %rd1110;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1111, {%r617, %r621};
	add.s64 	%rd1112, %rd1064, %rd10;
	add.s64 	%rd1113, %rd1112, %rd1083;
	xor.b64  	%rd1114, %rd1113, %rd1052;
	mov.b64	{%r2325, %r2326}, %rd1114;
	mov.b64	%rd1115, {%r2326, %r2325};
	add.s64 	%rd1116, %rd1115, %rd1039;
	xor.b64  	%rd1117, %rd1116, %rd1083;
	mov.b64	{%r2327, %r2328}, %rd1117;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1118, {%r2330, %r2329};
	add.s64 	%rd1119, %rd1113, %rd19;
	add.s64 	%rd1120, %rd1119, %rd1118;
	xor.b64  	%rd1121, %rd1120, %rd1115;
	mov.b64	{%r2331, %r2332}, %rd1121;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1122, {%r2334, %r2333};
	add.s64 	%rd1123, %rd1122, %rd1116;
	xor.b64  	%rd1124, %rd1123, %rd1118;
	mov.b64	{%r630, %r631}, %rd1124;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1125, {%r625, %r629};
	add.s64 	%rd1126, %rd1041, %rd20;
	add.s64 	%rd1127, %rd1126, %rd1078;
	xor.b64  	%rd1128, %rd1127, %rd1066;
	mov.b64	{%r2335, %r2336}, %rd1128;
	mov.b64	%rd1129, {%r2336, %r2335};
	add.s64 	%rd1130, %rd1129, %rd1053;
	xor.b64  	%rd1131, %rd1130, %rd1041;
	mov.b64	{%r2337, %r2338}, %rd1131;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1132, {%r2340, %r2339};
	add.s64 	%rd1133, %rd1127, %rd1;
	add.s64 	%rd1134, %rd1133, %rd1132;
	xor.b64  	%rd1135, %rd1134, %rd1129;
	mov.b64	{%r2341, %r2342}, %rd1135;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1136, {%r2344, %r2343};
	add.s64 	%rd1137, %rd1136, %rd1130;
	xor.b64  	%rd1138, %rd1137, %rd1132;
	mov.b64	{%r638, %r639}, %rd1138;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1139, {%r633, %r637};
	add.s64 	%rd1140, %rd1092, %rd1;
	add.s64 	%rd1141, %rd1140, %rd1139;
	xor.b64  	%rd1142, %rd1141, %rd1108;
	mov.b64	{%r2345, %r2346}, %rd1142;
	mov.b64	%rd1143, {%r2346, %r2345};
	add.s64 	%rd1144, %rd1143, %rd1123;
	xor.b64  	%rd1145, %rd1144, %rd1139;
	mov.b64	{%r2347, %r2348}, %rd1145;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1146, {%r2350, %r2349};
	add.s64 	%rd1147, %rd1141, %rd29;
	add.s64 	%rd1148, %rd1147, %rd1146;
	xor.b64  	%rd1149, %rd1143, %rd1148;
	mov.b64	{%r2351, %r2352}, %rd1149;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1150, {%r2354, %r2353};
	add.s64 	%rd1151, %rd1144, %rd1150;
	xor.b64  	%rd1152, %rd1151, %rd1146;
	mov.b64	{%r646, %r647}, %rd1152;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1153, {%r641, %r645};
	add.s64 	%rd1154, %rd1097, %rd9;
	add.s64 	%rd1155, %rd1154, %rd1106;
	xor.b64  	%rd1156, %rd1122, %rd1155;
	mov.b64	{%r2355, %r2356}, %rd1156;
	mov.b64	%rd1157, {%r2356, %r2355};
	add.s64 	%rd1158, %rd1137, %rd1157;
	xor.b64  	%rd1159, %rd1158, %rd1097;
	mov.b64	{%r2357, %r2358}, %rd1159;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1160, {%r2360, %r2359};
	add.s64 	%rd1161, %rd1155, %rd10;
	add.s64 	%rd1162, %rd1161, %rd1160;
	xor.b64  	%rd1163, %rd1162, %rd1157;
	mov.b64	{%r2361, %r2362}, %rd1163;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1164, {%r2364, %r2363};
	add.s64 	%rd1165, %rd1164, %rd1158;
	xor.b64  	%rd1166, %rd1165, %rd1160;
	mov.b64	{%r654, %r655}, %rd1166;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1167, {%r649, %r653};
	add.s64 	%rd1168, %rd1111, %rd11;
	add.s64 	%rd1169, %rd1168, %rd1120;
	xor.b64  	%rd1170, %rd1136, %rd1169;
	mov.b64	{%r2365, %r2366}, %rd1170;
	mov.b64	%rd1171, {%r2366, %r2365};
	add.s64 	%rd1172, %rd1171, %rd1095;
	xor.b64  	%rd1173, %rd1172, %rd1111;
	mov.b64	{%r2367, %r2368}, %rd1173;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1174, {%r2370, %r2369};
	add.s64 	%rd1175, %rd1169, %rd12;
	add.s64 	%rd1176, %rd1175, %rd1174;
	xor.b64  	%rd1177, %rd1176, %rd1171;
	mov.b64	{%r2371, %r2372}, %rd1177;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1178, {%r2374, %r2373};
	add.s64 	%rd1179, %rd1178, %rd1172;
	xor.b64  	%rd1180, %rd1179, %rd1174;
	mov.b64	{%r662, %r663}, %rd1180;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1181, {%r657, %r661};
	add.s64 	%rd1182, %rd1125, %rd13;
	add.s64 	%rd1183, %rd1182, %rd1134;
	xor.b64  	%rd1184, %rd1183, %rd1094;
	mov.b64	{%r2375, %r2376}, %rd1184;
	mov.b64	%rd1185, {%r2376, %r2375};
	add.s64 	%rd1186, %rd1185, %rd1109;
	xor.b64  	%rd1187, %rd1186, %rd1125;
	mov.b64	{%r2377, %r2378}, %rd1187;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1188, {%r2380, %r2379};
	add.s64 	%rd1189, %rd1183, %rd14;
	add.s64 	%rd1190, %rd1189, %rd1188;
	xor.b64  	%rd1191, %rd1190, %rd1185;
	mov.b64	{%r2381, %r2382}, %rd1191;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1192, {%r2384, %r2383};
	add.s64 	%rd1193, %rd1192, %rd1186;
	xor.b64  	%rd1194, %rd1193, %rd1188;
	mov.b64	{%r670, %r671}, %rd1194;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1195, {%r665, %r669};
	add.s64 	%rd1196, %rd1148, %rd15;
	add.s64 	%rd1197, %rd1196, %rd1167;
	xor.b64  	%rd1198, %rd1192, %rd1197;
	mov.b64	{%r2385, %r2386}, %rd1198;
	mov.b64	%rd1199, {%r2386, %r2385};
	add.s64 	%rd1200, %rd1199, %rd1179;
	xor.b64  	%rd1201, %rd1200, %rd1167;
	mov.b64	{%r2387, %r2388}, %rd1201;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1202, {%r2390, %r2389};
	add.s64 	%rd1203, %rd1197, %rd16;
	add.s64 	%rd1204, %rd1203, %rd1202;
	xor.b64  	%rd1205, %rd1199, %rd1204;
	mov.b64	{%r2391, %r2392}, %rd1205;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1206, {%r2394, %r2393};
	add.s64 	%rd1207, %rd1206, %rd1200;
	xor.b64  	%rd1208, %rd1207, %rd1202;
	mov.b64	{%r678, %r679}, %rd1208;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1209, {%r673, %r677};
	add.s64 	%rd1210, %rd1162, %rd17;
	add.s64 	%rd1211, %rd1210, %rd1181;
	xor.b64  	%rd1212, %rd1211, %rd1150;
	mov.b64	{%r2395, %r2396}, %rd1212;
	mov.b64	%rd1213, {%r2396, %r2395};
	add.s64 	%rd1214, %rd1213, %rd1193;
	xor.b64  	%rd1215, %rd1214, %rd1181;
	mov.b64	{%r2397, %r2398}, %rd1215;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1216, {%r2400, %r2399};
	add.s64 	%rd1217, %rd1211, %rd18;
	add.s64 	%rd1218, %rd1217, %rd1216;
	xor.b64  	%rd1219, %rd1218, %rd1213;
	mov.b64	{%r2401, %r2402}, %rd1219;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1220, {%r2404, %r2403};
	add.s64 	%rd1221, %rd1220, %rd1214;
	xor.b64  	%rd1222, %rd1221, %rd1216;
	mov.b64	{%r686, %r687}, %rd1222;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1223, {%r681, %r685};
	add.s64 	%rd1224, %rd1176, %rd19;
	add.s64 	%rd1225, %rd1224, %rd1195;
	xor.b64  	%rd1226, %rd1225, %rd1164;
	mov.b64	{%r2405, %r2406}, %rd1226;
	mov.b64	%rd1227, {%r2406, %r2405};
	add.s64 	%rd1228, %rd1227, %rd1151;
	xor.b64  	%rd1229, %rd1228, %rd1195;
	mov.b64	{%r2407, %r2408}, %rd1229;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1230, {%r2410, %r2409};
	add.s64 	%rd1231, %rd1225, %rd20;
	add.s64 	%rd1232, %rd1231, %rd1230;
	xor.b64  	%rd1233, %rd1232, %rd1227;
	mov.b64	{%r2411, %r2412}, %rd1233;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1234, {%r2414, %r2413};
	add.s64 	%rd1235, %rd1234, %rd1228;
	xor.b64  	%rd1236, %rd1235, %rd1230;
	mov.b64	{%r694, %r695}, %rd1236;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1237, {%r689, %r693};
	add.s64 	%rd1238, %rd1153, %rd21;
	add.s64 	%rd1239, %rd1238, %rd1190;
	xor.b64  	%rd1240, %rd1239, %rd1178;
	mov.b64	{%r2415, %r2416}, %rd1240;
	mov.b64	%rd1241, {%r2416, %r2415};
	add.s64 	%rd1242, %rd1241, %rd1165;
	xor.b64  	%rd1243, %rd1242, %rd1153;
	mov.b64	{%r2417, %r2418}, %rd1243;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1244, {%r2420, %r2419};
	add.s64 	%rd1245, %rd1239, %rd22;
	add.s64 	%rd1246, %rd1245, %rd1244;
	xor.b64  	%rd1247, %rd1246, %rd1241;
	mov.b64	{%r2421, %r2422}, %rd1247;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1248, {%r2424, %r2423};
	add.s64 	%rd1249, %rd1248, %rd1242;
	xor.b64  	%rd1250, %rd1249, %rd1244;
	mov.b64	{%r702, %r703}, %rd1250;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1251, {%r697, %r701};
	add.s64 	%rd1252, %rd1204, %rd21;
	add.s64 	%rd1253, %rd1252, %rd1251;
	xor.b64  	%rd1254, %rd1253, %rd1220;
	mov.b64	{%r2425, %r2426}, %rd1254;
	mov.b64	%rd1255, {%r2426, %r2425};
	add.s64 	%rd1256, %rd1255, %rd1235;
	xor.b64  	%rd1257, %rd1256, %rd1251;
	mov.b64	{%r2427, %r2428}, %rd1257;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1258, {%r2430, %r2429};
	add.s64 	%rd1259, %rd1253, %rd17;
	add.s64 	%rd1260, %rd1259, %rd1258;
	xor.b64  	%rd1261, %rd1255, %rd1260;
	mov.b64	{%r2431, %r2432}, %rd1261;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1262, {%r2434, %r2433};
	add.s64 	%rd1263, %rd1256, %rd1262;
	xor.b64  	%rd1264, %rd1263, %rd1258;
	mov.b64	{%r710, %r711}, %rd1264;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1265, {%r705, %r709};
	add.s64 	%rd1266, %rd1209, %rd11;
	add.s64 	%rd1267, %rd1266, %rd1218;
	xor.b64  	%rd1268, %rd1234, %rd1267;
	mov.b64	{%r2435, %r2436}, %rd1268;
	mov.b64	%rd1269, {%r2436, %r2435};
	add.s64 	%rd1270, %rd1249, %rd1269;
	xor.b64  	%rd1271, %rd1270, %rd1209;
	mov.b64	{%r2437, %r2438}, %rd1271;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1272, {%r2440, %r2439};
	add.s64 	%rd1273, %rd1267, %rd15;
	add.s64 	%rd1274, %rd1273, %rd1272;
	xor.b64  	%rd1275, %rd1274, %rd1269;
	mov.b64	{%r2441, %r2442}, %rd1275;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1276, {%r2444, %r2443};
	add.s64 	%rd1277, %rd1276, %rd1270;
	xor.b64  	%rd1278, %rd1277, %rd1272;
	mov.b64	{%r718, %r719}, %rd1278;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1279, {%r713, %r717};
	add.s64 	%rd1280, %rd1223, %rd16;
	add.s64 	%rd1281, %rd1280, %rd1232;
	xor.b64  	%rd1282, %rd1248, %rd1281;
	mov.b64	{%r2445, %r2446}, %rd1282;
	mov.b64	%rd1283, {%r2446, %r2445};
	add.s64 	%rd1284, %rd1283, %rd1207;
	xor.b64  	%rd1285, %rd1284, %rd1223;
	mov.b64	{%r2447, %r2448}, %rd1285;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1286, {%r2450, %r2449};
	add.s64 	%rd1287, %rd1281, %rd22;
	add.s64 	%rd1288, %rd1287, %rd1286;
	xor.b64  	%rd1289, %rd1288, %rd1283;
	mov.b64	{%r2451, %r2452}, %rd1289;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1290, {%r2454, %r2453};
	add.s64 	%rd1291, %rd1290, %rd1284;
	xor.b64  	%rd1292, %rd1291, %rd1286;
	mov.b64	{%r726, %r727}, %rd1292;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1293, {%r721, %r725};
	add.s64 	%rd1294, %rd1237, %rd20;
	add.s64 	%rd1295, %rd1294, %rd1246;
	xor.b64  	%rd1296, %rd1295, %rd1206;
	mov.b64	{%r2455, %r2456}, %rd1296;
	mov.b64	%rd1297, {%r2456, %r2455};
	add.s64 	%rd1298, %rd1297, %rd1221;
	xor.b64  	%rd1299, %rd1298, %rd1237;
	mov.b64	{%r2457, %r2458}, %rd1299;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1300, {%r2460, %r2459};
	add.s64 	%rd1301, %rd1295, %rd13;
	add.s64 	%rd1302, %rd1301, %rd1300;
	xor.b64  	%rd1303, %rd1302, %rd1297;
	mov.b64	{%r2461, %r2462}, %rd1303;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1304, {%r2464, %r2463};
	add.s64 	%rd1305, %rd1304, %rd1298;
	xor.b64  	%rd1306, %rd1305, %rd1300;
	mov.b64	{%r734, %r735}, %rd1306;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1307, {%r729, %r733};
	add.s64 	%rd1308, %rd1260, %rd29;
	add.s64 	%rd1309, %rd1308, %rd1279;
	xor.b64  	%rd1310, %rd1304, %rd1309;
	mov.b64	{%r2465, %r2466}, %rd1310;
	mov.b64	%rd1311, {%r2466, %r2465};
	add.s64 	%rd1312, %rd1311, %rd1291;
	xor.b64  	%rd1313, %rd1312, %rd1279;
	mov.b64	{%r2467, %r2468}, %rd1313;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1314, {%r2470, %r2469};
	add.s64 	%rd1315, %rd1309, %rd19;
	add.s64 	%rd1316, %rd1315, %rd1314;
	xor.b64  	%rd1317, %rd1311, %rd1316;
	mov.b64	{%r2471, %r2472}, %rd1317;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1318, {%r2474, %r2473};
	add.s64 	%rd1319, %rd1318, %rd1312;
	xor.b64  	%rd1320, %rd1319, %rd1314;
	mov.b64	{%r742, %r743}, %rd1320;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1321, {%r737, %r741};
	add.s64 	%rd1322, %rd1274, %rd1;
	add.s64 	%rd1323, %rd1322, %rd1293;
	xor.b64  	%rd1324, %rd1323, %rd1262;
	mov.b64	{%r2475, %r2476}, %rd1324;
	mov.b64	%rd1325, {%r2476, %r2475};
	add.s64 	%rd1326, %rd1325, %rd1305;
	xor.b64  	%rd1327, %rd1326, %rd1293;
	mov.b64	{%r2477, %r2478}, %rd1327;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1328, {%r2480, %r2479};
	add.s64 	%rd1329, %rd1323, %rd9;
	add.s64 	%rd1330, %rd1329, %rd1328;
	xor.b64  	%rd1331, %rd1330, %rd1325;
	mov.b64	{%r2481, %r2482}, %rd1331;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1332, {%r2484, %r2483};
	add.s64 	%rd1333, %rd1332, %rd1326;
	xor.b64  	%rd1334, %rd1333, %rd1328;
	mov.b64	{%r750, %r751}, %rd1334;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1335, {%r745, %r749};
	add.s64 	%rd1336, %rd1288, %rd18;
	add.s64 	%rd1337, %rd1336, %rd1307;
	xor.b64  	%rd1338, %rd1337, %rd1276;
	mov.b64	{%r2485, %r2486}, %rd1338;
	mov.b64	%rd1339, {%r2486, %r2485};
	add.s64 	%rd1340, %rd1339, %rd1263;
	xor.b64  	%rd1341, %rd1340, %rd1307;
	mov.b64	{%r2487, %r2488}, %rd1341;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1342, {%r2490, %r2489};
	add.s64 	%rd1343, %rd1337, %rd14;
	add.s64 	%rd1344, %rd1343, %rd1342;
	xor.b64  	%rd1345, %rd1344, %rd1339;
	mov.b64	{%r2491, %r2492}, %rd1345;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1346, {%r2494, %r2493};
	add.s64 	%rd1347, %rd1346, %rd1340;
	xor.b64  	%rd1348, %rd1347, %rd1342;
	mov.b64	{%r758, %r759}, %rd1348;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1349, {%r753, %r757};
	add.s64 	%rd1350, %rd1265, %rd12;
	add.s64 	%rd1351, %rd1350, %rd1302;
	xor.b64  	%rd1352, %rd1351, %rd1290;
	mov.b64	{%r2495, %r2496}, %rd1352;
	mov.b64	%rd1353, {%r2496, %r2495};
	add.s64 	%rd1354, %rd1353, %rd1277;
	xor.b64  	%rd1355, %rd1354, %rd1265;
	mov.b64	{%r2497, %r2498}, %rd1355;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1356, {%r2500, %r2499};
	add.s64 	%rd1357, %rd1351, %rd10;
	add.s64 	%rd1358, %rd1357, %rd1356;
	xor.b64  	%rd1359, %rd1358, %rd1353;
	mov.b64	{%r2501, %r2502}, %rd1359;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1360, {%r2504, %r2503};
	add.s64 	%rd1361, %rd1360, %rd1354;
	xor.b64  	%rd1362, %rd1361, %rd1356;
	mov.b64	{%r766, %r767}, %rd1362;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1363, {%r761, %r765};
	xor.b64  	%rd1364, %rd1316, %rd1347;
	xor.b64  	%rd1365, %rd1364, 7640891576939301192;
	xor.b64  	%rd1366, %rd1330, %rd1361;
	xor.b64  	%rd1367, %rd1366, -4942790177534073029;
	xor.b64  	%rd1368, %rd1319, %rd1344;
	xor.b64  	%rd1369, %rd1368, 4354685564936845355;
	xor.b64  	%rd1370, %rd1333, %rd1358;
	xor.b64  	%rd1371, %rd1370, -6534734903238641935;
	xor.b64  	%rd1372, %rd1332, %rd1363;
	xor.b64  	%rd1373, %rd1372, 5840696475078001361;
	xor.b64  	%rd1374, %rd1321, %rd1346;
	xor.b64  	%rd1375, %rd1374, -7276294671716946913;
	xor.b64  	%rd1376, %rd1335, %rd1360;
	xor.b64  	%rd1377, %rd1376, 2270897969802886507;
	xor.b64  	%rd1378, %rd1318, %rd1349;
	xor.b64  	%rd1379, %rd1378, 6620516959819538809;
	ld.global.u64 	%rd1380, [%rd5+144];
	ld.global.u64 	%rd1381, [%rd5+152];
	ld.global.u64 	%rd1382, [%rd5+160];
	ld.global.u64 	%rd1383, [%rd5+168];
	ld.global.u64 	%rd1384, [%rd5+176];
	ld.global.u64 	%rd1385, [%rd5+184];
	ld.global.u64 	%rd1386, [%rd5+192];
	ld.global.u64 	%rd1387, [%rd5+200];
	ld.global.u64 	%rd1388, [%rd5+208];
	ld.global.u64 	%rd1389, [%rd5+216];
	ld.global.u64 	%rd1390, [%rd5+224];
	ld.global.u64 	%rd1391, [%rd5+232];
	ld.global.u64 	%rd1392, [%rd5+240];
	ld.global.u64 	%rd1393, [%rd5+248];
	ld.global.u64 	%rd1394, [%rd5+128];
	add.s64 	%rd1395, %rd1394, %rd1365;
	add.s64 	%rd1396, %rd1395, %rd1373;
	xor.b64  	%rd1397, %rd1396, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1397;
	mov.b64	%rd1398, {%r2506, %r2505};
	add.s64 	%rd1399, %rd1398, 7640891576956012808;
	xor.b64  	%rd1400, %rd1399, %rd1373;
	mov.b64	{%r2507, %r2508}, %rd1400;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1401, {%r2510, %r2509};
	ld.global.u64 	%rd1402, [%rd5+136];
	add.s64 	%rd1403, %rd1396, %rd1402;
	add.s64 	%rd1404, %rd1403, %rd1401;
	xor.b64  	%rd1405, %rd1404, %rd1398;
	mov.b64	{%r2511, %r2512}, %rd1405;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1406, {%r2514, %r2513};
	add.s64 	%rd1407, %rd1406, %rd1399;
	xor.b64  	%rd1408, %rd1407, %rd1401;
	mov.b64	{%r774, %r775}, %rd1408;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1409, {%r769, %r773};
	add.s64 	%rd1410, %rd1367, %rd1375;
	add.s64 	%rd1411, %rd1410, %rd1380;
	xor.b64  	%rd1412, %rd1411, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1412;
	mov.b64	%rd1413, {%r2516, %r2515};
	add.s64 	%rd1414, %rd1413, -4942790177534073029;
	xor.b64  	%rd1415, %rd1414, %rd1375;
	mov.b64	{%r2517, %r2518}, %rd1415;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1416, {%r2520, %r2519};
	add.s64 	%rd1417, %rd1411, %rd1381;
	add.s64 	%rd1418, %rd1417, %rd1416;
	xor.b64  	%rd1419, %rd1418, %rd1413;
	mov.b64	{%r2521, %r2522}, %rd1419;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1420, {%r2524, %r2523};
	add.s64 	%rd1421, %rd1420, %rd1414;
	xor.b64  	%rd1422, %rd1421, %rd1416;
	mov.b64	{%r782, %r783}, %rd1422;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1423, {%r777, %r781};
	add.s64 	%rd1424, %rd1377, %rd1369;
	add.s64 	%rd1425, %rd1424, %rd1382;
	xor.b64  	%rd1426, %rd1425, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1426;
	mov.b64	%rd1427, {%r2526, %r2525};
	add.s64 	%rd1428, %rd1427, 4354685564936845355;
	xor.b64  	%rd1429, %rd1428, %rd1377;
	mov.b64	{%r2527, %r2528}, %rd1429;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1430, {%r2530, %r2529};
	add.s64 	%rd1431, %rd1383, %rd1425;
	add.s64 	%rd1432, %rd1431, %rd1430;
	xor.b64  	%rd1433, %rd1432, %rd1427;
	mov.b64	{%r2531, %r2532}, %rd1433;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1434, {%r2534, %r2533};
	add.s64 	%rd1435, %rd1434, %rd1428;
	xor.b64  	%rd1436, %rd1435, %rd1430;
	mov.b64	{%r790, %r791}, %rd1436;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1437, {%r785, %r789};
	add.s64 	%rd1438, %rd1371, %rd1379;
	add.s64 	%rd1439, %rd1438, %rd1384;
	xor.b64  	%rd1440, %rd1439, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1440;
	mov.b64	%rd1441, {%r2536, %r2535};
	add.s64 	%rd1442, %rd1441, -6534734903238641935;
	xor.b64  	%rd1443, %rd1442, %rd1379;
	mov.b64	{%r2537, %r2538}, %rd1443;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1444, {%r2540, %r2539};
	add.s64 	%rd1445, %rd1385, %rd1439;
	add.s64 	%rd1446, %rd1445, %rd1444;
	xor.b64  	%rd1447, %rd1446, %rd1441;
	mov.b64	{%r2541, %r2542}, %rd1447;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1448, {%r2544, %r2543};
	add.s64 	%rd1449, %rd1448, %rd1442;
	xor.b64  	%rd1450, %rd1449, %rd1444;
	mov.b64	{%r798, %r799}, %rd1450;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1451, {%r793, %r797};
	add.s64 	%rd1452, %rd1404, %rd1386;
	add.s64 	%rd1453, %rd1452, %rd1423;
	xor.b64  	%rd1454, %rd1448, %rd1453;
	mov.b64	{%r2545, %r2546}, %rd1454;
	mov.b64	%rd1455, {%r2546, %r2545};
	add.s64 	%rd1456, %rd1455, %rd1435;
	xor.b64  	%rd1457, %rd1456, %rd1423;
	mov.b64	{%r2547, %r2548}, %rd1457;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1458, {%r2550, %r2549};
	add.s64 	%rd1459, %rd1453, %rd1387;
	add.s64 	%rd1460, %rd1459, %rd1458;
	xor.b64  	%rd1461, %rd1455, %rd1460;
	mov.b64	{%r2551, %r2552}, %rd1461;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1462, {%r2554, %r2553};
	add.s64 	%rd1463, %rd1462, %rd1456;
	xor.b64  	%rd1464, %rd1463, %rd1458;
	mov.b64	{%r806, %r807}, %rd1464;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1465, {%r801, %r805};
	add.s64 	%rd1466, %rd1418, %rd1388;
	add.s64 	%rd1467, %rd1466, %rd1437;
	xor.b64  	%rd1468, %rd1467, %rd1406;
	mov.b64	{%r2555, %r2556}, %rd1468;
	mov.b64	%rd1469, {%r2556, %r2555};
	add.s64 	%rd1470, %rd1469, %rd1449;
	xor.b64  	%rd1471, %rd1470, %rd1437;
	mov.b64	{%r2557, %r2558}, %rd1471;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1472, {%r2560, %r2559};
	add.s64 	%rd1473, %rd1467, %rd1389;
	add.s64 	%rd1474, %rd1473, %rd1472;
	xor.b64  	%rd1475, %rd1474, %rd1469;
	mov.b64	{%r2561, %r2562}, %rd1475;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1476, {%r2564, %r2563};
	add.s64 	%rd1477, %rd1476, %rd1470;
	xor.b64  	%rd1478, %rd1477, %rd1472;
	mov.b64	{%r814, %r815}, %rd1478;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1479, {%r809, %r813};
	add.s64 	%rd1480, %rd1432, %rd1390;
	add.s64 	%rd1481, %rd1480, %rd1451;
	xor.b64  	%rd1482, %rd1481, %rd1420;
	mov.b64	{%r2565, %r2566}, %rd1482;
	mov.b64	%rd1483, {%r2566, %r2565};
	add.s64 	%rd1484, %rd1483, %rd1407;
	xor.b64  	%rd1485, %rd1484, %rd1451;
	mov.b64	{%r2567, %r2568}, %rd1485;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1486, {%r2570, %r2569};
	add.s64 	%rd1487, %rd1481, %rd1391;
	add.s64 	%rd1488, %rd1487, %rd1486;
	xor.b64  	%rd1489, %rd1488, %rd1483;
	mov.b64	{%r2571, %r2572}, %rd1489;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1490, {%r2574, %r2573};
	add.s64 	%rd1491, %rd1490, %rd1484;
	xor.b64  	%rd1492, %rd1491, %rd1486;
	mov.b64	{%r822, %r823}, %rd1492;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1493, {%r817, %r821};
	add.s64 	%rd1494, %rd1409, %rd1392;
	add.s64 	%rd1495, %rd1494, %rd1446;
	xor.b64  	%rd1496, %rd1495, %rd1434;
	mov.b64	{%r2575, %r2576}, %rd1496;
	mov.b64	%rd1497, {%r2576, %r2575};
	add.s64 	%rd1498, %rd1497, %rd1421;
	xor.b64  	%rd1499, %rd1498, %rd1409;
	mov.b64	{%r2577, %r2578}, %rd1499;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1500, {%r2580, %r2579};
	add.s64 	%rd1501, %rd1495, %rd1393;
	add.s64 	%rd1502, %rd1501, %rd1500;
	xor.b64  	%rd1503, %rd1502, %rd1497;
	mov.b64	{%r2581, %r2582}, %rd1503;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1504, {%r2584, %r2583};
	add.s64 	%rd1505, %rd1504, %rd1498;
	xor.b64  	%rd1506, %rd1505, %rd1500;
	mov.b64	{%r830, %r831}, %rd1506;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1507, {%r825, %r829};
	add.s64 	%rd1508, %rd1460, %rd1392;
	add.s64 	%rd1509, %rd1508, %rd1507;
	xor.b64  	%rd1510, %rd1509, %rd1476;
	mov.b64	{%r2585, %r2586}, %rd1510;
	mov.b64	%rd1511, {%r2586, %r2585};
	add.s64 	%rd1512, %rd1511, %rd1491;
	xor.b64  	%rd1513, %rd1512, %rd1507;
	mov.b64	{%r2587, %r2588}, %rd1513;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1514, {%r2590, %r2589};
	add.s64 	%rd1515, %rd1509, %rd1388;
	add.s64 	%rd1516, %rd1515, %rd1514;
	xor.b64  	%rd1517, %rd1511, %rd1516;
	mov.b64	{%r2591, %r2592}, %rd1517;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1518, {%r2594, %r2593};
	add.s64 	%rd1519, %rd1512, %rd1518;
	xor.b64  	%rd1520, %rd1519, %rd1514;
	mov.b64	{%r838, %r839}, %rd1520;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1521, {%r833, %r837};
	add.s64 	%rd1522, %rd1465, %rd1382;
	add.s64 	%rd1523, %rd1522, %rd1474;
	xor.b64  	%rd1524, %rd1490, %rd1523;
	mov.b64	{%r2595, %r2596}, %rd1524;
	mov.b64	%rd1525, {%r2596, %r2595};
	add.s64 	%rd1526, %rd1505, %rd1525;
	xor.b64  	%rd1527, %rd1526, %rd1465;
	mov.b64	{%r2597, %r2598}, %rd1527;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1528, {%r2600, %r2599};
	add.s64 	%rd1529, %rd1523, %rd1386;
	add.s64 	%rd1530, %rd1529, %rd1528;
	xor.b64  	%rd1531, %rd1530, %rd1525;
	mov.b64	{%r2601, %r2602}, %rd1531;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1532, {%r2604, %r2603};
	add.s64 	%rd1533, %rd1532, %rd1526;
	xor.b64  	%rd1534, %rd1533, %rd1528;
	mov.b64	{%r846, %r847}, %rd1534;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1535, {%r841, %r845};
	add.s64 	%rd1536, %rd1479, %rd1387;
	add.s64 	%rd1537, %rd1536, %rd1488;
	xor.b64  	%rd1538, %rd1504, %rd1537;
	mov.b64	{%r2605, %r2606}, %rd1538;
	mov.b64	%rd1539, {%r2606, %r2605};
	add.s64 	%rd1540, %rd1539, %rd1463;
	xor.b64  	%rd1541, %rd1540, %rd1479;
	mov.b64	{%r2607, %r2608}, %rd1541;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1542, {%r2610, %r2609};
	add.s64 	%rd1543, %rd1537, %rd1393;
	add.s64 	%rd1544, %rd1543, %rd1542;
	xor.b64  	%rd1545, %rd1544, %rd1539;
	mov.b64	{%r2611, %r2612}, %rd1545;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1546, {%r2614, %r2613};
	add.s64 	%rd1547, %rd1546, %rd1540;
	xor.b64  	%rd1548, %rd1547, %rd1542;
	mov.b64	{%r854, %r855}, %rd1548;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1549, {%r849, %r853};
	add.s64 	%rd1550, %rd1493, %rd1391;
	add.s64 	%rd1551, %rd1550, %rd1502;
	xor.b64  	%rd1552, %rd1551, %rd1462;
	mov.b64	{%r2615, %r2616}, %rd1552;
	mov.b64	%rd1553, {%r2616, %r2615};
	add.s64 	%rd1554, %rd1553, %rd1477;
	xor.b64  	%rd1555, %rd1554, %rd1493;
	mov.b64	{%r2617, %r2618}, %rd1555;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1556, {%r2620, %r2619};
	add.s64 	%rd1557, %rd1551, %rd1384;
	add.s64 	%rd1558, %rd1557, %rd1556;
	xor.b64  	%rd1559, %rd1558, %rd1553;
	mov.b64	{%r2621, %r2622}, %rd1559;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1560, {%r2624, %r2623};
	add.s64 	%rd1561, %rd1560, %rd1554;
	xor.b64  	%rd1562, %rd1561, %rd1556;
	mov.b64	{%r862, %r863}, %rd1562;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1563, {%r857, %r861};
	add.s64 	%rd1564, %rd1516, %rd1402;
	add.s64 	%rd1565, %rd1564, %rd1535;
	xor.b64  	%rd1566, %rd1560, %rd1565;
	mov.b64	{%r2625, %r2626}, %rd1566;
	mov.b64	%rd1567, {%r2626, %r2625};
	add.s64 	%rd1568, %rd1567, %rd1547;
	xor.b64  	%rd1569, %rd1568, %rd1535;
	mov.b64	{%r2627, %r2628}, %rd1569;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1570, {%r2630, %r2629};
	add.s64 	%rd1571, %rd1565, %rd1390;
	add.s64 	%rd1572, %rd1571, %rd1570;
	xor.b64  	%rd1573, %rd1567, %rd1572;
	mov.b64	{%r2631, %r2632}, %rd1573;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1574, {%r2634, %r2633};
	add.s64 	%rd1575, %rd1574, %rd1568;
	xor.b64  	%rd1576, %rd1575, %rd1570;
	mov.b64	{%r870, %r871}, %rd1576;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1577, {%r865, %r869};
	add.s64 	%rd1578, %rd1530, %rd1394;
	add.s64 	%rd1579, %rd1578, %rd1549;
	xor.b64  	%rd1580, %rd1579, %rd1518;
	mov.b64	{%r2635, %r2636}, %rd1580;
	mov.b64	%rd1581, {%r2636, %r2635};
	add.s64 	%rd1582, %rd1581, %rd1561;
	xor.b64  	%rd1583, %rd1582, %rd1549;
	mov.b64	{%r2637, %r2638}, %rd1583;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1584, {%r2640, %r2639};
	add.s64 	%rd1585, %rd1579, %rd1380;
	add.s64 	%rd1586, %rd1585, %rd1584;
	xor.b64  	%rd1587, %rd1586, %rd1581;
	mov.b64	{%r2641, %r2642}, %rd1587;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1588, {%r2644, %r2643};
	add.s64 	%rd1589, %rd1588, %rd1582;
	xor.b64  	%rd1590, %rd1589, %rd1584;
	mov.b64	{%r878, %r879}, %rd1590;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1591, {%r873, %r877};
	add.s64 	%rd1592, %rd1544, %rd1389;
	add.s64 	%rd1593, %rd1592, %rd1563;
	xor.b64  	%rd1594, %rd1593, %rd1532;
	mov.b64	{%r2645, %r2646}, %rd1594;
	mov.b64	%rd1595, {%r2646, %r2645};
	add.s64 	%rd1596, %rd1595, %rd1519;
	xor.b64  	%rd1597, %rd1596, %rd1563;
	mov.b64	{%r2647, %r2648}, %rd1597;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1598, {%r2650, %r2649};
	add.s64 	%rd1599, %rd1593, %rd1385;
	add.s64 	%rd1600, %rd1599, %rd1598;
	xor.b64  	%rd1601, %rd1600, %rd1595;
	mov.b64	{%r2651, %r2652}, %rd1601;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1602, {%r2654, %r2653};
	add.s64 	%rd1603, %rd1602, %rd1596;
	xor.b64  	%rd1604, %rd1603, %rd1598;
	mov.b64	{%r886, %r887}, %rd1604;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1605, {%r881, %r885};
	add.s64 	%rd1606, %rd1521, %rd1383;
	add.s64 	%rd1607, %rd1606, %rd1558;
	xor.b64  	%rd1608, %rd1607, %rd1546;
	mov.b64	{%r2655, %r2656}, %rd1608;
	mov.b64	%rd1609, {%r2656, %r2655};
	add.s64 	%rd1610, %rd1609, %rd1533;
	xor.b64  	%rd1611, %rd1610, %rd1521;
	mov.b64	{%r2657, %r2658}, %rd1611;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1612, {%r2660, %r2659};
	add.s64 	%rd1613, %rd1607, %rd1381;
	add.s64 	%rd1614, %rd1613, %rd1612;
	xor.b64  	%rd1615, %rd1614, %rd1609;
	mov.b64	{%r2661, %r2662}, %rd1615;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1616, {%r2664, %r2663};
	add.s64 	%rd1617, %rd1616, %rd1610;
	xor.b64  	%rd1618, %rd1617, %rd1612;
	mov.b64	{%r894, %r895}, %rd1618;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1619, {%r889, %r893};
	add.s64 	%rd1620, %rd1572, %rd1389;
	add.s64 	%rd1621, %rd1620, %rd1619;
	xor.b64  	%rd1622, %rd1621, %rd1588;
	mov.b64	{%r2665, %r2666}, %rd1622;
	mov.b64	%rd1623, {%r2666, %r2665};
	add.s64 	%rd1624, %rd1623, %rd1603;
	xor.b64  	%rd1625, %rd1624, %rd1619;
	mov.b64	{%r2667, %r2668}, %rd1625;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1626, {%r2670, %r2669};
	add.s64 	%rd1627, %rd1621, %rd1386;
	add.s64 	%rd1628, %rd1627, %rd1626;
	xor.b64  	%rd1629, %rd1623, %rd1628;
	mov.b64	{%r2671, %r2672}, %rd1629;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1630, {%r2674, %r2673};
	add.s64 	%rd1631, %rd1624, %rd1630;
	xor.b64  	%rd1632, %rd1631, %rd1626;
	mov.b64	{%r902, %r903}, %rd1632;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1633, {%r897, %r901};
	add.s64 	%rd1634, %rd1577, %rd1390;
	add.s64 	%rd1635, %rd1634, %rd1586;
	xor.b64  	%rd1636, %rd1602, %rd1635;
	mov.b64	{%r2675, %r2676}, %rd1636;
	mov.b64	%rd1637, {%r2676, %r2675};
	add.s64 	%rd1638, %rd1617, %rd1637;
	xor.b64  	%rd1639, %rd1638, %rd1577;
	mov.b64	{%r2677, %r2678}, %rd1639;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1640, {%r2680, %r2679};
	add.s64 	%rd1641, %rd1635, %rd1394;
	add.s64 	%rd1642, %rd1641, %rd1640;
	xor.b64  	%rd1643, %rd1642, %rd1637;
	mov.b64	{%r2681, %r2682}, %rd1643;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1644, {%r2684, %r2683};
	add.s64 	%rd1645, %rd1644, %rd1638;
	xor.b64  	%rd1646, %rd1645, %rd1640;
	mov.b64	{%r910, %r911}, %rd1646;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1647, {%r905, %r909};
	add.s64 	%rd1648, %rd1591, %rd1383;
	add.s64 	%rd1649, %rd1648, %rd1600;
	xor.b64  	%rd1650, %rd1616, %rd1649;
	mov.b64	{%r2685, %r2686}, %rd1650;
	mov.b64	%rd1651, {%r2686, %r2685};
	add.s64 	%rd1652, %rd1651, %rd1575;
	xor.b64  	%rd1653, %rd1652, %rd1591;
	mov.b64	{%r2687, %r2688}, %rd1653;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1654, {%r2690, %r2689};
	add.s64 	%rd1655, %rd1649, %rd1380;
	add.s64 	%rd1656, %rd1655, %rd1654;
	xor.b64  	%rd1657, %rd1656, %rd1651;
	mov.b64	{%r2691, %r2692}, %rd1657;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1658, {%r2694, %r2693};
	add.s64 	%rd1659, %rd1658, %rd1652;
	xor.b64  	%rd1660, %rd1659, %rd1654;
	mov.b64	{%r918, %r919}, %rd1660;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1661, {%r913, %r917};
	add.s64 	%rd1662, %rd1605, %rd1393;
	add.s64 	%rd1663, %rd1662, %rd1614;
	xor.b64  	%rd1664, %rd1663, %rd1574;
	mov.b64	{%r2695, %r2696}, %rd1664;
	mov.b64	%rd1665, {%r2696, %r2695};
	add.s64 	%rd1666, %rd1665, %rd1589;
	xor.b64  	%rd1667, %rd1666, %rd1605;
	mov.b64	{%r2697, %r2698}, %rd1667;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1668, {%r2700, %r2699};
	add.s64 	%rd1669, %rd1663, %rd1391;
	add.s64 	%rd1670, %rd1669, %rd1668;
	xor.b64  	%rd1671, %rd1670, %rd1665;
	mov.b64	{%r2701, %r2702}, %rd1671;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1672, {%r2704, %r2703};
	add.s64 	%rd1673, %rd1672, %rd1666;
	xor.b64  	%rd1674, %rd1673, %rd1668;
	mov.b64	{%r926, %r927}, %rd1674;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1675, {%r921, %r925};
	add.s64 	%rd1676, %rd1628, %rd1388;
	add.s64 	%rd1677, %rd1676, %rd1647;
	xor.b64  	%rd1678, %rd1672, %rd1677;
	mov.b64	{%r2705, %r2706}, %rd1678;
	mov.b64	%rd1679, {%r2706, %r2705};
	add.s64 	%rd1680, %rd1679, %rd1659;
	xor.b64  	%rd1681, %rd1680, %rd1647;
	mov.b64	{%r2707, %r2708}, %rd1681;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1682, {%r2710, %r2709};
	add.s64 	%rd1683, %rd1677, %rd1392;
	add.s64 	%rd1684, %rd1683, %rd1682;
	xor.b64  	%rd1685, %rd1679, %rd1684;
	mov.b64	{%r2711, %r2712}, %rd1685;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1686, {%r2714, %r2713};
	add.s64 	%rd1687, %rd1686, %rd1680;
	xor.b64  	%rd1688, %rd1687, %rd1682;
	mov.b64	{%r934, %r935}, %rd1688;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1689, {%r929, %r933};
	add.s64 	%rd1690, %rd1642, %rd1381;
	add.s64 	%rd1691, %rd1690, %rd1661;
	xor.b64  	%rd1692, %rd1691, %rd1630;
	mov.b64	{%r2715, %r2716}, %rd1692;
	mov.b64	%rd1693, {%r2716, %r2715};
	add.s64 	%rd1694, %rd1693, %rd1673;
	xor.b64  	%rd1695, %rd1694, %rd1661;
	mov.b64	{%r2717, %r2718}, %rd1695;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1696, {%r2720, %r2719};
	add.s64 	%rd1697, %rd1691, %rd1384;
	add.s64 	%rd1698, %rd1697, %rd1696;
	xor.b64  	%rd1699, %rd1698, %rd1693;
	mov.b64	{%r2721, %r2722}, %rd1699;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1700, {%r2724, %r2723};
	add.s64 	%rd1701, %rd1700, %rd1694;
	xor.b64  	%rd1702, %rd1701, %rd1696;
	mov.b64	{%r942, %r943}, %rd1702;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1703, {%r937, %r941};
	add.s64 	%rd1704, %rd1656, %rd1385;
	add.s64 	%rd1705, %rd1704, %rd1675;
	xor.b64  	%rd1706, %rd1705, %rd1644;
	mov.b64	{%r2725, %r2726}, %rd1706;
	mov.b64	%rd1707, {%r2726, %r2725};
	add.s64 	%rd1708, %rd1707, %rd1631;
	xor.b64  	%rd1709, %rd1708, %rd1675;
	mov.b64	{%r2727, %r2728}, %rd1709;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1710, {%r2730, %r2729};
	add.s64 	%rd1711, %rd1705, %rd1402;
	add.s64 	%rd1712, %rd1711, %rd1710;
	xor.b64  	%rd1713, %rd1712, %rd1707;
	mov.b64	{%r2731, %r2732}, %rd1713;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1714, {%r2734, %r2733};
	add.s64 	%rd1715, %rd1714, %rd1708;
	xor.b64  	%rd1716, %rd1715, %rd1710;
	mov.b64	{%r950, %r951}, %rd1716;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1717, {%r945, %r949};
	add.s64 	%rd1718, %rd1633, %rd1387;
	add.s64 	%rd1719, %rd1718, %rd1670;
	xor.b64  	%rd1720, %rd1719, %rd1658;
	mov.b64	{%r2735, %r2736}, %rd1720;
	mov.b64	%rd1721, {%r2736, %r2735};
	add.s64 	%rd1722, %rd1721, %rd1645;
	xor.b64  	%rd1723, %rd1722, %rd1633;
	mov.b64	{%r2737, %r2738}, %rd1723;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1724, {%r2740, %r2739};
	add.s64 	%rd1725, %rd1719, %rd1382;
	add.s64 	%rd1726, %rd1725, %rd1724;
	xor.b64  	%rd1727, %rd1726, %rd1721;
	mov.b64	{%r2741, %r2742}, %rd1727;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1728, {%r2744, %r2743};
	add.s64 	%rd1729, %rd1728, %rd1722;
	xor.b64  	%rd1730, %rd1729, %rd1724;
	mov.b64	{%r958, %r959}, %rd1730;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1731, {%r953, %r957};
	add.s64 	%rd1732, %rd1684, %rd1385;
	add.s64 	%rd1733, %rd1732, %rd1731;
	xor.b64  	%rd1734, %rd1733, %rd1700;
	mov.b64	{%r2745, %r2746}, %rd1734;
	mov.b64	%rd1735, {%r2746, %r2745};
	add.s64 	%rd1736, %rd1735, %rd1715;
	xor.b64  	%rd1737, %rd1736, %rd1731;
	mov.b64	{%r2747, %r2748}, %rd1737;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1738, {%r2750, %r2749};
	add.s64 	%rd1739, %rd1733, %rd1387;
	add.s64 	%rd1740, %rd1739, %rd1738;
	xor.b64  	%rd1741, %rd1735, %rd1740;
	mov.b64	{%r2751, %r2752}, %rd1741;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1742, {%r2754, %r2753};
	add.s64 	%rd1743, %rd1736, %rd1742;
	xor.b64  	%rd1744, %rd1743, %rd1738;
	mov.b64	{%r966, %r967}, %rd1744;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1745, {%r961, %r965};
	add.s64 	%rd1746, %rd1689, %rd1381;
	add.s64 	%rd1747, %rd1746, %rd1698;
	xor.b64  	%rd1748, %rd1714, %rd1747;
	mov.b64	{%r2755, %r2756}, %rd1748;
	mov.b64	%rd1749, {%r2756, %r2755};
	add.s64 	%rd1750, %rd1729, %rd1749;
	xor.b64  	%rd1751, %rd1750, %rd1689;
	mov.b64	{%r2757, %r2758}, %rd1751;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1752, {%r2760, %r2759};
	add.s64 	%rd1753, %rd1747, %rd1402;
	add.s64 	%rd1754, %rd1753, %rd1752;
	xor.b64  	%rd1755, %rd1754, %rd1749;
	mov.b64	{%r2761, %r2762}, %rd1755;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1756, {%r2764, %r2763};
	add.s64 	%rd1757, %rd1756, %rd1750;
	xor.b64  	%rd1758, %rd1757, %rd1752;
	mov.b64	{%r974, %r975}, %rd1758;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1759, {%r969, %r973};
	add.s64 	%rd1760, %rd1703, %rd1391;
	add.s64 	%rd1761, %rd1760, %rd1712;
	xor.b64  	%rd1762, %rd1728, %rd1761;
	mov.b64	{%r2765, %r2766}, %rd1762;
	mov.b64	%rd1763, {%r2766, %r2765};
	add.s64 	%rd1764, %rd1763, %rd1687;
	xor.b64  	%rd1765, %rd1764, %rd1703;
	mov.b64	{%r2767, %r2768}, %rd1765;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1766, {%r2770, %r2769};
	add.s64 	%rd1767, %rd1761, %rd1390;
	add.s64 	%rd1768, %rd1767, %rd1766;
	xor.b64  	%rd1769, %rd1768, %rd1763;
	mov.b64	{%r2771, %r2772}, %rd1769;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1770, {%r2774, %r2773};
	add.s64 	%rd1771, %rd1770, %rd1764;
	xor.b64  	%rd1772, %rd1771, %rd1766;
	mov.b64	{%r982, %r983}, %rd1772;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1773, {%r977, %r981};
	add.s64 	%rd1774, %rd1717, %rd1389;
	add.s64 	%rd1775, %rd1774, %rd1726;
	xor.b64  	%rd1776, %rd1775, %rd1686;
	mov.b64	{%r2775, %r2776}, %rd1776;
	mov.b64	%rd1777, {%r2776, %r2775};
	add.s64 	%rd1778, %rd1777, %rd1701;
	xor.b64  	%rd1779, %rd1778, %rd1717;
	mov.b64	{%r2777, %r2778}, %rd1779;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1780, {%r2780, %r2779};
	add.s64 	%rd1781, %rd1775, %rd1392;
	add.s64 	%rd1782, %rd1781, %rd1780;
	xor.b64  	%rd1783, %rd1782, %rd1777;
	mov.b64	{%r2781, %r2782}, %rd1783;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1784, {%r2784, %r2783};
	add.s64 	%rd1785, %rd1784, %rd1778;
	xor.b64  	%rd1786, %rd1785, %rd1780;
	mov.b64	{%r990, %r991}, %rd1786;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1787, {%r985, %r989};
	add.s64 	%rd1788, %rd1740, %rd1380;
	add.s64 	%rd1789, %rd1788, %rd1759;
	xor.b64  	%rd1790, %rd1784, %rd1789;
	mov.b64	{%r2785, %r2786}, %rd1790;
	mov.b64	%rd1791, {%r2786, %r2785};
	add.s64 	%rd1792, %rd1791, %rd1771;
	xor.b64  	%rd1793, %rd1792, %rd1759;
	mov.b64	{%r2787, %r2788}, %rd1793;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1794, {%r2790, %r2789};
	add.s64 	%rd1795, %rd1789, %rd1384;
	add.s64 	%rd1796, %rd1795, %rd1794;
	xor.b64  	%rd1797, %rd1791, %rd1796;
	mov.b64	{%r2791, %r2792}, %rd1797;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1798, {%r2794, %r2793};
	add.s64 	%rd1799, %rd1798, %rd1792;
	xor.b64  	%rd1800, %rd1799, %rd1794;
	mov.b64	{%r998, %r999}, %rd1800;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1801, {%r993, %r997};
	add.s64 	%rd1802, %rd1754, %rd1383;
	add.s64 	%rd1803, %rd1802, %rd1773;
	xor.b64  	%rd1804, %rd1803, %rd1742;
	mov.b64	{%r2795, %r2796}, %rd1804;
	mov.b64	%rd1805, {%r2796, %r2795};
	add.s64 	%rd1806, %rd1805, %rd1785;
	xor.b64  	%rd1807, %rd1806, %rd1773;
	mov.b64	{%r2797, %r2798}, %rd1807;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1808, {%r2800, %r2799};
	add.s64 	%rd1809, %rd1803, %rd1388;
	add.s64 	%rd1810, %rd1809, %rd1808;
	xor.b64  	%rd1811, %rd1810, %rd1805;
	mov.b64	{%r2801, %r2802}, %rd1811;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1812, {%r2804, %r2803};
	add.s64 	%rd1813, %rd1812, %rd1806;
	xor.b64  	%rd1814, %rd1813, %rd1808;
	mov.b64	{%r1006, %r1007}, %rd1814;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1815, {%r1001, %r1005};
	add.s64 	%rd1816, %rd1768, %rd1382;
	add.s64 	%rd1817, %rd1816, %rd1787;
	xor.b64  	%rd1818, %rd1817, %rd1756;
	mov.b64	{%r2805, %r2806}, %rd1818;
	mov.b64	%rd1819, {%r2806, %r2805};
	add.s64 	%rd1820, %rd1819, %rd1743;
	xor.b64  	%rd1821, %rd1820, %rd1787;
	mov.b64	{%r2807, %r2808}, %rd1821;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1822, {%r2810, %r2809};
	add.s64 	%rd1823, %rd1817, %rd1394;
	add.s64 	%rd1824, %rd1823, %rd1822;
	xor.b64  	%rd1825, %rd1824, %rd1819;
	mov.b64	{%r2811, %r2812}, %rd1825;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1826, {%r2814, %r2813};
	add.s64 	%rd1827, %rd1826, %rd1820;
	xor.b64  	%rd1828, %rd1827, %rd1822;
	mov.b64	{%r1014, %r1015}, %rd1828;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1829, {%r1009, %r1013};
	add.s64 	%rd1830, %rd1745, %rd1393;
	add.s64 	%rd1831, %rd1830, %rd1782;
	xor.b64  	%rd1832, %rd1831, %rd1770;
	mov.b64	{%r2815, %r2816}, %rd1832;
	mov.b64	%rd1833, {%r2816, %r2815};
	add.s64 	%rd1834, %rd1833, %rd1757;
	xor.b64  	%rd1835, %rd1834, %rd1745;
	mov.b64	{%r2817, %r2818}, %rd1835;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1836, {%r2820, %r2819};
	add.s64 	%rd1837, %rd1831, %rd1386;
	add.s64 	%rd1838, %rd1837, %rd1836;
	xor.b64  	%rd1839, %rd1838, %rd1833;
	mov.b64	{%r2821, %r2822}, %rd1839;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1840, {%r2824, %r2823};
	add.s64 	%rd1841, %rd1840, %rd1834;
	xor.b64  	%rd1842, %rd1841, %rd1836;
	mov.b64	{%r1022, %r1023}, %rd1842;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1843, {%r1017, %r1021};
	add.s64 	%rd1844, %rd1796, %rd1387;
	add.s64 	%rd1845, %rd1844, %rd1843;
	xor.b64  	%rd1846, %rd1845, %rd1812;
	mov.b64	{%r2825, %r2826}, %rd1846;
	mov.b64	%rd1847, {%r2826, %r2825};
	add.s64 	%rd1848, %rd1847, %rd1827;
	xor.b64  	%rd1849, %rd1848, %rd1843;
	mov.b64	{%r2827, %r2828}, %rd1849;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1850, {%r2830, %r2829};
	add.s64 	%rd1851, %rd1845, %rd1394;
	add.s64 	%rd1852, %rd1851, %rd1850;
	xor.b64  	%rd1853, %rd1847, %rd1852;
	mov.b64	{%r2831, %r2832}, %rd1853;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1854, {%r2834, %r2833};
	add.s64 	%rd1855, %rd1848, %rd1854;
	xor.b64  	%rd1856, %rd1855, %rd1850;
	mov.b64	{%r1030, %r1031}, %rd1856;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1857, {%r1025, %r1029};
	add.s64 	%rd1858, %rd1801, %rd1383;
	add.s64 	%rd1859, %rd1858, %rd1810;
	xor.b64  	%rd1860, %rd1826, %rd1859;
	mov.b64	{%r2835, %r2836}, %rd1860;
	mov.b64	%rd1861, {%r2836, %r2835};
	add.s64 	%rd1862, %rd1841, %rd1861;
	xor.b64  	%rd1863, %rd1862, %rd1801;
	mov.b64	{%r2837, %r2838}, %rd1863;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1864, {%r2840, %r2839};
	add.s64 	%rd1865, %rd1859, %rd1385;
	add.s64 	%rd1866, %rd1865, %rd1864;
	xor.b64  	%rd1867, %rd1866, %rd1861;
	mov.b64	{%r2841, %r2842}, %rd1867;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1868, {%r2844, %r2843};
	add.s64 	%rd1869, %rd1868, %rd1862;
	xor.b64  	%rd1870, %rd1869, %rd1864;
	mov.b64	{%r1038, %r1039}, %rd1870;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1871, {%r1033, %r1037};
	add.s64 	%rd1872, %rd1815, %rd1380;
	add.s64 	%rd1873, %rd1872, %rd1824;
	xor.b64  	%rd1874, %rd1840, %rd1873;
	mov.b64	{%r2845, %r2846}, %rd1874;
	mov.b64	%rd1875, {%r2846, %r2845};
	add.s64 	%rd1876, %rd1875, %rd1799;
	xor.b64  	%rd1877, %rd1876, %rd1815;
	mov.b64	{%r2847, %r2848}, %rd1877;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1878, {%r2850, %r2849};
	add.s64 	%rd1879, %rd1873, %rd1382;
	add.s64 	%rd1880, %rd1879, %rd1878;
	xor.b64  	%rd1881, %rd1880, %rd1875;
	mov.b64	{%r2851, %r2852}, %rd1881;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1882, {%r2854, %r2853};
	add.s64 	%rd1883, %rd1882, %rd1876;
	xor.b64  	%rd1884, %rd1883, %rd1878;
	mov.b64	{%r1046, %r1047}, %rd1884;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1885, {%r1041, %r1045};
	add.s64 	%rd1886, %rd1829, %rd1388;
	add.s64 	%rd1887, %rd1886, %rd1838;
	xor.b64  	%rd1888, %rd1887, %rd1798;
	mov.b64	{%r2855, %r2856}, %rd1888;
	mov.b64	%rd1889, {%r2856, %r2855};
	add.s64 	%rd1890, %rd1889, %rd1813;
	xor.b64  	%rd1891, %rd1890, %rd1829;
	mov.b64	{%r2857, %r2858}, %rd1891;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1892, {%r2860, %r2859};
	add.s64 	%rd1893, %rd1887, %rd1393;
	add.s64 	%rd1894, %rd1893, %rd1892;
	xor.b64  	%rd1895, %rd1894, %rd1889;
	mov.b64	{%r2861, %r2862}, %rd1895;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1896, {%r2864, %r2863};
	add.s64 	%rd1897, %rd1896, %rd1890;
	xor.b64  	%rd1898, %rd1897, %rd1892;
	mov.b64	{%r1054, %r1055}, %rd1898;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1899, {%r1049, %r1053};
	add.s64 	%rd1900, %rd1852, %rd1392;
	add.s64 	%rd1901, %rd1900, %rd1871;
	xor.b64  	%rd1902, %rd1896, %rd1901;
	mov.b64	{%r2865, %r2866}, %rd1902;
	mov.b64	%rd1903, {%r2866, %r2865};
	add.s64 	%rd1904, %rd1903, %rd1883;
	xor.b64  	%rd1905, %rd1904, %rd1871;
	mov.b64	{%r2867, %r2868}, %rd1905;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1906, {%r2870, %r2869};
	add.s64 	%rd1907, %rd1901, %rd1402;
	add.s64 	%rd1908, %rd1907, %rd1906;
	xor.b64  	%rd1909, %rd1903, %rd1908;
	mov.b64	{%r2871, %r2872}, %rd1909;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1910, {%r2874, %r2873};
	add.s64 	%rd1911, %rd1910, %rd1904;
	xor.b64  	%rd1912, %rd1911, %rd1906;
	mov.b64	{%r1062, %r1063}, %rd1912;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1913, {%r1057, %r1061};
	add.s64 	%rd1914, %rd1866, %rd1389;
	add.s64 	%rd1915, %rd1914, %rd1885;
	xor.b64  	%rd1916, %rd1915, %rd1854;
	mov.b64	{%r2875, %r2876}, %rd1916;
	mov.b64	%rd1917, {%r2876, %r2875};
	add.s64 	%rd1918, %rd1917, %rd1897;
	xor.b64  	%rd1919, %rd1918, %rd1885;
	mov.b64	{%r2877, %r2878}, %rd1919;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1920, {%r2880, %r2879};
	add.s64 	%rd1921, %rd1915, %rd1390;
	add.s64 	%rd1922, %rd1921, %rd1920;
	xor.b64  	%rd1923, %rd1922, %rd1917;
	mov.b64	{%r2881, %r2882}, %rd1923;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1924, {%r2884, %r2883};
	add.s64 	%rd1925, %rd1924, %rd1918;
	xor.b64  	%rd1926, %rd1925, %rd1920;
	mov.b64	{%r1070, %r1071}, %rd1926;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1927, {%r1065, %r1069};
	add.s64 	%rd1928, %rd1880, %rd1384;
	add.s64 	%rd1929, %rd1928, %rd1899;
	xor.b64  	%rd1930, %rd1929, %rd1868;
	mov.b64	{%r2885, %r2886}, %rd1930;
	mov.b64	%rd1931, {%r2886, %r2885};
	add.s64 	%rd1932, %rd1931, %rd1855;
	xor.b64  	%rd1933, %rd1932, %rd1899;
	mov.b64	{%r2887, %r2888}, %rd1933;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1934, {%r2890, %r2889};
	add.s64 	%rd1935, %rd1929, %rd1386;
	add.s64 	%rd1936, %rd1935, %rd1934;
	xor.b64  	%rd1937, %rd1936, %rd1931;
	mov.b64	{%r2891, %r2892}, %rd1937;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1938, {%r2894, %r2893};
	add.s64 	%rd1939, %rd1938, %rd1932;
	xor.b64  	%rd1940, %rd1939, %rd1934;
	mov.b64	{%r1078, %r1079}, %rd1940;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1941, {%r1073, %r1077};
	add.s64 	%rd1942, %rd1857, %rd1381;
	add.s64 	%rd1943, %rd1942, %rd1894;
	xor.b64  	%rd1944, %rd1943, %rd1882;
	mov.b64	{%r2895, %r2896}, %rd1944;
	mov.b64	%rd1945, {%r2896, %r2895};
	add.s64 	%rd1946, %rd1945, %rd1869;
	xor.b64  	%rd1947, %rd1946, %rd1857;
	mov.b64	{%r2897, %r2898}, %rd1947;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1948, {%r2900, %r2899};
	add.s64 	%rd1949, %rd1943, %rd1391;
	add.s64 	%rd1950, %rd1949, %rd1948;
	xor.b64  	%rd1951, %rd1950, %rd1945;
	mov.b64	{%r2901, %r2902}, %rd1951;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1952, {%r2904, %r2903};
	add.s64 	%rd1953, %rd1952, %rd1946;
	xor.b64  	%rd1954, %rd1953, %rd1948;
	mov.b64	{%r1086, %r1087}, %rd1954;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1955, {%r1081, %r1085};
	add.s64 	%rd1956, %rd1908, %rd1380;
	add.s64 	%rd1957, %rd1956, %rd1955;
	xor.b64  	%rd1958, %rd1957, %rd1924;
	mov.b64	{%r2905, %r2906}, %rd1958;
	mov.b64	%rd1959, {%r2906, %r2905};
	add.s64 	%rd1960, %rd1959, %rd1939;
	xor.b64  	%rd1961, %rd1960, %rd1955;
	mov.b64	{%r2907, %r2908}, %rd1961;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1962, {%r2910, %r2909};
	add.s64 	%rd1963, %rd1957, %rd1390;
	add.s64 	%rd1964, %rd1963, %rd1962;
	xor.b64  	%rd1965, %rd1959, %rd1964;
	mov.b64	{%r2911, %r2912}, %rd1965;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1966, {%r2914, %r2913};
	add.s64 	%rd1967, %rd1960, %rd1966;
	xor.b64  	%rd1968, %rd1967, %rd1962;
	mov.b64	{%r1094, %r1095}, %rd1968;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1969, {%r1089, %r1093};
	add.s64 	%rd1970, %rd1913, %rd1384;
	add.s64 	%rd1971, %rd1970, %rd1922;
	xor.b64  	%rd1972, %rd1938, %rd1971;
	mov.b64	{%r2915, %r2916}, %rd1972;
	mov.b64	%rd1973, {%r2916, %r2915};
	add.s64 	%rd1974, %rd1953, %rd1973;
	xor.b64  	%rd1975, %rd1974, %rd1913;
	mov.b64	{%r2917, %r2918}, %rd1975;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1976, {%r2920, %r2919};
	add.s64 	%rd1977, %rd1971, %rd1388;
	add.s64 	%rd1978, %rd1977, %rd1976;
	xor.b64  	%rd1979, %rd1978, %rd1973;
	mov.b64	{%r2921, %r2922}, %rd1979;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1980, {%r2924, %r2923};
	add.s64 	%rd1981, %rd1980, %rd1974;
	xor.b64  	%rd1982, %rd1981, %rd1976;
	mov.b64	{%r1102, %r1103}, %rd1982;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1983, {%r1097, %r1101};
	add.s64 	%rd1984, %rd1927, %rd1394;
	add.s64 	%rd1985, %rd1984, %rd1936;
	xor.b64  	%rd1986, %rd1952, %rd1985;
	mov.b64	{%r2925, %r2926}, %rd1986;
	mov.b64	%rd1987, {%r2926, %r2925};
	add.s64 	%rd1988, %rd1987, %rd1911;
	xor.b64  	%rd1989, %rd1988, %rd1927;
	mov.b64	{%r2927, %r2928}, %rd1989;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1990, {%r2930, %r2929};
	add.s64 	%rd1991, %rd1985, %rd1389;
	add.s64 	%rd1992, %rd1991, %rd1990;
	xor.b64  	%rd1993, %rd1992, %rd1987;
	mov.b64	{%r2931, %r2932}, %rd1993;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1994, {%r2934, %r2933};
	add.s64 	%rd1995, %rd1994, %rd1988;
	xor.b64  	%rd1996, %rd1995, %rd1990;
	mov.b64	{%r1110, %r1111}, %rd1996;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1997, {%r1105, %r1109};
	add.s64 	%rd1998, %rd1941, %rd1386;
	add.s64 	%rd1999, %rd1998, %rd1950;
	xor.b64  	%rd2000, %rd1999, %rd1910;
	mov.b64	{%r2935, %r2936}, %rd2000;
	mov.b64	%rd2001, {%r2936, %r2935};
	add.s64 	%rd2002, %rd2001, %rd1925;
	xor.b64  	%rd2003, %rd2002, %rd1941;
	mov.b64	{%r2937, %r2938}, %rd2003;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2004, {%r2940, %r2939};
	add.s64 	%rd2005, %rd1999, %rd1381;
	add.s64 	%rd2006, %rd2005, %rd2004;
	xor.b64  	%rd2007, %rd2006, %rd2001;
	mov.b64	{%r2941, %r2942}, %rd2007;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2008, {%r2944, %r2943};
	add.s64 	%rd2009, %rd2008, %rd2002;
	xor.b64  	%rd2010, %rd2009, %rd2004;
	mov.b64	{%r1118, %r1119}, %rd2010;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2011, {%r1113, %r1117};
	add.s64 	%rd2012, %rd1964, %rd1382;
	add.s64 	%rd2013, %rd2012, %rd1983;
	xor.b64  	%rd2014, %rd2008, %rd2013;
	mov.b64	{%r2945, %r2946}, %rd2014;
	mov.b64	%rd2015, {%r2946, %r2945};
	add.s64 	%rd2016, %rd2015, %rd1995;
	xor.b64  	%rd2017, %rd2016, %rd1983;
	mov.b64	{%r2947, %r2948}, %rd2017;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2018, {%r2950, %r2949};
	add.s64 	%rd2019, %rd2013, %rd1391;
	add.s64 	%rd2020, %rd2019, %rd2018;
	xor.b64  	%rd2021, %rd2015, %rd2020;
	mov.b64	{%r2951, %r2952}, %rd2021;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2022, {%r2954, %r2953};
	add.s64 	%rd2023, %rd2022, %rd2016;
	xor.b64  	%rd2024, %rd2023, %rd2018;
	mov.b64	{%r1126, %r1127}, %rd2024;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2025, {%r1121, %r1125};
	add.s64 	%rd2026, %rd1978, %rd1385;
	add.s64 	%rd2027, %rd2026, %rd1997;
	xor.b64  	%rd2028, %rd2027, %rd1966;
	mov.b64	{%r2955, %r2956}, %rd2028;
	mov.b64	%rd2029, {%r2956, %r2955};
	add.s64 	%rd2030, %rd2029, %rd2009;
	xor.b64  	%rd2031, %rd2030, %rd1997;
	mov.b64	{%r2957, %r2958}, %rd2031;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2032, {%r2960, %r2959};
	add.s64 	%rd2033, %rd2027, %rd1383;
	add.s64 	%rd2034, %rd2033, %rd2032;
	xor.b64  	%rd2035, %rd2034, %rd2029;
	mov.b64	{%r2961, %r2962}, %rd2035;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2036, {%r2964, %r2963};
	add.s64 	%rd2037, %rd2036, %rd2030;
	xor.b64  	%rd2038, %rd2037, %rd2032;
	mov.b64	{%r1134, %r1135}, %rd2038;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2039, {%r1129, %r1133};
	add.s64 	%rd2040, %rd1992, %rd1393;
	add.s64 	%rd2041, %rd2040, %rd2011;
	xor.b64  	%rd2042, %rd2041, %rd1980;
	mov.b64	{%r2965, %r2966}, %rd2042;
	mov.b64	%rd2043, {%r2966, %r2965};
	add.s64 	%rd2044, %rd2043, %rd1967;
	xor.b64  	%rd2045, %rd2044, %rd2011;
	mov.b64	{%r2967, %r2968}, %rd2045;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2046, {%r2970, %r2969};
	add.s64 	%rd2047, %rd2041, %rd1392;
	add.s64 	%rd2048, %rd2047, %rd2046;
	xor.b64  	%rd2049, %rd2048, %rd2043;
	mov.b64	{%r2971, %r2972}, %rd2049;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2050, {%r2974, %r2973};
	add.s64 	%rd2051, %rd2050, %rd2044;
	xor.b64  	%rd2052, %rd2051, %rd2046;
	mov.b64	{%r1142, %r1143}, %rd2052;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2053, {%r1137, %r1141};
	add.s64 	%rd2054, %rd1969, %rd1402;
	add.s64 	%rd2055, %rd2054, %rd2006;
	xor.b64  	%rd2056, %rd2055, %rd1994;
	mov.b64	{%r2975, %r2976}, %rd2056;
	mov.b64	%rd2057, {%r2976, %r2975};
	add.s64 	%rd2058, %rd2057, %rd1981;
	xor.b64  	%rd2059, %rd2058, %rd1969;
	mov.b64	{%r2977, %r2978}, %rd2059;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2060, {%r2980, %r2979};
	add.s64 	%rd2061, %rd2055, %rd1387;
	add.s64 	%rd2062, %rd2061, %rd2060;
	xor.b64  	%rd2063, %rd2062, %rd2057;
	mov.b64	{%r2981, %r2982}, %rd2063;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2064, {%r2984, %r2983};
	add.s64 	%rd2065, %rd2064, %rd2058;
	xor.b64  	%rd2066, %rd2065, %rd2060;
	mov.b64	{%r1150, %r1151}, %rd2066;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2067, {%r1145, %r1149};
	add.s64 	%rd2068, %rd2020, %rd1390;
	add.s64 	%rd2069, %rd2068, %rd2067;
	xor.b64  	%rd2070, %rd2069, %rd2036;
	mov.b64	{%r2985, %r2986}, %rd2070;
	mov.b64	%rd2071, {%r2986, %r2985};
	add.s64 	%rd2072, %rd2071, %rd2051;
	xor.b64  	%rd2073, %rd2072, %rd2067;
	mov.b64	{%r2987, %r2988}, %rd2073;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2074, {%r2990, %r2989};
	add.s64 	%rd2075, %rd2069, %rd1383;
	add.s64 	%rd2076, %rd2075, %rd2074;
	xor.b64  	%rd2077, %rd2071, %rd2076;
	mov.b64	{%r2991, %r2992}, %rd2077;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2078, {%r2994, %r2993};
	add.s64 	%rd2079, %rd2072, %rd2078;
	xor.b64  	%rd2080, %rd2079, %rd2074;
	mov.b64	{%r1158, %r1159}, %rd2080;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2081, {%r1153, %r1157};
	add.s64 	%rd2082, %rd2025, %rd1402;
	add.s64 	%rd2083, %rd2082, %rd2034;
	xor.b64  	%rd2084, %rd2050, %rd2083;
	mov.b64	{%r2995, %r2996}, %rd2084;
	mov.b64	%rd2085, {%r2996, %r2995};
	add.s64 	%rd2086, %rd2065, %rd2085;
	xor.b64  	%rd2087, %rd2086, %rd2025;
	mov.b64	{%r2997, %r2998}, %rd2087;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2088, {%r3000, %r2999};
	add.s64 	%rd2089, %rd2083, %rd1393;
	add.s64 	%rd2090, %rd2089, %rd2088;
	xor.b64  	%rd2091, %rd2090, %rd2085;
	mov.b64	{%r3001, %r3002}, %rd2091;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2092, {%r3004, %r3003};
	add.s64 	%rd2093, %rd2092, %rd2086;
	xor.b64  	%rd2094, %rd2093, %rd2088;
	mov.b64	{%r1166, %r1167}, %rd2094;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2095, {%r1161, %r1165};
	add.s64 	%rd2096, %rd2039, %rd1392;
	add.s64 	%rd2097, %rd2096, %rd2048;
	xor.b64  	%rd2098, %rd2064, %rd2097;
	mov.b64	{%r3005, %r3006}, %rd2098;
	mov.b64	%rd2099, {%r3006, %r3005};
	add.s64 	%rd2100, %rd2099, %rd2023;
	xor.b64  	%rd2101, %rd2100, %rd2039;
	mov.b64	{%r3007, %r3008}, %rd2101;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2102, {%r3010, %r3009};
	add.s64 	%rd2103, %rd2097, %rd1391;
	add.s64 	%rd2104, %rd2103, %rd2102;
	xor.b64  	%rd2105, %rd2104, %rd2099;
	mov.b64	{%r3011, %r3012}, %rd2105;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2106, {%r3014, %r3013};
	add.s64 	%rd2107, %rd2106, %rd2100;
	xor.b64  	%rd2108, %rd2107, %rd2102;
	mov.b64	{%r1174, %r1175}, %rd2108;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2109, {%r1169, %r1173};
	add.s64 	%rd2110, %rd2053, %rd1382;
	add.s64 	%rd2111, %rd2110, %rd2062;
	xor.b64  	%rd2112, %rd2111, %rd2022;
	mov.b64	{%r3015, %r3016}, %rd2112;
	mov.b64	%rd2113, {%r3016, %r3015};
	add.s64 	%rd2114, %rd2113, %rd2037;
	xor.b64  	%rd2115, %rd2114, %rd2053;
	mov.b64	{%r3017, %r3018}, %rd2115;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2116, {%r3020, %r3019};
	add.s64 	%rd2117, %rd2111, %rd1388;
	add.s64 	%rd2118, %rd2117, %rd2116;
	xor.b64  	%rd2119, %rd2118, %rd2113;
	mov.b64	{%r3021, %r3022}, %rd2119;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2120, {%r3024, %r3023};
	add.s64 	%rd2121, %rd2120, %rd2114;
	xor.b64  	%rd2122, %rd2121, %rd2116;
	mov.b64	{%r1182, %r1183}, %rd2122;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2123, {%r1177, %r1181};
	add.s64 	%rd2124, %rd2076, %rd1394;
	add.s64 	%rd2125, %rd2124, %rd2095;
	xor.b64  	%rd2126, %rd2120, %rd2125;
	mov.b64	{%r3025, %r3026}, %rd2126;
	mov.b64	%rd2127, {%r3026, %r3025};
	add.s64 	%rd2128, %rd2127, %rd2107;
	xor.b64  	%rd2129, %rd2128, %rd2095;
	mov.b64	{%r3027, %r3028}, %rd2129;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2130, {%r3030, %r3029};
	add.s64 	%rd2131, %rd2125, %rd1385;
	add.s64 	%rd2132, %rd2131, %rd2130;
	xor.b64  	%rd2133, %rd2127, %rd2132;
	mov.b64	{%r3031, %r3032}, %rd2133;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2134, {%r3034, %r3033};
	add.s64 	%rd2135, %rd2134, %rd2128;
	xor.b64  	%rd2136, %rd2135, %rd2130;
	mov.b64	{%r1190, %r1191}, %rd2136;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2137, {%r1185, %r1189};
	add.s64 	%rd2138, %rd2090, %rd1384;
	add.s64 	%rd2139, %rd2138, %rd2109;
	xor.b64  	%rd2140, %rd2139, %rd2078;
	mov.b64	{%r3035, %r3036}, %rd2140;
	mov.b64	%rd2141, {%r3036, %r3035};
	add.s64 	%rd2142, %rd2141, %rd2121;
	xor.b64  	%rd2143, %rd2142, %rd2109;
	mov.b64	{%r3037, %r3038}, %rd2143;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2144, {%r3040, %r3039};
	add.s64 	%rd2145, %rd2139, %rd1381;
	add.s64 	%rd2146, %rd2145, %rd2144;
	xor.b64  	%rd2147, %rd2146, %rd2141;
	mov.b64	{%r3041, %r3042}, %rd2147;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2148, {%r3044, %r3043};
	add.s64 	%rd2149, %rd2148, %rd2142;
	xor.b64  	%rd2150, %rd2149, %rd2144;
	mov.b64	{%r1198, %r1199}, %rd2150;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2151, {%r1193, %r1197};
	add.s64 	%rd2152, %rd2104, %rd1387;
	add.s64 	%rd2153, %rd2152, %rd2123;
	xor.b64  	%rd2154, %rd2153, %rd2092;
	mov.b64	{%r3045, %r3046}, %rd2154;
	mov.b64	%rd2155, {%r3046, %r3045};
	add.s64 	%rd2156, %rd2155, %rd2079;
	xor.b64  	%rd2157, %rd2156, %rd2123;
	mov.b64	{%r3047, %r3048}, %rd2157;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2158, {%r3050, %r3049};
	add.s64 	%rd2159, %rd2153, %rd1380;
	add.s64 	%rd2160, %rd2159, %rd2158;
	xor.b64  	%rd2161, %rd2160, %rd2155;
	mov.b64	{%r3051, %r3052}, %rd2161;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2162, {%r3054, %r3053};
	add.s64 	%rd2163, %rd2162, %rd2156;
	xor.b64  	%rd2164, %rd2163, %rd2158;
	mov.b64	{%r1206, %r1207}, %rd2164;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2165, {%r1201, %r1205};
	add.s64 	%rd2166, %rd2081, %rd1386;
	add.s64 	%rd2167, %rd2166, %rd2118;
	xor.b64  	%rd2168, %rd2167, %rd2106;
	mov.b64	{%r3055, %r3056}, %rd2168;
	mov.b64	%rd2169, {%r3056, %r3055};
	add.s64 	%rd2170, %rd2169, %rd2093;
	xor.b64  	%rd2171, %rd2170, %rd2081;
	mov.b64	{%r3057, %r3058}, %rd2171;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2172, {%r3060, %r3059};
	add.s64 	%rd2173, %rd2167, %rd1389;
	add.s64 	%rd2174, %rd2173, %rd2172;
	xor.b64  	%rd2175, %rd2174, %rd2169;
	mov.b64	{%r3061, %r3062}, %rd2175;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2176, {%r3064, %r3063};
	add.s64 	%rd2177, %rd2176, %rd2170;
	xor.b64  	%rd2178, %rd2177, %rd2172;
	mov.b64	{%r1214, %r1215}, %rd2178;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2179, {%r1209, %r1213};
	add.s64 	%rd2180, %rd2132, %rd1391;
	add.s64 	%rd2181, %rd2180, %rd2179;
	xor.b64  	%rd2182, %rd2181, %rd2148;
	mov.b64	{%r3065, %r3066}, %rd2182;
	mov.b64	%rd2183, {%r3066, %r3065};
	add.s64 	%rd2184, %rd2183, %rd2163;
	xor.b64  	%rd2185, %rd2184, %rd2179;
	mov.b64	{%r3067, %r3068}, %rd2185;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2186, {%r3070, %r3069};
	add.s64 	%rd2187, %rd2181, %rd1389;
	add.s64 	%rd2188, %rd2187, %rd2186;
	xor.b64  	%rd2189, %rd2183, %rd2188;
	mov.b64	{%r3071, %r3072}, %rd2189;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2190, {%r3074, %r3073};
	add.s64 	%rd2191, %rd2184, %rd2190;
	xor.b64  	%rd2192, %rd2191, %rd2186;
	mov.b64	{%r1222, %r1223}, %rd2192;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2193, {%r1217, %r1221};
	add.s64 	%rd2194, %rd2137, %rd1385;
	add.s64 	%rd2195, %rd2194, %rd2146;
	xor.b64  	%rd2196, %rd2162, %rd2195;
	mov.b64	{%r3075, %r3076}, %rd2196;
	mov.b64	%rd2197, {%r3076, %r3075};
	add.s64 	%rd2198, %rd2177, %rd2197;
	xor.b64  	%rd2199, %rd2198, %rd2137;
	mov.b64	{%r3077, %r3078}, %rd2199;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2200, {%r3080, %r3079};
	add.s64 	%rd2201, %rd2195, %rd1392;
	add.s64 	%rd2202, %rd2201, %rd2200;
	xor.b64  	%rd2203, %rd2202, %rd2197;
	mov.b64	{%r3081, %r3082}, %rd2203;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2204, {%r3084, %r3083};
	add.s64 	%rd2205, %rd2204, %rd2198;
	xor.b64  	%rd2206, %rd2205, %rd2200;
	mov.b64	{%r1230, %r1231}, %rd2206;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2207, {%r1225, %r1229};
	add.s64 	%rd2208, %rd2151, %rd1390;
	add.s64 	%rd2209, %rd2208, %rd2160;
	xor.b64  	%rd2210, %rd2176, %rd2209;
	mov.b64	{%r3085, %r3086}, %rd2210;
	mov.b64	%rd2211, {%r3086, %r3085};
	add.s64 	%rd2212, %rd2211, %rd2135;
	xor.b64  	%rd2213, %rd2212, %rd2151;
	mov.b64	{%r3087, %r3088}, %rd2213;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2214, {%r3090, %r3089};
	add.s64 	%rd2215, %rd2209, %rd1402;
	add.s64 	%rd2216, %rd2215, %rd2214;
	xor.b64  	%rd2217, %rd2216, %rd2211;
	mov.b64	{%r3091, %r3092}, %rd2217;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2218, {%r3094, %r3093};
	add.s64 	%rd2219, %rd2218, %rd2212;
	xor.b64  	%rd2220, %rd2219, %rd2214;
	mov.b64	{%r1238, %r1239}, %rd2220;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2221, {%r1233, %r1237};
	add.s64 	%rd2222, %rd2165, %rd1381;
	add.s64 	%rd2223, %rd2222, %rd2174;
	xor.b64  	%rd2224, %rd2223, %rd2134;
	mov.b64	{%r3095, %r3096}, %rd2224;
	mov.b64	%rd2225, {%r3096, %r3095};
	add.s64 	%rd2226, %rd2225, %rd2149;
	xor.b64  	%rd2227, %rd2226, %rd2165;
	mov.b64	{%r3097, %r3098}, %rd2227;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2228, {%r3100, %r3099};
	add.s64 	%rd2229, %rd2223, %rd1387;
	add.s64 	%rd2230, %rd2229, %rd2228;
	xor.b64  	%rd2231, %rd2230, %rd2225;
	mov.b64	{%r3101, %r3102}, %rd2231;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2232, {%r3104, %r3103};
	add.s64 	%rd2233, %rd2232, %rd2226;
	xor.b64  	%rd2234, %rd2233, %rd2228;
	mov.b64	{%r1246, %r1247}, %rd2234;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2235, {%r1241, %r1245};
	add.s64 	%rd2236, %rd2188, %rd1383;
	add.s64 	%rd2237, %rd2236, %rd2207;
	xor.b64  	%rd2238, %rd2232, %rd2237;
	mov.b64	{%r3105, %r3106}, %rd2238;
	mov.b64	%rd2239, {%r3106, %r3105};
	add.s64 	%rd2240, %rd2239, %rd2219;
	xor.b64  	%rd2241, %rd2240, %rd2207;
	mov.b64	{%r3107, %r3108}, %rd2241;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2242, {%r3110, %r3109};
	add.s64 	%rd2243, %rd2237, %rd1394;
	add.s64 	%rd2244, %rd2243, %rd2242;
	xor.b64  	%rd2245, %rd2239, %rd2244;
	mov.b64	{%r3111, %r3112}, %rd2245;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2246, {%r3114, %r3113};
	add.s64 	%rd2247, %rd2246, %rd2240;
	xor.b64  	%rd2248, %rd2247, %rd2242;
	mov.b64	{%r1254, %r1255}, %rd2248;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2249, {%r1249, %r1253};
	add.s64 	%rd2250, %rd2202, %rd1393;
	add.s64 	%rd2251, %rd2250, %rd2221;
	xor.b64  	%rd2252, %rd2251, %rd2190;
	mov.b64	{%r3115, %r3116}, %rd2252;
	mov.b64	%rd2253, {%r3116, %r3115};
	add.s64 	%rd2254, %rd2253, %rd2233;
	xor.b64  	%rd2255, %rd2254, %rd2221;
	mov.b64	{%r3117, %r3118}, %rd2255;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2256, {%r3120, %r3119};
	add.s64 	%rd2257, %rd2251, %rd1382;
	add.s64 	%rd2258, %rd2257, %rd2256;
	xor.b64  	%rd2259, %rd2258, %rd2253;
	mov.b64	{%r3121, %r3122}, %rd2259;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2260, {%r3124, %r3123};
	add.s64 	%rd2261, %rd2260, %rd2254;
	xor.b64  	%rd2262, %rd2261, %rd2256;
	mov.b64	{%r1262, %r1263}, %rd2262;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2263, {%r1257, %r1261};
	add.s64 	%rd2264, %rd2216, %rd1386;
	add.s64 	%rd2265, %rd2264, %rd2235;
	xor.b64  	%rd2266, %rd2265, %rd2204;
	mov.b64	{%r3125, %r3126}, %rd2266;
	mov.b64	%rd2267, {%r3126, %r3125};
	add.s64 	%rd2268, %rd2267, %rd2191;
	xor.b64  	%rd2269, %rd2268, %rd2235;
	mov.b64	{%r3127, %r3128}, %rd2269;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2270, {%r3130, %r3129};
	add.s64 	%rd2271, %rd2265, %rd1384;
	add.s64 	%rd2272, %rd2271, %rd2270;
	xor.b64  	%rd2273, %rd2272, %rd2267;
	mov.b64	{%r3131, %r3132}, %rd2273;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2274, {%r3134, %r3133};
	add.s64 	%rd2275, %rd2274, %rd2268;
	xor.b64  	%rd2276, %rd2275, %rd2270;
	mov.b64	{%r1270, %r1271}, %rd2276;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2277, {%r1265, %r1269};
	add.s64 	%rd2278, %rd2193, %rd1380;
	add.s64 	%rd2279, %rd2278, %rd2230;
	xor.b64  	%rd2280, %rd2279, %rd2218;
	mov.b64	{%r3135, %r3136}, %rd2280;
	mov.b64	%rd2281, {%r3136, %r3135};
	add.s64 	%rd2282, %rd2281, %rd2205;
	xor.b64  	%rd2283, %rd2282, %rd2193;
	mov.b64	{%r3137, %r3138}, %rd2283;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2284, {%r3140, %r3139};
	add.s64 	%rd2285, %rd2279, %rd1388;
	add.s64 	%rd2286, %rd2285, %rd2284;
	xor.b64  	%rd2287, %rd2286, %rd2281;
	mov.b64	{%r3141, %r3142}, %rd2287;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2288, {%r3144, %r3143};
	add.s64 	%rd2289, %rd2288, %rd2282;
	xor.b64  	%rd2290, %rd2289, %rd2284;
	mov.b64	{%r1278, %r1279}, %rd2290;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2291, {%r1273, %r1277};
	add.s64 	%rd2292, %rd2244, %rd1384;
	add.s64 	%rd2293, %rd2292, %rd2291;
	xor.b64  	%rd2294, %rd2293, %rd2260;
	mov.b64	{%r3145, %r3146}, %rd2294;
	mov.b64	%rd2295, {%r3146, %r3145};
	add.s64 	%rd2296, %rd2295, %rd2275;
	xor.b64  	%rd2297, %rd2296, %rd2291;
	mov.b64	{%r3147, %r3148}, %rd2297;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2298, {%r3150, %r3149};
	add.s64 	%rd2299, %rd2293, %rd1393;
	add.s64 	%rd2300, %rd2299, %rd2298;
	xor.b64  	%rd2301, %rd2295, %rd2300;
	mov.b64	{%r3151, %r3152}, %rd2301;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2302, {%r3154, %r3153};
	add.s64 	%rd2303, %rd2296, %rd2302;
	xor.b64  	%rd2304, %rd2303, %rd2298;
	mov.b64	{%r1286, %r1287}, %rd2304;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2305, {%r1281, %r1285};
	add.s64 	%rd2306, %rd2249, %rd1392;
	add.s64 	%rd2307, %rd2306, %rd2258;
	xor.b64  	%rd2308, %rd2274, %rd2307;
	mov.b64	{%r3155, %r3156}, %rd2308;
	mov.b64	%rd2309, {%r3156, %r3155};
	add.s64 	%rd2310, %rd2289, %rd2309;
	xor.b64  	%rd2311, %rd2310, %rd2249;
	mov.b64	{%r3157, %r3158}, %rd2311;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2312, {%r3160, %r3159};
	add.s64 	%rd2313, %rd2307, %rd1387;
	add.s64 	%rd2314, %rd2313, %rd2312;
	xor.b64  	%rd2315, %rd2314, %rd2309;
	mov.b64	{%r3161, %r3162}, %rd2315;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2316, {%r3164, %r3163};
	add.s64 	%rd2317, %rd2316, %rd2310;
	xor.b64  	%rd2318, %rd2317, %rd2312;
	mov.b64	{%r1294, %r1295}, %rd2318;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2319, {%r1289, %r1293};
	add.s64 	%rd2320, %rd2263, %rd1389;
	add.s64 	%rd2321, %rd2320, %rd2272;
	xor.b64  	%rd2322, %rd2288, %rd2321;
	mov.b64	{%r3165, %r3166}, %rd2322;
	mov.b64	%rd2323, {%r3166, %r3165};
	add.s64 	%rd2324, %rd2323, %rd2247;
	xor.b64  	%rd2325, %rd2324, %rd2263;
	mov.b64	{%r3167, %r3168}, %rd2325;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2326, {%r3170, %r3169};
	add.s64 	%rd2327, %rd2321, %rd1381;
	add.s64 	%rd2328, %rd2327, %rd2326;
	xor.b64  	%rd2329, %rd2328, %rd2323;
	mov.b64	{%r3171, %r3172}, %rd2329;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2330, {%r3174, %r3173};
	add.s64 	%rd2331, %rd2330, %rd2324;
	xor.b64  	%rd2332, %rd2331, %rd2326;
	mov.b64	{%r1302, %r1303}, %rd2332;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2333, {%r1297, %r1301};
	add.s64 	%rd2334, %rd2277, %rd1394;
	add.s64 	%rd2335, %rd2334, %rd2286;
	xor.b64  	%rd2336, %rd2335, %rd2246;
	mov.b64	{%r3175, %r3176}, %rd2336;
	mov.b64	%rd2337, {%r3176, %r3175};
	add.s64 	%rd2338, %rd2337, %rd2261;
	xor.b64  	%rd2339, %rd2338, %rd2277;
	mov.b64	{%r3177, %r3178}, %rd2339;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2340, {%r3180, %r3179};
	add.s64 	%rd2341, %rd2335, %rd1386;
	add.s64 	%rd2342, %rd2341, %rd2340;
	xor.b64  	%rd2343, %rd2342, %rd2337;
	mov.b64	{%r3181, %r3182}, %rd2343;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2344, {%r3184, %r3183};
	add.s64 	%rd2345, %rd2344, %rd2338;
	xor.b64  	%rd2346, %rd2345, %rd2340;
	mov.b64	{%r1310, %r1311}, %rd2346;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2347, {%r1305, %r1309};
	add.s64 	%rd2348, %rd2300, %rd1390;
	add.s64 	%rd2349, %rd2348, %rd2319;
	xor.b64  	%rd2350, %rd2344, %rd2349;
	mov.b64	{%r3185, %r3186}, %rd2350;
	mov.b64	%rd2351, {%r3186, %r3185};
	add.s64 	%rd2352, %rd2351, %rd2331;
	xor.b64  	%rd2353, %rd2352, %rd2319;
	mov.b64	{%r3187, %r3188}, %rd2353;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2354, {%r3190, %r3189};
	add.s64 	%rd2355, %rd2349, %rd1380;
	add.s64 	%rd2356, %rd2355, %rd2354;
	xor.b64  	%rd2357, %rd2351, %rd2356;
	mov.b64	{%r3191, %r3192}, %rd2357;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2358, {%r3194, %r3193};
	add.s64 	%rd2359, %rd2358, %rd2352;
	xor.b64  	%rd2360, %rd2359, %rd2354;
	mov.b64	{%r1318, %r1319}, %rd2360;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2361, {%r1313, %r1317};
	add.s64 	%rd2362, %rd2314, %rd1391;
	add.s64 	%rd2363, %rd2362, %rd2333;
	xor.b64  	%rd2364, %rd2363, %rd2302;
	mov.b64	{%r3195, %r3196}, %rd2364;
	mov.b64	%rd2365, {%r3196, %r3195};
	add.s64 	%rd2366, %rd2365, %rd2345;
	xor.b64  	%rd2367, %rd2366, %rd2333;
	mov.b64	{%r3197, %r3198}, %rd2367;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2368, {%r3200, %r3199};
	add.s64 	%rd2369, %rd2363, %rd1385;
	add.s64 	%rd2370, %rd2369, %rd2368;
	xor.b64  	%rd2371, %rd2370, %rd2365;
	mov.b64	{%r3201, %r3202}, %rd2371;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2372, {%r3204, %r3203};
	add.s64 	%rd2373, %rd2372, %rd2366;
	xor.b64  	%rd2374, %rd2373, %rd2368;
	mov.b64	{%r1326, %r1327}, %rd2374;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2375, {%r1321, %r1325};
	add.s64 	%rd2376, %rd2328, %rd1402;
	add.s64 	%rd2377, %rd2376, %rd2347;
	xor.b64  	%rd2378, %rd2377, %rd2316;
	mov.b64	{%r3205, %r3206}, %rd2378;
	mov.b64	%rd2379, {%r3206, %r3205};
	add.s64 	%rd2380, %rd2379, %rd2303;
	xor.b64  	%rd2381, %rd2380, %rd2347;
	mov.b64	{%r3207, %r3208}, %rd2381;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2382, {%r3210, %r3209};
	add.s64 	%rd2383, %rd2377, %rd1382;
	add.s64 	%rd2384, %rd2383, %rd2382;
	xor.b64  	%rd2385, %rd2384, %rd2379;
	mov.b64	{%r3211, %r3212}, %rd2385;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2386, {%r3214, %r3213};
	add.s64 	%rd2387, %rd2386, %rd2380;
	xor.b64  	%rd2388, %rd2387, %rd2382;
	mov.b64	{%r1334, %r1335}, %rd2388;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2389, {%r1329, %r1333};
	add.s64 	%rd2390, %rd2305, %rd1388;
	add.s64 	%rd2391, %rd2390, %rd2342;
	xor.b64  	%rd2392, %rd2391, %rd2330;
	mov.b64	{%r3215, %r3216}, %rd2392;
	mov.b64	%rd2393, {%r3216, %r3215};
	add.s64 	%rd2394, %rd2393, %rd2317;
	xor.b64  	%rd2395, %rd2394, %rd2305;
	mov.b64	{%r3217, %r3218}, %rd2395;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2396, {%r3220, %r3219};
	add.s64 	%rd2397, %rd2391, %rd1383;
	add.s64 	%rd2398, %rd2397, %rd2396;
	xor.b64  	%rd2399, %rd2398, %rd2393;
	mov.b64	{%r3221, %r3222}, %rd2399;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2400, {%r3224, %r3223};
	add.s64 	%rd2401, %rd2400, %rd2394;
	xor.b64  	%rd2402, %rd2401, %rd2396;
	mov.b64	{%r1342, %r1343}, %rd2402;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2403, {%r1337, %r1341};
	add.s64 	%rd2404, %rd2356, %rd1388;
	add.s64 	%rd2405, %rd2404, %rd2403;
	xor.b64  	%rd2406, %rd2405, %rd2372;
	mov.b64	{%r3225, %r3226}, %rd2406;
	mov.b64	%rd2407, {%r3226, %r3225};
	add.s64 	%rd2408, %rd2407, %rd2387;
	xor.b64  	%rd2409, %rd2408, %rd2403;
	mov.b64	{%r3227, %r3228}, %rd2409;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2410, {%r3230, %r3229};
	add.s64 	%rd2411, %rd2405, %rd1380;
	add.s64 	%rd2412, %rd2411, %rd2410;
	xor.b64  	%rd2413, %rd2407, %rd2412;
	mov.b64	{%r3231, %r3232}, %rd2413;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2414, {%r3234, %r3233};
	add.s64 	%rd2415, %rd2408, %rd2414;
	xor.b64  	%rd2416, %rd2415, %rd2410;
	mov.b64	{%r1350, %r1351}, %rd2416;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2417, {%r1345, %r1349};
	add.s64 	%rd2418, %rd2361, %rd1386;
	add.s64 	%rd2419, %rd2418, %rd2370;
	xor.b64  	%rd2420, %rd2386, %rd2419;
	mov.b64	{%r3235, %r3236}, %rd2420;
	mov.b64	%rd2421, {%r3236, %r3235};
	add.s64 	%rd2422, %rd2401, %rd2421;
	xor.b64  	%rd2423, %rd2422, %rd2361;
	mov.b64	{%r3237, %r3238}, %rd2423;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2424, {%r3240, %r3239};
	add.s64 	%rd2425, %rd2419, %rd1382;
	add.s64 	%rd2426, %rd2425, %rd2424;
	xor.b64  	%rd2427, %rd2426, %rd2421;
	mov.b64	{%r3241, %r3242}, %rd2427;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2428, {%r3244, %r3243};
	add.s64 	%rd2429, %rd2428, %rd2422;
	xor.b64  	%rd2430, %rd2429, %rd2424;
	mov.b64	{%r1358, %r1359}, %rd2430;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2431, {%r1353, %r1357};
	add.s64 	%rd2432, %rd2375, %rd1385;
	add.s64 	%rd2433, %rd2432, %rd2384;
	xor.b64  	%rd2434, %rd2400, %rd2433;
	mov.b64	{%r3245, %r3246}, %rd2434;
	mov.b64	%rd2435, {%r3246, %r3245};
	add.s64 	%rd2436, %rd2435, %rd2359;
	xor.b64  	%rd2437, %rd2436, %rd2375;
	mov.b64	{%r3247, %r3248}, %rd2437;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2438, {%r3250, %r3249};
	add.s64 	%rd2439, %rd2433, %rd1384;
	add.s64 	%rd2440, %rd2439, %rd2438;
	xor.b64  	%rd2441, %rd2440, %rd2435;
	mov.b64	{%r3251, %r3252}, %rd2441;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2442, {%r3254, %r3253};
	add.s64 	%rd2443, %rd2442, %rd2436;
	xor.b64  	%rd2444, %rd2443, %rd2438;
	mov.b64	{%r1366, %r1367}, %rd2444;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2445, {%r1361, %r1365};
	add.s64 	%rd2446, %rd2389, %rd1402;
	add.s64 	%rd2447, %rd2446, %rd2398;
	xor.b64  	%rd2448, %rd2447, %rd2358;
	mov.b64	{%r3255, %r3256}, %rd2448;
	mov.b64	%rd2449, {%r3256, %r3255};
	add.s64 	%rd2450, %rd2449, %rd2373;
	xor.b64  	%rd2451, %rd2450, %rd2389;
	mov.b64	{%r3257, %r3258}, %rd2451;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2452, {%r3260, %r3259};
	add.s64 	%rd2453, %rd2447, %rd1383;
	add.s64 	%rd2454, %rd2453, %rd2452;
	xor.b64  	%rd2455, %rd2454, %rd2449;
	mov.b64	{%r3261, %r3262}, %rd2455;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2456, {%r3264, %r3263};
	add.s64 	%rd2457, %rd2456, %rd2450;
	xor.b64  	%rd2458, %rd2457, %rd2452;
	mov.b64	{%r1374, %r1375}, %rd2458;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2459, {%r1369, %r1373};
	add.s64 	%rd2460, %rd2412, %rd1393;
	add.s64 	%rd2461, %rd2460, %rd2431;
	xor.b64  	%rd2462, %rd2456, %rd2461;
	mov.b64	{%r3265, %r3266}, %rd2462;
	mov.b64	%rd2463, {%r3266, %r3265};
	add.s64 	%rd2464, %rd2463, %rd2443;
	xor.b64  	%rd2465, %rd2464, %rd2431;
	mov.b64	{%r3267, %r3268}, %rd2465;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2466, {%r3270, %r3269};
	add.s64 	%rd2467, %rd2461, %rd1389;
	add.s64 	%rd2468, %rd2467, %rd2466;
	xor.b64  	%rd2469, %rd2463, %rd2468;
	mov.b64	{%r3271, %r3272}, %rd2469;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2470, {%r3274, %r3273};
	add.s64 	%rd2471, %rd2470, %rd2464;
	xor.b64  	%rd2472, %rd2471, %rd2466;
	mov.b64	{%r1382, %r1383}, %rd2472;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2473, {%r1377, %r1381};
	add.s64 	%rd2474, %rd2426, %rd1387;
	add.s64 	%rd2475, %rd2474, %rd2445;
	xor.b64  	%rd2476, %rd2475, %rd2414;
	mov.b64	{%r3275, %r3276}, %rd2476;
	mov.b64	%rd2477, {%r3276, %r3275};
	add.s64 	%rd2478, %rd2477, %rd2457;
	xor.b64  	%rd2479, %rd2478, %rd2445;
	mov.b64	{%r3277, %r3278}, %rd2479;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2480, {%r3280, %r3279};
	add.s64 	%rd2481, %rd2475, %rd1392;
	add.s64 	%rd2482, %rd2481, %rd2480;
	xor.b64  	%rd2483, %rd2482, %rd2477;
	mov.b64	{%r3281, %r3282}, %rd2483;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2484, {%r3284, %r3283};
	add.s64 	%rd2485, %rd2484, %rd2478;
	xor.b64  	%rd2486, %rd2485, %rd2480;
	mov.b64	{%r1390, %r1391}, %rd2486;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2487, {%r1385, %r1389};
	add.s64 	%rd2488, %rd2440, %rd1381;
	add.s64 	%rd2489, %rd2488, %rd2459;
	xor.b64  	%rd2490, %rd2489, %rd2428;
	mov.b64	{%r3285, %r3286}, %rd2490;
	mov.b64	%rd2491, {%r3286, %r3285};
	add.s64 	%rd2492, %rd2491, %rd2415;
	xor.b64  	%rd2493, %rd2492, %rd2459;
	mov.b64	{%r3287, %r3288}, %rd2493;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2494, {%r3290, %r3289};
	add.s64 	%rd2495, %rd2489, %rd1390;
	add.s64 	%rd2496, %rd2495, %rd2494;
	xor.b64  	%rd2497, %rd2496, %rd2491;
	mov.b64	{%r3291, %r3292}, %rd2497;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2498, {%r3294, %r3293};
	add.s64 	%rd2499, %rd2498, %rd2492;
	xor.b64  	%rd2500, %rd2499, %rd2494;
	mov.b64	{%r1398, %r1399}, %rd2500;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2501, {%r1393, %r1397};
	add.s64 	%rd2502, %rd2417, %rd1391;
	add.s64 	%rd2503, %rd2502, %rd2454;
	xor.b64  	%rd2504, %rd2503, %rd2442;
	mov.b64	{%r3295, %r3296}, %rd2504;
	mov.b64	%rd2505, {%r3296, %r3295};
	add.s64 	%rd2506, %rd2505, %rd2429;
	xor.b64  	%rd2507, %rd2506, %rd2417;
	mov.b64	{%r3297, %r3298}, %rd2507;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2508, {%r3300, %r3299};
	add.s64 	%rd2509, %rd2503, %rd1394;
	add.s64 	%rd2510, %rd2509, %rd2508;
	xor.b64  	%rd2511, %rd2510, %rd2505;
	mov.b64	{%r3301, %r3302}, %rd2511;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2512, {%r3304, %r3303};
	add.s64 	%rd2513, %rd2512, %rd2506;
	xor.b64  	%rd2514, %rd2513, %rd2508;
	mov.b64	{%r1406, %r1407}, %rd2514;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2515, {%r1401, %r1405};
	add.s64 	%rd2516, %rd2468, %rd1394;
	add.s64 	%rd2517, %rd2516, %rd2515;
	xor.b64  	%rd2518, %rd2517, %rd2484;
	mov.b64	{%r3305, %r3306}, %rd2518;
	mov.b64	%rd2519, {%r3306, %r3305};
	add.s64 	%rd2520, %rd2519, %rd2499;
	xor.b64  	%rd2521, %rd2520, %rd2515;
	mov.b64	{%r3307, %r3308}, %rd2521;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2522, {%r3310, %r3309};
	add.s64 	%rd2523, %rd2517, %rd1402;
	add.s64 	%rd2524, %rd2523, %rd2522;
	xor.b64  	%rd2525, %rd2519, %rd2524;
	mov.b64	{%r3311, %r3312}, %rd2525;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2526, {%r3314, %r3313};
	add.s64 	%rd2527, %rd2520, %rd2526;
	xor.b64  	%rd2528, %rd2527, %rd2522;
	mov.b64	{%r1414, %r1415}, %rd2528;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2529, {%r1409, %r1413};
	add.s64 	%rd2530, %rd2473, %rd1380;
	add.s64 	%rd2531, %rd2530, %rd2482;
	xor.b64  	%rd2532, %rd2498, %rd2531;
	mov.b64	{%r3315, %r3316}, %rd2532;
	mov.b64	%rd2533, {%r3316, %r3315};
	add.s64 	%rd2534, %rd2513, %rd2533;
	xor.b64  	%rd2535, %rd2534, %rd2473;
	mov.b64	{%r3317, %r3318}, %rd2535;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2536, {%r3320, %r3319};
	add.s64 	%rd2537, %rd2531, %rd1381;
	add.s64 	%rd2538, %rd2537, %rd2536;
	xor.b64  	%rd2539, %rd2538, %rd2533;
	mov.b64	{%r3321, %r3322}, %rd2539;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2540, {%r3324, %r3323};
	add.s64 	%rd2541, %rd2540, %rd2534;
	xor.b64  	%rd2542, %rd2541, %rd2536;
	mov.b64	{%r1422, %r1423}, %rd2542;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2543, {%r1417, %r1421};
	add.s64 	%rd2544, %rd2487, %rd1382;
	add.s64 	%rd2545, %rd2544, %rd2496;
	xor.b64  	%rd2546, %rd2512, %rd2545;
	mov.b64	{%r3325, %r3326}, %rd2546;
	mov.b64	%rd2547, {%r3326, %r3325};
	add.s64 	%rd2548, %rd2547, %rd2471;
	xor.b64  	%rd2549, %rd2548, %rd2487;
	mov.b64	{%r3327, %r3328}, %rd2549;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2550, {%r3330, %r3329};
	add.s64 	%rd2551, %rd2545, %rd1383;
	add.s64 	%rd2552, %rd2551, %rd2550;
	xor.b64  	%rd2553, %rd2552, %rd2547;
	mov.b64	{%r3331, %r3332}, %rd2553;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2554, {%r3334, %r3333};
	add.s64 	%rd2555, %rd2554, %rd2548;
	xor.b64  	%rd2556, %rd2555, %rd2550;
	mov.b64	{%r1430, %r1431}, %rd2556;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2557, {%r1425, %r1429};
	add.s64 	%rd2558, %rd2501, %rd1384;
	add.s64 	%rd2559, %rd2558, %rd2510;
	xor.b64  	%rd2560, %rd2559, %rd2470;
	mov.b64	{%r3335, %r3336}, %rd2560;
	mov.b64	%rd2561, {%r3336, %r3335};
	add.s64 	%rd2562, %rd2561, %rd2485;
	xor.b64  	%rd2563, %rd2562, %rd2501;
	mov.b64	{%r3337, %r3338}, %rd2563;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2564, {%r3340, %r3339};
	add.s64 	%rd2565, %rd2559, %rd1385;
	add.s64 	%rd2566, %rd2565, %rd2564;
	xor.b64  	%rd2567, %rd2566, %rd2561;
	mov.b64	{%r3341, %r3342}, %rd2567;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2568, {%r3344, %r3343};
	add.s64 	%rd2569, %rd2568, %rd2562;
	xor.b64  	%rd2570, %rd2569, %rd2564;
	mov.b64	{%r1438, %r1439}, %rd2570;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2571, {%r1433, %r1437};
	add.s64 	%rd2572, %rd2524, %rd1386;
	add.s64 	%rd2573, %rd2572, %rd2543;
	xor.b64  	%rd2574, %rd2568, %rd2573;
	mov.b64	{%r3345, %r3346}, %rd2574;
	mov.b64	%rd2575, {%r3346, %r3345};
	add.s64 	%rd2576, %rd2575, %rd2555;
	xor.b64  	%rd2577, %rd2576, %rd2543;
	mov.b64	{%r3347, %r3348}, %rd2577;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2578, {%r3350, %r3349};
	add.s64 	%rd2579, %rd2573, %rd1387;
	add.s64 	%rd2580, %rd2579, %rd2578;
	xor.b64  	%rd2581, %rd2575, %rd2580;
	mov.b64	{%r3351, %r3352}, %rd2581;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2582, {%r3354, %r3353};
	add.s64 	%rd2583, %rd2582, %rd2576;
	xor.b64  	%rd2584, %rd2583, %rd2578;
	mov.b64	{%r1446, %r1447}, %rd2584;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2585, {%r1441, %r1445};
	add.s64 	%rd2586, %rd2538, %rd1388;
	add.s64 	%rd2587, %rd2586, %rd2557;
	xor.b64  	%rd2588, %rd2587, %rd2526;
	mov.b64	{%r3355, %r3356}, %rd2588;
	mov.b64	%rd2589, {%r3356, %r3355};
	add.s64 	%rd2590, %rd2589, %rd2569;
	xor.b64  	%rd2591, %rd2590, %rd2557;
	mov.b64	{%r3357, %r3358}, %rd2591;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2592, {%r3360, %r3359};
	add.s64 	%rd2593, %rd2587, %rd1389;
	add.s64 	%rd2594, %rd2593, %rd2592;
	xor.b64  	%rd2595, %rd2594, %rd2589;
	mov.b64	{%r3361, %r3362}, %rd2595;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2596, {%r3364, %r3363};
	add.s64 	%rd2597, %rd2596, %rd2590;
	xor.b64  	%rd2598, %rd2597, %rd2592;
	mov.b64	{%r1454, %r1455}, %rd2598;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2599, {%r1449, %r1453};
	add.s64 	%rd2600, %rd2552, %rd1390;
	add.s64 	%rd2601, %rd2600, %rd2571;
	xor.b64  	%rd2602, %rd2601, %rd2540;
	mov.b64	{%r3365, %r3366}, %rd2602;
	mov.b64	%rd2603, {%r3366, %r3365};
	add.s64 	%rd2604, %rd2603, %rd2527;
	xor.b64  	%rd2605, %rd2604, %rd2571;
	mov.b64	{%r3367, %r3368}, %rd2605;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2606, {%r3370, %r3369};
	add.s64 	%rd2607, %rd2601, %rd1391;
	add.s64 	%rd2608, %rd2607, %rd2606;
	xor.b64  	%rd2609, %rd2608, %rd2603;
	mov.b64	{%r3371, %r3372}, %rd2609;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2610, {%r3374, %r3373};
	add.s64 	%rd2611, %rd2610, %rd2604;
	xor.b64  	%rd2612, %rd2611, %rd2606;
	mov.b64	{%r1462, %r1463}, %rd2612;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2613, {%r1457, %r1461};
	add.s64 	%rd2614, %rd2529, %rd1392;
	add.s64 	%rd2615, %rd2614, %rd2566;
	xor.b64  	%rd2616, %rd2615, %rd2554;
	mov.b64	{%r3375, %r3376}, %rd2616;
	mov.b64	%rd2617, {%r3376, %r3375};
	add.s64 	%rd2618, %rd2617, %rd2541;
	xor.b64  	%rd2619, %rd2618, %rd2529;
	mov.b64	{%r3377, %r3378}, %rd2619;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2620, {%r3380, %r3379};
	add.s64 	%rd2621, %rd2615, %rd1393;
	add.s64 	%rd2622, %rd2621, %rd2620;
	xor.b64  	%rd2623, %rd2622, %rd2617;
	mov.b64	{%r3381, %r3382}, %rd2623;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2624, {%r3384, %r3383};
	add.s64 	%rd2625, %rd2624, %rd2618;
	xor.b64  	%rd2626, %rd2625, %rd2620;
	mov.b64	{%r1470, %r1471}, %rd2626;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2627, {%r1465, %r1469};
	add.s64 	%rd2628, %rd2580, %rd1392;
	add.s64 	%rd2629, %rd2628, %rd2627;
	xor.b64  	%rd2630, %rd2629, %rd2596;
	mov.b64	{%r3385, %r3386}, %rd2630;
	mov.b64	%rd2631, {%r3386, %r3385};
	add.s64 	%rd2632, %rd2631, %rd2611;
	xor.b64  	%rd2633, %rd2632, %rd2627;
	mov.b64	{%r3387, %r3388}, %rd2633;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2634, {%r3390, %r3389};
	add.s64 	%rd2635, %rd2629, %rd1388;
	add.s64 	%rd2636, %rd2635, %rd2634;
	xor.b64  	%rd2637, %rd2631, %rd2636;
	mov.b64	{%r3391, %r3392}, %rd2637;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2638, {%r3394, %r3393};
	add.s64 	%rd2639, %rd2632, %rd2638;
	xor.b64  	%rd2640, %rd2639, %rd2634;
	mov.b64	{%r1478, %r1479}, %rd2640;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2641, {%r1473, %r1477};
	add.s64 	%rd2642, %rd2585, %rd1382;
	add.s64 	%rd2643, %rd2642, %rd2594;
	xor.b64  	%rd2644, %rd2610, %rd2643;
	mov.b64	{%r3395, %r3396}, %rd2644;
	mov.b64	%rd2645, {%r3396, %r3395};
	add.s64 	%rd2646, %rd2625, %rd2645;
	xor.b64  	%rd2647, %rd2646, %rd2585;
	mov.b64	{%r3397, %r3398}, %rd2647;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2648, {%r3400, %r3399};
	add.s64 	%rd2649, %rd2643, %rd1386;
	add.s64 	%rd2650, %rd2649, %rd2648;
	xor.b64  	%rd2651, %rd2650, %rd2645;
	mov.b64	{%r3401, %r3402}, %rd2651;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2652, {%r3404, %r3403};
	add.s64 	%rd2653, %rd2652, %rd2646;
	xor.b64  	%rd2654, %rd2653, %rd2648;
	mov.b64	{%r1486, %r1487}, %rd2654;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2655, {%r1481, %r1485};
	add.s64 	%rd2656, %rd2599, %rd1387;
	add.s64 	%rd2657, %rd2656, %rd2608;
	xor.b64  	%rd2658, %rd2624, %rd2657;
	mov.b64	{%r3405, %r3406}, %rd2658;
	mov.b64	%rd2659, {%r3406, %r3405};
	add.s64 	%rd2660, %rd2659, %rd2583;
	xor.b64  	%rd2661, %rd2660, %rd2599;
	mov.b64	{%r3407, %r3408}, %rd2661;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2662, {%r3410, %r3409};
	add.s64 	%rd2663, %rd2657, %rd1393;
	add.s64 	%rd2664, %rd2663, %rd2662;
	xor.b64  	%rd2665, %rd2664, %rd2659;
	mov.b64	{%r3411, %r3412}, %rd2665;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2666, {%r3414, %r3413};
	add.s64 	%rd2667, %rd2666, %rd2660;
	xor.b64  	%rd2668, %rd2667, %rd2662;
	mov.b64	{%r1494, %r1495}, %rd2668;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2669, {%r1489, %r1493};
	add.s64 	%rd2670, %rd2613, %rd1391;
	add.s64 	%rd2671, %rd2670, %rd2622;
	xor.b64  	%rd2672, %rd2671, %rd2582;
	mov.b64	{%r3415, %r3416}, %rd2672;
	mov.b64	%rd2673, {%r3416, %r3415};
	add.s64 	%rd2674, %rd2673, %rd2597;
	xor.b64  	%rd2675, %rd2674, %rd2613;
	mov.b64	{%r3417, %r3418}, %rd2675;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2676, {%r3420, %r3419};
	add.s64 	%rd2677, %rd2671, %rd1384;
	add.s64 	%rd2678, %rd2677, %rd2676;
	xor.b64  	%rd2679, %rd2678, %rd2673;
	mov.b64	{%r3421, %r3422}, %rd2679;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2680, {%r3424, %r3423};
	add.s64 	%rd2681, %rd2680, %rd2674;
	xor.b64  	%rd2682, %rd2681, %rd2676;
	mov.b64	{%r1502, %r1503}, %rd2682;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2683, {%r1497, %r1501};
	add.s64 	%rd2684, %rd2636, %rd1402;
	add.s64 	%rd2685, %rd2684, %rd2655;
	xor.b64  	%rd2686, %rd2680, %rd2685;
	mov.b64	{%r3425, %r3426}, %rd2686;
	mov.b64	%rd2687, {%r3426, %r3425};
	add.s64 	%rd2688, %rd2687, %rd2667;
	xor.b64  	%rd2689, %rd2688, %rd2655;
	mov.b64	{%r3427, %r3428}, %rd2689;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2690, {%r3430, %r3429};
	add.s64 	%rd2691, %rd2685, %rd1390;
	add.s64 	%rd2692, %rd2691, %rd2690;
	xor.b64  	%rd2693, %rd2687, %rd2692;
	mov.b64	{%r3431, %r3432}, %rd2693;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2694, {%r3434, %r3433};
	add.s64 	%rd2695, %rd2694, %rd2688;
	xor.b64  	%rd2696, %rd2695, %rd2690;
	mov.b64	{%r1510, %r1511}, %rd2696;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	add.s64 	%rd2697, %rd2650, %rd1394;
	add.s64 	%rd2698, %rd2697, %rd2669;
	xor.b64  	%rd2699, %rd2698, %rd2638;
	mov.b64	{%r3435, %r3436}, %rd2699;
	mov.b64	%rd2700, {%r3436, %r3435};
	add.s64 	%rd2701, %rd2700, %rd2681;
	xor.b64  	%rd2702, %rd2701, %rd2669;
	mov.b64	{%r3437, %r3438}, %rd2702;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2703, {%r3440, %r3439};
	add.s64 	%rd2704, %rd2698, %rd1380;
	add.s64 	%rd2705, %rd2704, %rd2703;
	xor.b64  	%rd2706, %rd2705, %rd2700;
	mov.b64	{%r3441, %r3442}, %rd2706;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2707, {%r3444, %r3443};
	add.s64 	%rd2708, %rd2707, %rd2701;
	xor.b64  	%rd2709, %rd2708, %rd2703;
	mov.b64	{%r1518, %r1519}, %rd2709;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	add.s64 	%rd2710, %rd2664, %rd1389;
	add.s64 	%rd2711, %rd2710, %rd2683;
	xor.b64  	%rd2712, %rd2711, %rd2652;
	mov.b64	{%r3445, %r3446}, %rd2712;
	mov.b64	%rd2713, {%r3446, %r3445};
	add.s64 	%rd2714, %rd2713, %rd2639;
	xor.b64  	%rd2715, %rd2714, %rd2683;
	mov.b64	{%r3447, %r3448}, %rd2715;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2716, {%r3450, %r3449};
	add.s64 	%rd2717, %rd2711, %rd1385;
	add.s64 	%rd2718, %rd2717, %rd2716;
	xor.b64  	%rd2719, %rd2718, %rd2713;
	mov.b64	{%r3451, %r3452}, %rd2719;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2720, {%r3454, %r3453};
	add.s64 	%rd2721, %rd2720, %rd2714;
	xor.b64  	%rd2722, %rd2721, %rd2716;
	mov.b64	{%r1526, %r1527}, %rd2722;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	mov.b64	%rd2723, {%r1521, %r1525};
	add.s64 	%rd2724, %rd2641, %rd1383;
	add.s64 	%rd2725, %rd2724, %rd2678;
	xor.b64  	%rd2726, %rd2725, %rd2666;
	mov.b64	{%r3455, %r3456}, %rd2726;
	mov.b64	%rd2727, {%r3456, %r3455};
	add.s64 	%rd2728, %rd2727, %rd2653;
	xor.b64  	%rd2729, %rd2728, %rd2641;
	mov.b64	{%r3457, %r3458}, %rd2729;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2730, {%r3460, %r3459};
	add.s64 	%rd2731, %rd2725, %rd1381;
	add.s64 	%rd2732, %rd2731, %rd2730;
	xor.b64  	%rd2733, %rd2732, %rd2727;
	mov.b64	{%r3461, %r3462}, %rd2733;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2734, {%r3464, %r3463};
	add.s64 	%rd2735, %rd2734, %rd2728;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r1534, %r1535}, %rd2736;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	xor.b64  	%rd2737, %rd2694, %rd1379;
	xor.b64  	%rd2738, %rd2737, %rd2723;
	mov.b64	{%r3465, %r3466}, %rd2738;
	setp.ne.s32	%p1, %r3466, 0;
	@%p1 bra 	BB23_2;

	ld.param.u64 	%rd2740, [_Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_0];
	cvta.to.global.u64 	%rd2739, %rd2740;
	st.global.u64 	[%rd2739], %rd1;

BB23_2:
	ret;
}


