//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-25769353
// Cuda compilation tools, release 10.1, V10.1.105
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_35
.address_size 64

	// .globl	_Z7init_vmPKvPv
.const .align 1 .b8 blake2b_sigma[192] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 10, 4, 8, 9, 15, 13, 6, 1, 12, 0, 2, 11, 7, 5, 3, 11, 8, 12, 0, 5, 2, 15, 13, 10, 14, 3, 6, 7, 1, 9, 4, 7, 9, 3, 1, 13, 12, 11, 14, 2, 6, 5, 10, 4, 0, 15, 8, 9, 0, 5, 7, 2, 4, 10, 15, 14, 1, 11, 12, 6, 8, 3, 13, 2, 12, 6, 10, 0, 11, 8, 3, 4, 13, 7, 5, 15, 14, 1, 9, 12, 5, 1, 15, 14, 13, 4, 10, 0, 7, 6, 3, 9, 2, 8, 11, 13, 11, 7, 14, 12, 1, 3, 9, 5, 0, 15, 4, 8, 6, 2, 10, 6, 15, 14, 9, 11, 3, 0, 8, 12, 2, 13, 7, 1, 4, 10, 5, 10, 2, 8, 4, 7, 6, 1, 5, 15, 11, 9, 14, 3, 12, 13, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 14, 10, 4, 8, 9, 15, 13, 6, 1, 12, 0, 2, 11, 7, 5, 3};
.const .align 4 .b8 AES_TABLE[8192] = {198, 99, 99, 165, 248, 124, 124, 132, 238, 119, 119, 153, 246, 123, 123, 141, 255, 242, 242, 13, 214, 107, 107, 189, 222, 111, 111, 177, 145, 197, 197, 84, 96, 48, 48, 80, 2, 1, 1, 3, 206, 103, 103, 169, 86, 43, 43, 125, 231, 254, 254, 25, 181, 215, 215, 98, 77, 171, 171, 230, 236, 118, 118, 154, 143, 202, 202, 69, 31, 130, 130, 157, 137, 201, 201, 64, 250, 125, 125, 135, 239, 250, 250, 21, 178, 89, 89, 235, 142, 71, 71, 201, 251, 240, 240, 11, 65, 173, 173, 236, 179, 212, 212, 103, 95, 162, 162, 253, 69, 175, 175, 234, 35, 156, 156, 191, 83, 164, 164, 247, 228, 114, 114, 150, 155, 192, 192, 91, 117, 183, 183, 194, 225, 253, 253, 28, 61, 147, 147, 174, 76, 38, 38, 106, 108, 54, 54, 90, 126, 63, 63, 65, 245, 247, 247, 2, 131, 204, 204, 79, 104, 52, 52, 92, 81, 165, 165, 244, 209, 229, 229, 52, 249, 241, 241, 8, 226, 113, 113, 147, 171, 216, 216, 115, 98, 49, 49, 83, 42, 21, 21, 63, 8, 4, 4, 12, 149, 199, 199, 82, 70, 35, 35, 101, 157, 195, 195, 94, 48, 24, 24, 40, 55, 150, 150, 161, 10, 5, 5, 15, 47, 154, 154, 181, 14, 7, 7, 9, 36, 18, 18, 54, 27, 128, 128, 155, 223, 226, 226, 61, 205, 235, 235, 38, 78, 39, 39, 105, 127, 178, 178, 205, 234, 117, 117, 159, 18, 9, 9, 27, 29, 131, 131, 158, 88, 44, 44, 116, 52, 26, 26, 46, 54, 27, 27, 45, 220, 110, 110, 178, 180, 90, 90, 238, 91, 160, 160, 251, 164, 82, 82, 246, 118, 59, 59, 77, 183, 214, 214, 97, 125, 179, 179, 206, 82, 41, 41, 123, 221, 227, 227, 62, 94, 47, 47, 113, 19, 132, 132, 151, 166, 83, 83, 245, 185, 209, 209, 104, 0, 0, 0, 0, 193, 237, 237, 44, 64, 32, 32, 96, 227, 252, 252, 31, 121, 177, 177, 200, 182, 91, 91, 237, 212, 106, 106, 190, 141, 203, 203, 70, 103, 190, 190, 217, 114, 57, 57, 75, 148, 74, 74, 222, 152, 76, 76, 212, 176, 88, 88, 232, 133, 207, 207, 74, 187, 208, 208, 107, 197, 239, 239, 42, 79, 170, 170, 229, 237, 251, 251, 22, 134, 67, 67, 197, 154, 77, 77, 215, 102, 51, 51, 85, 17, 133, 133, 148, 138, 69, 69, 207, 233, 249, 249, 16, 4, 2, 2, 6, 254, 127, 127, 129, 160, 80, 80, 240, 120, 60, 60, 68, 37, 159, 159, 186, 75, 168, 168, 227, 162, 81, 81, 243, 93, 163, 163, 254, 128, 64, 64, 192, 5, 143, 143, 138, 63, 146, 146, 173, 33, 157, 157, 188, 112, 56, 56, 72, 241, 245, 245, 4, 99, 188, 188, 223, 119, 182, 182, 193, 175, 218, 218, 117, 66, 33, 33, 99, 32, 16, 16, 48, 229, 255, 255, 26, 253, 243, 243, 14, 191, 210, 210, 109, 129, 205, 205, 76, 24, 12, 12, 20, 38, 19, 19, 53, 195, 236, 236, 47, 190, 95, 95, 225, 53, 151, 151, 162, 136, 68, 68, 204, 46, 23, 23, 57, 147, 196, 196, 87, 85, 167, 167, 242, 252, 126, 126, 130, 122, 61, 61, 71, 200, 100, 100, 172, 186, 93, 93, 231, 50, 25, 25, 43, 230, 115, 115, 149, 192, 96, 96, 160, 25, 129, 129, 152, 158, 79, 79, 209, 163, 220, 220, 127, 68, 34, 34, 102, 84, 42, 42, 126, 59, 144, 144, 171, 11, 136, 136, 131, 140, 70, 70, 202, 199, 238, 238, 41, 107, 184, 184, 211, 40, 20, 20, 60, 167, 222, 222, 121, 188, 94, 94, 226, 22, 11, 11, 29, 173, 219, 219, 118, 219, 224, 224, 59, 100, 50, 50, 86, 116, 58, 58, 78, 20, 10, 10, 30, 146, 73, 73, 219, 12, 6, 6, 10, 72, 36, 36, 108, 184, 92, 92, 228, 159, 194, 194, 93, 189, 211, 211, 110, 67, 172, 172, 239, 196, 98, 98, 166, 57, 145, 145, 168, 49, 149, 149, 164, 211, 228, 228, 55, 242, 121, 121, 139, 213, 231, 231, 50, 139, 200, 200, 67, 110, 55, 55, 89, 218, 109, 109, 183, 1, 141, 141, 140, 177, 213, 213, 100, 156, 78, 78, 210, 73, 169, 169, 224, 216, 108, 108, 180, 172, 86, 86, 250, 243, 244, 244, 7, 207, 234, 234, 37, 202, 101, 101, 175, 244, 122, 122, 142, 71, 174, 174, 233, 16, 8, 8, 24, 111, 186, 186, 213, 240, 120, 120, 136, 74, 37, 37, 111, 92, 46, 46, 114, 56, 28, 28, 36, 87, 166, 166, 241, 115, 180, 180, 199, 151, 198, 198, 81, 203, 232, 232, 35, 161, 221, 221, 124, 232, 116, 116, 156, 62, 31, 31, 33, 150, 75, 75, 221, 97, 189, 189, 220, 13, 139, 139, 134, 15, 138, 138, 133, 224, 112, 112, 144, 124, 62, 62, 66, 113, 181, 181, 196, 204, 102, 102, 170, 144, 72, 72, 216, 6, 3, 3, 5, 247, 246, 246, 1, 28, 14, 14, 18, 194, 97, 97, 163, 106, 53, 53, 95, 174, 87, 87, 249, 105, 185, 185, 208, 23, 134, 134, 145, 153, 193, 193, 88, 58, 29, 29, 39, 39, 158, 158, 185, 217, 225, 225, 56, 235, 248, 248, 19, 43, 152, 152, 179, 34, 17, 17, 51, 210, 105, 105, 187, 169, 217, 217, 112, 7, 142, 142, 137, 51, 148, 148, 167, 45, 155, 155, 182, 60, 30, 30, 34, 21, 135, 135, 146, 201, 233, 233, 32, 135, 206, 206, 73, 170, 85, 85, 255, 80, 40, 40, 120, 165, 223, 223, 122, 3, 140, 140, 143, 89, 161, 161, 248, 9, 137, 137, 128, 26, 13, 13, 23, 101, 191, 191, 218, 215, 230, 230, 49, 132, 66, 66, 198, 208, 104, 104, 184, 130, 65, 65, 195, 41, 153, 153, 176, 90, 45, 45, 119, 30, 15, 15, 17, 123, 176, 176, 203, 168, 84, 84, 252, 109, 187, 187, 214, 44, 22, 22, 58, 165, 198, 99, 99, 132, 248, 124, 124, 153, 238, 119, 119, 141, 246, 123, 123, 13, 255, 242, 242, 189, 214, 107, 107, 177, 222, 111, 111, 84, 145, 197, 197, 80, 96, 48, 48, 3, 2, 1, 1, 169, 206, 103, 103, 125, 86, 43, 43, 25, 231, 254, 254, 98, 181, 215, 215, 230, 77, 171, 171, 154, 236, 118, 118, 69, 143, 202, 202, 157, 31, 130, 130, 64, 137, 201, 201, 135, 250, 125, 125, 21, 239, 250, 250, 235, 178, 89, 89, 201, 142, 71, 71, 11, 251, 240, 240, 236, 65, 173, 173, 103, 179, 212, 212, 253, 95, 162, 162, 234, 69, 175, 175, 191, 35, 156, 156, 247, 83, 164, 164, 150, 228, 114, 114, 91, 155, 192, 192, 194, 117, 183, 183, 28, 225, 253, 253, 174, 61, 147, 147, 106, 76, 38, 38, 90, 108, 54, 54, 65, 126, 63, 63, 2, 245, 247, 247, 79, 131, 204, 204, 92, 104, 52, 52, 244, 81, 165, 165, 52, 209, 229, 229, 8, 249, 241, 241, 147, 226, 113, 113, 115, 171, 216, 216, 83, 98, 49, 49, 63, 42, 21, 21, 12, 8, 4, 4, 82, 149, 199, 199, 101, 70, 35, 35, 94, 157, 195, 195, 40, 48, 24, 24, 161, 55, 150, 150, 15, 10, 5, 5, 181, 47, 154, 154, 9, 14, 7, 7, 54, 36, 18, 18, 155, 27, 128, 128, 61, 223, 226, 226, 38, 205, 235, 235, 105, 78, 39, 39, 205, 127, 178, 178, 159, 234, 117, 117, 27, 18, 9, 9, 158, 29, 131, 131, 116, 88, 44, 44, 46, 52, 26, 26, 45, 54, 27, 27, 178, 220, 110, 110, 238, 180, 90, 90, 251, 91, 160, 160, 246, 164, 82, 82, 77, 118, 59, 59, 97, 183, 214, 214, 206, 125, 179, 179, 123, 82, 41, 41, 62, 221, 227, 227, 113, 94, 47, 47, 151, 19, 132, 132, 245, 166, 83, 83, 104, 185, 209, 209, 0, 0, 0, 0, 44, 193, 237, 237, 96, 64, 32, 32, 31, 227, 252, 252, 200, 121, 177, 177, 237, 182, 91, 91, 190, 212, 106, 106, 70, 141, 203, 203, 217, 103, 190, 190, 75, 114, 57, 57, 222, 148, 74, 74, 212, 152, 76, 76, 232, 176, 88, 88, 74, 133, 207, 207, 107, 187, 208, 208, 42, 197, 239, 239, 229, 79, 170, 170, 22, 237, 251, 251, 197, 134, 67, 67, 215, 154, 77, 77, 85, 102, 51, 51, 148, 17, 133, 133, 207, 138, 69, 69, 16, 233, 249, 249, 6, 4, 2, 2, 129, 254, 127, 127, 240, 160, 80, 80, 68, 120, 60, 60, 186, 37, 159, 159, 227, 75, 168, 168, 243, 162, 81, 81, 254, 93, 163, 163, 192, 128, 64, 64, 138, 5, 143, 143, 173, 63, 146, 146, 188, 33, 157, 157, 72, 112, 56, 56, 4, 241, 245, 245, 223, 99, 188, 188, 193, 119, 182, 182, 117, 175, 218, 218, 99, 66, 33, 33, 48, 32, 16, 16, 26, 229, 255, 255, 14, 253, 243, 243, 109, 191, 210, 210, 76, 129, 205, 205, 20, 24, 12, 12, 53, 38, 19, 19, 47, 195, 236, 236, 225, 190, 95, 95, 162, 53, 151, 151, 204, 136, 68, 68, 57, 46, 23, 23, 87, 147, 196, 196, 242, 85, 167, 167, 130, 252, 126, 126, 71, 122, 61, 61, 172, 200, 100, 100, 231, 186, 93, 93, 43, 50, 25, 25, 149, 230, 115, 115, 160, 192, 96, 96, 152, 25, 129, 129, 209, 158, 79, 79, 127, 163, 220, 220, 102, 68, 34, 34, 126, 84, 42, 42, 171, 59, 144, 144, 131, 11, 136, 136, 202, 140, 70, 70, 41, 199, 238, 238, 211, 107, 184, 184, 60, 40, 20, 20, 121, 167, 222, 222, 226, 188, 94, 94, 29, 22, 11, 11, 118, 173, 219, 219, 59, 219, 224, 224, 86, 100, 50, 50, 78, 116, 58, 58, 30, 20, 10, 10, 219, 146, 73, 73, 10, 12, 6, 6, 108, 72, 36, 36, 228, 184, 92, 92, 93, 159, 194, 194, 110, 189, 211, 211, 239, 67, 172, 172, 166, 196, 98, 98, 168, 57, 145, 145, 164, 49, 149, 149, 55, 211, 228, 228, 139, 242, 121, 121, 50, 213, 231, 231, 67, 139, 200, 200, 89, 110, 55, 55, 183, 218, 109, 109, 140, 1, 141, 141, 100, 177, 213, 213, 210, 156, 78, 78, 224, 73, 169, 169, 180, 216, 108, 108, 250, 172, 86, 86, 7, 243, 244, 244, 37, 207, 234, 234, 175, 202, 101, 101, 142, 244, 122, 122, 233, 71, 174, 174, 24, 16, 8, 8, 213, 111, 186, 186, 136, 240, 120, 120, 111, 74, 37, 37, 114, 92, 46, 46, 36, 56, 28, 28, 241, 87, 166, 166, 199, 115, 180, 180, 81, 151, 198, 198, 35, 203, 232, 232, 124, 161, 221, 221, 156, 232, 116, 116, 33, 62, 31, 31, 221, 150, 75, 75, 220, 97, 189, 189, 134, 13, 139, 139, 133, 15, 138, 138, 144, 224, 112, 112, 66, 124, 62, 62, 196, 113, 181, 181, 170, 204, 102, 102, 216, 144, 72, 72, 5, 6, 3, 3, 1, 247, 246, 246, 18, 28, 14, 14, 163, 194, 97, 97, 95, 106, 53, 53, 249, 174, 87, 87, 208, 105, 185, 185, 145, 23, 134, 134, 88, 153, 193, 193, 39, 58, 29, 29, 185, 39, 158, 158, 56, 217, 225, 225, 19, 235, 248, 248, 179, 43, 152, 152, 51, 34, 17, 17, 187, 210, 105, 105, 112, 169, 217, 217, 137, 7, 142, 142, 167, 51, 148, 148, 182, 45, 155, 155, 34, 60, 30, 30, 146, 21, 135, 135, 32, 201, 233, 233, 73, 135, 206, 206, 255, 170, 85, 85, 120, 80, 40, 40, 122, 165, 223, 223, 143, 3, 140, 140, 248, 89, 161, 161, 128, 9, 137, 137, 23, 26, 13, 13, 218, 101, 191, 191, 49, 215, 230, 230, 198, 132, 66, 66, 184, 208, 104, 104, 195, 130, 65, 65, 176, 41, 153, 153, 119, 90, 45, 45, 17, 30, 15, 15, 203, 123, 176, 176, 252, 168, 84, 84, 214, 109, 187, 187, 58, 44, 22, 22, 99, 165, 198, 99, 124, 132, 248, 124, 119, 153, 238, 119, 123, 141, 246, 123, 242, 13, 255, 242, 107, 189, 214, 107, 111, 177, 222, 111, 197, 84, 145, 197, 48, 80, 96, 48, 1, 3, 2, 1, 103, 169, 206, 103, 43, 125, 86, 43, 254, 25, 231, 254, 215, 98, 181, 215, 171, 230, 77, 171, 118, 154, 236, 118, 202, 69, 143, 202, 130, 157, 31, 130, 201, 64, 137, 201, 125, 135, 250, 125, 250, 21, 239, 250, 89, 235, 178, 89, 71, 201, 142, 71, 240, 11, 251, 240, 173, 236, 65, 173, 212, 103, 179, 212, 162, 253, 95, 162, 175, 234, 69, 175, 156, 191, 35, 156, 164, 247, 83, 164, 114, 150, 228, 114, 192, 91, 155, 192, 183, 194, 117, 183, 253, 28, 225, 253, 147, 174, 61, 147, 38, 106, 76, 38, 54, 90, 108, 54, 63, 65, 126, 63, 247, 2, 245, 247, 204, 79, 131, 204, 52, 92, 104, 52, 165, 244, 81, 165, 229, 52, 209, 229, 241, 8, 249, 241, 113, 147, 226, 113, 216, 115, 171, 216, 49, 83, 98, 49, 21, 63, 42, 21, 4, 12, 8, 4, 199, 82, 149, 199, 35, 101, 70, 35, 195, 94, 157, 195, 24, 40, 48, 24, 150, 161, 55, 150, 5, 15, 10, 5, 154, 181, 47, 154, 7, 9, 14, 7, 18, 54, 36, 18, 128, 155, 27, 128, 226, 61, 223, 226, 235, 38, 205, 235, 39, 105, 78, 39, 178, 205, 127, 178, 117, 159, 234, 117, 9, 27, 18, 9, 131, 158, 29, 131, 44, 116, 88, 44, 26, 46, 52, 26, 27, 45, 54, 27, 110, 178, 220, 110, 90, 238, 180, 90, 160, 251, 91, 160, 82, 246, 164, 82, 59, 77, 118, 59, 214, 97, 183, 214, 179, 206, 125, 179, 41, 123, 82, 41, 227, 62, 221, 227, 47, 113, 94, 47, 132, 151, 19, 132, 83, 245, 166, 83, 209, 104, 185, 209, 0, 0, 0, 0, 237, 44, 193, 237, 32, 96, 64, 32, 252, 31, 227, 252, 177, 200, 121, 177, 91, 237, 182, 91, 106, 190, 212, 106, 203, 70, 141, 203, 190, 217, 103, 190, 57, 75, 114, 57, 74, 222, 148, 74, 76, 212, 152, 76, 88, 232, 176, 88, 207, 74, 133, 207, 208, 107, 187, 208, 239, 42, 197, 239, 170, 229, 79, 170, 251, 22, 237, 251, 67, 197, 134, 67, 77, 215, 154, 77, 51, 85, 102, 51, 133, 148, 17, 133, 69, 207, 138, 69, 249, 16, 233, 249, 2, 6, 4, 2, 127, 129, 254, 127, 80, 240, 160, 80, 60, 68, 120, 60, 159, 186, 37, 159, 168, 227, 75, 168, 81, 243, 162, 81, 163, 254, 93, 163, 64, 192, 128, 64, 143, 138, 5, 143, 146, 173, 63, 146, 157, 188, 33, 157, 56, 72, 112, 56, 245, 4, 241, 245, 188, 223, 99, 188, 182, 193, 119, 182, 218, 117, 175, 218, 33, 99, 66, 33, 16, 48, 32, 16, 255, 26, 229, 255, 243, 14, 253, 243, 210, 109, 191, 210, 205, 76, 129, 205, 12, 20, 24, 12, 19, 53, 38, 19, 236, 47, 195, 236, 95, 225, 190, 95, 151, 162, 53, 151, 68, 204, 136, 68, 23, 57, 46, 23, 196, 87, 147, 196, 167, 242, 85, 167, 126, 130, 252, 126, 61, 71, 122, 61, 100, 172, 200, 100, 93, 231, 186, 93, 25, 43, 50, 25, 115, 149, 230, 115, 96, 160, 192, 96, 129, 152, 25, 129, 79, 209, 158, 79, 220, 127, 163, 220, 34, 102, 68, 34, 42, 126, 84, 42, 144, 171, 59, 144, 136, 131, 11, 136, 70, 202, 140, 70, 238, 41, 199, 238, 184, 211, 107, 184, 20, 60, 40, 20, 222, 121, 167, 222, 94, 226, 188, 94, 11, 29, 22, 11, 219, 118, 173, 219, 224, 59, 219, 224, 50, 86, 100, 50, 58, 78, 116, 58, 10, 30, 20, 10, 73, 219, 146, 73, 6, 10, 12, 6, 36, 108, 72, 36, 92, 228, 184, 92, 194, 93, 159, 194, 211, 110, 189, 211, 172, 239, 67, 172, 98, 166, 196, 98, 145, 168, 57, 145, 149, 164, 49, 149, 228, 55, 211, 228, 121, 139, 242, 121, 231, 50, 213, 231, 200, 67, 139, 200, 55, 89, 110, 55, 109, 183, 218, 109, 141, 140, 1, 141, 213, 100, 177, 213, 78, 210, 156, 78, 169, 224, 73, 169, 108, 180, 216, 108, 86, 250, 172, 86, 244, 7, 243, 244, 234, 37, 207, 234, 101, 175, 202, 101, 122, 142, 244, 122, 174, 233, 71, 174, 8, 24, 16, 8, 186, 213, 111, 186, 120, 136, 240, 120, 37, 111, 74, 37, 46, 114, 92, 46, 28, 36, 56, 28, 166, 241, 87, 166, 180, 199, 115, 180, 198, 81, 151, 198, 232, 35, 203, 232, 221, 124, 161, 221, 116, 156, 232, 116, 31, 33, 62, 31, 75, 221, 150, 75, 189, 220, 97, 189, 139, 134, 13, 139, 138, 133, 15, 138, 112, 144, 224, 112, 62, 66, 124, 62, 181, 196, 113, 181, 102, 170, 204, 102, 72, 216, 144, 72, 3, 5, 6, 3, 246, 1, 247, 246, 14, 18, 28, 14, 97, 163, 194, 97, 53, 95, 106, 53, 87, 249, 174, 87, 185, 208, 105, 185, 134, 145, 23, 134, 193, 88, 153, 193, 29, 39, 58, 29, 158, 185, 39, 158, 225, 56, 217, 225, 248, 19, 235, 248, 152, 179, 43, 152, 17, 51, 34, 17, 105, 187, 210, 105, 217, 112, 169, 217, 142, 137, 7, 142, 148, 167, 51, 148, 155, 182, 45, 155, 30, 34, 60, 30, 135, 146, 21, 135, 233, 32, 201, 233, 206, 73, 135, 206, 85, 255, 170, 85, 40, 120, 80, 40, 223, 122, 165, 223, 140, 143, 3, 140, 161, 248, 89, 161, 137, 128, 9, 137, 13, 23, 26, 13, 191, 218, 101, 191, 230, 49, 215, 230, 66, 198, 132, 66, 104, 184, 208, 104, 65, 195, 130, 65, 153, 176, 41, 153, 45, 119, 90, 45, 15, 17, 30, 15, 176, 203, 123, 176, 84, 252, 168, 84, 187, 214, 109, 187, 22, 58, 44, 22, 99, 99, 165, 198, 124, 124, 132, 248, 119, 119, 153, 238, 123, 123, 141, 246, 242, 242, 13, 255, 107, 107, 189, 214, 111, 111, 177, 222, 197, 197, 84, 145, 48, 48, 80, 96, 1, 1, 3, 2, 103, 103, 169, 206, 43, 43, 125, 86, 254, 254, 25, 231, 215, 215, 98, 181, 171, 171, 230, 77, 118, 118, 154, 236, 202, 202, 69, 143, 130, 130, 157, 31, 201, 201, 64, 137, 125, 125, 135, 250, 250, 250, 21, 239, 89, 89, 235, 178, 71, 71, 201, 142, 240, 240, 11, 251, 173, 173, 236, 65, 212, 212, 103, 179, 162, 162, 253, 95, 175, 175, 234, 69, 156, 156, 191, 35, 164, 164, 247, 83, 114, 114, 150, 228, 192, 192, 91, 155, 183, 183, 194, 117, 253, 253, 28, 225, 147, 147, 174, 61, 38, 38, 106, 76, 54, 54, 90, 108, 63, 63, 65, 126, 247, 247, 2, 245, 204, 204, 79, 131, 52, 52, 92, 104, 165, 165, 244, 81, 229, 229, 52, 209, 241, 241, 8, 249, 113, 113, 147, 226, 216, 216, 115, 171, 49, 49, 83, 98, 21, 21, 63, 42, 4, 4, 12, 8, 199, 199, 82, 149, 35, 35, 101, 70, 195, 195, 94, 157, 24, 24, 40, 48, 150, 150, 161, 55, 5, 5, 15, 10, 154, 154, 181, 47, 7, 7, 9, 14, 18, 18, 54, 36, 128, 128, 155, 27, 226, 226, 61, 223, 235, 235, 38, 205, 39, 39, 105, 78, 178, 178, 205, 127, 117, 117, 159, 234, 9, 9, 27, 18, 131, 131, 158, 29, 44, 44, 116, 88, 26, 26, 46, 52, 27, 27, 45, 54, 110, 110, 178, 220, 90, 90, 238, 180, 160, 160, 251, 91, 82, 82, 246, 164, 59, 59, 77, 118, 214, 214, 97, 183, 179, 179, 206, 125, 41, 41, 123, 82, 227, 227, 62, 221, 47, 47, 113, 94, 132, 132, 151, 19, 83, 83, 245, 166, 209, 209, 104, 185, 0, 0, 0, 0, 237, 237, 44, 193, 32, 32, 96, 64, 252, 252, 31, 227, 177, 177, 200, 121, 91, 91, 237, 182, 106, 106, 190, 212, 203, 203, 70, 141, 190, 190, 217, 103, 57, 57, 75, 114, 74, 74, 222, 148, 76, 76, 212, 152, 88, 88, 232, 176, 207, 207, 74, 133, 208, 208, 107, 187, 239, 239, 42, 197, 170, 170, 229, 79, 251, 251, 22, 237, 67, 67, 197, 134, 77, 77, 215, 154, 51, 51, 85, 102, 133, 133, 148, 17, 69, 69, 207, 138, 249, 249, 16, 233, 2, 2, 6, 4, 127, 127, 129, 254, 80, 80, 240, 160, 60, 60, 68, 120, 159, 159, 186, 37, 168, 168, 227, 75, 81, 81, 243, 162, 163, 163, 254, 93, 64, 64, 192, 128, 143, 143, 138, 5, 146, 146, 173, 63, 157, 157, 188, 33, 56, 56, 72, 112, 245, 245, 4, 241, 188, 188, 223, 99, 182, 182, 193, 119, 218, 218, 117, 175, 33, 33, 99, 66, 16, 16, 48, 32, 255, 255, 26, 229, 243, 243, 14, 253, 210, 210, 109, 191, 205, 205, 76, 129, 12, 12, 20, 24, 19, 19, 53, 38, 236, 236, 47, 195, 95, 95, 225, 190, 151, 151, 162, 53, 68, 68, 204, 136, 23, 23, 57, 46, 196, 196, 87, 147, 167, 167, 242, 85, 126, 126, 130, 252, 61, 61, 71, 122, 100, 100, 172, 200, 93, 93, 231, 186, 25, 25, 43, 50, 115, 115, 149, 230, 96, 96, 160, 192, 129, 129, 152, 25, 79, 79, 209, 158, 220, 220, 127, 163, 34, 34, 102, 68, 42, 42, 126, 84, 144, 144, 171, 59, 136, 136, 131, 11, 70, 70, 202, 140, 238, 238, 41, 199, 184, 184, 211, 107, 20, 20, 60, 40, 222, 222, 121, 167, 94, 94, 226, 188, 11, 11, 29, 22, 219, 219, 118, 173, 224, 224, 59, 219, 50, 50, 86, 100, 58, 58, 78, 116, 10, 10, 30, 20, 73, 73, 219, 146, 6, 6, 10, 12, 36, 36, 108, 72, 92, 92, 228, 184, 194, 194, 93, 159, 211, 211, 110, 189, 172, 172, 239, 67, 98, 98, 166, 196, 145, 145, 168, 57, 149, 149, 164, 49, 228, 228, 55, 211, 121, 121, 139, 242, 231, 231, 50, 213, 200, 200, 67, 139, 55, 55, 89, 110, 109, 109, 183, 218, 141, 141, 140, 1, 213, 213, 100, 177, 78, 78, 210, 156, 169, 169, 224, 73, 108, 108, 180, 216, 86, 86, 250, 172, 244, 244, 7, 243, 234, 234, 37, 207, 101, 101, 175, 202, 122, 122, 142, 244, 174, 174, 233, 71, 8, 8, 24, 16, 186, 186, 213, 111, 120, 120, 136, 240, 37, 37, 111, 74, 46, 46, 114, 92, 28, 28, 36, 56, 166, 166, 241, 87, 180, 180, 199, 115, 198, 198, 81, 151, 232, 232, 35, 203, 221, 221, 124, 161, 116, 116, 156, 232, 31, 31, 33, 62, 75, 75, 221, 150, 189, 189, 220, 97, 139, 139, 134, 13, 138, 138, 133, 15, 112, 112, 144, 224, 62, 62, 66, 124, 181, 181, 196, 113, 102, 102, 170, 204, 72, 72, 216, 144, 3, 3, 5, 6, 246, 246, 1, 247, 14, 14, 18, 28, 97, 97, 163, 194, 53, 53, 95, 106, 87, 87, 249, 174, 185, 185, 208, 105, 134, 134, 145, 23, 193, 193, 88, 153, 29, 29, 39, 58, 158, 158, 185, 39, 225, 225, 56, 217, 248, 248, 19, 235, 152, 152, 179, 43, 17, 17, 51, 34, 105, 105, 187, 210, 217, 217, 112, 169, 142, 142, 137, 7, 148, 148, 167, 51, 155, 155, 182, 45, 30, 30, 34, 60, 135, 135, 146, 21, 233, 233, 32, 201, 206, 206, 73, 135, 85, 85, 255, 170, 40, 40, 120, 80, 223, 223, 122, 165, 140, 140, 143, 3, 161, 161, 248, 89, 137, 137, 128, 9, 13, 13, 23, 26, 191, 191, 218, 101, 230, 230, 49, 215, 66, 66, 198, 132, 104, 104, 184, 208, 65, 65, 195, 130, 153, 153, 176, 41, 45, 45, 119, 90, 15, 15, 17, 30, 176, 176, 203, 123, 84, 84, 252, 168, 187, 187, 214, 109, 22, 22, 58, 44, 81, 244, 167, 80, 126, 65, 101, 83, 26, 23, 164, 195, 58, 39, 94, 150, 59, 171, 107, 203, 31, 157, 69, 241, 172, 250, 88, 171, 75, 227, 3, 147, 32, 48, 250, 85, 173, 118, 109, 246, 136, 204, 118, 145, 245, 2, 76, 37, 79, 229, 215, 252, 197, 42, 203, 215, 38, 53, 68, 128, 181, 98, 163, 143, 222, 177, 90, 73, 37, 186, 27, 103, 69, 234, 14, 152, 93, 254, 192, 225, 195, 47, 117, 2, 129, 76, 240, 18, 141, 70, 151, 163, 107, 211, 249, 198, 3, 143, 95, 231, 21, 146, 156, 149, 191, 109, 122, 235, 149, 82, 89, 218, 212, 190, 131, 45, 88, 116, 33, 211, 73, 224, 105, 41, 142, 201, 200, 68, 117, 194, 137, 106, 244, 142, 121, 120, 153, 88, 62, 107, 39, 185, 113, 221, 190, 225, 79, 182, 240, 136, 173, 23, 201, 32, 172, 102, 125, 206, 58, 180, 99, 223, 74, 24, 229, 26, 49, 130, 151, 81, 51, 96, 98, 83, 127, 69, 177, 100, 119, 224, 187, 107, 174, 132, 254, 129, 160, 28, 249, 8, 43, 148, 112, 72, 104, 88, 143, 69, 253, 25, 148, 222, 108, 135, 82, 123, 248, 183, 171, 115, 211, 35, 114, 75, 2, 226, 227, 31, 143, 87, 102, 85, 171, 42, 178, 235, 40, 7, 47, 181, 194, 3, 134, 197, 123, 154, 211, 55, 8, 165, 48, 40, 135, 242, 35, 191, 165, 178, 2, 3, 106, 186, 237, 22, 130, 92, 138, 207, 28, 43, 167, 121, 180, 146, 243, 7, 242, 240, 78, 105, 226, 161, 101, 218, 244, 205, 6, 5, 190, 213, 209, 52, 98, 31, 196, 166, 254, 138, 52, 46, 83, 157, 162, 243, 85, 160, 5, 138, 225, 50, 164, 246, 235, 117, 11, 131, 236, 57, 64, 96, 239, 170, 94, 113, 159, 6, 189, 110, 16, 81, 62, 33, 138, 249, 150, 221, 6, 61, 221, 62, 5, 174, 77, 230, 189, 70, 145, 84, 141, 181, 113, 196, 93, 5, 4, 6, 212, 111, 96, 80, 21, 255, 25, 152, 251, 36, 214, 189, 233, 151, 137, 64, 67, 204, 103, 217, 158, 119, 176, 232, 66, 189, 7, 137, 139, 136, 231, 25, 91, 56, 121, 200, 238, 219, 161, 124, 10, 71, 124, 66, 15, 233, 248, 132, 30, 201, 0, 0, 0, 0, 9, 128, 134, 131, 50, 43, 237, 72, 30, 17, 112, 172, 108, 90, 114, 78, 253, 14, 255, 251, 15, 133, 56, 86, 61, 174, 213, 30, 54, 45, 57, 39, 10, 15, 217, 100, 104, 92, 166, 33, 155, 91, 84, 209, 36, 54, 46, 58, 12, 10, 103, 177, 147, 87, 231, 15, 180, 238, 150, 210, 27, 155, 145, 158, 128, 192, 197, 79, 97, 220, 32, 162, 90, 119, 75, 105, 28, 18, 26, 22, 226, 147, 186, 10, 192, 160, 42, 229, 60, 34, 224, 67, 18, 27, 23, 29, 14, 9, 13, 11, 242, 139, 199, 173, 45, 182, 168, 185, 20, 30, 169, 200, 87, 241, 25, 133, 175, 117, 7, 76, 238, 153, 221, 187, 163, 127, 96, 253, 247, 1, 38, 159, 92, 114, 245, 188, 68, 102, 59, 197, 91, 251, 126, 52, 139, 67, 41, 118, 203, 35, 198, 220, 182, 237, 252, 104, 184, 228, 241, 99, 215, 49, 220, 202, 66, 99, 133, 16, 19, 151, 34, 64, 132, 198, 17, 32, 133, 74, 36, 125, 210, 187, 61, 248, 174, 249, 50, 17, 199, 41, 161, 109, 29, 158, 47, 75, 220, 178, 48, 243, 13, 134, 82, 236, 119, 193, 227, 208, 43, 179, 22, 108, 169, 112, 185, 153, 17, 148, 72, 250, 71, 233, 100, 34, 168, 252, 140, 196, 160, 240, 63, 26, 86, 125, 44, 216, 34, 51, 144, 239, 135, 73, 78, 199, 217, 56, 209, 193, 140, 202, 162, 254, 152, 212, 11, 54, 166, 245, 129, 207, 165, 122, 222, 40, 218, 183, 142, 38, 63, 173, 191, 164, 44, 58, 157, 228, 80, 120, 146, 13, 106, 95, 204, 155, 84, 126, 70, 98, 246, 141, 19, 194, 144, 216, 184, 232, 46, 57, 247, 94, 130, 195, 175, 245, 159, 93, 128, 190, 105, 208, 147, 124, 111, 213, 45, 169, 207, 37, 18, 179, 200, 172, 153, 59, 16, 24, 125, 167, 232, 156, 99, 110, 219, 59, 187, 123, 205, 38, 120, 9, 110, 89, 24, 244, 236, 154, 183, 1, 131, 79, 154, 168, 230, 149, 110, 101, 170, 255, 230, 126, 33, 188, 207, 8, 239, 21, 232, 230, 186, 231, 155, 217, 74, 111, 54, 206, 234, 159, 9, 212, 41, 176, 124, 214, 49, 164, 178, 175, 42, 63, 35, 49, 198, 165, 148, 48, 53, 162, 102, 192, 116, 78, 188, 55, 252, 130, 202, 166, 224, 144, 208, 176, 51, 167, 216, 21, 241, 4, 152, 74, 65, 236, 218, 247, 127, 205, 80, 14, 23, 145, 246, 47, 118, 77, 214, 141, 67, 239, 176, 77, 204, 170, 77, 84, 228, 150, 4, 223, 158, 209, 181, 227, 76, 106, 136, 27, 193, 44, 31, 184, 70, 101, 81, 127, 157, 94, 234, 4, 1, 140, 53, 93, 250, 135, 116, 115, 251, 11, 65, 46, 179, 103, 29, 90, 146, 219, 210, 82, 233, 16, 86, 51, 109, 214, 71, 19, 154, 215, 97, 140, 55, 161, 12, 122, 89, 248, 20, 142, 235, 19, 60, 137, 206, 169, 39, 238, 183, 97, 201, 53, 225, 28, 229, 237, 122, 71, 177, 60, 156, 210, 223, 89, 85, 242, 115, 63, 24, 20, 206, 121, 115, 199, 55, 191, 83, 247, 205, 234, 95, 253, 170, 91, 223, 61, 111, 20, 120, 68, 219, 134, 202, 175, 243, 129, 185, 104, 196, 62, 56, 36, 52, 44, 194, 163, 64, 95, 22, 29, 195, 114, 188, 226, 37, 12, 40, 60, 73, 139, 255, 13, 149, 65, 57, 168, 1, 113, 8, 12, 179, 222, 216, 180, 228, 156, 100, 86, 193, 144, 123, 203, 132, 97, 213, 50, 182, 112, 72, 108, 92, 116, 208, 184, 87, 66, 80, 81, 244, 167, 83, 126, 65, 101, 195, 26, 23, 164, 150, 58, 39, 94, 203, 59, 171, 107, 241, 31, 157, 69, 171, 172, 250, 88, 147, 75, 227, 3, 85, 32, 48, 250, 246, 173, 118, 109, 145, 136, 204, 118, 37, 245, 2, 76, 252, 79, 229, 215, 215, 197, 42, 203, 128, 38, 53, 68, 143, 181, 98, 163, 73, 222, 177, 90, 103, 37, 186, 27, 152, 69, 234, 14, 225, 93, 254, 192, 2, 195, 47, 117, 18, 129, 76, 240, 163, 141, 70, 151, 198, 107, 211, 249, 231, 3, 143, 95, 149, 21, 146, 156, 235, 191, 109, 122, 218, 149, 82, 89, 45, 212, 190, 131, 211, 88, 116, 33, 41, 73, 224, 105, 68, 142, 201, 200, 106, 117, 194, 137, 120, 244, 142, 121, 107, 153, 88, 62, 221, 39, 185, 113, 182, 190, 225, 79, 23, 240, 136, 173, 102, 201, 32, 172, 180, 125, 206, 58, 24, 99, 223, 74, 130, 229, 26, 49, 96, 151, 81, 51, 69, 98, 83, 127, 224, 177, 100, 119, 132, 187, 107, 174, 28, 254, 129, 160, 148, 249, 8, 43, 88, 112, 72, 104, 25, 143, 69, 253, 135, 148, 222, 108, 183, 82, 123, 248, 35, 171, 115, 211, 226, 114, 75, 2, 87, 227, 31, 143, 42, 102, 85, 171, 7, 178, 235, 40, 3, 47, 181, 194, 154, 134, 197, 123, 165, 211, 55, 8, 242, 48, 40, 135, 178, 35, 191, 165, 186, 2, 3, 106, 92, 237, 22, 130, 43, 138, 207, 28, 146, 167, 121, 180, 240, 243, 7, 242, 161, 78, 105, 226, 205, 101, 218, 244, 213, 6, 5, 190, 31, 209, 52, 98, 138, 196, 166, 254, 157, 52, 46, 83, 160, 162, 243, 85, 50, 5, 138, 225, 117, 164, 246, 235, 57, 11, 131, 236, 170, 64, 96, 239, 6, 94, 113, 159, 81, 189, 110, 16, 249, 62, 33, 138, 61, 150, 221, 6, 174, 221, 62, 5, 70, 77, 230, 189, 181, 145, 84, 141, 5, 113, 196, 93, 111, 4, 6, 212, 255, 96, 80, 21, 36, 25, 152, 251, 151, 214, 189, 233, 204, 137, 64, 67, 119, 103, 217, 158, 189, 176, 232, 66, 136, 7, 137, 139, 56, 231, 25, 91, 219, 121, 200, 238, 71, 161, 124, 10, 233, 124, 66, 15, 201, 248, 132, 30, 0, 0, 0, 0, 131, 9, 128, 134, 72, 50, 43, 237, 172, 30, 17, 112, 78, 108, 90, 114, 251, 253, 14, 255, 86, 15, 133, 56, 30, 61, 174, 213, 39, 54, 45, 57, 100, 10, 15, 217, 33, 104, 92, 166, 209, 155, 91, 84, 58, 36, 54, 46, 177, 12, 10, 103, 15, 147, 87, 231, 210, 180, 238, 150, 158, 27, 155, 145, 79, 128, 192, 197, 162, 97, 220, 32, 105, 90, 119, 75, 22, 28, 18, 26, 10, 226, 147, 186, 229, 192, 160, 42, 67, 60, 34, 224, 29, 18, 27, 23, 11, 14, 9, 13, 173, 242, 139, 199, 185, 45, 182, 168, 200, 20, 30, 169, 133, 87, 241, 25, 76, 175, 117, 7, 187, 238, 153, 221, 253, 163, 127, 96, 159, 247, 1, 38, 188, 92, 114, 245, 197, 68, 102, 59, 52, 91, 251, 126, 118, 139, 67, 41, 220, 203, 35, 198, 104, 182, 237, 252, 99, 184, 228, 241, 202, 215, 49, 220, 16, 66, 99, 133, 64, 19, 151, 34, 32, 132, 198, 17, 125, 133, 74, 36, 248, 210, 187, 61, 17, 174, 249, 50, 109, 199, 41, 161, 75, 29, 158, 47, 243, 220, 178, 48, 236, 13, 134, 82, 208, 119, 193, 227, 108, 43, 179, 22, 153, 169, 112, 185, 250, 17, 148, 72, 34, 71, 233, 100, 196, 168, 252, 140, 26, 160, 240, 63, 216, 86, 125, 44, 239, 34, 51, 144, 199, 135, 73, 78, 193, 217, 56, 209, 254, 140, 202, 162, 54, 152, 212, 11, 207, 166, 245, 129, 40, 165, 122, 222, 38, 218, 183, 142, 164, 63, 173, 191, 228, 44, 58, 157, 13, 80, 120, 146, 155, 106, 95, 204, 98, 84, 126, 70, 194, 246, 141, 19, 232, 144, 216, 184, 94, 46, 57, 247, 245, 130, 195, 175, 190, 159, 93, 128, 124, 105, 208, 147, 169, 111, 213, 45, 179, 207, 37, 18, 59, 200, 172, 153, 167, 16, 24, 125, 110, 232, 156, 99, 123, 219, 59, 187, 9, 205, 38, 120, 244, 110, 89, 24, 1, 236, 154, 183, 168, 131, 79, 154, 101, 230, 149, 110, 126, 170, 255, 230, 8, 33, 188, 207, 230, 239, 21, 232, 217, 186, 231, 155, 206, 74, 111, 54, 212, 234, 159, 9, 214, 41, 176, 124, 175, 49, 164, 178, 49, 42, 63, 35, 48, 198, 165, 148, 192, 53, 162, 102, 55, 116, 78, 188, 166, 252, 130, 202, 176, 224, 144, 208, 21, 51, 167, 216, 74, 241, 4, 152, 247, 65, 236, 218, 14, 127, 205, 80, 47, 23, 145, 246, 141, 118, 77, 214, 77, 67, 239, 176, 84, 204, 170, 77, 223, 228, 150, 4, 227, 158, 209, 181, 27, 76, 106, 136, 184, 193, 44, 31, 127, 70, 101, 81, 4, 157, 94, 234, 93, 1, 140, 53, 115, 250, 135, 116, 46, 251, 11, 65, 90, 179, 103, 29, 82, 146, 219, 210, 51, 233, 16, 86, 19, 109, 214, 71, 140, 154, 215, 97, 122, 55, 161, 12, 142, 89, 248, 20, 137, 235, 19, 60, 238, 206, 169, 39, 53, 183, 97, 201, 237, 225, 28, 229, 60, 122, 71, 177, 89, 156, 210, 223, 63, 85, 242, 115, 121, 24, 20, 206, 191, 115, 199, 55, 234, 83, 247, 205, 91, 95, 253, 170, 20, 223, 61, 111, 134, 120, 68, 219, 129, 202, 175, 243, 62, 185, 104, 196, 44, 56, 36, 52, 95, 194, 163, 64, 114, 22, 29, 195, 12, 188, 226, 37, 139, 40, 60, 73, 65, 255, 13, 149, 113, 57, 168, 1, 222, 8, 12, 179, 156, 216, 180, 228, 144, 100, 86, 193, 97, 123, 203, 132, 112, 213, 50, 182, 116, 72, 108, 92, 66, 208, 184, 87, 167, 80, 81, 244, 101, 83, 126, 65, 164, 195, 26, 23, 94, 150, 58, 39, 107, 203, 59, 171, 69, 241, 31, 157, 88, 171, 172, 250, 3, 147, 75, 227, 250, 85, 32, 48, 109, 246, 173, 118, 118, 145, 136, 204, 76, 37, 245, 2, 215, 252, 79, 229, 203, 215, 197, 42, 68, 128, 38, 53, 163, 143, 181, 98, 90, 73, 222, 177, 27, 103, 37, 186, 14, 152, 69, 234, 192, 225, 93, 254, 117, 2, 195, 47, 240, 18, 129, 76, 151, 163, 141, 70, 249, 198, 107, 211, 95, 231, 3, 143, 156, 149, 21, 146, 122, 235, 191, 109, 89, 218, 149, 82, 131, 45, 212, 190, 33, 211, 88, 116, 105, 41, 73, 224, 200, 68, 142, 201, 137, 106, 117, 194, 121, 120, 244, 142, 62, 107, 153, 88, 113, 221, 39, 185, 79, 182, 190, 225, 173, 23, 240, 136, 172, 102, 201, 32, 58, 180, 125, 206, 74, 24, 99, 223, 49, 130, 229, 26, 51, 96, 151, 81, 127, 69, 98, 83, 119, 224, 177, 100, 174, 132, 187, 107, 160, 28, 254, 129, 43, 148, 249, 8, 104, 88, 112, 72, 253, 25, 143, 69, 108, 135, 148, 222, 248, 183, 82, 123, 211, 35, 171, 115, 2, 226, 114, 75, 143, 87, 227, 31, 171, 42, 102, 85, 40, 7, 178, 235, 194, 3, 47, 181, 123, 154, 134, 197, 8, 165, 211, 55, 135, 242, 48, 40, 165, 178, 35, 191, 106, 186, 2, 3, 130, 92, 237, 22, 28, 43, 138, 207, 180, 146, 167, 121, 242, 240, 243, 7, 226, 161, 78, 105, 244, 205, 101, 218, 190, 213, 6, 5, 98, 31, 209, 52, 254, 138, 196, 166, 83, 157, 52, 46, 85, 160, 162, 243, 225, 50, 5, 138, 235, 117, 164, 246, 236, 57, 11, 131, 239, 170, 64, 96, 159, 6, 94, 113, 16, 81, 189, 110, 138, 249, 62, 33, 6, 61, 150, 221, 5, 174, 221, 62, 189, 70, 77, 230, 141, 181, 145, 84, 93, 5, 113, 196, 212, 111, 4, 6, 21, 255, 96, 80, 251, 36, 25, 152, 233, 151, 214, 189, 67, 204, 137, 64, 158, 119, 103, 217, 66, 189, 176, 232, 139, 136, 7, 137, 91, 56, 231, 25, 238, 219, 121, 200, 10, 71, 161, 124, 15, 233, 124, 66, 30, 201, 248, 132, 0, 0, 0, 0, 134, 131, 9, 128, 237, 72, 50, 43, 112, 172, 30, 17, 114, 78, 108, 90, 255, 251, 253, 14, 56, 86, 15, 133, 213, 30, 61, 174, 57, 39, 54, 45, 217, 100, 10, 15, 166, 33, 104, 92, 84, 209, 155, 91, 46, 58, 36, 54, 103, 177, 12, 10, 231, 15, 147, 87, 150, 210, 180, 238, 145, 158, 27, 155, 197, 79, 128, 192, 32, 162, 97, 220, 75, 105, 90, 119, 26, 22, 28, 18, 186, 10, 226, 147, 42, 229, 192, 160, 224, 67, 60, 34, 23, 29, 18, 27, 13, 11, 14, 9, 199, 173, 242, 139, 168, 185, 45, 182, 169, 200, 20, 30, 25, 133, 87, 241, 7, 76, 175, 117, 221, 187, 238, 153, 96, 253, 163, 127, 38, 159, 247, 1, 245, 188, 92, 114, 59, 197, 68, 102, 126, 52, 91, 251, 41, 118, 139, 67, 198, 220, 203, 35, 252, 104, 182, 237, 241, 99, 184, 228, 220, 202, 215, 49, 133, 16, 66, 99, 34, 64, 19, 151, 17, 32, 132, 198, 36, 125, 133, 74, 61, 248, 210, 187, 50, 17, 174, 249, 161, 109, 199, 41, 47, 75, 29, 158, 48, 243, 220, 178, 82, 236, 13, 134, 227, 208, 119, 193, 22, 108, 43, 179, 185, 153, 169, 112, 72, 250, 17, 148, 100, 34, 71, 233, 140, 196, 168, 252, 63, 26, 160, 240, 44, 216, 86, 125, 144, 239, 34, 51, 78, 199, 135, 73, 209, 193, 217, 56, 162, 254, 140, 202, 11, 54, 152, 212, 129, 207, 166, 245, 222, 40, 165, 122, 142, 38, 218, 183, 191, 164, 63, 173, 157, 228, 44, 58, 146, 13, 80, 120, 204, 155, 106, 95, 70, 98, 84, 126, 19, 194, 246, 141, 184, 232, 144, 216, 247, 94, 46, 57, 175, 245, 130, 195, 128, 190, 159, 93, 147, 124, 105, 208, 45, 169, 111, 213, 18, 179, 207, 37, 153, 59, 200, 172, 125, 167, 16, 24, 99, 110, 232, 156, 187, 123, 219, 59, 120, 9, 205, 38, 24, 244, 110, 89, 183, 1, 236, 154, 154, 168, 131, 79, 110, 101, 230, 149, 230, 126, 170, 255, 207, 8, 33, 188, 232, 230, 239, 21, 155, 217, 186, 231, 54, 206, 74, 111, 9, 212, 234, 159, 124, 214, 41, 176, 178, 175, 49, 164, 35, 49, 42, 63, 148, 48, 198, 165, 102, 192, 53, 162, 188, 55, 116, 78, 202, 166, 252, 130, 208, 176, 224, 144, 216, 21, 51, 167, 152, 74, 241, 4, 218, 247, 65, 236, 80, 14, 127, 205, 246, 47, 23, 145, 214, 141, 118, 77, 176, 77, 67, 239, 77, 84, 204, 170, 4, 223, 228, 150, 181, 227, 158, 209, 136, 27, 76, 106, 31, 184, 193, 44, 81, 127, 70, 101, 234, 4, 157, 94, 53, 93, 1, 140, 116, 115, 250, 135, 65, 46, 251, 11, 29, 90, 179, 103, 210, 82, 146, 219, 86, 51, 233, 16, 71, 19, 109, 214, 97, 140, 154, 215, 12, 122, 55, 161, 20, 142, 89, 248, 60, 137, 235, 19, 39, 238, 206, 169, 201, 53, 183, 97, 229, 237, 225, 28, 177, 60, 122, 71, 223, 89, 156, 210, 115, 63, 85, 242, 206, 121, 24, 20, 55, 191, 115, 199, 205, 234, 83, 247, 170, 91, 95, 253, 111, 20, 223, 61, 219, 134, 120, 68, 243, 129, 202, 175, 196, 62, 185, 104, 52, 44, 56, 36, 64, 95, 194, 163, 195, 114, 22, 29, 37, 12, 188, 226, 73, 139, 40, 60, 149, 65, 255, 13, 1, 113, 57, 168, 179, 222, 8, 12, 228, 156, 216, 180, 193, 144, 100, 86, 132, 97, 123, 203, 182, 112, 213, 50, 92, 116, 72, 108, 87, 66, 208, 184, 244, 167, 80, 81, 65, 101, 83, 126, 23, 164, 195, 26, 39, 94, 150, 58, 171, 107, 203, 59, 157, 69, 241, 31, 250, 88, 171, 172, 227, 3, 147, 75, 48, 250, 85, 32, 118, 109, 246, 173, 204, 118, 145, 136, 2, 76, 37, 245, 229, 215, 252, 79, 42, 203, 215, 197, 53, 68, 128, 38, 98, 163, 143, 181, 177, 90, 73, 222, 186, 27, 103, 37, 234, 14, 152, 69, 254, 192, 225, 93, 47, 117, 2, 195, 76, 240, 18, 129, 70, 151, 163, 141, 211, 249, 198, 107, 143, 95, 231, 3, 146, 156, 149, 21, 109, 122, 235, 191, 82, 89, 218, 149, 190, 131, 45, 212, 116, 33, 211, 88, 224, 105, 41, 73, 201, 200, 68, 142, 194, 137, 106, 117, 142, 121, 120, 244, 88, 62, 107, 153, 185, 113, 221, 39, 225, 79, 182, 190, 136, 173, 23, 240, 32, 172, 102, 201, 206, 58, 180, 125, 223, 74, 24, 99, 26, 49, 130, 229, 81, 51, 96, 151, 83, 127, 69, 98, 100, 119, 224, 177, 107, 174, 132, 187, 129, 160, 28, 254, 8, 43, 148, 249, 72, 104, 88, 112, 69, 253, 25, 143, 222, 108, 135, 148, 123, 248, 183, 82, 115, 211, 35, 171, 75, 2, 226, 114, 31, 143, 87, 227, 85, 171, 42, 102, 235, 40, 7, 178, 181, 194, 3, 47, 197, 123, 154, 134, 55, 8, 165, 211, 40, 135, 242, 48, 191, 165, 178, 35, 3, 106, 186, 2, 22, 130, 92, 237, 207, 28, 43, 138, 121, 180, 146, 167, 7, 242, 240, 243, 105, 226, 161, 78, 218, 244, 205, 101, 5, 190, 213, 6, 52, 98, 31, 209, 166, 254, 138, 196, 46, 83, 157, 52, 243, 85, 160, 162, 138, 225, 50, 5, 246, 235, 117, 164, 131, 236, 57, 11, 96, 239, 170, 64, 113, 159, 6, 94, 110, 16, 81, 189, 33, 138, 249, 62, 221, 6, 61, 150, 62, 5, 174, 221, 230, 189, 70, 77, 84, 141, 181, 145, 196, 93, 5, 113, 6, 212, 111, 4, 80, 21, 255, 96, 152, 251, 36, 25, 189, 233, 151, 214, 64, 67, 204, 137, 217, 158, 119, 103, 232, 66, 189, 176, 137, 139, 136, 7, 25, 91, 56, 231, 200, 238, 219, 121, 124, 10, 71, 161, 66, 15, 233, 124, 132, 30, 201, 248, 0, 0, 0, 0, 128, 134, 131, 9, 43, 237, 72, 50, 17, 112, 172, 30, 90, 114, 78, 108, 14, 255, 251, 253, 133, 56, 86, 15, 174, 213, 30, 61, 45, 57, 39, 54, 15, 217, 100, 10, 92, 166, 33, 104, 91, 84, 209, 155, 54, 46, 58, 36, 10, 103, 177, 12, 87, 231, 15, 147, 238, 150, 210, 180, 155, 145, 158, 27, 192, 197, 79, 128, 220, 32, 162, 97, 119, 75, 105, 90, 18, 26, 22, 28, 147, 186, 10, 226, 160, 42, 229, 192, 34, 224, 67, 60, 27, 23, 29, 18, 9, 13, 11, 14, 139, 199, 173, 242, 182, 168, 185, 45, 30, 169, 200, 20, 241, 25, 133, 87, 117, 7, 76, 175, 153, 221, 187, 238, 127, 96, 253, 163, 1, 38, 159, 247, 114, 245, 188, 92, 102, 59, 197, 68, 251, 126, 52, 91, 67, 41, 118, 139, 35, 198, 220, 203, 237, 252, 104, 182, 228, 241, 99, 184, 49, 220, 202, 215, 99, 133, 16, 66, 151, 34, 64, 19, 198, 17, 32, 132, 74, 36, 125, 133, 187, 61, 248, 210, 249, 50, 17, 174, 41, 161, 109, 199, 158, 47, 75, 29, 178, 48, 243, 220, 134, 82, 236, 13, 193, 227, 208, 119, 179, 22, 108, 43, 112, 185, 153, 169, 148, 72, 250, 17, 233, 100, 34, 71, 252, 140, 196, 168, 240, 63, 26, 160, 125, 44, 216, 86, 51, 144, 239, 34, 73, 78, 199, 135, 56, 209, 193, 217, 202, 162, 254, 140, 212, 11, 54, 152, 245, 129, 207, 166, 122, 222, 40, 165, 183, 142, 38, 218, 173, 191, 164, 63, 58, 157, 228, 44, 120, 146, 13, 80, 95, 204, 155, 106, 126, 70, 98, 84, 141, 19, 194, 246, 216, 184, 232, 144, 57, 247, 94, 46, 195, 175, 245, 130, 93, 128, 190, 159, 208, 147, 124, 105, 213, 45, 169, 111, 37, 18, 179, 207, 172, 153, 59, 200, 24, 125, 167, 16, 156, 99, 110, 232, 59, 187, 123, 219, 38, 120, 9, 205, 89, 24, 244, 110, 154, 183, 1, 236, 79, 154, 168, 131, 149, 110, 101, 230, 255, 230, 126, 170, 188, 207, 8, 33, 21, 232, 230, 239, 231, 155, 217, 186, 111, 54, 206, 74, 159, 9, 212, 234, 176, 124, 214, 41, 164, 178, 175, 49, 63, 35, 49, 42, 165, 148, 48, 198, 162, 102, 192, 53, 78, 188, 55, 116, 130, 202, 166, 252, 144, 208, 176, 224, 167, 216, 21, 51, 4, 152, 74, 241, 236, 218, 247, 65, 205, 80, 14, 127, 145, 246, 47, 23, 77, 214, 141, 118, 239, 176, 77, 67, 170, 77, 84, 204, 150, 4, 223, 228, 209, 181, 227, 158, 106, 136, 27, 76, 44, 31, 184, 193, 101, 81, 127, 70, 94, 234, 4, 157, 140, 53, 93, 1, 135, 116, 115, 250, 11, 65, 46, 251, 103, 29, 90, 179, 219, 210, 82, 146, 16, 86, 51, 233, 214, 71, 19, 109, 215, 97, 140, 154, 161, 12, 122, 55, 248, 20, 142, 89, 19, 60, 137, 235, 169, 39, 238, 206, 97, 201, 53, 183, 28, 229, 237, 225, 71, 177, 60, 122, 210, 223, 89, 156, 242, 115, 63, 85, 20, 206, 121, 24, 199, 55, 191, 115, 247, 205, 234, 83, 253, 170, 91, 95, 61, 111, 20, 223, 68, 219, 134, 120, 175, 243, 129, 202, 104, 196, 62, 185, 36, 52, 44, 56, 163, 64, 95, 194, 29, 195, 114, 22, 226, 37, 12, 188, 60, 73, 139, 40, 13, 149, 65, 255, 168, 1, 113, 57, 12, 179, 222, 8, 180, 228, 156, 216, 86, 193, 144, 100, 203, 132, 97, 123, 50, 182, 112, 213, 108, 92, 116, 72, 184, 87, 66, 208};
.const .align 4 .b8 AES_KEY_FILL[64] = {45, 236, 238, 132, 213, 246, 79, 69, 50, 145, 50, 202, 227, 162, 32, 223, 208, 99, 123, 1, 120, 197, 15, 241, 127, 56, 208, 254, 113, 89, 235, 29, 82, 122, 125, 50, 161, 112, 44, 47, 180, 206, 23, 165, 179, 38, 201, 223, 211, 119, 141, 92, 94, 218, 23, 61, 169, 224, 236, 160, 28, 243, 28, 52};
.const .align 4 .b8 AES_STATE_HASH[64] = {0, 142, 119, 196, 171, 245, 122, 136, 103, 209, 70, 17, 253, 38, 49, 141, 75, 239, 52, 184, 137, 175, 149, 27, 43, 99, 218, 88, 161, 159, 254, 25, 58, 221, 66, 119, 0, 58, 40, 171, 68, 215, 90, 195, 116, 205, 178, 27, 154, 68, 139, 225, 204, 151, 93, 220, 87, 60, 89, 73, 138, 165, 48, 187};
// _ZZ10execute_vmPvS_PKvjjbbE15vm_states_local has been demoted
// _ZZ11fillAes1Rx4ILy2097152ELb1EEvPvS0_jE1T has been demoted
// _ZZ11fillAes1Rx4ILy2176ELb0EEvPvS0_jE1T has been demoted
// _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvjE1T has been demoted
// _ZZ11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvjE1T has been demoted
// _ZZ11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvjE1T has been demoted

.visible .entry _Z7init_vmPKvPv(
	.param .u64 _Z7init_vmPKvPv_param_0,
	.param .u64 _Z7init_vmPKvPv_param_1
)
.maxntid 32, 1, 1
{
	.reg .pred 	%p<46>;
	.reg .b32 	%r<239>;
	.reg .b64 	%rd<161>;


	ld.param.u64 	%rd42, [_Z7init_vmPKvPv_param_0];
	ld.param.u64 	%rd43, [_Z7init_vmPKvPv_param_1];
	mov.u32 	%r46, %ctaid.x;
	mov.u32 	%r47, %ntid.x;
	mov.u32 	%r48, %tid.x;
	mad.lo.s32 	%r49, %r47, %r46, %r48;
	shr.u32 	%r50, %r49, 3;
	and.b32  	%r51, %r49, 7;
	mul.wide.u32 	%rd1, %r50, 256;
	cvt.u64.u32	%rd44, %r51;
	or.b64  	%rd45, %rd1, %rd44;
	cvta.to.global.u64 	%rd2, %rd43;
	shl.b64 	%rd46, %rd45, 3;
	add.s64 	%rd47, %rd2, %rd46;
	mov.u64 	%rd160, 0;
	st.global.u64 	[%rd47], %rd160;
	mul.wide.u32 	%rd49, %r50, 2176;
	shr.u64 	%rd3, %rd49, 3;
	or.b64  	%rd50, %rd3, %rd44;
	cvta.to.global.u64 	%rd4, %rd42;
	shl.b64 	%rd51, %rd50, 3;
	add.s64 	%rd52, %rd4, %rd51;
	ld.global.u64 	%rd53, [%rd52];
	shr.u64 	%rd54, %rd53, 7;
	and.b64  	%rd55, %rd54, 139611588448485376;
	and.b64  	%rd56, %rd53, 4503599627370495;
	add.s64 	%rd57, %rd55, 4607182418800017408;
	or.b64  	%rd58, %rd57, %rd56;
	st.global.u64 	[%rd47+192], %rd58;
	setp.ne.s32	%p1, %r51, 0;
	@%p1 bra 	BB0_57;

	shl.b64 	%rd60, %rd3, 3;
	add.s64 	%rd61, %rd4, %rd60;
	ld.global.u32 	%r54, [%rd61+64];
	ld.global.u32 	%r55, [%rd61+80];
	ld.global.v2.u64 	{%rd62, %rd63}, [%rd61+96];
	and.b64  	%rd65, %rd62, 1;
	and.b64  	%rd66, %rd62, 2;
	shl.b64 	%rd67, %rd66, 7;
	or.b64  	%rd68, %rd67, %rd65;
	and.b64  	%rd69, %rd62, 4;
	shl.b64 	%rd70, %rd69, 14;
	or.b64  	%rd71, %rd68, %rd70;
	and.b64  	%rd72, %rd62, 8;
	shl.b64 	%rd73, %rd72, 21;
	or.b64  	%rd74, %rd71, %rd73;
	shl.b64 	%rd75, %rd74, 3;
	and.b64  	%rd77, %rd63, 524287;
	shl.b64 	%rd78, %rd77, 6;
	ld.global.v2.u64 	{%rd79, %rd80}, [%rd61+112];
	mov.u32 	%r225, 0;
	and.b64  	%rd83, %rd79, 4194303;
	and.b64  	%rd84, %rd80, 4194303;
	shl.b64 	%rd85, %rd1, 3;
	cvt.u32.u64	%r56, %rd75;
	or.b32  	%r57, %r56, 807407616;
	and.b32  	%r58, %r55, 2147483584;
	and.b32  	%r59, %r54, 2147483584;
	cvt.u32.u64	%r60, %rd78;
	add.s64 	%rd86, %rd2, %rd85;
	st.global.v4.u32 	[%rd86+128], {%r59, %r58, %r57, %r60};
	or.b64  	%rd87, %rd84, 3526318508231098368;
	or.b64  	%rd88, %rd83, 3526318508231098368;
	st.global.v2.u64 	[%rd86+144], {%rd88, %rd87};
	add.s64 	%rd5, %rd3, 16;
	add.s64 	%rd6, %rd86, 256;
	add.s64 	%rd153, %rd86, 768;
	mov.u32 	%r2, %r225;
	bra.uni 	BB0_2;

BB0_15:
	add.s32 	%r73, %r5, -118;
	setp.lt.u32	%p15, %r73, 5;
	@%p15 bra 	BB0_27;
	bra.uni 	BB0_16;

BB0_27:
	setp.eq.s32	%p24, %r7, %r6;
	and.b32  	%r103, %r61, 50331648;
	setp.eq.s32	%p25, %r103, 0;
	selp.b32	%r104, 65536, 32768, %p25;
	selp.b32	%r105, 98304, %r104, %p24;
	shl.b32 	%r106, %r2, 8;
	shl.b32 	%r107, %r7, 5;
	or.b32  	%r108, %r106, %r107;
	shl.b32 	%r109, %r6, 2;
	or.b32  	%r110, %r108, %r109;
	or.b32  	%r111, %r110, %r105;
	or.b32  	%r112, %r111, 14680064;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;
	shl.b32 	%r102, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r102,8;
	// inline asm
	st.global.u32 	[%rd153], %r112;
	bra.uni 	BB0_56;

BB0_16:
	add.s32 	%r74, %r5, -123;
	setp.lt.u32	%p16, %r74, 10;
	@%p16 bra 	BB0_24;
	bra.uni 	BB0_17;

BB0_24:
	shl.b32 	%r96, %r6, 2;
	shl.b32 	%r97, %r7, 5;
	or.b32  	%r98, %r97, %r96;
	or.b32  	%r227, %r98, 16777216;
	setp.ne.s32	%p23, %r7, %r6;
	@%p23 bra 	BB0_26;

	shl.b32 	%r99, %r2, 8;
	or.b32  	%r100, %r99, %r227;
	or.b32  	%r227, %r100, 524288;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;

BB0_26:
	shl.b32 	%r101, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r101,8;
	// inline asm
	st.global.u32 	[%rd153], %r227;
	bra.uni 	BB0_56;

BB0_17:
	add.s32 	%r75, %r5, -133;
	setp.lt.u32	%p17, %r75, 4;
	@%p17 bra 	BB0_21;
	bra.uni 	BB0_18;

BB0_21:
	setp.eq.s32	%p21, %r7, %r6;
	@%p21 bra 	BB0_23;

	shl.b32 	%r89, %r6, 3;
	// inline asm
	bfi.b64 %rd93,%rd10,%rd160,%r89,8;
	// inline asm
	shl.b32 	%r90, %r7, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd93,%r90,8;
	// inline asm

BB0_23:
	shl.b32 	%r91, %r7, 5;
	shl.b32 	%r92, %r6, 2;
	or.b32  	%r93, %r91, %r92;
	or.b32  	%r94, %r93, 18874368;
	selp.b32	%r95, 3, %r94, %p21;
	st.global.u32 	[%rd153], %r95;
	bra.uni 	BB0_56;

BB0_18:
	add.s32 	%r76, %r5, -137;
	setp.lt.u32	%p18, %r76, 16;
	@%p18 bra 	BB0_20;
	bra.uni 	BB0_19;

BB0_20:
	and.b32  	%r78, %r61, 469762048;
	setp.eq.s32	%p19, %r78, 0;
	and.b32  	%r79, %r61, 50331648;
	setp.eq.s32	%p20, %r79, 0;
	selp.b32	%r80, 65536, 32768, %p20;
	selp.b32	%r81, 98304, %r80, %p19;
	shl.b32 	%r82, %r2, 8;
	shl.b32 	%r83, %r7, 5;
	or.b32  	%r84, %r82, %r83;
	shl.b32 	%r85, %r6, 2;
	or.b32  	%r86, %r84, %r85;
	or.b32  	%r87, %r86, %r81;
	or.b32  	%r88, %r87, 20971520;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;
	st.global.u32 	[%rd153], %r88;
	bra.uni 	BB0_56;

BB0_19:
	mov.u32 	%r77, 3;
	st.global.u32 	[%rd153], %r77;
	bra.uni 	BB0_56;

BB0_2:
	cvt.u64.u32	%rd10, %r225;
	add.s64 	%rd89, %rd5, %rd10;
	shl.b64 	%rd90, %rd89, 3;
	add.s64 	%rd91, %rd4, %rd90;
	ld.global.v2.u32 	{%r61, %r62}, [%rd91];
	and.b32  	%r5, %r61, 255;
	bfe.u32 	%r6, %r61, 8, 3;
	bfe.u32 	%r7, %r61, 16, 3;
	setp.lt.u32	%p2, %r5, 32;
	mul.wide.u32 	%rd92, %r2, 4;
	add.s64 	%rd11, %rd6, %rd92;
	@%p2 bra 	BB0_53;
	bra.uni 	BB0_3;

BB0_53:
	mov.u32 	%r236, 2097152;
	setp.ne.s32	%p44, %r6, 5;
	@%p44 bra 	BB0_55;

	shl.b32 	%r236, %r2, 8;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;

BB0_55:
	shr.u32 	%r217, %r61, 13;
	and.b32  	%r218, %r217, 393216;
	shl.b32 	%r219, %r7, 5;
	or.b32  	%r220, %r219, %r218;
	shl.b32 	%r221, %r6, 2;
	or.b32  	%r222, %r220, %r221;
	or.b32  	%r223, %r222, %r236;
	shl.b32 	%r216, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r216,8;
	// inline asm
	st.global.u32 	[%rd153], %r223;
	bra.uni 	BB0_56;

BB0_3:
	add.s32 	%r63, %r5, -32;
	setp.lt.u32	%p3, %r63, 7;
	@%p3 bra 	BB0_52;
	bra.uni 	BB0_4;

BB0_52:
	setp.eq.s32	%p42, %r7, %r6;
	and.b32  	%r205, %r61, 50331648;
	setp.eq.s32	%p43, %r205, 0;
	selp.b32	%r206, 65536, 32768, %p43;
	selp.b32	%r207, 98304, %r206, %p42;
	shl.b32 	%r208, %r2, 8;
	shl.b32 	%r209, %r7, 5;
	or.b32  	%r210, %r208, %r209;
	shl.b32 	%r211, %r6, 2;
	or.b32  	%r212, %r210, %r211;
	or.b32  	%r213, %r212, %r207;
	or.b32  	%r214, %r213, 2097152;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;
	shl.b32 	%r204, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r204,8;
	// inline asm
	st.global.u32 	[%rd153], %r214;
	bra.uni 	BB0_56;

BB0_4:
	add.s32 	%r64, %r5, -39;
	setp.lt.u32	%p4, %r64, 17;
	@%p4 bra 	BB0_49;
	bra.uni 	BB0_5;

BB0_49:
	shl.b32 	%r198, %r6, 2;
	shl.b32 	%r199, %r7, 5;
	or.b32  	%r200, %r199, %r198;
	or.b32  	%r234, %r200, 4194304;
	setp.ne.s32	%p41, %r7, %r6;
	@%p41 bra 	BB0_51;

	shl.b32 	%r201, %r2, 8;
	or.b32  	%r202, %r201, %r234;
	or.b32  	%r234, %r202, 524288;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;

BB0_51:
	shl.b32 	%r203, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r203,8;
	// inline asm
	st.global.u32 	[%rd153], %r234;
	bra.uni 	BB0_56;

BB0_5:
	add.s32 	%r65, %r5, -56;
	setp.lt.u32	%p5, %r65, 7;
	@%p5 bra 	BB0_48;
	bra.uni 	BB0_6;

BB0_48:
	setp.eq.s32	%p39, %r7, %r6;
	and.b32  	%r188, %r61, 50331648;
	setp.eq.s32	%p40, %r188, 0;
	selp.b32	%r189, 65536, 32768, %p40;
	selp.b32	%r190, 98304, %r189, %p39;
	shl.b32 	%r191, %r2, 8;
	shl.b32 	%r192, %r7, 5;
	or.b32  	%r193, %r191, %r192;
	shl.b32 	%r194, %r6, 2;
	or.b32  	%r195, %r193, %r194;
	or.b32  	%r196, %r195, %r190;
	or.b32  	%r197, %r196, 4194304;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;
	shl.b32 	%r187, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r187,8;
	// inline asm
	st.global.u32 	[%rd153], %r197;
	bra.uni 	BB0_56;

BB0_6:
	add.s32 	%r66, %r5, -63;
	setp.lt.u32	%p6, %r66, 16;
	@%p6 bra 	BB0_45;
	bra.uni 	BB0_7;

BB0_45:
	shl.b32 	%r181, %r6, 2;
	shl.b32 	%r182, %r7, 5;
	or.b32  	%r183, %r182, %r181;
	or.b32  	%r232, %r183, 6291456;
	setp.ne.s32	%p38, %r7, %r6;
	@%p38 bra 	BB0_47;

	shl.b32 	%r184, %r2, 8;
	or.b32  	%r185, %r184, %r232;
	or.b32  	%r232, %r185, 524288;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;

BB0_47:
	shl.b32 	%r186, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r186,8;
	// inline asm
	st.global.u32 	[%rd153], %r232;
	bra.uni 	BB0_56;

BB0_7:
	add.s32 	%r67, %r5, -79;
	setp.lt.u32	%p7, %r67, 4;
	@%p7 bra 	BB0_44;
	bra.uni 	BB0_8;

BB0_44:
	setp.eq.s32	%p36, %r7, %r6;
	and.b32  	%r171, %r61, 50331648;
	setp.eq.s32	%p37, %r171, 0;
	selp.b32	%r172, 65536, 32768, %p37;
	selp.b32	%r173, 98304, %r172, %p36;
	shl.b32 	%r174, %r2, 8;
	shl.b32 	%r175, %r7, 5;
	or.b32  	%r176, %r174, %r175;
	shl.b32 	%r177, %r6, 2;
	or.b32  	%r178, %r176, %r177;
	or.b32  	%r179, %r178, %r173;
	or.b32  	%r180, %r179, 6291456;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;
	shl.b32 	%r170, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r170,8;
	// inline asm
	st.global.u32 	[%rd153], %r180;
	bra.uni 	BB0_56;

BB0_8:
	add.s32 	%r68, %r5, -83;
	setp.lt.u32	%p8, %r68, 4;
	@%p8 bra 	BB0_43;
	bra.uni 	BB0_9;

BB0_43:
	shl.b32 	%r166, %r6, 2;
	shl.b32 	%r167, %r7, 5;
	or.b32  	%r168, %r167, %r166;
	or.b32  	%r169, %r168, 8388608;
	shl.b32 	%r165, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r165,8;
	// inline asm
	st.global.u32 	[%rd153], %r169;
	bra.uni 	BB0_56;

BB0_9:
	setp.eq.s32	%p9, %r5, 87;
	@%p9 bra 	BB0_42;
	bra.uni 	BB0_10;

BB0_42:
	setp.eq.s32	%p34, %r7, %r6;
	and.b32  	%r155, %r61, 50331648;
	setp.eq.s32	%p35, %r155, 0;
	selp.b32	%r156, 65536, 32768, %p35;
	selp.b32	%r157, 98304, %r156, %p34;
	shl.b32 	%r158, %r2, 8;
	shl.b32 	%r159, %r7, 5;
	or.b32  	%r160, %r158, %r159;
	shl.b32 	%r161, %r6, 2;
	or.b32  	%r162, %r160, %r161;
	or.b32  	%r163, %r162, %r157;
	or.b32  	%r164, %r163, 8388608;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;
	shl.b32 	%r154, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r154,8;
	// inline asm
	st.global.u32 	[%rd153], %r164;
	bra.uni 	BB0_56;

BB0_10:
	and.b32  	%r69, %r61, 252;
	setp.eq.s32	%p10, %r69, 88;
	@%p10 bra 	BB0_41;
	bra.uni 	BB0_11;

BB0_41:
	shl.b32 	%r150, %r6, 2;
	shl.b32 	%r151, %r7, 5;
	or.b32  	%r152, %r151, %r150;
	or.b32  	%r153, %r152, 10485760;
	shl.b32 	%r149, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r149,8;
	// inline asm
	st.global.u32 	[%rd153], %r153;
	bra.uni 	BB0_56;

BB0_11:
	setp.eq.s32	%p11, %r5, 92;
	@%p11 bra 	BB0_40;
	bra.uni 	BB0_12;

BB0_40:
	setp.eq.s32	%p32, %r7, %r6;
	and.b32  	%r139, %r61, 50331648;
	setp.eq.s32	%p33, %r139, 0;
	selp.b32	%r140, 65536, 32768, %p33;
	selp.b32	%r141, 98304, %r140, %p32;
	shl.b32 	%r142, %r2, 8;
	shl.b32 	%r143, %r7, 5;
	or.b32  	%r144, %r142, %r143;
	shl.b32 	%r145, %r6, 2;
	or.b32  	%r146, %r144, %r145;
	or.b32  	%r147, %r146, %r141;
	or.b32  	%r148, %r147, 10485760;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;
	shl.b32 	%r138, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r138,8;
	// inline asm
	st.global.u32 	[%rd153], %r148;
	bra.uni 	BB0_56;

BB0_12:
	add.s32 	%r70, %r5, -93;
	setp.lt.u32	%p12, %r70, 8;
	@%p12 bra 	BB0_32;
	bra.uni 	BB0_13;

BB0_32:
	setp.eq.s32	%p27, %r62, 0;
	mov.u64 	%rd157, 1;
	@%p27 bra 	BB0_37;

	cvt.u64.u32	%rd18, %r62;
	// inline asm
	bfind.u32 %r122,%r62;
	// inline asm
	mov.pred 	%p28, 0;
	@%p28 bra 	BB0_35;
	bra.uni 	BB0_34;

BB0_35:
	cvt.u32.u64	%r126, %rd18;
	mov.u32 	%r231, 0;
	div.u32 	%r127, %r231, %r126;
	rem.u32 	%r128, %r231, %r126;
	cvt.u64.u32	%rd157, %r127;
	cvt.u64.u32	%rd156, %r128;
	bra.uni 	BB0_36;

BB0_13:
	add.s32 	%r71, %r5, -101;
	setp.lt.u32	%p13, %r71, 2;
	@%p13 bra 	BB0_31;
	bra.uni 	BB0_14;

BB0_31:
	shl.b32 	%r120, %r6, 2;
	or.b32  	%r121, %r120, 12582912;
	shl.b32 	%r119, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r119,8;
	// inline asm
	st.global.u32 	[%rd153], %r121;
	bra.uni 	BB0_56;

BB0_14:
	add.s32 	%r72, %r5, -103;
	setp.lt.u32	%p14, %r72, 15;
	@%p14 bra 	BB0_28;
	bra.uni 	BB0_15;

BB0_28:
	shl.b32 	%r113, %r6, 2;
	shl.b32 	%r114, %r7, 5;
	or.b32  	%r115, %r114, %r113;
	or.b32  	%r229, %r115, 14680064;
	setp.ne.s32	%p26, %r7, %r6;
	@%p26 bra 	BB0_30;

	shl.b32 	%r116, %r2, 8;
	or.b32  	%r117, %r116, %r229;
	or.b32  	%r229, %r117, 524288;
	add.s32 	%r2, %r2, 1;
	st.global.u32 	[%rd11], %r62;

BB0_30:
	shl.b32 	%r118, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r118,8;
	// inline asm
	st.global.u32 	[%rd153], %r229;
	bra.uni 	BB0_56;

BB0_34:
	mov.u64 	%rd112, -9223372036854775808;
	div.u64 	%rd157, %rd112, %rd18;
	rem.u64 	%rd156, %rd112, %rd18;
	mov.u32 	%r231, 0;

BB0_36:
	sub.s64 	%rd113, %rd18, %rd156;
	setp.ge.u64	%p29, %rd156, %rd113;
	selp.u64	%rd114, 1, 0, %p29;
	shl.b64 	%rd115, %rd157, 1;
	or.b64  	%rd157, %rd114, %rd115;
	selp.b64	%rd116, %rd18, 0, %p29;
	shl.b64 	%rd117, %rd156, 1;
	sub.s64 	%rd156, %rd117, %rd116;
	add.s32 	%r231, %r231, 1;
	setp.le.u32	%p30, %r231, %r122;
	@%p30 bra 	BB0_36;

BB0_37:
	mov.b64	{%r129, %r130}, %rd157;
	st.global.u32 	[%rd11], %r129;
	add.s32 	%r131, %r2, 1;
	mul.wide.u32 	%rd118, %r131, 4;
	add.s64 	%rd119, %rd6, %rd118;
	st.global.u32 	[%rd119], %r130;
	shl.b32 	%r132, %r7, 5;
	shl.b32 	%r133, %r2, 8;
	or.b32  	%r23, %r133, %r132;
	@%p27 bra 	BB0_39;

	shl.b32 	%r134, %r6, 3;
	// inline asm
	bfi.b64 %rd160,%rd10,%rd160,%r134,8;
	// inline asm

BB0_39:
	shl.b32 	%r135, %r6, 2;
	or.b32  	%r136, %r23, %r135;
	or.b32  	%r137, %r136, 7340032;
	st.global.u32 	[%rd153], %r137;
	add.s32 	%r2, %r2, 2;

BB0_56:
	add.s64 	%rd153, %rd153, 4;
	cvt.u32.u64	%r224, %rd10;
	add.s32 	%r225, %r224, 1;
	setp.lt.u32	%p45, %r225, 256;
	@%p45 bra 	BB0_2;

BB0_57:
	ret;
}

	// .globl	_Z10execute_vmPvS_PKvjjbb
.visible .entry _Z10execute_vmPvS_PKvjjbb(
	.param .u64 _Z10execute_vmPvS_PKvjjbb_param_0,
	.param .u64 _Z10execute_vmPvS_PKvjjbb_param_1,
	.param .u64 _Z10execute_vmPvS_PKvjjbb_param_2,
	.param .u32 _Z10execute_vmPvS_PKvjjbb_param_3,
	.param .u32 _Z10execute_vmPvS_PKvjjbb_param_4,
	.param .u8 _Z10execute_vmPvS_PKvjjbb_param_5,
	.param .u8 _Z10execute_vmPvS_PKvjjbb_param_6
)
.maxntid 16, 1, 1
{
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<151>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<115>;
	// demoted variable
	.shared .align 8 .b8 _ZZ10execute_vmPvS_PKvjjbbE15vm_states_local[4096];

	ld.param.u64 	%rd47, [_Z10execute_vmPvS_PKvjjbb_param_0];
	ld.param.u64 	%rd49, [_Z10execute_vmPvS_PKvjjbb_param_1];
	ld.param.u64 	%rd48, [_Z10execute_vmPvS_PKvjjbb_param_2];
	ld.param.u32 	%r48, [_Z10execute_vmPvS_PKvjjbb_param_3];
	ld.param.u32 	%r49, [_Z10execute_vmPvS_PKvjjbb_param_4];
	ld.param.s8 	%rs1, [_Z10execute_vmPvS_PKvjjbb_param_5];
	cvta.to.global.u64 	%rd1, %rd49;
	mov.u32 	%r1, %tid.x;
	shl.b32 	%r141, %r1, 3;
	mov.u32 	%r3, %ntid.x;
	shl.b32 	%r4, %r3, 3;
	mov.u32 	%r5, %ctaid.x;
	setp.gt.u32	%p1, %r141, 4095;
	@%p1 bra 	BB1_3;

	mov.u32 	%r51, _ZZ10execute_vmPvS_PKvjjbbE15vm_states_local;
	add.s32 	%r140, %r51, %r141;
	cvt.u64.u32	%rd2, %r4;
	cvta.to.global.u64 	%rd50, %rd47;
	cvt.u64.u32	%rd51, %r141;
	mul.wide.u32 	%rd52, %r5, 4096;
	add.s64 	%rd53, %rd52, %rd51;
	add.s64 	%rd107, %rd50, %rd53;

BB1_2:
	ld.global.u64 	%rd54, [%rd107];
	st.shared.u64 	[%r140], %rd54;
	add.s64 	%rd107, %rd107, %rd2;
	add.s32 	%r140, %r140, %r4;
	add.s32 	%r141, %r141, %r4;
	setp.lt.u32	%p2, %r141, 4096;
	@%p2 bra 	BB1_2;

BB1_3:
	bar.warp.sync 	-1;
	shr.u32 	%r52, %r1, 3;
	mul.wide.u32 	%rd6, %r52, 256;
	or.b64  	%rd7, %rd6, 16;
	mad.lo.s32 	%r53, %r5, %r3, %r1;
	shr.u32 	%r11, %r53, 3;
	and.b32  	%r12, %r53, 7;
	cvt.u32.u64	%r54, %rd6;
	shl.b32 	%r55, %r54, 3;
	or.b32  	%r56, %r55, 128;
	mov.u32 	%r57, _ZZ10execute_vmPvS_PKvjjbbE15vm_states_local;
	add.s32 	%r13, %r57, %r56;
	ld.shared.v2.u32 	{%r150, %r149}, [%r13];
	shl.b32 	%r60, %r11, 6;
	cvt.u64.u32	%rd8, %r60;
	cvt.u64.u32	%rd9, %r12;
	or.b32  	%r61, %r12, %r54;
	shl.b32 	%r62, %r61, 3;
	add.s32 	%r16, %r57, %r62;
	setp.eq.s32	%p3, %r49, 0;
	@%p3 bra 	BB1_40;

	add.s32 	%r67, %r57, %r55;
	ld.shared.v2.u32 	{%r68, %r69}, [%r13+8];
	cvt.u64.u32	%rd55, %r69;
	ld.shared.v2.u64 	{%rd56, %rd57}, [%r13+16];
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p4, %rs3, 0;
	setp.lt.u32	%p5, %r12, 4;
	add.s32 	%r71, %r12, 2147483644;
	shl.b32 	%r72, %r71, 1;
	cvt.u64.u32	%rd58, %r72;
	add.s64 	%rd59, %rd58, %rd7;
	shl.b32 	%r73, %r12, 1;
	cvt.u64.u32	%rd60, %r73;
	add.s64 	%rd61, %rd6, %rd60;
	add.s64 	%rd62, %rd61, 8;
	selp.b64	%rd63, %rd62, %rd59, %p5;
	selp.b64	%rd10, -1, 4503599627370495, %p5;
	cvt.u32.u64	%r74, %rd9;
	shl.b32 	%r76, %r74, 3;
	cvt.u64.u32	%rd11, %r76;
	add.s64 	%rd64, %rd63, 1;
	cvt.u32.u64	%r77, %rd64;
	shl.b32 	%r78, %r77, 3;
	add.s32 	%r17, %r57, %r78;
	add.s64 	%rd12, %rd55, %rd11;
	ld.shared.u64 	%rd114, [%r16];
	shl.b32 	%r79, %r1, 8;
	and.b32  	%r80, %r79, -2048;
	add.s32 	%r81, %r80, %r57;
	add.s32 	%r18, %r81, 768;
	cvta.to.global.u64 	%rd14, %rd48;
	selp.b64	%rd15, 0, %rd57, %p5;
	selp.b64	%rd16, 0, %rd56, %p5;
	cvt.u32.u64	%r82, %rd63;
	shl.b32 	%r83, %r82, 3;
	add.s32 	%r19, %r57, %r83;
	selp.b32	%r143, 0, %r150, %p4;
	selp.b32	%r144, 0, %r149, %p4;
	shr.u32 	%r84, %r68, 24;
	add.s32 	%r22, %r67, %r84;
	bfe.u32 	%r85, %r68, 16, 8;
	add.s32 	%r23, %r67, %r85;
	bfe.u32 	%r86, %r68, 8, 8;
	add.s32 	%r24, %r67, %r86;
	and.b32  	%r87, %r68, 255;
	add.s32 	%r25, %r67, %r87;
	mov.u32 	%r142, 0;

BB1_5:
	.pragma "nounroll";
	mov.u32 	%r29, %r149;
	mov.u32 	%r149, %r150;
	ld.shared.u64 	%rd70, [%r24];
	ld.shared.u64 	%rd71, [%r25];
	xor.b64  	%rd72, %rd70, %rd71;
	mov.b64	{%r91, %r92}, %rd72;
	xor.b32  	%r93, %r91, %r144;
	xor.b32  	%r94, %r92, %r143;
	and.b32  	%r88, %r93, 2097088;
	and.b32  	%r89, %r94, 2097088;
	// inline asm
	mad.wide.u32 %rd67,%r88,%r48,%rd11;
	mad.wide.u32 %rd68,%r89,%r48,%rd11;
	// inline asm
	add.s64 	%rd73, %rd67, %rd8;
	add.s64 	%rd18, %rd1, %rd73;
	add.s64 	%rd74, %rd68, %rd8;
	add.s64 	%rd19, %rd1, %rd74;
	ld.global.u64 	%rd75, [%rd18];
	xor.b64  	%rd76, %rd114, %rd75;
	st.shared.u64 	[%r16], %rd76;
	ld.global.v2.u32 	{%r95, %r96}, [%rd19];
	cvt.rn.f64.s32	%fd1, %r95;
	mov.b64 	 %rd77, %fd1;
	and.b64  	%rd78, %rd77, %rd10;
	or.b64  	%rd79, %rd78, %rd16;
	st.shared.u64 	[%r19], %rd79;
	cvt.rn.f64.s32	%fd2, %r96;
	mov.b64 	 %rd80, %fd2;
	and.b64  	%rd81, %rd80, %rd10;
	or.b64  	%rd82, %rd81, %rd15;
	st.shared.u64 	[%r17], %rd82;
	bar.warp.sync 	-1;
	mov.u32 	%r148, -256;
	setp.ne.s32	%p6, %r12, 0;
	mov.u32 	%r147, %r18;
	@%p6 bra 	BB1_39;

BB1_6:
	mov.u32 	%r138, _ZZ10execute_vmPvS_PKvjjbbE15vm_states_local;
	ld.shared.u32 	%r34, [%r147];
	shr.u32 	%r100, %r34, 6;
	and.b32  	%r101, %r100, 508;
	add.s32 	%r102, %r13, %r101;
	// inline asm
	// INSTRUCTION DECODING BEGIN
	// inline asm
	bfe.u32 	%r103, %r34, 2, 3;
	cvt.u64.u32	%rd83, %r103;
	add.s64 	%rd84, %rd83, %rd6;
	cvt.u32.u64	%r104, %rd84;
	shl.b32 	%r105, %r104, 3;
	add.s32 	%r35, %r138, %r105;
	bfe.u32 	%r107, %r34, 5, 3;
	cvt.u64.u32	%rd85, %r107;
	add.s64 	%rd86, %rd85, %rd6;
	cvt.u32.u64	%r108, %rd86;
	shl.b32 	%r109, %r108, 3;
	add.s32 	%r36, %r138, %r109;
	ld.shared.u64 	%rd113, [%r35];
	ld.shared.u64 	%rd110, [%r36];
	ld.shared.u32 	%r37, [%r102+128];
	cvt.s64.s32	%rd22, %r37;
	ld.shared.u32 	%r38, [%r102+132];
	bfe.u32 	%r39, %r34, 21, 4;
	shr.u32 	%r110, %r34, 12;
	and.b32  	%r40, %r110, 24;
	setp.eq.s32	%p7, %r40, 0;
	@%p7 bra 	BB1_11;

	cvt.u32.u64	%r116, %rd22;
	// inline asm
	// SCRATCHPAD ACCESS BEGIN
	// inline asm
	mov.u32 	%r112, 185471488;
	// inline asm
	bfe.u32 %r111,%r112,%r40,8;
	// inline asm
	mov.u32 	%r117, -1;
	shr.u32 	%r118, %r117, %r111;
	cvt.u32.u64	%r119, %rd110;
	setp.eq.s32	%p8, %r40, 24;
	selp.b32	%r120, 0, %r119, %p8;
	cvt.u32.u64	%r121, %rd113;
	setp.ne.s32	%p9, %r39, 10;
	selp.b32	%r122, %r120, %r121, %p9;
	add.s32 	%r123, %r122, %r116;
	and.b32  	%r124, %r118, %r123;
	and.b32  	%r114, %r124, -64;
	and.b32  	%r125, %r124, 56;
	cvt.u64.u32	%rd88, %r125;
	// inline asm
	mad.wide.u32 %rd87,%r114,%r48,%rd88;
	// inline asm
	add.s64 	%rd89, %rd87, %rd8;
	add.s64 	%rd23, %rd1, %rd89;
	@%p9 bra 	BB1_9;
	bra.uni 	BB1_8;

BB1_9:
	ld.global.u64 	%rd110, [%rd23];
	bra.uni 	BB1_10;

BB1_8:
	st.global.u64 	[%rd23], %rd110;

BB1_10:
	// inline asm
	// SCRATCHPAD ACCESS END
	// inline asm

BB1_11:
	// inline asm
	// INSTRUCTION DECODING END
	// inline asm
	and.b32  	%r126, %r34, 3;
	setp.ne.s32	%p10, %r126, 0;
	@%p10 bra 	BB1_38;

	// inline asm
	// INTEGER GROUP BEGIN
	// inline asm
	and.b32  	%r127, %r34, 524288;
	setp.eq.s32	%p11, %r127, 0;
	selp.b64	%rd27, %rd110, %rd22, %p11;
	setp.gt.s32	%p12, %r39, 4;
	@%p12 bra 	BB1_20;

	setp.gt.s32	%p19, %r39, 1;
	@%p19 bra 	BB1_16;

	setp.eq.s32	%p23, %r39, 0;
	@%p23 bra 	BB1_35;
	bra.uni 	BB1_15;

BB1_35:
	add.s64 	%rd113, %rd22, %rd113;
	bra.uni 	BB1_36;

BB1_20:
	setp.gt.s32	%p13, %r39, 6;
	@%p13 bra 	BB1_24;

	setp.eq.s32	%p17, %r39, 5;
	@%p17 bra 	BB1_30;
	bra.uni 	BB1_22;

BB1_30:
	mul.hi.s64 	%rd113, %rd113, %rd27;
	bra.uni 	BB1_37;

BB1_16:
	setp.eq.s32	%p20, %r39, 2;
	@%p20 bra 	BB1_34;

	setp.eq.s32	%p21, %r39, 3;
	@%p21 bra 	BB1_31;
	bra.uni 	BB1_18;

BB1_31:
	and.b32  	%r130, %r34, 1048576;
	setp.eq.s32	%p25, %r130, 0;
	@%p25 bra 	BB1_33;

	mov.b64	%rd27, {%r37, %r38};

BB1_33:
	mul.lo.s64 	%rd113, %rd27, %rd113;
	bra.uni 	BB1_37;

BB1_24:
	setp.eq.s32	%p14, %r39, 7;
	@%p14 bra 	BB1_29;

	setp.eq.s32	%p15, %r39, 8;
	@%p15 bra 	BB1_28;
	bra.uni 	BB1_26;

BB1_28:
	cvt.u32.u64	%r128, %rd27;
	and.b32  	%r129, %r128, 63;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	.reg .u32 %amt2;
	shr.b64 	%lhs, %rd113, %r129;
	sub.u32 	%amt2, 64, %r129;
	shl.b64 	%rhs, %rd113, %amt2;
	add.u64 	%rd113, %lhs, %rhs;
	}
	bra.uni 	BB1_37;

BB1_15:
	setp.eq.s32	%p24, %r39, 1;
	@%p24 bra 	BB1_36;
	bra.uni 	BB1_37;

BB1_36:
	bfe.u32 	%r131, %r34, 17, 2;
	shl.b64 	%rd90, %rd27, %r131;
	add.s64 	%rd113, %rd113, %rd90;
	bra.uni 	BB1_37;

BB1_22:
	setp.eq.s32	%p18, %r39, 6;
	@%p18 bra 	BB1_23;
	bra.uni 	BB1_37;

BB1_23:
	neg.s64 	%rd113, %rd113;
	bra.uni 	BB1_37;

BB1_34:
	sub.s64 	%rd113, %rd113, %rd27;
	bra.uni 	BB1_37;

BB1_18:
	setp.eq.s32	%p22, %r39, 4;
	@%p22 bra 	BB1_19;
	bra.uni 	BB1_37;

BB1_19:
	mul.hi.u64 	%rd113, %rd113, %rd27;
	bra.uni 	BB1_37;

BB1_29:
	xor.b64  	%rd113, %rd27, %rd113;
	bra.uni 	BB1_37;

BB1_26:
	setp.ne.s32	%p16, %r39, 9;
	@%p16 bra 	BB1_37;

	st.shared.u64 	[%r36], %rd113;
	mov.u64 	%rd113, %rd27;

BB1_37:
	st.shared.u64 	[%r35], %rd113;
	// inline asm
	// INTEGER GROUP END
	// inline asm

BB1_38:
	bar.warp.sync 	9;
	add.s32 	%r147, %r147, 4;
	add.s32 	%r148, %r148, 1;
	setp.ne.s32	%p26, %r148, 0;
	@%p26 bra 	BB1_6;

BB1_39:
	ld.param.u32 	%r139, [_Z10execute_vmPvS_PKvjjbb_param_4];
	ld.shared.u32 	%r134, [%r23];
	xor.b32  	%r135, %r134, %r29;
	ld.shared.u32 	%r136, [%r22];
	xor.b32  	%r137, %r135, %r136;
	cvt.u64.u32	%rd91, %r149;
	add.s64 	%rd92, %rd12, %rd91;
	add.s64 	%rd93, %rd14, %rd92;
	ld.global.u64 	%rd94, [%rd93];
	ld.shared.u64 	%rd95, [%r16];
	xor.b64  	%rd114, %rd94, %rd95;
	ld.shared.u64 	%rd96, [%r16+64];
	ld.shared.u64 	%rd97, [%r16+128];
	st.shared.u64 	[%r16], %rd114;
	xor.b64  	%rd98, %rd97, %rd96;
	st.global.u64 	[%rd19], %rd114;
	st.global.u64 	[%rd18], %rd98;
	and.b32  	%r150, %r137, 2147483584;
	add.s32 	%r142, %r142, 1;
	setp.lt.u32	%p27, %r142, %r139;
	mov.u32 	%r143, 0;
	mov.u32 	%r144, %r143;
	@%p27 bra 	BB1_5;
	bra.uni 	BB1_41;

BB1_40:
	ld.shared.u64 	%rd114, [%r16];

BB1_41:
	ld.param.s8 	%rs5, [_Z10execute_vmPvS_PKvjjbb_param_6];
	cvt.u64.u32	%rd106, %r12;
	mul.wide.u32 	%rd44, %r11, 256;
	add.s64 	%rd99, %rd44, %rd106;
	cvta.to.global.u64 	%rd45, %rd47;
	shl.b64 	%rd100, %rd99, 3;
	add.s64 	%rd46, %rd45, %rd100;
	st.global.u64 	[%rd46], %rd114;
	and.b16  	%rs4, %rs5, 255;
	setp.eq.s16	%p28, %rs4, 0;
	@%p28 bra 	BB1_43;

	ld.shared.u64 	%rd101, [%r16+64];
	ld.shared.f64 	%fd3, [%r16+128];
	mov.b64 	 %rd102, %fd3;
	xor.b64  	%rd103, %rd102, %rd101;
	st.global.u64 	[%rd46+64], %rd103;
	st.global.f64 	[%rd46+128], %fd3;
	bra.uni 	BB1_45;

BB1_43:
	setp.ne.s32	%p29, %r12, 0;
	@%p29 bra 	BB1_45;

	shl.b64 	%rd104, %rd44, 3;
	add.s64 	%rd105, %rd45, %rd104;
	st.global.u32 	[%rd105+128], %r150;
	st.global.u32 	[%rd105+132], %r149;

BB1_45:
	ret;
}

	// .globl	_Z20blake2b_initial_hashILj76EEvPvPKvj
.visible .entry _Z20blake2b_initial_hashILj76EEvPvPKvj(
	.param .u64 _Z20blake2b_initial_hashILj76EEvPvPKvj_param_0,
	.param .u64 _Z20blake2b_initial_hashILj76EEvPvPKvj_param_1,
	.param .u32 _Z20blake2b_initial_hashILj76EEvPvPKvj_param_2
)
{
	.reg .b32 	%r<1740>;
	.reg .b64 	%rd<1308>;


	ld.param.u64 	%rd1, [_Z20blake2b_initial_hashILj76EEvPvPKvj_param_0];
	ld.param.u64 	%rd2, [_Z20blake2b_initial_hashILj76EEvPvPKvj_param_1];
	ld.param.u32 	%r769, [_Z20blake2b_initial_hashILj76EEvPvPKvj_param_2];
	mov.u32 	%r770, %ctaid.x;
	mov.u32 	%r771, %ntid.x;
	mov.u32 	%r772, %tid.x;
	mad.lo.s32 	%r773, %r771, %r770, %r772;
	cvta.to.global.u64 	%rd3, %rd2;
	ld.global.u64 	%rd4, [%rd3];
	ld.global.u64 	%rd5, [%rd3+8];
	ld.global.u64 	%rd6, [%rd3+16];
	ld.global.u64 	%rd7, [%rd3+24];
	ld.global.u64 	%rd8, [%rd3+32];
	ld.global.u64 	%rd9, [%rd3+40];
	ld.global.u64 	%rd10, [%rd3+48];
	ld.global.u64 	%rd11, [%rd3+56];
	ld.global.u64 	%rd12, [%rd3+64];
	ld.global.u32 	%rd13, [%rd3+72];
	add.s32 	%r774, %r773, %r769;
	cvt.u64.u32	%rd14, %r774;
	and.b64  	%rd15, %rd8, 72057594037927935;
	shl.b64 	%rd16, %rd14, 56;
	or.b64  	%rd17, %rd15, %rd16;
	and.b64  	%rd18, %rd9, -16777216;
	shr.u64 	%rd19, %rd14, 8;
	or.b64  	%rd20, %rd18, %rd19;
	add.s64 	%rd21, %rd4, -4965156021692249063;
	xor.b64  	%rd22, %rd21, 5840696475078001309;
	mov.b64	{%r775, %r776}, %rd22;
	mov.b64	%rd23, {%r776, %r775};
	add.s64 	%rd24, %rd23, 7640891576956012808;
	xor.b64  	%rd25, %rd24, 5840696475078001361;
	mov.b64	{%r777, %r778}, %rd25;
	mov.u32 	%r768, 1;
	mov.u32 	%r779, 25923;
	mov.u32 	%r780, 8455;
	prmt.b32 	%r781, %r777, %r778, %r780;
	prmt.b32 	%r782, %r777, %r778, %r779;
	mov.b64	%rd26, {%r782, %r781};
	add.s64 	%rd27, %rd5, %rd21;
	add.s64 	%rd28, %rd27, %rd26;
	xor.b64  	%rd29, %rd28, %rd23;
	mov.b64	{%r783, %r784}, %rd29;
	mov.u32 	%r785, 21554;
	mov.u32 	%r786, 4214;
	prmt.b32 	%r787, %r783, %r784, %r786;
	prmt.b32 	%r788, %r783, %r784, %r785;
	mov.b64	%rd30, {%r788, %r787};
	add.s64 	%rd31, %rd30, %rd24;
	xor.b64  	%rd32, %rd31, %rd26;
	mov.b64	{%r6, %r7}, %rd32;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r768;
	// inline asm
	mov.b64	%rd33, {%r1, %r5};
	add.s64 	%rd34, %rd6, 6227659224458531674;
	xor.b64  	%rd35, %rd34, -7276294671716946913;
	mov.b64	{%r789, %r790}, %rd35;
	mov.b64	%rd36, {%r790, %r789};
	add.s64 	%rd37, %rd36, -4942790177534073029;
	xor.b64  	%rd38, %rd37, -7276294671716946913;
	mov.b64	{%r791, %r792}, %rd38;
	prmt.b32 	%r793, %r791, %r792, %r780;
	prmt.b32 	%r794, %r791, %r792, %r779;
	mov.b64	%rd39, {%r794, %r793};
	add.s64 	%rd40, %rd7, %rd34;
	add.s64 	%rd41, %rd40, %rd39;
	xor.b64  	%rd42, %rd41, %rd36;
	mov.b64	{%r795, %r796}, %rd42;
	prmt.b32 	%r797, %r795, %r796, %r786;
	prmt.b32 	%r798, %r795, %r796, %r785;
	mov.b64	%rd43, {%r798, %r797};
	add.s64 	%rd44, %rd43, %rd37;
	xor.b64  	%rd45, %rd44, %rd39;
	mov.b64	{%r14, %r15}, %rd45;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r768;
	// inline asm
	mov.b64	%rd46, {%r9, %r13};
	add.s64 	%rd47, %rd17, 6625583534739731862;
	xor.b64  	%rd48, %rd47, -2270897969802886508;
	mov.b64	{%r799, %r800}, %rd48;
	mov.b64	%rd49, {%r800, %r799};
	add.s64 	%rd50, %rd49, 4354685564936845355;
	xor.b64  	%rd51, %rd50, 2270897969802886507;
	mov.b64	{%r801, %r802}, %rd51;
	prmt.b32 	%r803, %r801, %r802, %r780;
	prmt.b32 	%r804, %r801, %r802, %r779;
	mov.b64	%rd52, {%r804, %r803};
	add.s64 	%rd53, %rd20, %rd47;
	add.s64 	%rd54, %rd53, %rd52;
	xor.b64  	%rd55, %rd54, %rd49;
	mov.b64	{%r805, %r806}, %rd55;
	prmt.b32 	%r807, %r805, %r806, %r786;
	prmt.b32 	%r808, %r805, %r806, %r785;
	mov.b64	%rd56, {%r808, %r807};
	add.s64 	%rd57, %rd56, %rd50;
	xor.b64  	%rd58, %rd57, %rd52;
	mov.b64	{%r22, %r23}, %rd58;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r768;
	// inline asm
	mov.b64	%rd59, {%r17, %r21};
	add.s64 	%rd60, %rd10, 85782056580896874;
	xor.b64  	%rd61, %rd60, 6620516959819538809;
	mov.b64	{%r809, %r810}, %rd61;
	mov.b64	%rd62, {%r810, %r809};
	add.s64 	%rd63, %rd62, -6534734903238641935;
	xor.b64  	%rd64, %rd63, 6620516959819538809;
	mov.b64	{%r811, %r812}, %rd64;
	prmt.b32 	%r813, %r811, %r812, %r780;
	prmt.b32 	%r814, %r811, %r812, %r779;
	mov.b64	%rd65, {%r814, %r813};
	add.s64 	%rd66, %rd11, %rd60;
	add.s64 	%rd67, %rd66, %rd65;
	xor.b64  	%rd68, %rd67, %rd62;
	mov.b64	{%r815, %r816}, %rd68;
	prmt.b32 	%r817, %r815, %r816, %r786;
	prmt.b32 	%r818, %r815, %r816, %r785;
	mov.b64	%rd69, {%r818, %r817};
	add.s64 	%rd70, %rd69, %rd63;
	xor.b64  	%rd71, %rd70, %rd65;
	mov.b64	{%r30, %r31}, %rd71;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r768;
	// inline asm
	mov.b64	%rd72, {%r25, %r29};
	add.s64 	%rd73, %rd28, %rd12;
	add.s64 	%rd74, %rd73, %rd46;
	xor.b64  	%rd75, %rd69, %rd74;
	mov.b64	{%r819, %r820}, %rd75;
	mov.b64	%rd76, {%r820, %r819};
	add.s64 	%rd77, %rd76, %rd57;
	xor.b64  	%rd78, %rd77, %rd46;
	mov.b64	{%r821, %r822}, %rd78;
	prmt.b32 	%r823, %r821, %r822, %r780;
	prmt.b32 	%r824, %r821, %r822, %r779;
	mov.b64	%rd79, {%r824, %r823};
	add.s64 	%rd80, %rd74, %rd13;
	add.s64 	%rd81, %rd80, %rd79;
	xor.b64  	%rd82, %rd76, %rd81;
	mov.b64	{%r825, %r826}, %rd82;
	prmt.b32 	%r827, %r825, %r826, %r786;
	prmt.b32 	%r828, %r825, %r826, %r785;
	mov.b64	%rd83, {%r828, %r827};
	add.s64 	%rd84, %rd77, %rd83;
	xor.b64  	%rd85, %rd84, %rd79;
	mov.b64	{%r38, %r39}, %rd85;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r768;
	// inline asm
	mov.b64	%rd86, {%r33, %r37};
	add.s64 	%rd87, %rd59, %rd41;
	xor.b64  	%rd88, %rd87, %rd30;
	mov.b64	{%r829, %r830}, %rd88;
	mov.b64	%rd89, {%r830, %r829};
	add.s64 	%rd90, %rd89, %rd70;
	xor.b64  	%rd91, %rd90, %rd59;
	mov.b64	{%r831, %r832}, %rd91;
	prmt.b32 	%r833, %r831, %r832, %r780;
	prmt.b32 	%r834, %r831, %r832, %r779;
	mov.b64	%rd92, {%r834, %r833};
	add.s64 	%rd93, %rd92, %rd87;
	xor.b64  	%rd94, %rd93, %rd89;
	mov.b64	{%r835, %r836}, %rd94;
	prmt.b32 	%r837, %r835, %r836, %r786;
	prmt.b32 	%r838, %r835, %r836, %r785;
	mov.b64	%rd95, {%r838, %r837};
	add.s64 	%rd96, %rd95, %rd90;
	xor.b64  	%rd97, %rd96, %rd92;
	mov.b64	{%r46, %r47}, %rd97;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r768;
	// inline asm
	mov.b64	%rd98, {%r41, %r45};
	add.s64 	%rd99, %rd72, %rd54;
	xor.b64  	%rd100, %rd99, %rd43;
	mov.b64	{%r839, %r840}, %rd100;
	mov.b64	%rd101, {%r840, %r839};
	add.s64 	%rd102, %rd101, %rd31;
	xor.b64  	%rd103, %rd102, %rd72;
	mov.b64	{%r841, %r842}, %rd103;
	prmt.b32 	%r843, %r841, %r842, %r780;
	prmt.b32 	%r844, %r841, %r842, %r779;
	mov.b64	%rd104, {%r844, %r843};
	add.s64 	%rd105, %rd104, %rd99;
	xor.b64  	%rd106, %rd105, %rd101;
	mov.b64	{%r845, %r846}, %rd106;
	prmt.b32 	%r847, %r845, %r846, %r786;
	prmt.b32 	%r848, %r845, %r846, %r785;
	mov.b64	%rd107, {%r848, %r847};
	add.s64 	%rd108, %rd107, %rd102;
	xor.b64  	%rd109, %rd108, %rd104;
	mov.b64	{%r54, %r55}, %rd109;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r768;
	// inline asm
	mov.b64	%rd110, {%r49, %r53};
	add.s64 	%rd111, %rd67, %rd33;
	xor.b64  	%rd112, %rd111, %rd56;
	mov.b64	{%r849, %r850}, %rd112;
	mov.b64	%rd113, {%r850, %r849};
	add.s64 	%rd114, %rd113, %rd44;
	xor.b64  	%rd115, %rd114, %rd33;
	mov.b64	{%r851, %r852}, %rd115;
	prmt.b32 	%r853, %r851, %r852, %r780;
	prmt.b32 	%r854, %r851, %r852, %r779;
	mov.b64	%rd116, {%r854, %r853};
	add.s64 	%rd117, %rd116, %rd111;
	xor.b64  	%rd118, %rd117, %rd113;
	mov.b64	{%r855, %r856}, %rd118;
	prmt.b32 	%r857, %r855, %r856, %r786;
	prmt.b32 	%r858, %r855, %r856, %r785;
	mov.b64	%rd119, {%r858, %r857};
	add.s64 	%rd120, %rd119, %rd114;
	xor.b64  	%rd121, %rd120, %rd116;
	mov.b64	{%r62, %r63}, %rd121;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r768;
	// inline asm
	mov.b64	%rd122, {%r57, %r61};
	add.s64 	%rd123, %rd122, %rd81;
	xor.b64  	%rd124, %rd123, %rd95;
	mov.b64	{%r859, %r860}, %rd124;
	mov.b64	%rd125, {%r860, %r859};
	add.s64 	%rd126, %rd125, %rd108;
	xor.b64  	%rd127, %rd126, %rd122;
	mov.b64	{%r861, %r862}, %rd127;
	prmt.b32 	%r863, %r861, %r862, %r780;
	prmt.b32 	%r864, %r861, %r862, %r779;
	mov.b64	%rd128, {%r864, %r863};
	add.s64 	%rd129, %rd128, %rd123;
	xor.b64  	%rd130, %rd125, %rd129;
	mov.b64	{%r865, %r866}, %rd130;
	prmt.b32 	%r867, %r865, %r866, %r786;
	prmt.b32 	%r868, %r865, %r866, %r785;
	mov.b64	%rd131, {%r868, %r867};
	add.s64 	%rd132, %rd126, %rd131;
	xor.b64  	%rd133, %rd132, %rd128;
	mov.b64	{%r70, %r71}, %rd133;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r768;
	// inline asm
	mov.b64	%rd134, {%r65, %r69};
	add.s64 	%rd135, %rd86, %rd17;
	add.s64 	%rd136, %rd135, %rd93;
	xor.b64  	%rd137, %rd107, %rd136;
	mov.b64	{%r869, %r870}, %rd137;
	mov.b64	%rd138, {%r870, %r869};
	add.s64 	%rd139, %rd120, %rd138;
	xor.b64  	%rd140, %rd139, %rd86;
	mov.b64	{%r871, %r872}, %rd140;
	prmt.b32 	%r873, %r871, %r872, %r780;
	prmt.b32 	%r874, %r871, %r872, %r779;
	mov.b64	%rd141, {%r874, %r873};
	add.s64 	%rd142, %rd136, %rd12;
	add.s64 	%rd143, %rd142, %rd141;
	xor.b64  	%rd144, %rd143, %rd138;
	mov.b64	{%r875, %r876}, %rd144;
	prmt.b32 	%r877, %r875, %r876, %r786;
	prmt.b32 	%r878, %r875, %r876, %r785;
	mov.b64	%rd145, {%r878, %r877};
	add.s64 	%rd146, %rd145, %rd139;
	xor.b64  	%rd147, %rd146, %rd141;
	mov.b64	{%r78, %r79}, %rd147;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r768;
	// inline asm
	mov.b64	%rd148, {%r73, %r77};
	add.s64 	%rd149, %rd98, %rd13;
	add.s64 	%rd150, %rd149, %rd105;
	xor.b64  	%rd151, %rd119, %rd150;
	mov.b64	{%r879, %r880}, %rd151;
	mov.b64	%rd152, {%r880, %r879};
	add.s64 	%rd153, %rd152, %rd84;
	xor.b64  	%rd154, %rd153, %rd98;
	mov.b64	{%r881, %r882}, %rd154;
	prmt.b32 	%r883, %r881, %r882, %r780;
	prmt.b32 	%r884, %r881, %r882, %r779;
	mov.b64	%rd155, {%r884, %r883};
	add.s64 	%rd156, %rd155, %rd150;
	xor.b64  	%rd157, %rd156, %rd152;
	mov.b64	{%r885, %r886}, %rd157;
	prmt.b32 	%r887, %r885, %r886, %r786;
	prmt.b32 	%r888, %r885, %r886, %r785;
	mov.b64	%rd158, {%r888, %r887};
	add.s64 	%rd159, %rd158, %rd153;
	xor.b64  	%rd160, %rd159, %rd155;
	mov.b64	{%r86, %r87}, %rd160;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r768;
	// inline asm
	mov.b64	%rd161, {%r81, %r85};
	add.s64 	%rd162, %rd117, %rd110;
	xor.b64  	%rd163, %rd162, %rd83;
	mov.b64	{%r889, %r890}, %rd163;
	mov.b64	%rd164, {%r890, %r889};
	add.s64 	%rd165, %rd164, %rd96;
	xor.b64  	%rd166, %rd165, %rd110;
	mov.b64	{%r891, %r892}, %rd166;
	prmt.b32 	%r893, %r891, %r892, %r780;
	prmt.b32 	%r894, %r891, %r892, %r779;
	mov.b64	%rd167, {%r894, %r893};
	add.s64 	%rd168, %rd162, %rd10;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r895, %r896}, %rd170;
	prmt.b32 	%r897, %r895, %r896, %r786;
	prmt.b32 	%r898, %r895, %r896, %r785;
	mov.b64	%rd171, {%r898, %r897};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r94, %r95}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r768;
	// inline asm
	mov.b64	%rd174, {%r89, %r93};
	add.s64 	%rd175, %rd129, %rd5;
	add.s64 	%rd176, %rd175, %rd148;
	xor.b64  	%rd177, %rd171, %rd176;
	mov.b64	{%r899, %r900}, %rd177;
	mov.b64	%rd178, {%r900, %r899};
	add.s64 	%rd179, %rd178, %rd159;
	xor.b64  	%rd180, %rd179, %rd148;
	mov.b64	{%r901, %r902}, %rd180;
	prmt.b32 	%r903, %r901, %r902, %r780;
	prmt.b32 	%r904, %r901, %r902, %r779;
	mov.b64	%rd181, {%r904, %r903};
	add.s64 	%rd182, %rd181, %rd176;
	xor.b64  	%rd183, %rd178, %rd182;
	mov.b64	{%r905, %r906}, %rd183;
	prmt.b32 	%r907, %r905, %r906, %r786;
	prmt.b32 	%r908, %r905, %r906, %r785;
	mov.b64	%rd184, {%r908, %r907};
	add.s64 	%rd185, %rd179, %rd184;
	xor.b64  	%rd186, %rd185, %rd181;
	mov.b64	{%r102, %r103}, %rd186;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r768;
	// inline asm
	mov.b64	%rd187, {%r97, %r101};
	add.s64 	%rd188, %rd143, %rd4;
	add.s64 	%rd189, %rd188, %rd161;
	xor.b64  	%rd190, %rd189, %rd131;
	mov.b64	{%r909, %r910}, %rd190;
	mov.b64	%rd191, {%r910, %r909};
	add.s64 	%rd192, %rd191, %rd172;
	xor.b64  	%rd193, %rd192, %rd161;
	mov.b64	{%r911, %r912}, %rd193;
	prmt.b32 	%r913, %r911, %r912, %r780;
	prmt.b32 	%r914, %r911, %r912, %r779;
	mov.b64	%rd194, {%r914, %r913};
	add.s64 	%rd195, %rd189, %rd6;
	add.s64 	%rd196, %rd195, %rd194;
	xor.b64  	%rd197, %rd196, %rd191;
	mov.b64	{%r915, %r916}, %rd197;
	prmt.b32 	%r917, %r915, %r916, %r786;
	prmt.b32 	%r918, %r915, %r916, %r785;
	mov.b64	%rd198, {%r918, %r917};
	add.s64 	%rd199, %rd198, %rd192;
	xor.b64  	%rd200, %rd199, %rd194;
	mov.b64	{%r110, %r111}, %rd200;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r768;
	// inline asm
	mov.b64	%rd201, {%r105, %r109};
	add.s64 	%rd202, %rd174, %rd156;
	xor.b64  	%rd203, %rd202, %rd145;
	mov.b64	{%r919, %r920}, %rd203;
	mov.b64	%rd204, {%r920, %r919};
	add.s64 	%rd205, %rd204, %rd132;
	xor.b64  	%rd206, %rd205, %rd174;
	mov.b64	{%r921, %r922}, %rd206;
	prmt.b32 	%r923, %r921, %r922, %r780;
	prmt.b32 	%r924, %r921, %r922, %r779;
	mov.b64	%rd207, {%r924, %r923};
	add.s64 	%rd208, %rd202, %rd11;
	add.s64 	%rd209, %rd208, %rd207;
	xor.b64  	%rd210, %rd209, %rd204;
	mov.b64	{%r925, %r926}, %rd210;
	prmt.b32 	%r927, %r925, %r926, %r786;
	prmt.b32 	%r928, %r925, %r926, %r785;
	mov.b64	%rd211, {%r928, %r927};
	add.s64 	%rd212, %rd211, %rd205;
	xor.b64  	%rd213, %rd212, %rd207;
	mov.b64	{%r118, %r119}, %rd213;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r768;
	// inline asm
	mov.b64	%rd214, {%r113, %r117};
	add.s64 	%rd215, %rd134, %rd20;
	add.s64 	%rd216, %rd215, %rd169;
	xor.b64  	%rd217, %rd216, %rd158;
	mov.b64	{%r929, %r930}, %rd217;
	mov.b64	%rd218, {%r930, %r929};
	add.s64 	%rd219, %rd218, %rd146;
	xor.b64  	%rd220, %rd219, %rd134;
	mov.b64	{%r931, %r932}, %rd220;
	prmt.b32 	%r933, %r931, %r932, %r780;
	prmt.b32 	%r934, %r931, %r932, %r779;
	mov.b64	%rd221, {%r934, %r933};
	add.s64 	%rd222, %rd216, %rd7;
	add.s64 	%rd223, %rd222, %rd221;
	xor.b64  	%rd224, %rd223, %rd218;
	mov.b64	{%r935, %r936}, %rd224;
	prmt.b32 	%r937, %r935, %r936, %r786;
	prmt.b32 	%r938, %r935, %r936, %r785;
	mov.b64	%rd225, {%r938, %r937};
	add.s64 	%rd226, %rd225, %rd219;
	xor.b64  	%rd227, %rd226, %rd221;
	mov.b64	{%r126, %r127}, %rd227;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r768;
	// inline asm
	mov.b64	%rd228, {%r121, %r125};
	add.s64 	%rd229, %rd228, %rd182;
	xor.b64  	%rd230, %rd229, %rd198;
	mov.b64	{%r939, %r940}, %rd230;
	mov.b64	%rd231, {%r940, %r939};
	add.s64 	%rd232, %rd231, %rd212;
	xor.b64  	%rd233, %rd232, %rd228;
	mov.b64	{%r941, %r942}, %rd233;
	prmt.b32 	%r943, %r941, %r942, %r780;
	prmt.b32 	%r944, %r941, %r942, %r779;
	mov.b64	%rd234, {%r944, %r943};
	add.s64 	%rd235, %rd229, %rd12;
	add.s64 	%rd236, %rd235, %rd234;
	xor.b64  	%rd237, %rd231, %rd236;
	mov.b64	{%r945, %r946}, %rd237;
	prmt.b32 	%r947, %r945, %r946, %r786;
	prmt.b32 	%r948, %r945, %r946, %r785;
	mov.b64	%rd238, {%r948, %r947};
	add.s64 	%rd239, %rd232, %rd238;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r134, %r135}, %rd240;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r768;
	// inline asm
	mov.b64	%rd241, {%r129, %r133};
	add.s64 	%rd242, %rd196, %rd187;
	xor.b64  	%rd243, %rd211, %rd242;
	mov.b64	{%r949, %r950}, %rd243;
	mov.b64	%rd244, {%r950, %r949};
	add.s64 	%rd245, %rd226, %rd244;
	xor.b64  	%rd246, %rd245, %rd187;
	mov.b64	{%r951, %r952}, %rd246;
	prmt.b32 	%r953, %r951, %r952, %r780;
	prmt.b32 	%r954, %r951, %r952, %r779;
	mov.b64	%rd247, {%r954, %r953};
	add.s64 	%rd248, %rd242, %rd4;
	add.s64 	%rd249, %rd248, %rd247;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r955, %r956}, %rd250;
	prmt.b32 	%r957, %r955, %r956, %r786;
	prmt.b32 	%r958, %r955, %r956, %r785;
	mov.b64	%rd251, {%r958, %r957};
	add.s64 	%rd252, %rd251, %rd245;
	xor.b64  	%rd253, %rd252, %rd247;
	mov.b64	{%r142, %r143}, %rd253;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r768;
	// inline asm
	mov.b64	%rd254, {%r137, %r141};
	add.s64 	%rd255, %rd201, %rd20;
	add.s64 	%rd256, %rd255, %rd209;
	xor.b64  	%rd257, %rd225, %rd256;
	mov.b64	{%r959, %r960}, %rd257;
	mov.b64	%rd258, {%r960, %r959};
	add.s64 	%rd259, %rd258, %rd185;
	xor.b64  	%rd260, %rd259, %rd201;
	mov.b64	{%r961, %r962}, %rd260;
	prmt.b32 	%r963, %r961, %r962, %r780;
	prmt.b32 	%r964, %r961, %r962, %r779;
	mov.b64	%rd261, {%r964, %r963};
	add.s64 	%rd262, %rd256, %rd6;
	add.s64 	%rd263, %rd262, %rd261;
	xor.b64  	%rd264, %rd263, %rd258;
	mov.b64	{%r965, %r966}, %rd264;
	prmt.b32 	%r967, %r965, %r966, %r786;
	prmt.b32 	%r968, %r965, %r966, %r785;
	mov.b64	%rd265, {%r968, %r967};
	add.s64 	%rd266, %rd265, %rd259;
	xor.b64  	%rd267, %rd266, %rd261;
	mov.b64	{%r150, %r151}, %rd267;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r768;
	// inline asm
	mov.b64	%rd268, {%r145, %r149};
	add.s64 	%rd269, %rd223, %rd214;
	xor.b64  	%rd270, %rd269, %rd184;
	mov.b64	{%r969, %r970}, %rd270;
	mov.b64	%rd271, {%r970, %r969};
	add.s64 	%rd272, %rd271, %rd199;
	xor.b64  	%rd273, %rd272, %rd214;
	mov.b64	{%r971, %r972}, %rd273;
	prmt.b32 	%r973, %r971, %r972, %r780;
	prmt.b32 	%r974, %r971, %r972, %r779;
	mov.b64	%rd274, {%r974, %r973};
	add.s64 	%rd275, %rd274, %rd269;
	xor.b64  	%rd276, %rd275, %rd271;
	mov.b64	{%r975, %r976}, %rd276;
	prmt.b32 	%r977, %r975, %r976, %r786;
	prmt.b32 	%r978, %r975, %r976, %r785;
	mov.b64	%rd277, {%r978, %r977};
	add.s64 	%rd278, %rd277, %rd272;
	xor.b64  	%rd279, %rd278, %rd274;
	mov.b64	{%r158, %r159}, %rd279;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r768;
	// inline asm
	mov.b64	%rd280, {%r153, %r157};
	add.s64 	%rd281, %rd254, %rd236;
	xor.b64  	%rd282, %rd277, %rd281;
	mov.b64	{%r979, %r980}, %rd282;
	mov.b64	%rd283, {%r980, %r979};
	add.s64 	%rd284, %rd283, %rd266;
	xor.b64  	%rd285, %rd284, %rd254;
	mov.b64	{%r981, %r982}, %rd285;
	prmt.b32 	%r983, %r981, %r982, %r780;
	prmt.b32 	%r984, %r981, %r982, %r779;
	mov.b64	%rd286, {%r984, %r983};
	add.s64 	%rd287, %rd286, %rd281;
	xor.b64  	%rd288, %rd283, %rd287;
	mov.b64	{%r985, %r986}, %rd288;
	prmt.b32 	%r987, %r985, %r986, %r786;
	prmt.b32 	%r988, %r985, %r986, %r785;
	mov.b64	%rd289, {%r988, %r987};
	add.s64 	%rd290, %rd284, %rd289;
	xor.b64  	%rd291, %rd290, %rd286;
	mov.b64	{%r166, %r167}, %rd291;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r768;
	// inline asm
	mov.b64	%rd292, {%r161, %r165};
	add.s64 	%rd293, %rd249, %rd7;
	add.s64 	%rd294, %rd293, %rd268;
	xor.b64  	%rd295, %rd294, %rd238;
	mov.b64	{%r989, %r990}, %rd295;
	mov.b64	%rd296, {%r990, %r989};
	add.s64 	%rd297, %rd296, %rd278;
	xor.b64  	%rd298, %rd297, %rd268;
	mov.b64	{%r991, %r992}, %rd298;
	prmt.b32 	%r993, %r991, %r992, %r780;
	prmt.b32 	%r994, %r991, %r992, %r779;
	mov.b64	%rd299, {%r994, %r993};
	add.s64 	%rd300, %rd294, %rd10;
	add.s64 	%rd301, %rd300, %rd299;
	xor.b64  	%rd302, %rd301, %rd296;
	mov.b64	{%r995, %r996}, %rd302;
	prmt.b32 	%r997, %r995, %r996, %r786;
	prmt.b32 	%r998, %r995, %r996, %r785;
	mov.b64	%rd303, {%r998, %r997};
	add.s64 	%rd304, %rd303, %rd297;
	xor.b64  	%rd305, %rd304, %rd299;
	mov.b64	{%r174, %r175}, %rd305;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r768;
	// inline asm
	mov.b64	%rd306, {%r169, %r173};
	add.s64 	%rd307, %rd263, %rd11;
	add.s64 	%rd308, %rd307, %rd280;
	xor.b64  	%rd309, %rd308, %rd251;
	mov.b64	{%r999, %r1000}, %rd309;
	mov.b64	%rd310, {%r1000, %r999};
	add.s64 	%rd311, %rd310, %rd239;
	xor.b64  	%rd312, %rd311, %rd280;
	mov.b64	{%r1001, %r1002}, %rd312;
	prmt.b32 	%r1003, %r1001, %r1002, %r780;
	prmt.b32 	%r1004, %r1001, %r1002, %r779;
	mov.b64	%rd313, {%r1004, %r1003};
	add.s64 	%rd314, %rd308, %rd5;
	add.s64 	%rd315, %rd314, %rd313;
	xor.b64  	%rd316, %rd315, %rd310;
	mov.b64	{%r1005, %r1006}, %rd316;
	prmt.b32 	%r1007, %r1005, %r1006, %r786;
	prmt.b32 	%r1008, %r1005, %r1006, %r785;
	mov.b64	%rd317, {%r1008, %r1007};
	add.s64 	%rd318, %rd317, %rd311;
	xor.b64  	%rd319, %rd318, %rd313;
	mov.b64	{%r182, %r183}, %rd319;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r768;
	// inline asm
	mov.b64	%rd320, {%r177, %r181};
	add.s64 	%rd321, %rd241, %rd13;
	add.s64 	%rd322, %rd321, %rd275;
	xor.b64  	%rd323, %rd322, %rd265;
	mov.b64	{%r1009, %r1010}, %rd323;
	mov.b64	%rd324, {%r1010, %r1009};
	add.s64 	%rd325, %rd324, %rd252;
	xor.b64  	%rd326, %rd325, %rd241;
	mov.b64	{%r1011, %r1012}, %rd326;
	prmt.b32 	%r1013, %r1011, %r1012, %r780;
	prmt.b32 	%r1014, %r1011, %r1012, %r779;
	mov.b64	%rd327, {%r1014, %r1013};
	add.s64 	%rd328, %rd322, %rd17;
	add.s64 	%rd329, %rd328, %rd327;
	xor.b64  	%rd330, %rd329, %rd324;
	mov.b64	{%r1015, %r1016}, %rd330;
	prmt.b32 	%r1017, %r1015, %r1016, %r786;
	prmt.b32 	%r1018, %r1015, %r1016, %r785;
	mov.b64	%rd331, {%r1018, %r1017};
	add.s64 	%rd332, %rd331, %rd325;
	xor.b64  	%rd333, %rd332, %rd327;
	mov.b64	{%r190, %r191}, %rd333;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r768;
	// inline asm
	mov.b64	%rd334, {%r185, %r189};
	add.s64 	%rd335, %rd287, %rd11;
	add.s64 	%rd336, %rd335, %rd334;
	xor.b64  	%rd337, %rd336, %rd303;
	mov.b64	{%r1019, %r1020}, %rd337;
	mov.b64	%rd338, {%r1020, %r1019};
	add.s64 	%rd339, %rd338, %rd318;
	xor.b64  	%rd340, %rd339, %rd334;
	mov.b64	{%r1021, %r1022}, %rd340;
	prmt.b32 	%r1023, %r1021, %r1022, %r780;
	prmt.b32 	%r1024, %r1021, %r1022, %r779;
	mov.b64	%rd341, {%r1024, %r1023};
	add.s64 	%rd342, %rd336, %rd13;
	add.s64 	%rd343, %rd342, %rd341;
	xor.b64  	%rd344, %rd338, %rd343;
	mov.b64	{%r1025, %r1026}, %rd344;
	prmt.b32 	%r1027, %r1025, %r1026, %r786;
	prmt.b32 	%r1028, %r1025, %r1026, %r785;
	mov.b64	%rd345, {%r1028, %r1027};
	add.s64 	%rd346, %rd339, %rd345;
	xor.b64  	%rd347, %rd346, %rd341;
	mov.b64	{%r198, %r199}, %rd347;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r768;
	// inline asm
	mov.b64	%rd348, {%r193, %r197};
	add.s64 	%rd349, %rd292, %rd7;
	add.s64 	%rd350, %rd349, %rd301;
	xor.b64  	%rd351, %rd317, %rd350;
	mov.b64	{%r1029, %r1030}, %rd351;
	mov.b64	%rd352, {%r1030, %r1029};
	add.s64 	%rd353, %rd332, %rd352;
	xor.b64  	%rd354, %rd353, %rd292;
	mov.b64	{%r1031, %r1032}, %rd354;
	prmt.b32 	%r1033, %r1031, %r1032, %r780;
	prmt.b32 	%r1034, %r1031, %r1032, %r779;
	mov.b64	%rd355, {%r1034, %r1033};
	add.s64 	%rd356, %rd350, %rd5;
	add.s64 	%rd357, %rd356, %rd355;
	xor.b64  	%rd358, %rd357, %rd352;
	mov.b64	{%r1035, %r1036}, %rd358;
	prmt.b32 	%r1037, %r1035, %r1036, %r786;
	prmt.b32 	%r1038, %r1035, %r1036, %r785;
	mov.b64	%rd359, {%r1038, %r1037};
	add.s64 	%rd360, %rd359, %rd353;
	xor.b64  	%rd361, %rd360, %rd355;
	mov.b64	{%r206, %r207}, %rd361;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r768;
	// inline asm
	mov.b64	%rd362, {%r201, %r205};
	add.s64 	%rd363, %rd315, %rd306;
	xor.b64  	%rd364, %rd331, %rd363;
	mov.b64	{%r1039, %r1040}, %rd364;
	mov.b64	%rd365, {%r1040, %r1039};
	add.s64 	%rd366, %rd365, %rd290;
	xor.b64  	%rd367, %rd366, %rd306;
	mov.b64	{%r1041, %r1042}, %rd367;
	prmt.b32 	%r1043, %r1041, %r1042, %r780;
	prmt.b32 	%r1044, %r1041, %r1042, %r779;
	mov.b64	%rd368, {%r1044, %r1043};
	add.s64 	%rd369, %rd368, %rd363;
	xor.b64  	%rd370, %rd369, %rd365;
	mov.b64	{%r1045, %r1046}, %rd370;
	prmt.b32 	%r1047, %r1045, %r1046, %r786;
	prmt.b32 	%r1048, %r1045, %r1046, %r785;
	mov.b64	%rd371, {%r1048, %r1047};
	add.s64 	%rd372, %rd371, %rd366;
	xor.b64  	%rd373, %rd372, %rd368;
	mov.b64	{%r214, %r215}, %rd373;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r768;
	// inline asm
	mov.b64	%rd374, {%r209, %r213};
	add.s64 	%rd375, %rd329, %rd320;
	xor.b64  	%rd376, %rd375, %rd289;
	mov.b64	{%r1049, %r1050}, %rd376;
	mov.b64	%rd377, {%r1050, %r1049};
	add.s64 	%rd378, %rd377, %rd304;
	xor.b64  	%rd379, %rd378, %rd320;
	mov.b64	{%r1051, %r1052}, %rd379;
	prmt.b32 	%r1053, %r1051, %r1052, %r780;
	prmt.b32 	%r1054, %r1051, %r1052, %r779;
	mov.b64	%rd380, {%r1054, %r1053};
	add.s64 	%rd381, %rd380, %rd375;
	xor.b64  	%rd382, %rd381, %rd377;
	mov.b64	{%r1055, %r1056}, %rd382;
	prmt.b32 	%r1057, %r1055, %r1056, %r786;
	prmt.b32 	%r1058, %r1055, %r1056, %r785;
	mov.b64	%rd383, {%r1058, %r1057};
	add.s64 	%rd384, %rd383, %rd378;
	xor.b64  	%rd385, %rd384, %rd380;
	mov.b64	{%r222, %r223}, %rd385;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r768;
	// inline asm
	mov.b64	%rd386, {%r217, %r221};
	add.s64 	%rd387, %rd343, %rd6;
	add.s64 	%rd388, %rd387, %rd362;
	xor.b64  	%rd389, %rd383, %rd388;
	mov.b64	{%r1059, %r1060}, %rd389;
	mov.b64	%rd390, {%r1060, %r1059};
	add.s64 	%rd391, %rd390, %rd372;
	xor.b64  	%rd392, %rd391, %rd362;
	mov.b64	{%r1061, %r1062}, %rd392;
	prmt.b32 	%r1063, %r1061, %r1062, %r780;
	prmt.b32 	%r1064, %r1061, %r1062, %r779;
	mov.b64	%rd393, {%r1064, %r1063};
	add.s64 	%rd394, %rd388, %rd10;
	add.s64 	%rd395, %rd394, %rd393;
	xor.b64  	%rd396, %rd390, %rd395;
	mov.b64	{%r1065, %r1066}, %rd396;
	prmt.b32 	%r1067, %r1065, %r1066, %r786;
	prmt.b32 	%r1068, %r1065, %r1066, %r785;
	mov.b64	%rd397, {%r1068, %r1067};
	add.s64 	%rd398, %rd391, %rd397;
	xor.b64  	%rd399, %rd398, %rd393;
	mov.b64	{%r230, %r231}, %rd399;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r768;
	// inline asm
	mov.b64	%rd400, {%r225, %r229};
	add.s64 	%rd401, %rd357, %rd20;
	add.s64 	%rd402, %rd401, %rd374;
	xor.b64  	%rd403, %rd402, %rd345;
	mov.b64	{%r1069, %r1070}, %rd403;
	mov.b64	%rd404, {%r1070, %r1069};
	add.s64 	%rd405, %rd404, %rd384;
	xor.b64  	%rd406, %rd405, %rd374;
	mov.b64	{%r1071, %r1072}, %rd406;
	prmt.b32 	%r1073, %r1071, %r1072, %r780;
	prmt.b32 	%r1074, %r1071, %r1072, %r779;
	mov.b64	%rd407, {%r1074, %r1073};
	add.s64 	%rd408, %rd407, %rd402;
	xor.b64  	%rd409, %rd408, %rd404;
	mov.b64	{%r1075, %r1076}, %rd409;
	prmt.b32 	%r1077, %r1075, %r1076, %r786;
	prmt.b32 	%r1078, %r1075, %r1076, %r785;
	mov.b64	%rd410, {%r1078, %r1077};
	add.s64 	%rd411, %rd410, %rd405;
	xor.b64  	%rd412, %rd411, %rd407;
	mov.b64	{%r238, %r239}, %rd412;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r768;
	// inline asm
	mov.b64	%rd413, {%r233, %r237};
	add.s64 	%rd414, %rd369, %rd17;
	add.s64 	%rd415, %rd414, %rd386;
	xor.b64  	%rd416, %rd415, %rd359;
	mov.b64	{%r1079, %r1080}, %rd416;
	mov.b64	%rd417, {%r1080, %r1079};
	add.s64 	%rd418, %rd417, %rd346;
	xor.b64  	%rd419, %rd418, %rd386;
	mov.b64	{%r1081, %r1082}, %rd419;
	prmt.b32 	%r1083, %r1081, %r1082, %r780;
	prmt.b32 	%r1084, %r1081, %r1082, %r779;
	mov.b64	%rd420, {%r1084, %r1083};
	add.s64 	%rd421, %rd415, %rd4;
	add.s64 	%rd422, %rd421, %rd420;
	xor.b64  	%rd423, %rd422, %rd417;
	mov.b64	{%r1085, %r1086}, %rd423;
	prmt.b32 	%r1087, %r1085, %r1086, %r786;
	prmt.b32 	%r1088, %r1085, %r1086, %r785;
	mov.b64	%rd424, {%r1088, %r1087};
	add.s64 	%rd425, %rd424, %rd418;
	xor.b64  	%rd426, %rd425, %rd420;
	mov.b64	{%r246, %r247}, %rd426;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r768;
	// inline asm
	mov.b64	%rd427, {%r241, %r245};
	add.s64 	%rd428, %rd381, %rd348;
	xor.b64  	%rd429, %rd428, %rd371;
	mov.b64	{%r1089, %r1090}, %rd429;
	mov.b64	%rd430, {%r1090, %r1089};
	add.s64 	%rd431, %rd430, %rd360;
	xor.b64  	%rd432, %rd431, %rd348;
	mov.b64	{%r1091, %r1092}, %rd432;
	prmt.b32 	%r1093, %r1091, %r1092, %r780;
	prmt.b32 	%r1094, %r1091, %r1092, %r779;
	mov.b64	%rd433, {%r1094, %r1093};
	add.s64 	%rd434, %rd428, %rd12;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1095, %r1096}, %rd436;
	prmt.b32 	%r1097, %r1095, %r1096, %r786;
	prmt.b32 	%r1098, %r1095, %r1096, %r785;
	mov.b64	%rd437, {%r1098, %r1097};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r254, %r255}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r768;
	// inline asm
	mov.b64	%rd440, {%r249, %r253};
	add.s64 	%rd441, %rd395, %rd13;
	add.s64 	%rd442, %rd441, %rd440;
	xor.b64  	%rd443, %rd442, %rd410;
	mov.b64	{%r1099, %r1100}, %rd443;
	mov.b64	%rd444, {%r1100, %r1099};
	add.s64 	%rd445, %rd444, %rd425;
	xor.b64  	%rd446, %rd445, %rd440;
	mov.b64	{%r1101, %r1102}, %rd446;
	prmt.b32 	%r1103, %r1101, %r1102, %r780;
	prmt.b32 	%r1104, %r1101, %r1102, %r779;
	mov.b64	%rd447, {%r1104, %r1103};
	add.s64 	%rd448, %rd442, %rd4;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd444, %rd449;
	mov.b64	{%r1105, %r1106}, %rd450;
	prmt.b32 	%r1107, %r1105, %r1106, %r786;
	prmt.b32 	%r1108, %r1105, %r1106, %r785;
	mov.b64	%rd451, {%r1108, %r1107};
	add.s64 	%rd452, %rd445, %rd451;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r262, %r263}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r768;
	// inline asm
	mov.b64	%rd454, {%r257, %r261};
	add.s64 	%rd455, %rd400, %rd20;
	add.s64 	%rd456, %rd455, %rd408;
	xor.b64  	%rd457, %rd424, %rd456;
	mov.b64	{%r1109, %r1110}, %rd457;
	mov.b64	%rd458, {%r1110, %r1109};
	add.s64 	%rd459, %rd438, %rd458;
	xor.b64  	%rd460, %rd459, %rd400;
	mov.b64	{%r1111, %r1112}, %rd460;
	prmt.b32 	%r1113, %r1111, %r1112, %r780;
	prmt.b32 	%r1114, %r1111, %r1112, %r779;
	mov.b64	%rd461, {%r1114, %r1113};
	add.s64 	%rd462, %rd456, %rd11;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1115, %r1116}, %rd464;
	prmt.b32 	%r1117, %r1115, %r1116, %r786;
	prmt.b32 	%r1118, %r1115, %r1116, %r785;
	mov.b64	%rd465, {%r1118, %r1117};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r270, %r271}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r768;
	// inline asm
	mov.b64	%rd468, {%r265, %r269};
	add.s64 	%rd469, %rd413, %rd6;
	add.s64 	%rd470, %rd469, %rd422;
	xor.b64  	%rd471, %rd437, %rd470;
	mov.b64	{%r1119, %r1120}, %rd471;
	mov.b64	%rd472, {%r1120, %r1119};
	add.s64 	%rd473, %rd472, %rd398;
	xor.b64  	%rd474, %rd473, %rd413;
	mov.b64	{%r1121, %r1122}, %rd474;
	prmt.b32 	%r1123, %r1121, %r1122, %r780;
	prmt.b32 	%r1124, %r1121, %r1122, %r779;
	mov.b64	%rd475, {%r1124, %r1123};
	add.s64 	%rd476, %rd470, %rd17;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd477, %rd472;
	mov.b64	{%r1125, %r1126}, %rd478;
	prmt.b32 	%r1127, %r1125, %r1126, %r786;
	prmt.b32 	%r1128, %r1125, %r1126, %r785;
	mov.b64	%rd479, {%r1128, %r1127};
	add.s64 	%rd480, %rd479, %rd473;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r278, %r279}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r768;
	// inline asm
	mov.b64	%rd482, {%r273, %r277};
	add.s64 	%rd483, %rd435, %rd427;
	xor.b64  	%rd484, %rd483, %rd397;
	mov.b64	{%r1129, %r1130}, %rd484;
	mov.b64	%rd485, {%r1130, %r1129};
	add.s64 	%rd486, %rd485, %rd411;
	xor.b64  	%rd487, %rd486, %rd427;
	mov.b64	{%r1131, %r1132}, %rd487;
	prmt.b32 	%r1133, %r1131, %r1132, %r780;
	prmt.b32 	%r1134, %r1131, %r1132, %r779;
	mov.b64	%rd488, {%r1134, %r1133};
	add.s64 	%rd489, %rd488, %rd483;
	xor.b64  	%rd490, %rd489, %rd485;
	mov.b64	{%r1135, %r1136}, %rd490;
	prmt.b32 	%r1137, %r1135, %r1136, %r786;
	prmt.b32 	%r1138, %r1135, %r1136, %r785;
	mov.b64	%rd491, {%r1138, %r1137};
	add.s64 	%rd492, %rd491, %rd486;
	xor.b64  	%rd493, %rd492, %rd488;
	mov.b64	{%r286, %r287}, %rd493;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r768;
	// inline asm
	mov.b64	%rd494, {%r281, %r285};
	add.s64 	%rd495, %rd468, %rd449;
	xor.b64  	%rd496, %rd491, %rd495;
	mov.b64	{%r1139, %r1140}, %rd496;
	mov.b64	%rd497, {%r1140, %r1139};
	add.s64 	%rd498, %rd497, %rd480;
	xor.b64  	%rd499, %rd498, %rd468;
	mov.b64	{%r1141, %r1142}, %rd499;
	prmt.b32 	%r1143, %r1141, %r1142, %r780;
	prmt.b32 	%r1144, %r1141, %r1142, %r779;
	mov.b64	%rd500, {%r1144, %r1143};
	add.s64 	%rd501, %rd495, %rd5;
	add.s64 	%rd502, %rd501, %rd500;
	xor.b64  	%rd503, %rd497, %rd502;
	mov.b64	{%r1145, %r1146}, %rd503;
	prmt.b32 	%r1147, %r1145, %r1146, %r786;
	prmt.b32 	%r1148, %r1145, %r1146, %r785;
	mov.b64	%rd504, {%r1148, %r1147};
	add.s64 	%rd505, %rd498, %rd504;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r294, %r295}, %rd506;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r768;
	// inline asm
	mov.b64	%rd507, {%r289, %r293};
	add.s64 	%rd508, %rd482, %rd463;
	xor.b64  	%rd509, %rd508, %rd451;
	mov.b64	{%r1149, %r1150}, %rd509;
	mov.b64	%rd510, {%r1150, %r1149};
	add.s64 	%rd511, %rd510, %rd492;
	xor.b64  	%rd512, %rd511, %rd482;
	mov.b64	{%r1151, %r1152}, %rd512;
	prmt.b32 	%r1153, %r1151, %r1152, %r780;
	prmt.b32 	%r1154, %r1151, %r1152, %r779;
	mov.b64	%rd513, {%r1154, %r1153};
	add.s64 	%rd514, %rd513, %rd508;
	xor.b64  	%rd515, %rd514, %rd510;
	mov.b64	{%r1155, %r1156}, %rd515;
	prmt.b32 	%r1157, %r1155, %r1156, %r786;
	prmt.b32 	%r1158, %r1155, %r1156, %r785;
	mov.b64	%rd516, {%r1158, %r1157};
	add.s64 	%rd517, %rd516, %rd511;
	xor.b64  	%rd518, %rd517, %rd513;
	mov.b64	{%r302, %r303}, %rd518;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r768;
	// inline asm
	mov.b64	%rd519, {%r297, %r301};
	add.s64 	%rd520, %rd477, %rd10;
	add.s64 	%rd521, %rd520, %rd494;
	xor.b64  	%rd522, %rd521, %rd465;
	mov.b64	{%r1159, %r1160}, %rd522;
	mov.b64	%rd523, {%r1160, %r1159};
	add.s64 	%rd524, %rd523, %rd452;
	xor.b64  	%rd525, %rd524, %rd494;
	mov.b64	{%r1161, %r1162}, %rd525;
	prmt.b32 	%r1163, %r1161, %r1162, %r780;
	prmt.b32 	%r1164, %r1161, %r1162, %r779;
	mov.b64	%rd526, {%r1164, %r1163};
	add.s64 	%rd527, %rd521, %rd12;
	add.s64 	%rd528, %rd527, %rd526;
	xor.b64  	%rd529, %rd528, %rd523;
	mov.b64	{%r1165, %r1166}, %rd529;
	prmt.b32 	%r1167, %r1165, %r1166, %r786;
	prmt.b32 	%r1168, %r1165, %r1166, %r785;
	mov.b64	%rd530, {%r1168, %r1167};
	add.s64 	%rd531, %rd530, %rd524;
	xor.b64  	%rd532, %rd531, %rd526;
	mov.b64	{%r310, %r311}, %rd532;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r768;
	// inline asm
	mov.b64	%rd533, {%r305, %r309};
	add.s64 	%rd534, %rd454, %rd7;
	add.s64 	%rd535, %rd534, %rd489;
	xor.b64  	%rd536, %rd535, %rd479;
	mov.b64	{%r1169, %r1170}, %rd536;
	mov.b64	%rd537, {%r1170, %r1169};
	add.s64 	%rd538, %rd537, %rd466;
	xor.b64  	%rd539, %rd538, %rd454;
	mov.b64	{%r1171, %r1172}, %rd539;
	prmt.b32 	%r1173, %r1171, %r1172, %r780;
	prmt.b32 	%r1174, %r1171, %r1172, %r779;
	mov.b64	%rd540, {%r1174, %r1173};
	add.s64 	%rd541, %rd540, %rd535;
	xor.b64  	%rd542, %rd541, %rd537;
	mov.b64	{%r1175, %r1176}, %rd542;
	prmt.b32 	%r1177, %r1175, %r1176, %r786;
	prmt.b32 	%r1178, %r1175, %r1176, %r785;
	mov.b64	%rd543, {%r1178, %r1177};
	add.s64 	%rd544, %rd543, %rd538;
	xor.b64  	%rd545, %rd544, %rd540;
	mov.b64	{%r318, %r319}, %rd545;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r768;
	// inline asm
	mov.b64	%rd546, {%r313, %r317};
	add.s64 	%rd547, %rd502, %rd6;
	add.s64 	%rd548, %rd547, %rd546;
	xor.b64  	%rd549, %rd548, %rd516;
	mov.b64	{%r1179, %r1180}, %rd549;
	mov.b64	%rd550, {%r1180, %r1179};
	add.s64 	%rd551, %rd550, %rd531;
	xor.b64  	%rd552, %rd551, %rd546;
	mov.b64	{%r1181, %r1182}, %rd552;
	prmt.b32 	%r1183, %r1181, %r1182, %r780;
	prmt.b32 	%r1184, %r1181, %r1182, %r779;
	mov.b64	%rd553, {%r1184, %r1183};
	add.s64 	%rd554, %rd553, %rd548;
	xor.b64  	%rd555, %rd550, %rd554;
	mov.b64	{%r1185, %r1186}, %rd555;
	prmt.b32 	%r1187, %r1185, %r1186, %r786;
	prmt.b32 	%r1188, %r1185, %r1186, %r785;
	mov.b64	%rd556, {%r1188, %r1187};
	add.s64 	%rd557, %rd551, %rd556;
	xor.b64  	%rd558, %rd557, %rd553;
	mov.b64	{%r326, %r327}, %rd558;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r768;
	// inline asm
	mov.b64	%rd559, {%r321, %r325};
	add.s64 	%rd560, %rd507, %rd10;
	add.s64 	%rd561, %rd560, %rd514;
	xor.b64  	%rd562, %rd530, %rd561;
	mov.b64	{%r1189, %r1190}, %rd562;
	mov.b64	%rd563, {%r1190, %r1189};
	add.s64 	%rd564, %rd544, %rd563;
	xor.b64  	%rd565, %rd564, %rd507;
	mov.b64	{%r1191, %r1192}, %rd565;
	prmt.b32 	%r1193, %r1191, %r1192, %r780;
	prmt.b32 	%r1194, %r1191, %r1192, %r779;
	mov.b64	%rd566, {%r1194, %r1193};
	add.s64 	%rd567, %rd566, %rd561;
	xor.b64  	%rd568, %rd567, %rd563;
	mov.b64	{%r1195, %r1196}, %rd568;
	prmt.b32 	%r1197, %r1195, %r1196, %r786;
	prmt.b32 	%r1198, %r1195, %r1196, %r785;
	mov.b64	%rd569, {%r1198, %r1197};
	add.s64 	%rd570, %rd569, %rd564;
	xor.b64  	%rd571, %rd570, %rd566;
	mov.b64	{%r334, %r335}, %rd571;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r768;
	// inline asm
	mov.b64	%rd572, {%r329, %r333};
	add.s64 	%rd573, %rd519, %rd4;
	add.s64 	%rd574, %rd573, %rd528;
	xor.b64  	%rd575, %rd543, %rd574;
	mov.b64	{%r1199, %r1200}, %rd575;
	mov.b64	%rd576, {%r1200, %r1199};
	add.s64 	%rd577, %rd576, %rd505;
	xor.b64  	%rd578, %rd577, %rd519;
	mov.b64	{%r1201, %r1202}, %rd578;
	prmt.b32 	%r1203, %r1201, %r1202, %r780;
	prmt.b32 	%r1204, %r1201, %r1202, %r779;
	mov.b64	%rd579, {%r1204, %r1203};
	add.s64 	%rd580, %rd579, %rd574;
	xor.b64  	%rd581, %rd580, %rd576;
	mov.b64	{%r1205, %r1206}, %rd581;
	prmt.b32 	%r1207, %r1205, %r1206, %r786;
	prmt.b32 	%r1208, %r1205, %r1206, %r785;
	mov.b64	%rd582, {%r1208, %r1207};
	add.s64 	%rd583, %rd582, %rd577;
	xor.b64  	%rd584, %rd583, %rd579;
	mov.b64	{%r342, %r343}, %rd584;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r768;
	// inline asm
	mov.b64	%rd585, {%r337, %r341};
	add.s64 	%rd586, %rd533, %rd12;
	add.s64 	%rd587, %rd586, %rd541;
	xor.b64  	%rd588, %rd587, %rd504;
	mov.b64	{%r1209, %r1210}, %rd588;
	mov.b64	%rd589, {%r1210, %r1209};
	add.s64 	%rd590, %rd589, %rd517;
	xor.b64  	%rd591, %rd590, %rd533;
	mov.b64	{%r1211, %r1212}, %rd591;
	prmt.b32 	%r1213, %r1211, %r1212, %r780;
	prmt.b32 	%r1214, %r1211, %r1212, %r779;
	mov.b64	%rd592, {%r1214, %r1213};
	add.s64 	%rd593, %rd587, %rd7;
	add.s64 	%rd594, %rd593, %rd592;
	xor.b64  	%rd595, %rd594, %rd589;
	mov.b64	{%r1215, %r1216}, %rd595;
	prmt.b32 	%r1217, %r1215, %r1216, %r786;
	prmt.b32 	%r1218, %r1215, %r1216, %r785;
	mov.b64	%rd596, {%r1218, %r1217};
	add.s64 	%rd597, %rd596, %rd590;
	xor.b64  	%rd598, %rd597, %rd592;
	mov.b64	{%r350, %r351}, %rd598;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r768;
	// inline asm
	mov.b64	%rd599, {%r345, %r349};
	add.s64 	%rd600, %rd554, %rd17;
	add.s64 	%rd601, %rd600, %rd572;
	xor.b64  	%rd602, %rd596, %rd601;
	mov.b64	{%r1219, %r1220}, %rd602;
	mov.b64	%rd603, {%r1220, %r1219};
	add.s64 	%rd604, %rd603, %rd583;
	xor.b64  	%rd605, %rd604, %rd572;
	mov.b64	{%r1221, %r1222}, %rd605;
	prmt.b32 	%r1223, %r1221, %r1222, %r780;
	prmt.b32 	%r1224, %r1221, %r1222, %r779;
	mov.b64	%rd606, {%r1224, %r1223};
	add.s64 	%rd607, %rd606, %rd601;
	xor.b64  	%rd608, %rd603, %rd607;
	mov.b64	{%r1225, %r1226}, %rd608;
	prmt.b32 	%r1227, %r1225, %r1226, %r786;
	prmt.b32 	%r1228, %r1225, %r1226, %r785;
	mov.b64	%rd609, {%r1228, %r1227};
	add.s64 	%rd610, %rd604, %rd609;
	xor.b64  	%rd611, %rd610, %rd606;
	mov.b64	{%r358, %r359}, %rd611;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r768;
	// inline asm
	mov.b64	%rd612, {%r353, %r357};
	add.s64 	%rd613, %rd567, %rd11;
	add.s64 	%rd614, %rd613, %rd585;
	xor.b64  	%rd615, %rd614, %rd556;
	mov.b64	{%r1229, %r1230}, %rd615;
	mov.b64	%rd616, {%r1230, %r1229};
	add.s64 	%rd617, %rd616, %rd597;
	xor.b64  	%rd618, %rd617, %rd585;
	mov.b64	{%r1231, %r1232}, %rd618;
	prmt.b32 	%r1233, %r1231, %r1232, %r780;
	prmt.b32 	%r1234, %r1231, %r1232, %r779;
	mov.b64	%rd619, {%r1234, %r1233};
	add.s64 	%rd620, %rd614, %rd20;
	add.s64 	%rd621, %rd620, %rd619;
	xor.b64  	%rd622, %rd621, %rd616;
	mov.b64	{%r1235, %r1236}, %rd622;
	prmt.b32 	%r1237, %r1235, %r1236, %r786;
	prmt.b32 	%r1238, %r1235, %r1236, %r785;
	mov.b64	%rd623, {%r1238, %r1237};
	add.s64 	%rd624, %rd623, %rd617;
	xor.b64  	%rd625, %rd624, %rd619;
	mov.b64	{%r366, %r367}, %rd625;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r768;
	// inline asm
	mov.b64	%rd626, {%r361, %r365};
	add.s64 	%rd627, %rd599, %rd580;
	xor.b64  	%rd628, %rd627, %rd569;
	mov.b64	{%r1239, %r1240}, %rd628;
	mov.b64	%rd629, {%r1240, %r1239};
	add.s64 	%rd630, %rd629, %rd557;
	xor.b64  	%rd631, %rd630, %rd599;
	mov.b64	{%r1241, %r1242}, %rd631;
	prmt.b32 	%r1243, %r1241, %r1242, %r780;
	prmt.b32 	%r1244, %r1241, %r1242, %r779;
	mov.b64	%rd632, {%r1244, %r1243};
	add.s64 	%rd633, %rd632, %rd627;
	xor.b64  	%rd634, %rd633, %rd629;
	mov.b64	{%r1245, %r1246}, %rd634;
	prmt.b32 	%r1247, %r1245, %r1246, %r786;
	prmt.b32 	%r1248, %r1245, %r1246, %r785;
	mov.b64	%rd635, {%r1248, %r1247};
	add.s64 	%rd636, %rd635, %rd630;
	xor.b64  	%rd637, %rd636, %rd632;
	mov.b64	{%r374, %r375}, %rd637;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r768;
	// inline asm
	mov.b64	%rd638, {%r369, %r373};
	add.s64 	%rd639, %rd559, %rd5;
	add.s64 	%rd640, %rd639, %rd594;
	xor.b64  	%rd641, %rd640, %rd582;
	mov.b64	{%r1249, %r1250}, %rd641;
	mov.b64	%rd642, {%r1250, %r1249};
	add.s64 	%rd643, %rd642, %rd570;
	xor.b64  	%rd644, %rd643, %rd559;
	mov.b64	{%r1251, %r1252}, %rd644;
	prmt.b32 	%r1253, %r1251, %r1252, %r780;
	prmt.b32 	%r1254, %r1251, %r1252, %r779;
	mov.b64	%rd645, {%r1254, %r1253};
	add.s64 	%rd646, %rd640, %rd13;
	add.s64 	%rd647, %rd646, %rd645;
	xor.b64  	%rd648, %rd647, %rd642;
	mov.b64	{%r1255, %r1256}, %rd648;
	prmt.b32 	%r1257, %r1255, %r1256, %r786;
	prmt.b32 	%r1258, %r1255, %r1256, %r785;
	mov.b64	%rd649, {%r1258, %r1257};
	add.s64 	%rd650, %rd649, %rd643;
	xor.b64  	%rd651, %rd650, %rd645;
	mov.b64	{%r382, %r383}, %rd651;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r768;
	// inline asm
	mov.b64	%rd652, {%r377, %r381};
	add.s64 	%rd653, %rd652, %rd607;
	xor.b64  	%rd654, %rd653, %rd623;
	mov.b64	{%r1259, %r1260}, %rd654;
	mov.b64	%rd655, {%r1260, %r1259};
	add.s64 	%rd656, %rd655, %rd636;
	xor.b64  	%rd657, %rd656, %rd652;
	mov.b64	{%r1261, %r1262}, %rd657;
	prmt.b32 	%r1263, %r1261, %r1262, %r780;
	prmt.b32 	%r1264, %r1261, %r1262, %r779;
	mov.b64	%rd658, {%r1264, %r1263};
	add.s64 	%rd659, %rd653, %rd20;
	add.s64 	%rd660, %rd659, %rd658;
	xor.b64  	%rd661, %rd655, %rd660;
	mov.b64	{%r1265, %r1266}, %rd661;
	prmt.b32 	%r1267, %r1265, %r1266, %r786;
	prmt.b32 	%r1268, %r1265, %r1266, %r785;
	mov.b64	%rd662, {%r1268, %r1267};
	add.s64 	%rd663, %rd656, %rd662;
	xor.b64  	%rd664, %rd663, %rd658;
	mov.b64	{%r390, %r391}, %rd664;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r768;
	// inline asm
	mov.b64	%rd665, {%r385, %r389};
	add.s64 	%rd666, %rd612, %rd5;
	add.s64 	%rd667, %rd666, %rd621;
	xor.b64  	%rd668, %rd635, %rd667;
	mov.b64	{%r1269, %r1270}, %rd668;
	mov.b64	%rd669, {%r1270, %r1269};
	add.s64 	%rd670, %rd650, %rd669;
	xor.b64  	%rd671, %rd670, %rd612;
	mov.b64	{%r1271, %r1272}, %rd671;
	prmt.b32 	%r1273, %r1271, %r1272, %r780;
	prmt.b32 	%r1274, %r1271, %r1272, %r779;
	mov.b64	%rd672, {%r1274, %r1273};
	add.s64 	%rd673, %rd672, %rd667;
	xor.b64  	%rd674, %rd673, %rd669;
	mov.b64	{%r1275, %r1276}, %rd674;
	prmt.b32 	%r1277, %r1275, %r1276, %r786;
	prmt.b32 	%r1278, %r1275, %r1276, %r785;
	mov.b64	%rd675, {%r1278, %r1277};
	add.s64 	%rd676, %rd675, %rd670;
	xor.b64  	%rd677, %rd676, %rd672;
	mov.b64	{%r398, %r399}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r768;
	// inline asm
	mov.b64	%rd678, {%r393, %r397};
	add.s64 	%rd679, %rd633, %rd626;
	xor.b64  	%rd680, %rd649, %rd679;
	mov.b64	{%r1279, %r1280}, %rd680;
	mov.b64	%rd681, {%r1280, %r1279};
	add.s64 	%rd682, %rd681, %rd610;
	xor.b64  	%rd683, %rd682, %rd626;
	mov.b64	{%r1281, %r1282}, %rd683;
	prmt.b32 	%r1283, %r1281, %r1282, %r780;
	prmt.b32 	%r1284, %r1281, %r1282, %r779;
	mov.b64	%rd684, {%r1284, %r1283};
	add.s64 	%rd685, %rd684, %rd679;
	xor.b64  	%rd686, %rd685, %rd681;
	mov.b64	{%r1285, %r1286}, %rd686;
	prmt.b32 	%r1287, %r1285, %r1286, %r786;
	prmt.b32 	%r1288, %r1285, %r1286, %r785;
	mov.b64	%rd687, {%r1288, %r1287};
	add.s64 	%rd688, %rd687, %rd682;
	xor.b64  	%rd689, %rd688, %rd684;
	mov.b64	{%r406, %r407}, %rd689;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r768;
	// inline asm
	mov.b64	%rd690, {%r401, %r405};
	add.s64 	%rd691, %rd638, %rd17;
	add.s64 	%rd692, %rd691, %rd647;
	xor.b64  	%rd693, %rd692, %rd609;
	mov.b64	{%r1289, %r1290}, %rd693;
	mov.b64	%rd694, {%r1290, %r1289};
	add.s64 	%rd695, %rd694, %rd624;
	xor.b64  	%rd696, %rd695, %rd638;
	mov.b64	{%r1291, %r1292}, %rd696;
	prmt.b32 	%r1293, %r1291, %r1292, %r780;
	prmt.b32 	%r1294, %r1291, %r1292, %r779;
	mov.b64	%rd697, {%r1294, %r1293};
	add.s64 	%rd698, %rd697, %rd692;
	xor.b64  	%rd699, %rd698, %rd694;
	mov.b64	{%r1295, %r1296}, %rd699;
	prmt.b32 	%r1297, %r1295, %r1296, %r786;
	prmt.b32 	%r1298, %r1295, %r1296, %r785;
	mov.b64	%rd700, {%r1298, %r1297};
	add.s64 	%rd701, %rd700, %rd695;
	xor.b64  	%rd702, %rd701, %rd697;
	mov.b64	{%r414, %r415}, %rd702;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r768;
	// inline asm
	mov.b64	%rd703, {%r409, %r413};
	add.s64 	%rd704, %rd660, %rd4;
	add.s64 	%rd705, %rd704, %rd678;
	xor.b64  	%rd706, %rd700, %rd705;
	mov.b64	{%r1299, %r1300}, %rd706;
	mov.b64	%rd707, {%r1300, %r1299};
	add.s64 	%rd708, %rd707, %rd688;
	xor.b64  	%rd709, %rd708, %rd678;
	mov.b64	{%r1301, %r1302}, %rd709;
	prmt.b32 	%r1303, %r1301, %r1302, %r780;
	prmt.b32 	%r1304, %r1301, %r1302, %r779;
	mov.b64	%rd710, {%r1304, %r1303};
	add.s64 	%rd711, %rd705, %rd11;
	add.s64 	%rd712, %rd711, %rd710;
	xor.b64  	%rd713, %rd707, %rd712;
	mov.b64	{%r1305, %r1306}, %rd713;
	prmt.b32 	%r1307, %r1305, %r1306, %r786;
	prmt.b32 	%r1308, %r1305, %r1306, %r785;
	mov.b64	%rd714, {%r1308, %r1307};
	add.s64 	%rd715, %rd708, %rd714;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r422, %r423}, %rd716;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r768;
	// inline asm
	mov.b64	%rd717, {%r417, %r421};
	add.s64 	%rd718, %rd673, %rd10;
	add.s64 	%rd719, %rd718, %rd690;
	xor.b64  	%rd720, %rd719, %rd662;
	mov.b64	{%r1309, %r1310}, %rd720;
	mov.b64	%rd721, {%r1310, %r1309};
	add.s64 	%rd722, %rd721, %rd701;
	xor.b64  	%rd723, %rd722, %rd690;
	mov.b64	{%r1311, %r1312}, %rd723;
	prmt.b32 	%r1313, %r1311, %r1312, %r780;
	prmt.b32 	%r1314, %r1311, %r1312, %r779;
	mov.b64	%rd724, {%r1314, %r1313};
	add.s64 	%rd725, %rd719, %rd7;
	add.s64 	%rd726, %rd725, %rd724;
	xor.b64  	%rd727, %rd726, %rd721;
	mov.b64	{%r1315, %r1316}, %rd727;
	prmt.b32 	%r1317, %r1315, %r1316, %r786;
	prmt.b32 	%r1318, %r1315, %r1316, %r785;
	mov.b64	%rd728, {%r1318, %r1317};
	add.s64 	%rd729, %rd728, %rd722;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r430, %r431}, %rd730;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r768;
	// inline asm
	mov.b64	%rd731, {%r425, %r429};
	add.s64 	%rd732, %rd685, %rd13;
	add.s64 	%rd733, %rd732, %rd703;
	xor.b64  	%rd734, %rd733, %rd675;
	mov.b64	{%r1319, %r1320}, %rd734;
	mov.b64	%rd735, {%r1320, %r1319};
	add.s64 	%rd736, %rd735, %rd663;
	xor.b64  	%rd737, %rd736, %rd703;
	mov.b64	{%r1321, %r1322}, %rd737;
	prmt.b32 	%r1323, %r1321, %r1322, %r780;
	prmt.b32 	%r1324, %r1321, %r1322, %r779;
	mov.b64	%rd738, {%r1324, %r1323};
	add.s64 	%rd739, %rd733, %rd6;
	add.s64 	%rd740, %rd739, %rd738;
	xor.b64  	%rd741, %rd740, %rd735;
	mov.b64	{%r1325, %r1326}, %rd741;
	prmt.b32 	%r1327, %r1325, %r1326, %r786;
	prmt.b32 	%r1328, %r1325, %r1326, %r785;
	mov.b64	%rd742, {%r1328, %r1327};
	add.s64 	%rd743, %rd742, %rd736;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r438, %r439}, %rd744;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r768;
	// inline asm
	mov.b64	%rd745, {%r433, %r437};
	add.s64 	%rd746, %rd665, %rd12;
	add.s64 	%rd747, %rd746, %rd698;
	xor.b64  	%rd748, %rd747, %rd687;
	mov.b64	{%r1329, %r1330}, %rd748;
	mov.b64	%rd749, {%r1330, %r1329};
	add.s64 	%rd750, %rd749, %rd676;
	xor.b64  	%rd751, %rd750, %rd665;
	mov.b64	{%r1331, %r1332}, %rd751;
	prmt.b32 	%r1333, %r1331, %r1332, %r780;
	prmt.b32 	%r1334, %r1331, %r1332, %r779;
	mov.b64	%rd752, {%r1334, %r1333};
	add.s64 	%rd753, %rd752, %rd747;
	xor.b64  	%rd754, %rd753, %rd749;
	mov.b64	{%r1335, %r1336}, %rd754;
	prmt.b32 	%r1337, %r1335, %r1336, %r786;
	prmt.b32 	%r1338, %r1335, %r1336, %r785;
	mov.b64	%rd755, {%r1338, %r1337};
	add.s64 	%rd756, %rd755, %rd750;
	xor.b64  	%rd757, %rd756, %rd752;
	mov.b64	{%r446, %r447}, %rd757;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r768;
	// inline asm
	mov.b64	%rd758, {%r441, %r445};
	add.s64 	%rd759, %rd758, %rd712;
	xor.b64  	%rd760, %rd759, %rd728;
	mov.b64	{%r1339, %r1340}, %rd760;
	mov.b64	%rd761, {%r1340, %r1339};
	add.s64 	%rd762, %rd761, %rd743;
	xor.b64  	%rd763, %rd762, %rd758;
	mov.b64	{%r1341, %r1342}, %rd763;
	prmt.b32 	%r1343, %r1341, %r1342, %r780;
	prmt.b32 	%r1344, %r1341, %r1342, %r779;
	mov.b64	%rd764, {%r1344, %r1343};
	add.s64 	%rd765, %rd764, %rd759;
	xor.b64  	%rd766, %rd761, %rd765;
	mov.b64	{%r1345, %r1346}, %rd766;
	prmt.b32 	%r1347, %r1345, %r1346, %r786;
	prmt.b32 	%r1348, %r1345, %r1346, %r785;
	mov.b64	%rd767, {%r1348, %r1347};
	add.s64 	%rd768, %rd762, %rd767;
	xor.b64  	%rd769, %rd768, %rd764;
	mov.b64	{%r454, %r455}, %rd769;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r768;
	// inline asm
	mov.b64	%rd770, {%r449, %r453};
	add.s64 	%rd771, %rd717, %rd11;
	add.s64 	%rd772, %rd771, %rd726;
	xor.b64  	%rd773, %rd742, %rd772;
	mov.b64	{%r1349, %r1350}, %rd773;
	mov.b64	%rd774, {%r1350, %r1349};
	add.s64 	%rd775, %rd756, %rd774;
	xor.b64  	%rd776, %rd775, %rd717;
	mov.b64	{%r1351, %r1352}, %rd776;
	prmt.b32 	%r1353, %r1351, %r1352, %r780;
	prmt.b32 	%r1354, %r1351, %r1352, %r779;
	mov.b64	%rd777, {%r1354, %r1353};
	add.s64 	%rd778, %rd777, %rd772;
	xor.b64  	%rd779, %rd778, %rd774;
	mov.b64	{%r1355, %r1356}, %rd779;
	prmt.b32 	%r1357, %r1355, %r1356, %r786;
	prmt.b32 	%r1358, %r1355, %r1356, %r785;
	mov.b64	%rd780, {%r1358, %r1357};
	add.s64 	%rd781, %rd780, %rd775;
	xor.b64  	%rd782, %rd781, %rd777;
	mov.b64	{%r462, %r463}, %rd782;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r768;
	// inline asm
	mov.b64	%rd783, {%r457, %r461};
	add.s64 	%rd784, %rd740, %rd731;
	xor.b64  	%rd785, %rd755, %rd784;
	mov.b64	{%r1359, %r1360}, %rd785;
	mov.b64	%rd786, {%r1360, %r1359};
	add.s64 	%rd787, %rd786, %rd715;
	xor.b64  	%rd788, %rd787, %rd731;
	mov.b64	{%r1361, %r1362}, %rd788;
	prmt.b32 	%r1363, %r1361, %r1362, %r780;
	prmt.b32 	%r1364, %r1361, %r1362, %r779;
	mov.b64	%rd789, {%r1364, %r1363};
	add.s64 	%rd790, %rd784, %rd5;
	add.s64 	%rd791, %rd790, %rd789;
	xor.b64  	%rd792, %rd791, %rd786;
	mov.b64	{%r1365, %r1366}, %rd792;
	prmt.b32 	%r1367, %r1365, %r1366, %r786;
	prmt.b32 	%r1368, %r1365, %r1366, %r785;
	mov.b64	%rd793, {%r1368, %r1367};
	add.s64 	%rd794, %rd793, %rd787;
	xor.b64  	%rd795, %rd794, %rd789;
	mov.b64	{%r470, %r471}, %rd795;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r768;
	// inline asm
	mov.b64	%rd796, {%r465, %r469};
	add.s64 	%rd797, %rd745, %rd7;
	add.s64 	%rd798, %rd797, %rd753;
	xor.b64  	%rd799, %rd798, %rd714;
	mov.b64	{%r1369, %r1370}, %rd799;
	mov.b64	%rd800, {%r1370, %r1369};
	add.s64 	%rd801, %rd800, %rd729;
	xor.b64  	%rd802, %rd801, %rd745;
	mov.b64	{%r1371, %r1372}, %rd802;
	prmt.b32 	%r1373, %r1371, %r1372, %r780;
	prmt.b32 	%r1374, %r1371, %r1372, %r779;
	mov.b64	%rd803, {%r1374, %r1373};
	add.s64 	%rd804, %rd798, %rd13;
	add.s64 	%rd805, %rd804, %rd803;
	xor.b64  	%rd806, %rd805, %rd800;
	mov.b64	{%r1375, %r1376}, %rd806;
	prmt.b32 	%r1377, %r1375, %r1376, %r786;
	prmt.b32 	%r1378, %r1375, %r1376, %r785;
	mov.b64	%rd807, {%r1378, %r1377};
	add.s64 	%rd808, %rd807, %rd801;
	xor.b64  	%rd809, %rd808, %rd803;
	mov.b64	{%r478, %r479}, %rd809;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r768;
	// inline asm
	mov.b64	%rd810, {%r473, %r477};
	add.s64 	%rd811, %rd765, %rd20;
	add.s64 	%rd812, %rd811, %rd783;
	xor.b64  	%rd813, %rd807, %rd812;
	mov.b64	{%r1379, %r1380}, %rd813;
	mov.b64	%rd814, {%r1380, %r1379};
	add.s64 	%rd815, %rd814, %rd794;
	xor.b64  	%rd816, %rd815, %rd783;
	mov.b64	{%r1381, %r1382}, %rd816;
	prmt.b32 	%r1383, %r1381, %r1382, %r780;
	prmt.b32 	%r1384, %r1381, %r1382, %r779;
	mov.b64	%rd817, {%r1384, %r1383};
	add.s64 	%rd818, %rd812, %rd4;
	add.s64 	%rd819, %rd818, %rd817;
	xor.b64  	%rd820, %rd814, %rd819;
	mov.b64	{%r1385, %r1386}, %rd820;
	prmt.b32 	%r1387, %r1385, %r1386, %r786;
	prmt.b32 	%r1388, %r1385, %r1386, %r785;
	mov.b64	%rd821, {%r1388, %r1387};
	add.s64 	%rd822, %rd815, %rd821;
	xor.b64  	%rd823, %rd822, %rd817;
	mov.b64	{%r486, %r487}, %rd823;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r768;
	// inline asm
	mov.b64	%rd824, {%r481, %r485};
	add.s64 	%rd825, %rd796, %rd778;
	xor.b64  	%rd826, %rd825, %rd767;
	mov.b64	{%r1389, %r1390}, %rd826;
	mov.b64	%rd827, {%r1390, %r1389};
	add.s64 	%rd828, %rd827, %rd808;
	xor.b64  	%rd829, %rd828, %rd796;
	mov.b64	{%r1391, %r1392}, %rd829;
	prmt.b32 	%r1393, %r1391, %r1392, %r780;
	prmt.b32 	%r1394, %r1391, %r1392, %r779;
	mov.b64	%rd830, {%r1394, %r1393};
	add.s64 	%rd831, %rd825, %rd17;
	add.s64 	%rd832, %rd831, %rd830;
	xor.b64  	%rd833, %rd832, %rd827;
	mov.b64	{%r1395, %r1396}, %rd833;
	prmt.b32 	%r1397, %r1395, %r1396, %r786;
	prmt.b32 	%r1398, %r1395, %r1396, %r785;
	mov.b64	%rd834, {%r1398, %r1397};
	add.s64 	%rd835, %rd834, %rd828;
	xor.b64  	%rd836, %rd835, %rd830;
	mov.b64	{%r494, %r495}, %rd836;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r768;
	// inline asm
	mov.b64	%rd837, {%r489, %r493};
	add.s64 	%rd838, %rd791, %rd12;
	add.s64 	%rd839, %rd838, %rd810;
	xor.b64  	%rd840, %rd839, %rd780;
	mov.b64	{%r1399, %r1400}, %rd840;
	mov.b64	%rd841, {%r1400, %r1399};
	add.s64 	%rd842, %rd841, %rd768;
	xor.b64  	%rd843, %rd842, %rd810;
	mov.b64	{%r1401, %r1402}, %rd843;
	prmt.b32 	%r1403, %r1401, %r1402, %r780;
	prmt.b32 	%r1404, %r1401, %r1402, %r779;
	mov.b64	%rd844, {%r1404, %r1403};
	add.s64 	%rd845, %rd839, %rd10;
	add.s64 	%rd846, %rd845, %rd844;
	xor.b64  	%rd847, %rd846, %rd841;
	mov.b64	{%r1405, %r1406}, %rd847;
	prmt.b32 	%r1407, %r1405, %r1406, %r786;
	prmt.b32 	%r1408, %r1405, %r1406, %r785;
	mov.b64	%rd848, {%r1408, %r1407};
	add.s64 	%rd849, %rd848, %rd842;
	xor.b64  	%rd850, %rd849, %rd844;
	mov.b64	{%r502, %r503}, %rd850;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r768;
	// inline asm
	mov.b64	%rd851, {%r497, %r501};
	add.s64 	%rd852, %rd770, %rd6;
	add.s64 	%rd853, %rd852, %rd805;
	xor.b64  	%rd854, %rd853, %rd793;
	mov.b64	{%r1409, %r1410}, %rd854;
	mov.b64	%rd855, {%r1410, %r1409};
	add.s64 	%rd856, %rd855, %rd781;
	xor.b64  	%rd857, %rd856, %rd770;
	mov.b64	{%r1411, %r1412}, %rd857;
	prmt.b32 	%r1413, %r1411, %r1412, %r780;
	prmt.b32 	%r1414, %r1411, %r1412, %r779;
	mov.b64	%rd858, {%r1414, %r1413};
	add.s64 	%rd859, %rd858, %rd853;
	xor.b64  	%rd860, %rd859, %rd855;
	mov.b64	{%r1415, %r1416}, %rd860;
	prmt.b32 	%r1417, %r1415, %r1416, %r786;
	prmt.b32 	%r1418, %r1415, %r1416, %r785;
	mov.b64	%rd861, {%r1418, %r1417};
	add.s64 	%rd862, %rd861, %rd856;
	xor.b64  	%rd863, %rd862, %rd858;
	mov.b64	{%r510, %r511}, %rd863;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r768;
	// inline asm
	mov.b64	%rd864, {%r505, %r509};
	add.s64 	%rd865, %rd819, %rd10;
	add.s64 	%rd866, %rd865, %rd864;
	xor.b64  	%rd867, %rd866, %rd834;
	mov.b64	{%r1419, %r1420}, %rd867;
	mov.b64	%rd868, {%r1420, %r1419};
	add.s64 	%rd869, %rd868, %rd849;
	xor.b64  	%rd870, %rd869, %rd864;
	mov.b64	{%r1421, %r1422}, %rd870;
	prmt.b32 	%r1423, %r1421, %r1422, %r780;
	prmt.b32 	%r1424, %r1421, %r1422, %r779;
	mov.b64	%rd871, {%r1424, %r1423};
	add.s64 	%rd872, %rd871, %rd866;
	xor.b64  	%rd873, %rd868, %rd872;
	mov.b64	{%r1425, %r1426}, %rd873;
	prmt.b32 	%r1427, %r1425, %r1426, %r786;
	prmt.b32 	%r1428, %r1425, %r1426, %r785;
	mov.b64	%rd874, {%r1428, %r1427};
	add.s64 	%rd875, %rd869, %rd874;
	xor.b64  	%rd876, %rd875, %rd871;
	mov.b64	{%r518, %r519}, %rd876;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r768;
	// inline asm
	mov.b64	%rd877, {%r513, %r517};
	add.s64 	%rd878, %rd832, %rd824;
	xor.b64  	%rd879, %rd848, %rd878;
	mov.b64	{%r1429, %r1430}, %rd879;
	mov.b64	%rd880, {%r1430, %r1429};
	add.s64 	%rd881, %rd862, %rd880;
	xor.b64  	%rd882, %rd881, %rd824;
	mov.b64	{%r1431, %r1432}, %rd882;
	prmt.b32 	%r1433, %r1431, %r1432, %r780;
	prmt.b32 	%r1434, %r1431, %r1432, %r779;
	mov.b64	%rd883, {%r1434, %r1433};
	add.s64 	%rd884, %rd878, %rd13;
	add.s64 	%rd885, %rd884, %rd883;
	xor.b64  	%rd886, %rd885, %rd880;
	mov.b64	{%r1435, %r1436}, %rd886;
	prmt.b32 	%r1437, %r1435, %r1436, %r786;
	prmt.b32 	%r1438, %r1435, %r1436, %r785;
	mov.b64	%rd887, {%r1438, %r1437};
	add.s64 	%rd888, %rd887, %rd881;
	xor.b64  	%rd889, %rd888, %rd883;
	mov.b64	{%r526, %r527}, %rd889;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r768;
	// inline asm
	mov.b64	%rd890, {%r521, %r525};
	add.s64 	%rd891, %rd846, %rd837;
	xor.b64  	%rd892, %rd861, %rd891;
	mov.b64	{%r1439, %r1440}, %rd892;
	mov.b64	%rd893, {%r1440, %r1439};
	add.s64 	%rd894, %rd893, %rd822;
	xor.b64  	%rd895, %rd894, %rd837;
	mov.b64	{%r1441, %r1442}, %rd895;
	prmt.b32 	%r1443, %r1441, %r1442, %r780;
	prmt.b32 	%r1444, %r1441, %r1442, %r779;
	mov.b64	%rd896, {%r1444, %r1443};
	add.s64 	%rd897, %rd891, %rd7;
	add.s64 	%rd898, %rd897, %rd896;
	xor.b64  	%rd899, %rd898, %rd893;
	mov.b64	{%r1445, %r1446}, %rd899;
	prmt.b32 	%r1447, %r1445, %r1446, %r786;
	prmt.b32 	%r1448, %r1445, %r1446, %r785;
	mov.b64	%rd900, {%r1448, %r1447};
	add.s64 	%rd901, %rd900, %rd894;
	xor.b64  	%rd902, %rd901, %rd896;
	mov.b64	{%r534, %r535}, %rd902;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r768;
	// inline asm
	mov.b64	%rd903, {%r529, %r533};
	add.s64 	%rd904, %rd851, %rd4;
	add.s64 	%rd905, %rd904, %rd859;
	xor.b64  	%rd906, %rd905, %rd821;
	mov.b64	{%r1449, %r1450}, %rd906;
	mov.b64	%rd907, {%r1450, %r1449};
	add.s64 	%rd908, %rd907, %rd835;
	xor.b64  	%rd909, %rd908, %rd851;
	mov.b64	{%r1451, %r1452}, %rd909;
	prmt.b32 	%r1453, %r1451, %r1452, %r780;
	prmt.b32 	%r1454, %r1451, %r1452, %r779;
	mov.b64	%rd910, {%r1454, %r1453};
	add.s64 	%rd911, %rd905, %rd12;
	add.s64 	%rd912, %rd911, %rd910;
	xor.b64  	%rd913, %rd912, %rd907;
	mov.b64	{%r1455, %r1456}, %rd913;
	prmt.b32 	%r1457, %r1455, %r1456, %r786;
	prmt.b32 	%r1458, %r1455, %r1456, %r785;
	mov.b64	%rd914, {%r1458, %r1457};
	add.s64 	%rd915, %rd914, %rd908;
	xor.b64  	%rd916, %rd915, %rd910;
	mov.b64	{%r542, %r543}, %rd916;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r768;
	// inline asm
	mov.b64	%rd917, {%r537, %r541};
	add.s64 	%rd918, %rd890, %rd872;
	xor.b64  	%rd919, %rd914, %rd918;
	mov.b64	{%r1459, %r1460}, %rd919;
	mov.b64	%rd920, {%r1460, %r1459};
	add.s64 	%rd921, %rd920, %rd901;
	xor.b64  	%rd922, %rd921, %rd890;
	mov.b64	{%r1461, %r1462}, %rd922;
	prmt.b32 	%r1463, %r1461, %r1462, %r780;
	prmt.b32 	%r1464, %r1461, %r1462, %r779;
	mov.b64	%rd923, {%r1464, %r1463};
	add.s64 	%rd924, %rd918, %rd6;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r1465, %r1466}, %rd926;
	prmt.b32 	%r1467, %r1465, %r1466, %r786;
	prmt.b32 	%r1468, %r1465, %r1466, %r785;
	mov.b64	%rd927, {%r1468, %r1467};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r550, %r551}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r768;
	// inline asm
	mov.b64	%rd930, {%r545, %r549};
	add.s64 	%rd931, %rd903, %rd885;
	xor.b64  	%rd932, %rd931, %rd874;
	mov.b64	{%r1469, %r1470}, %rd932;
	mov.b64	%rd933, {%r1470, %r1469};
	add.s64 	%rd934, %rd933, %rd915;
	xor.b64  	%rd935, %rd934, %rd903;
	mov.b64	{%r1471, %r1472}, %rd935;
	prmt.b32 	%r1473, %r1471, %r1472, %r780;
	prmt.b32 	%r1474, %r1471, %r1472, %r779;
	mov.b64	%rd936, {%r1474, %r1473};
	add.s64 	%rd937, %rd931, %rd11;
	add.s64 	%rd938, %rd937, %rd936;
	xor.b64  	%rd939, %rd938, %rd933;
	mov.b64	{%r1475, %r1476}, %rd939;
	prmt.b32 	%r1477, %r1475, %r1476, %r786;
	prmt.b32 	%r1478, %r1475, %r1476, %r785;
	mov.b64	%rd940, {%r1478, %r1477};
	add.s64 	%rd941, %rd940, %rd934;
	xor.b64  	%rd942, %rd941, %rd936;
	mov.b64	{%r558, %r559}, %rd942;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r768;
	// inline asm
	mov.b64	%rd943, {%r553, %r557};
	add.s64 	%rd944, %rd898, %rd5;
	add.s64 	%rd945, %rd944, %rd917;
	xor.b64  	%rd946, %rd945, %rd887;
	mov.b64	{%r1479, %r1480}, %rd946;
	mov.b64	%rd947, {%r1480, %r1479};
	add.s64 	%rd948, %rd947, %rd875;
	xor.b64  	%rd949, %rd948, %rd917;
	mov.b64	{%r1481, %r1482}, %rd949;
	prmt.b32 	%r1483, %r1481, %r1482, %r780;
	prmt.b32 	%r1484, %r1481, %r1482, %r779;
	mov.b64	%rd950, {%r1484, %r1483};
	add.s64 	%rd951, %rd945, %rd17;
	add.s64 	%rd952, %rd951, %rd950;
	xor.b64  	%rd953, %rd952, %rd947;
	mov.b64	{%r1485, %r1486}, %rd953;
	prmt.b32 	%r1487, %r1485, %r1486, %r786;
	prmt.b32 	%r1488, %r1485, %r1486, %r785;
	mov.b64	%rd954, {%r1488, %r1487};
	add.s64 	%rd955, %rd954, %rd948;
	xor.b64  	%rd956, %rd955, %rd950;
	mov.b64	{%r566, %r567}, %rd956;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r768;
	// inline asm
	mov.b64	%rd957, {%r561, %r565};
	add.s64 	%rd958, %rd912, %rd877;
	xor.b64  	%rd959, %rd958, %rd900;
	mov.b64	{%r1489, %r1490}, %rd959;
	mov.b64	%rd960, {%r1490, %r1489};
	add.s64 	%rd961, %rd960, %rd888;
	xor.b64  	%rd962, %rd961, %rd877;
	mov.b64	{%r1491, %r1492}, %rd962;
	prmt.b32 	%r1493, %r1491, %r1492, %r780;
	prmt.b32 	%r1494, %r1491, %r1492, %r779;
	mov.b64	%rd963, {%r1494, %r1493};
	add.s64 	%rd964, %rd958, %rd20;
	add.s64 	%rd965, %rd964, %rd963;
	xor.b64  	%rd966, %rd965, %rd960;
	mov.b64	{%r1495, %r1496}, %rd966;
	prmt.b32 	%r1497, %r1495, %r1496, %r786;
	prmt.b32 	%r1498, %r1495, %r1496, %r785;
	mov.b64	%rd967, {%r1498, %r1497};
	add.s64 	%rd968, %rd967, %rd961;
	xor.b64  	%rd969, %rd968, %rd963;
	mov.b64	{%r574, %r575}, %rd969;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r768;
	// inline asm
	mov.b64	%rd970, {%r569, %r573};
	add.s64 	%rd971, %rd970, %rd925;
	xor.b64  	%rd972, %rd971, %rd940;
	mov.b64	{%r1499, %r1500}, %rd972;
	mov.b64	%rd973, {%r1500, %r1499};
	add.s64 	%rd974, %rd973, %rd955;
	xor.b64  	%rd975, %rd974, %rd970;
	mov.b64	{%r1501, %r1502}, %rd975;
	prmt.b32 	%r1503, %r1501, %r1502, %r780;
	prmt.b32 	%r1504, %r1501, %r1502, %r779;
	mov.b64	%rd976, {%r1504, %r1503};
	add.s64 	%rd977, %rd971, %rd6;
	add.s64 	%rd978, %rd977, %rd976;
	xor.b64  	%rd979, %rd973, %rd978;
	mov.b64	{%r1505, %r1506}, %rd979;
	prmt.b32 	%r1507, %r1505, %r1506, %r786;
	prmt.b32 	%r1508, %r1505, %r1506, %r785;
	mov.b64	%rd980, {%r1508, %r1507};
	add.s64 	%rd981, %rd974, %rd980;
	xor.b64  	%rd982, %rd981, %rd976;
	mov.b64	{%r582, %r583}, %rd982;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r768;
	// inline asm
	mov.b64	%rd983, {%r577, %r581};
	add.s64 	%rd984, %rd930, %rd12;
	add.s64 	%rd985, %rd984, %rd938;
	xor.b64  	%rd986, %rd954, %rd985;
	mov.b64	{%r1509, %r1510}, %rd986;
	mov.b64	%rd987, {%r1510, %r1509};
	add.s64 	%rd988, %rd968, %rd987;
	xor.b64  	%rd989, %rd988, %rd930;
	mov.b64	{%r1511, %r1512}, %rd989;
	prmt.b32 	%r1513, %r1511, %r1512, %r780;
	prmt.b32 	%r1514, %r1511, %r1512, %r779;
	mov.b64	%rd990, {%r1514, %r1513};
	add.s64 	%rd991, %rd985, %rd17;
	add.s64 	%rd992, %rd991, %rd990;
	xor.b64  	%rd993, %rd992, %rd987;
	mov.b64	{%r1515, %r1516}, %rd993;
	prmt.b32 	%r1517, %r1515, %r1516, %r786;
	prmt.b32 	%r1518, %r1515, %r1516, %r785;
	mov.b64	%rd994, {%r1518, %r1517};
	add.s64 	%rd995, %rd994, %rd988;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r590, %r591}, %rd996;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r768;
	// inline asm
	mov.b64	%rd997, {%r585, %r589};
	add.s64 	%rd998, %rd943, %rd11;
	add.s64 	%rd999, %rd998, %rd952;
	xor.b64  	%rd1000, %rd967, %rd999;
	mov.b64	{%r1519, %r1520}, %rd1000;
	mov.b64	%rd1001, {%r1520, %r1519};
	add.s64 	%rd1002, %rd1001, %rd928;
	xor.b64  	%rd1003, %rd1002, %rd943;
	mov.b64	{%r1521, %r1522}, %rd1003;
	prmt.b32 	%r1523, %r1521, %r1522, %r780;
	prmt.b32 	%r1524, %r1521, %r1522, %r779;
	mov.b64	%rd1004, {%r1524, %r1523};
	add.s64 	%rd1005, %rd999, %rd10;
	add.s64 	%rd1006, %rd1005, %rd1004;
	xor.b64  	%rd1007, %rd1006, %rd1001;
	mov.b64	{%r1525, %r1526}, %rd1007;
	prmt.b32 	%r1527, %r1525, %r1526, %r786;
	prmt.b32 	%r1528, %r1525, %r1526, %r785;
	mov.b64	%rd1008, {%r1528, %r1527};
	add.s64 	%rd1009, %rd1008, %rd1002;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r598, %r599}, %rd1010;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r768;
	// inline asm
	mov.b64	%rd1011, {%r593, %r597};
	add.s64 	%rd1012, %rd957, %rd5;
	add.s64 	%rd1013, %rd1012, %rd965;
	xor.b64  	%rd1014, %rd1013, %rd927;
	mov.b64	{%r1529, %r1530}, %rd1014;
	mov.b64	%rd1015, {%r1530, %r1529};
	add.s64 	%rd1016, %rd1015, %rd941;
	xor.b64  	%rd1017, %rd1016, %rd957;
	mov.b64	{%r1531, %r1532}, %rd1017;
	prmt.b32 	%r1533, %r1531, %r1532, %r780;
	prmt.b32 	%r1534, %r1531, %r1532, %r779;
	mov.b64	%rd1018, {%r1534, %r1533};
	add.s64 	%rd1019, %rd1013, %rd20;
	add.s64 	%rd1020, %rd1019, %rd1018;
	xor.b64  	%rd1021, %rd1020, %rd1015;
	mov.b64	{%r1535, %r1536}, %rd1021;
	prmt.b32 	%r1537, %r1535, %r1536, %r786;
	prmt.b32 	%r1538, %r1535, %r1536, %r785;
	mov.b64	%rd1022, {%r1538, %r1537};
	add.s64 	%rd1023, %rd1022, %rd1016;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r606, %r607}, %rd1024;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r768;
	// inline asm
	mov.b64	%rd1025, {%r601, %r605};
	add.s64 	%rd1026, %rd997, %rd978;
	xor.b64  	%rd1027, %rd1022, %rd1026;
	mov.b64	{%r1539, %r1540}, %rd1027;
	mov.b64	%rd1028, {%r1540, %r1539};
	add.s64 	%rd1029, %rd1028, %rd1009;
	xor.b64  	%rd1030, %rd1029, %rd997;
	mov.b64	{%r1541, %r1542}, %rd1030;
	prmt.b32 	%r1543, %r1541, %r1542, %r780;
	prmt.b32 	%r1544, %r1541, %r1542, %r779;
	mov.b64	%rd1031, {%r1544, %r1543};
	add.s64 	%rd1032, %rd1031, %rd1026;
	xor.b64  	%rd1033, %rd1028, %rd1032;
	mov.b64	{%r1545, %r1546}, %rd1033;
	prmt.b32 	%r1547, %r1545, %r1546, %r786;
	prmt.b32 	%r1548, %r1545, %r1546, %r785;
	mov.b64	%rd1034, {%r1548, %r1547};
	add.s64 	%rd1035, %rd1029, %rd1034;
	xor.b64  	%rd1036, %rd1035, %rd1031;
	mov.b64	{%r614, %r615}, %rd1036;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r768;
	// inline asm
	mov.b64	%rd1037, {%r609, %r613};
	add.s64 	%rd1038, %rd992, %rd13;
	add.s64 	%rd1039, %rd1038, %rd1011;
	xor.b64  	%rd1040, %rd1039, %rd980;
	mov.b64	{%r1549, %r1550}, %rd1040;
	mov.b64	%rd1041, {%r1550, %r1549};
	add.s64 	%rd1042, %rd1041, %rd1023;
	xor.b64  	%rd1043, %rd1042, %rd1011;
	mov.b64	{%r1551, %r1552}, %rd1043;
	prmt.b32 	%r1553, %r1551, %r1552, %r780;
	prmt.b32 	%r1554, %r1551, %r1552, %r779;
	mov.b64	%rd1044, {%r1554, %r1553};
	add.s64 	%rd1045, %rd1044, %rd1039;
	xor.b64  	%rd1046, %rd1045, %rd1041;
	mov.b64	{%r1555, %r1556}, %rd1046;
	prmt.b32 	%r1557, %r1555, %r1556, %r786;
	prmt.b32 	%r1558, %r1555, %r1556, %r785;
	mov.b64	%rd1047, {%r1558, %r1557};
	add.s64 	%rd1048, %rd1047, %rd1042;
	xor.b64  	%rd1049, %rd1048, %rd1044;
	mov.b64	{%r622, %r623}, %rd1049;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r768;
	// inline asm
	mov.b64	%rd1050, {%r617, %r621};
	add.s64 	%rd1051, %rd1006, %rd7;
	add.s64 	%rd1052, %rd1051, %rd1025;
	xor.b64  	%rd1053, %rd1052, %rd994;
	mov.b64	{%r1559, %r1560}, %rd1053;
	mov.b64	%rd1054, {%r1560, %r1559};
	add.s64 	%rd1055, %rd1054, %rd981;
	xor.b64  	%rd1056, %rd1055, %rd1025;
	mov.b64	{%r1561, %r1562}, %rd1056;
	prmt.b32 	%r1563, %r1561, %r1562, %r780;
	prmt.b32 	%r1564, %r1561, %r1562, %r779;
	mov.b64	%rd1057, {%r1564, %r1563};
	add.s64 	%rd1058, %rd1057, %rd1052;
	xor.b64  	%rd1059, %rd1058, %rd1054;
	mov.b64	{%r1565, %r1566}, %rd1059;
	prmt.b32 	%r1567, %r1565, %r1566, %r786;
	prmt.b32 	%r1568, %r1565, %r1566, %r785;
	mov.b64	%rd1060, {%r1568, %r1567};
	add.s64 	%rd1061, %rd1060, %rd1055;
	xor.b64  	%rd1062, %rd1061, %rd1057;
	mov.b64	{%r630, %r631}, %rd1062;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r768;
	// inline asm
	mov.b64	%rd1063, {%r625, %r629};
	add.s64 	%rd1064, %rd1020, %rd983;
	xor.b64  	%rd1065, %rd1064, %rd1008;
	mov.b64	{%r1569, %r1570}, %rd1065;
	mov.b64	%rd1066, {%r1570, %r1569};
	add.s64 	%rd1067, %rd1066, %rd995;
	xor.b64  	%rd1068, %rd1067, %rd983;
	mov.b64	{%r1571, %r1572}, %rd1068;
	prmt.b32 	%r1573, %r1571, %r1572, %r780;
	prmt.b32 	%r1574, %r1571, %r1572, %r779;
	mov.b64	%rd1069, {%r1574, %r1573};
	add.s64 	%rd1070, %rd1064, %rd4;
	add.s64 	%rd1071, %rd1070, %rd1069;
	xor.b64  	%rd1072, %rd1071, %rd1066;
	mov.b64	{%r1575, %r1576}, %rd1072;
	prmt.b32 	%r1577, %r1575, %r1576, %r786;
	prmt.b32 	%r1578, %r1575, %r1576, %r785;
	mov.b64	%rd1073, {%r1578, %r1577};
	add.s64 	%rd1074, %rd1073, %rd1067;
	xor.b64  	%rd1075, %rd1074, %rd1069;
	mov.b64	{%r638, %r639}, %rd1075;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r768;
	// inline asm
	mov.b64	%rd1076, {%r633, %r637};
	add.s64 	%rd1077, %rd1032, %rd4;
	add.s64 	%rd1078, %rd1077, %rd1076;
	xor.b64  	%rd1079, %rd1078, %rd1047;
	mov.b64	{%r1579, %r1580}, %rd1079;
	mov.b64	%rd1080, {%r1580, %r1579};
	add.s64 	%rd1081, %rd1080, %rd1061;
	xor.b64  	%rd1082, %rd1081, %rd1076;
	mov.b64	{%r1581, %r1582}, %rd1082;
	prmt.b32 	%r1583, %r1581, %r1582, %r780;
	prmt.b32 	%r1584, %r1581, %r1582, %r779;
	mov.b64	%rd1083, {%r1584, %r1583};
	add.s64 	%rd1084, %rd1078, %rd5;
	add.s64 	%rd1085, %rd1084, %rd1083;
	xor.b64  	%rd1086, %rd1080, %rd1085;
	mov.b64	{%r1585, %r1586}, %rd1086;
	prmt.b32 	%r1587, %r1585, %r1586, %r786;
	prmt.b32 	%r1588, %r1585, %r1586, %r785;
	mov.b64	%rd1087, {%r1588, %r1587};
	add.s64 	%rd1088, %rd1081, %rd1087;
	xor.b64  	%rd1089, %rd1088, %rd1083;
	mov.b64	{%r646, %r647}, %rd1089;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r768;
	// inline asm
	mov.b64	%rd1090, {%r641, %r645};
	add.s64 	%rd1091, %rd1037, %rd6;
	add.s64 	%rd1092, %rd1091, %rd1045;
	xor.b64  	%rd1093, %rd1060, %rd1092;
	mov.b64	{%r1589, %r1590}, %rd1093;
	mov.b64	%rd1094, {%r1590, %r1589};
	add.s64 	%rd1095, %rd1074, %rd1094;
	xor.b64  	%rd1096, %rd1095, %rd1037;
	mov.b64	{%r1591, %r1592}, %rd1096;
	prmt.b32 	%r1593, %r1591, %r1592, %r780;
	prmt.b32 	%r1594, %r1591, %r1592, %r779;
	mov.b64	%rd1097, {%r1594, %r1593};
	add.s64 	%rd1098, %rd1092, %rd7;
	add.s64 	%rd1099, %rd1098, %rd1097;
	xor.b64  	%rd1100, %rd1099, %rd1094;
	mov.b64	{%r1595, %r1596}, %rd1100;
	prmt.b32 	%r1597, %r1595, %r1596, %r786;
	prmt.b32 	%r1598, %r1595, %r1596, %r785;
	mov.b64	%rd1101, {%r1598, %r1597};
	add.s64 	%rd1102, %rd1101, %rd1095;
	xor.b64  	%rd1103, %rd1102, %rd1097;
	mov.b64	{%r654, %r655}, %rd1103;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r768;
	// inline asm
	mov.b64	%rd1104, {%r649, %r653};
	add.s64 	%rd1105, %rd1050, %rd17;
	add.s64 	%rd1106, %rd1105, %rd1058;
	xor.b64  	%rd1107, %rd1073, %rd1106;
	mov.b64	{%r1599, %r1600}, %rd1107;
	mov.b64	%rd1108, {%r1600, %r1599};
	add.s64 	%rd1109, %rd1108, %rd1035;
	xor.b64  	%rd1110, %rd1109, %rd1050;
	mov.b64	{%r1601, %r1602}, %rd1110;
	prmt.b32 	%r1603, %r1601, %r1602, %r780;
	prmt.b32 	%r1604, %r1601, %r1602, %r779;
	mov.b64	%rd1111, {%r1604, %r1603};
	add.s64 	%rd1112, %rd1106, %rd20;
	add.s64 	%rd1113, %rd1112, %rd1111;
	xor.b64  	%rd1114, %rd1113, %rd1108;
	mov.b64	{%r1605, %r1606}, %rd1114;
	prmt.b32 	%r1607, %r1605, %r1606, %r786;
	prmt.b32 	%r1608, %r1605, %r1606, %r785;
	mov.b64	%rd1115, {%r1608, %r1607};
	add.s64 	%rd1116, %rd1115, %rd1109;
	xor.b64  	%rd1117, %rd1116, %rd1111;
	mov.b64	{%r662, %r663}, %rd1117;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r768;
	// inline asm
	mov.b64	%rd1118, {%r657, %r661};
	add.s64 	%rd1119, %rd1063, %rd10;
	add.s64 	%rd1120, %rd1119, %rd1071;
	xor.b64  	%rd1121, %rd1120, %rd1034;
	mov.b64	{%r1609, %r1610}, %rd1121;
	mov.b64	%rd1122, {%r1610, %r1609};
	add.s64 	%rd1123, %rd1122, %rd1048;
	xor.b64  	%rd1124, %rd1123, %rd1063;
	mov.b64	{%r1611, %r1612}, %rd1124;
	prmt.b32 	%r1613, %r1611, %r1612, %r780;
	prmt.b32 	%r1614, %r1611, %r1612, %r779;
	mov.b64	%rd1125, {%r1614, %r1613};
	add.s64 	%rd1126, %rd1120, %rd11;
	add.s64 	%rd1127, %rd1126, %rd1125;
	xor.b64  	%rd1128, %rd1127, %rd1122;
	mov.b64	{%r1615, %r1616}, %rd1128;
	prmt.b32 	%r1617, %r1615, %r1616, %r786;
	prmt.b32 	%r1618, %r1615, %r1616, %r785;
	mov.b64	%rd1129, {%r1618, %r1617};
	add.s64 	%rd1130, %rd1129, %rd1123;
	xor.b64  	%rd1131, %rd1130, %rd1125;
	mov.b64	{%r670, %r671}, %rd1131;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r768;
	// inline asm
	mov.b64	%rd1132, {%r665, %r669};
	add.s64 	%rd1133, %rd1085, %rd12;
	add.s64 	%rd1134, %rd1133, %rd1104;
	xor.b64  	%rd1135, %rd1129, %rd1134;
	mov.b64	{%r1619, %r1620}, %rd1135;
	mov.b64	%rd1136, {%r1620, %r1619};
	add.s64 	%rd1137, %rd1136, %rd1116;
	xor.b64  	%rd1138, %rd1137, %rd1104;
	mov.b64	{%r1621, %r1622}, %rd1138;
	prmt.b32 	%r1623, %r1621, %r1622, %r780;
	prmt.b32 	%r1624, %r1621, %r1622, %r779;
	mov.b64	%rd1139, {%r1624, %r1623};
	add.s64 	%rd1140, %rd1134, %rd13;
	add.s64 	%rd1141, %rd1140, %rd1139;
	xor.b64  	%rd1142, %rd1136, %rd1141;
	mov.b64	{%r1625, %r1626}, %rd1142;
	prmt.b32 	%r1627, %r1625, %r1626, %r786;
	prmt.b32 	%r1628, %r1625, %r1626, %r785;
	mov.b64	%rd1143, {%r1628, %r1627};
	add.s64 	%rd1144, %rd1137, %rd1143;
	xor.b64  	%rd1145, %rd1144, %rd1139;
	mov.b64	{%r678, %r679}, %rd1145;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r768;
	// inline asm
	mov.b64	%rd1146, {%r673, %r677};
	add.s64 	%rd1147, %rd1118, %rd1099;
	xor.b64  	%rd1148, %rd1147, %rd1087;
	mov.b64	{%r1629, %r1630}, %rd1148;
	mov.b64	%rd1149, {%r1630, %r1629};
	add.s64 	%rd1150, %rd1149, %rd1130;
	xor.b64  	%rd1151, %rd1150, %rd1118;
	mov.b64	{%r1631, %r1632}, %rd1151;
	prmt.b32 	%r1633, %r1631, %r1632, %r780;
	prmt.b32 	%r1634, %r1631, %r1632, %r779;
	mov.b64	%rd1152, {%r1634, %r1633};
	add.s64 	%rd1153, %rd1152, %rd1147;
	xor.b64  	%rd1154, %rd1153, %rd1149;
	mov.b64	{%r1635, %r1636}, %rd1154;
	prmt.b32 	%r1637, %r1635, %r1636, %r786;
	prmt.b32 	%r1638, %r1635, %r1636, %r785;
	mov.b64	%rd1155, {%r1638, %r1637};
	add.s64 	%rd1156, %rd1155, %rd1150;
	xor.b64  	%rd1157, %rd1156, %rd1152;
	mov.b64	{%r686, %r687}, %rd1157;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r768;
	// inline asm
	mov.b64	%rd1158, {%r681, %r685};
	add.s64 	%rd1159, %rd1132, %rd1113;
	xor.b64  	%rd1160, %rd1159, %rd1101;
	mov.b64	{%r1639, %r1640}, %rd1160;
	mov.b64	%rd1161, {%r1640, %r1639};
	add.s64 	%rd1162, %rd1161, %rd1088;
	xor.b64  	%rd1163, %rd1162, %rd1132;
	mov.b64	{%r1641, %r1642}, %rd1163;
	prmt.b32 	%r1643, %r1641, %r1642, %r780;
	prmt.b32 	%r1644, %r1641, %r1642, %r779;
	mov.b64	%rd1164, {%r1644, %r1643};
	add.s64 	%rd1165, %rd1164, %rd1159;
	xor.b64  	%rd1166, %rd1165, %rd1161;
	mov.b64	{%r1645, %r1646}, %rd1166;
	prmt.b32 	%r1647, %r1645, %r1646, %r786;
	prmt.b32 	%r1648, %r1645, %r1646, %r785;
	mov.b64	%rd1167, {%r1648, %r1647};
	add.s64 	%rd1168, %rd1167, %rd1162;
	xor.b64  	%rd1169, %rd1168, %rd1164;
	mov.b64	{%r694, %r695}, %rd1169;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r768;
	// inline asm
	mov.b64	%rd1170, {%r689, %r693};
	add.s64 	%rd1171, %rd1127, %rd1090;
	xor.b64  	%rd1172, %rd1171, %rd1115;
	mov.b64	{%r1649, %r1650}, %rd1172;
	mov.b64	%rd1173, {%r1650, %r1649};
	add.s64 	%rd1174, %rd1173, %rd1102;
	xor.b64  	%rd1175, %rd1174, %rd1090;
	mov.b64	{%r1651, %r1652}, %rd1175;
	prmt.b32 	%r1653, %r1651, %r1652, %r780;
	prmt.b32 	%r1654, %r1651, %r1652, %r779;
	mov.b64	%rd1176, {%r1654, %r1653};
	add.s64 	%rd1177, %rd1176, %rd1171;
	xor.b64  	%rd1178, %rd1177, %rd1173;
	mov.b64	{%r1655, %r1656}, %rd1178;
	prmt.b32 	%r1657, %r1655, %r1656, %r786;
	prmt.b32 	%r1658, %r1655, %r1656, %r785;
	mov.b64	%rd1179, {%r1658, %r1657};
	add.s64 	%rd1180, %rd1179, %rd1174;
	xor.b64  	%rd1181, %rd1180, %rd1176;
	mov.b64	{%r702, %r703}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r768;
	// inline asm
	mov.b64	%rd1182, {%r697, %r701};
	add.s64 	%rd1183, %rd1182, %rd1141;
	xor.b64  	%rd1184, %rd1183, %rd1155;
	mov.b64	{%r1659, %r1660}, %rd1184;
	mov.b64	%rd1185, {%r1660, %r1659};
	add.s64 	%rd1186, %rd1185, %rd1168;
	xor.b64  	%rd1187, %rd1186, %rd1182;
	mov.b64	{%r1661, %r1662}, %rd1187;
	prmt.b32 	%r1663, %r1661, %r1662, %r780;
	prmt.b32 	%r1664, %r1661, %r1662, %r779;
	mov.b64	%rd1188, {%r1664, %r1663};
	add.s64 	%rd1189, %rd1188, %rd1183;
	xor.b64  	%rd1190, %rd1185, %rd1189;
	mov.b64	{%r1665, %r1666}, %rd1190;
	prmt.b32 	%r1667, %r1665, %r1666, %r786;
	prmt.b32 	%r1668, %r1665, %r1666, %r785;
	mov.b64	%rd1191, {%r1668, %r1667};
	add.s64 	%rd1192, %rd1186, %rd1191;
	xor.b64  	%rd1193, %rd1192, %rd1188;
	mov.b64	{%r710, %r711}, %rd1193;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r768;
	// inline asm
	mov.b64	%rd1194, {%r705, %r709};
	add.s64 	%rd1195, %rd1146, %rd17;
	add.s64 	%rd1196, %rd1195, %rd1153;
	xor.b64  	%rd1197, %rd1167, %rd1196;
	mov.b64	{%r1669, %r1670}, %rd1197;
	mov.b64	%rd1198, {%r1670, %r1669};
	add.s64 	%rd1199, %rd1180, %rd1198;
	xor.b64  	%rd1200, %rd1199, %rd1146;
	mov.b64	{%r1671, %r1672}, %rd1200;
	prmt.b32 	%r1673, %r1671, %r1672, %r780;
	prmt.b32 	%r1674, %r1671, %r1672, %r779;
	mov.b64	%rd1201, {%r1674, %r1673};
	add.s64 	%rd1202, %rd1196, %rd12;
	add.s64 	%rd1203, %rd1202, %rd1201;
	xor.b64  	%rd1204, %rd1203, %rd1198;
	mov.b64	{%r1675, %r1676}, %rd1204;
	prmt.b32 	%r1677, %r1675, %r1676, %r786;
	prmt.b32 	%r1678, %r1675, %r1676, %r785;
	mov.b64	%rd1205, {%r1678, %r1677};
	add.s64 	%rd1206, %rd1205, %rd1199;
	xor.b64  	%rd1207, %rd1206, %rd1201;
	mov.b64	{%r718, %r719}, %rd1207;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r768;
	// inline asm
	mov.b64	%rd1208, {%r713, %r717};
	add.s64 	%rd1209, %rd1158, %rd13;
	add.s64 	%rd1210, %rd1209, %rd1165;
	xor.b64  	%rd1211, %rd1179, %rd1210;
	mov.b64	{%r1679, %r1680}, %rd1211;
	mov.b64	%rd1212, {%r1680, %r1679};
	add.s64 	%rd1213, %rd1212, %rd1144;
	xor.b64  	%rd1214, %rd1213, %rd1158;
	mov.b64	{%r1681, %r1682}, %rd1214;
	prmt.b32 	%r1683, %r1681, %r1682, %r780;
	prmt.b32 	%r1684, %r1681, %r1682, %r779;
	mov.b64	%rd1215, {%r1684, %r1683};
	add.s64 	%rd1216, %rd1215, %rd1210;
	xor.b64  	%rd1217, %rd1216, %rd1212;
	mov.b64	{%r1685, %r1686}, %rd1217;
	prmt.b32 	%r1687, %r1685, %r1686, %r786;
	prmt.b32 	%r1688, %r1685, %r1686, %r785;
	mov.b64	%rd1218, {%r1688, %r1687};
	add.s64 	%rd1219, %rd1218, %rd1213;
	xor.b64  	%rd1220, %rd1219, %rd1215;
	mov.b64	{%r726, %r727}, %rd1220;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r768;
	// inline asm
	mov.b64	%rd1221, {%r721, %r725};
	add.s64 	%rd1222, %rd1177, %rd1170;
	xor.b64  	%rd1223, %rd1222, %rd1143;
	mov.b64	{%r1689, %r1690}, %rd1223;
	mov.b64	%rd1224, {%r1690, %r1689};
	add.s64 	%rd1225, %rd1224, %rd1156;
	xor.b64  	%rd1226, %rd1225, %rd1170;
	mov.b64	{%r1691, %r1692}, %rd1226;
	prmt.b32 	%r1693, %r1691, %r1692, %r780;
	prmt.b32 	%r1694, %r1691, %r1692, %r779;
	mov.b64	%rd1227, {%r1694, %r1693};
	add.s64 	%rd1228, %rd1222, %rd10;
	add.s64 	%rd1229, %rd1228, %rd1227;
	xor.b64  	%rd1230, %rd1229, %rd1224;
	mov.b64	{%r1695, %r1696}, %rd1230;
	prmt.b32 	%r1697, %r1695, %r1696, %r786;
	prmt.b32 	%r1698, %r1695, %r1696, %r785;
	mov.b64	%rd1231, {%r1698, %r1697};
	add.s64 	%rd1232, %rd1231, %rd1225;
	xor.b64  	%rd1233, %rd1232, %rd1227;
	mov.b64	{%r734, %r735}, %rd1233;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r768;
	// inline asm
	mov.b64	%rd1234, {%r729, %r733};
	add.s64 	%rd1235, %rd1189, %rd5;
	add.s64 	%rd1236, %rd1235, %rd1208;
	xor.b64  	%rd1237, %rd1231, %rd1236;
	mov.b64	{%r1699, %r1700}, %rd1237;
	mov.b64	%rd1238, {%r1700, %r1699};
	add.s64 	%rd1239, %rd1238, %rd1219;
	xor.b64  	%rd1240, %rd1239, %rd1208;
	mov.b64	{%r1701, %r1702}, %rd1240;
	prmt.b32 	%r1703, %r1701, %r1702, %r780;
	prmt.b32 	%r1704, %r1701, %r1702, %r779;
	mov.b64	%rd1241, {%r1704, %r1703};
	add.s64 	%rd1242, %rd1241, %rd1236;
	xor.b64  	%rd1243, %rd1238, %rd1242;
	mov.b64	{%r1705, %r1706}, %rd1243;
	prmt.b32 	%r1707, %r1705, %r1706, %r786;
	prmt.b32 	%r1708, %r1705, %r1706, %r785;
	mov.b64	%rd1244, {%r1708, %r1707};
	add.s64 	%rd1245, %rd1239, %rd1244;
	xor.b64  	%rd1246, %rd1245, %rd1241;
	mov.b64	{%r742, %r743}, %rd1246;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r768;
	// inline asm
	mov.b64	%rd1247, {%r737, %r741};
	add.s64 	%rd1248, %rd1203, %rd4;
	add.s64 	%rd1249, %rd1248, %rd1221;
	xor.b64  	%rd1250, %rd1249, %rd1191;
	mov.b64	{%r1709, %r1710}, %rd1250;
	mov.b64	%rd1251, {%r1710, %r1709};
	add.s64 	%rd1252, %rd1251, %rd1232;
	xor.b64  	%rd1253, %rd1252, %rd1221;
	mov.b64	{%r1711, %r1712}, %rd1253;
	prmt.b32 	%r1713, %r1711, %r1712, %r780;
	prmt.b32 	%r1714, %r1711, %r1712, %r779;
	mov.b64	%rd1254, {%r1714, %r1713};
	add.s64 	%rd1255, %rd1249, %rd6;
	add.s64 	%rd1256, %rd1255, %rd1254;
	xor.b64  	%rd1257, %rd1256, %rd1251;
	mov.b64	{%r1715, %r1716}, %rd1257;
	prmt.b32 	%r1717, %r1715, %r1716, %r786;
	prmt.b32 	%r1718, %r1715, %r1716, %r785;
	mov.b64	%rd1258, {%r1718, %r1717};
	add.s64 	%rd1259, %rd1258, %rd1252;
	xor.b64  	%rd1260, %rd1259, %rd1254;
	mov.b64	{%r750, %r751}, %rd1260;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r768;
	// inline asm
	mov.b64	%rd1261, {%r745, %r749};
	add.s64 	%rd1262, %rd1234, %rd1216;
	xor.b64  	%rd1263, %rd1262, %rd1205;
	mov.b64	{%r1719, %r1720}, %rd1263;
	mov.b64	%rd1264, {%r1720, %r1719};
	add.s64 	%rd1265, %rd1264, %rd1192;
	xor.b64  	%rd1266, %rd1265, %rd1234;
	mov.b64	{%r1721, %r1722}, %rd1266;
	prmt.b32 	%r1723, %r1721, %r1722, %r780;
	prmt.b32 	%r1724, %r1721, %r1722, %r779;
	mov.b64	%rd1267, {%r1724, %r1723};
	add.s64 	%rd1268, %rd1262, %rd11;
	add.s64 	%rd1269, %rd1268, %rd1267;
	xor.b64  	%rd1270, %rd1269, %rd1264;
	mov.b64	{%r1725, %r1726}, %rd1270;
	prmt.b32 	%r1727, %r1725, %r1726, %r786;
	prmt.b32 	%r1728, %r1725, %r1726, %r785;
	mov.b64	%rd1271, {%r1728, %r1727};
	add.s64 	%rd1272, %rd1271, %rd1265;
	xor.b64  	%rd1273, %rd1272, %rd1267;
	mov.b64	{%r758, %r759}, %rd1273;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r768;
	// inline asm
	mov.b64	%rd1274, {%r753, %r757};
	add.s64 	%rd1275, %rd1194, %rd20;
	add.s64 	%rd1276, %rd1275, %rd1229;
	xor.b64  	%rd1277, %rd1276, %rd1218;
	mov.b64	{%r1729, %r1730}, %rd1277;
	mov.b64	%rd1278, {%r1730, %r1729};
	add.s64 	%rd1279, %rd1278, %rd1206;
	xor.b64  	%rd1280, %rd1279, %rd1194;
	mov.b64	{%r1731, %r1732}, %rd1280;
	prmt.b32 	%r1733, %r1731, %r1732, %r780;
	prmt.b32 	%r1734, %r1731, %r1732, %r779;
	mov.b64	%rd1281, {%r1734, %r1733};
	add.s64 	%rd1282, %rd1276, %rd7;
	add.s64 	%rd1283, %rd1282, %rd1281;
	xor.b64  	%rd1284, %rd1283, %rd1278;
	mov.b64	{%r1735, %r1736}, %rd1284;
	prmt.b32 	%r1737, %r1735, %r1736, %r786;
	prmt.b32 	%r1738, %r1735, %r1736, %r785;
	mov.b64	%rd1285, {%r1738, %r1737};
	add.s64 	%rd1286, %rd1285, %rd1279;
	xor.b64  	%rd1287, %rd1286, %rd1281;
	mov.b64	{%r766, %r767}, %rd1287;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r768;
	// inline asm
	mov.b64	%rd1288, {%r761, %r765};
	xor.b64  	%rd1289, %rd1242, %rd1272;
	xor.b64  	%rd1290, %rd1289, 7640891576939301192;
	xor.b64  	%rd1291, %rd1256, %rd1286;
	xor.b64  	%rd1292, %rd1291, -4942790177534073029;
	xor.b64  	%rd1293, %rd1245, %rd1269;
	xor.b64  	%rd1294, %rd1293, 4354685564936845355;
	xor.b64  	%rd1295, %rd1259, %rd1283;
	xor.b64  	%rd1296, %rd1295, -6534734903238641935;
	xor.b64  	%rd1297, %rd1258, %rd1288;
	xor.b64  	%rd1298, %rd1297, 5840696475078001361;
	xor.b64  	%rd1299, %rd1247, %rd1271;
	xor.b64  	%rd1300, %rd1299, -7276294671716946913;
	xor.b64  	%rd1301, %rd1261, %rd1285;
	xor.b64  	%rd1302, %rd1301, 2270897969802886507;
	xor.b64  	%rd1303, %rd1244, %rd1274;
	xor.b64  	%rd1304, %rd1303, 6620516959819538809;
	cvta.to.global.u64 	%rd1305, %rd1;
	shl.b32 	%r1739, %r773, 3;
	mul.wide.u32 	%rd1306, %r1739, 8;
	add.s64 	%rd1307, %rd1305, %rd1306;
	st.global.u64 	[%rd1307], %rd1290;
	st.global.u64 	[%rd1307+8], %rd1292;
	st.global.u64 	[%rd1307+16], %rd1294;
	st.global.u64 	[%rd1307+24], %rd1296;
	st.global.u64 	[%rd1307+32], %rd1298;
	st.global.u64 	[%rd1307+40], %rd1300;
	st.global.u64 	[%rd1307+48], %rd1302;
	st.global.u64 	[%rd1307+56], %rd1304;
	ret;
}

	// .globl	_Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j
.visible .entry _Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j(
	.param .u64 _Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j_param_0,
	.param .u64 _Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j_param_1,
	.param .u32 _Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<926>;
	.reg .b64 	%rd<33>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11fillAes1Rx4ILy2097152ELb1EEvPvS0_jE1T[8192];

	ld.param.u64 	%rd6, [_Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j_param_0];
	ld.param.u64 	%rd7, [_Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j_param_1];
	ld.param.u32 	%r32, [_Z11fillAes1Rx4ILy2097152ELb1EEvPvS0_j_param_2];
	shl.b32 	%r33, %r32, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r920, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r920;
	setp.ge.u32	%p1, %r4, %r33;
	@%p1 bra 	BB3_6;

	setp.gt.s32	%p2, %r920, 2047;
	@%p2 bra 	BB3_3;

BB3_2:
	mul.wide.s32 	%rd8, %r920, 4;
	mov.u64 	%rd9, AES_TABLE;
	add.s64 	%rd10, %rd9, %rd8;
	ld.const.u32 	%r34, [%rd10];
	shl.b32 	%r35, %r920, 2;
	mov.u32 	%r36, _ZZ11fillAes1Rx4ILy2097152ELb1EEvPvS0_jE1T;
	add.s32 	%r37, %r36, %r35;
	st.shared.u32 	[%r37], %r34;
	add.s32 	%r920, %r920, %r1;
	setp.lt.s32	%p3, %r920, 2048;
	@%p3 bra 	BB3_2;

BB3_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r39, %r7, 2;
	mul.wide.u32 	%rd11, %r39, 4;
	mov.u64 	%rd12, AES_KEY_FILL;
	add.s64 	%rd13, %rd12, %rd11;
	ld.const.u32 	%r8, [%rd13];
	ld.const.u32 	%r9, [%rd13+4];
	ld.const.u32 	%r10, [%rd13+8];
	ld.const.u32 	%r11, [%rd13+12];
	shr.u32 	%r40, %r4, 2;
	mul.wide.u32 	%rd14, %r40, 16;
	mul.wide.u32 	%rd15, %r7, 4;
	add.s64 	%rd16, %rd14, %rd15;
	cvta.to.global.u64 	%rd17, %rd6;
	shl.b64 	%rd18, %rd16, 2;
	add.s64 	%rd1, %rd17, %rd18;
	ld.global.u32 	%r925, [%rd1];
	ld.global.u32 	%r924, [%rd1+4];
	ld.global.u32 	%r923, [%rd1+8];
	ld.global.u32 	%r922, [%rd1+12];
	and.b32  	%r41, %r4, 1;
	shl.b32 	%r42, %r4, 4;
	and.b32  	%r43, %r42, 16;
	xor.b32  	%r44, %r43, 16;
	add.s32 	%r16, %r44, 8;
	add.s32 	%r17, %r43, 8;
	setp.eq.s32	%p4, %r41, 0;
	mov.u32 	%r38, 0;
	mov.u32 	%r45, _ZZ11fillAes1Rx4ILy2097152ELb1EEvPvS0_jE1T;
	add.s32 	%r46, %r45, 4096;
	selp.b32	%r18, %r46, %r45, %p4;
	add.s32 	%r47, %r45, 1024;
	add.s32 	%r48, %r45, 7168;
	selp.b32	%r19, %r48, %r47, %p4;
	add.s32 	%r49, %r45, 2048;
	add.s32 	%r50, %r45, 6144;
	selp.b32	%r20, %r50, %r49, %p4;
	add.s32 	%r51, %r45, 3072;
	add.s32 	%r52, %r45, 5120;
	selp.b32	%r21, %r52, %r51, %p4;
	and.b32  	%r54, %r4, -4;
	cvt.u64.u32	%rd19, %r54;
	cvt.u64.u32	%rd20, %r4;
	and.b64  	%rd21, %rd20, 3;
	or.b64  	%rd22, %rd19, %rd21;
	cvta.to.global.u64 	%rd23, %rd7;
	shl.b64 	%rd24, %rd22, 4;
	add.s64 	%rd32, %rd23, %rd24;
	mul.wide.u32 	%rd3, %r33, 16;
	mov.u32 	%r921, %r38;

BB3_4:
	.pragma "nounroll";
	// inline asm
	bfe.u32 %r56, %r925, %r38, 8;
	// inline asm
	shl.b32 	%r440, %r56, 2;
	add.s32 	%r441, %r18, %r440;
	ld.shared.u32 	%r442, [%r441];
	// inline asm
	bfe.u32 %r59, %r924, %r16, 8;
	// inline asm
	shl.b32 	%r443, %r59, 2;
	add.s32 	%r444, %r19, %r443;
	ld.shared.u32 	%r445, [%r444];
	mov.u32 	%r436, 16;
	// inline asm
	bfe.u32 %r62, %r923, %r436, 8;
	// inline asm
	shl.b32 	%r446, %r62, 2;
	add.s32 	%r447, %r20, %r446;
	ld.shared.u32 	%r448, [%r447];
	// inline asm
	bfe.u32 %r65, %r922, %r17, 8;
	// inline asm
	shl.b32 	%r449, %r65, 2;
	add.s32 	%r450, %r21, %r449;
	xor.b32  	%r451, %r442, %r8;
	xor.b32  	%r452, %r451, %r445;
	xor.b32  	%r453, %r452, %r448;
	ld.shared.u32 	%r454, [%r450];
	// inline asm
	bfe.u32 %r68, %r924, %r38, 8;
	// inline asm
	shl.b32 	%r455, %r68, 2;
	add.s32 	%r456, %r18, %r455;
	ld.shared.u32 	%r457, [%r456];
	// inline asm
	bfe.u32 %r71, %r923, %r16, 8;
	// inline asm
	shl.b32 	%r458, %r71, 2;
	add.s32 	%r459, %r19, %r458;
	ld.shared.u32 	%r460, [%r459];
	// inline asm
	bfe.u32 %r74, %r922, %r436, 8;
	// inline asm
	shl.b32 	%r461, %r74, 2;
	add.s32 	%r462, %r20, %r461;
	ld.shared.u32 	%r463, [%r462];
	// inline asm
	bfe.u32 %r77, %r925, %r17, 8;
	// inline asm
	shl.b32 	%r464, %r77, 2;
	add.s32 	%r465, %r21, %r464;
	xor.b32  	%r466, %r457, %r9;
	xor.b32  	%r467, %r466, %r460;
	xor.b32  	%r468, %r467, %r463;
	ld.shared.u32 	%r469, [%r465];
	// inline asm
	bfe.u32 %r80, %r923, %r38, 8;
	// inline asm
	shl.b32 	%r470, %r80, 2;
	add.s32 	%r471, %r18, %r470;
	ld.shared.u32 	%r472, [%r471];
	// inline asm
	bfe.u32 %r83, %r922, %r16, 8;
	// inline asm
	shl.b32 	%r473, %r83, 2;
	add.s32 	%r474, %r19, %r473;
	ld.shared.u32 	%r475, [%r474];
	// inline asm
	bfe.u32 %r86, %r925, %r436, 8;
	// inline asm
	shl.b32 	%r476, %r86, 2;
	add.s32 	%r477, %r20, %r476;
	ld.shared.u32 	%r478, [%r477];
	// inline asm
	bfe.u32 %r89, %r924, %r17, 8;
	// inline asm
	shl.b32 	%r479, %r89, 2;
	add.s32 	%r480, %r21, %r479;
	xor.b32  	%r481, %r472, %r10;
	xor.b32  	%r482, %r481, %r475;
	xor.b32  	%r483, %r482, %r478;
	ld.shared.u32 	%r484, [%r480];
	// inline asm
	bfe.u32 %r92, %r922, %r38, 8;
	// inline asm
	shl.b32 	%r485, %r92, 2;
	add.s32 	%r486, %r18, %r485;
	ld.shared.u32 	%r487, [%r486];
	// inline asm
	bfe.u32 %r95, %r925, %r16, 8;
	// inline asm
	shl.b32 	%r488, %r95, 2;
	add.s32 	%r489, %r19, %r488;
	ld.shared.u32 	%r490, [%r489];
	// inline asm
	bfe.u32 %r98, %r924, %r436, 8;
	// inline asm
	shl.b32 	%r491, %r98, 2;
	add.s32 	%r492, %r20, %r491;
	ld.shared.u32 	%r493, [%r492];
	// inline asm
	bfe.u32 %r101, %r923, %r17, 8;
	// inline asm
	shl.b32 	%r494, %r101, 2;
	add.s32 	%r495, %r21, %r494;
	xor.b32  	%r496, %r487, %r11;
	xor.b32  	%r497, %r496, %r490;
	xor.b32  	%r498, %r497, %r493;
	ld.shared.u32 	%r499, [%r495];
	xor.b32  	%r141, %r498, %r499;
	xor.b32  	%r150, %r483, %r484;
	xor.b32  	%r147, %r468, %r469;
	xor.b32  	%r144, %r453, %r454;
	st.global.v4.u32 	[%rd32], {%r144, %r147, %r150, %r141};
	// inline asm
	bfe.u32 %r104, %r144, %r38, 8;
	// inline asm
	shl.b32 	%r500, %r104, 2;
	add.s32 	%r501, %r18, %r500;
	ld.shared.u32 	%r502, [%r501];
	// inline asm
	bfe.u32 %r107, %r147, %r16, 8;
	// inline asm
	shl.b32 	%r503, %r107, 2;
	add.s32 	%r504, %r19, %r503;
	ld.shared.u32 	%r505, [%r504];
	// inline asm
	bfe.u32 %r110, %r150, %r436, 8;
	// inline asm
	shl.b32 	%r506, %r110, 2;
	add.s32 	%r507, %r20, %r506;
	ld.shared.u32 	%r508, [%r507];
	// inline asm
	bfe.u32 %r113, %r141, %r17, 8;
	// inline asm
	shl.b32 	%r509, %r113, 2;
	add.s32 	%r510, %r21, %r509;
	xor.b32  	%r511, %r502, %r8;
	xor.b32  	%r512, %r511, %r505;
	xor.b32  	%r513, %r512, %r508;
	ld.shared.u32 	%r514, [%r510];
	// inline asm
	bfe.u32 %r116, %r147, %r38, 8;
	// inline asm
	shl.b32 	%r515, %r116, 2;
	add.s32 	%r516, %r18, %r515;
	ld.shared.u32 	%r517, [%r516];
	// inline asm
	bfe.u32 %r119, %r150, %r16, 8;
	// inline asm
	shl.b32 	%r518, %r119, 2;
	add.s32 	%r519, %r19, %r518;
	ld.shared.u32 	%r520, [%r519];
	// inline asm
	bfe.u32 %r122, %r141, %r436, 8;
	// inline asm
	shl.b32 	%r521, %r122, 2;
	add.s32 	%r522, %r20, %r521;
	ld.shared.u32 	%r523, [%r522];
	// inline asm
	bfe.u32 %r125, %r144, %r17, 8;
	// inline asm
	shl.b32 	%r524, %r125, 2;
	add.s32 	%r525, %r21, %r524;
	xor.b32  	%r526, %r517, %r9;
	xor.b32  	%r527, %r526, %r520;
	xor.b32  	%r528, %r527, %r523;
	ld.shared.u32 	%r529, [%r525];
	// inline asm
	bfe.u32 %r128, %r150, %r38, 8;
	// inline asm
	shl.b32 	%r530, %r128, 2;
	add.s32 	%r531, %r18, %r530;
	ld.shared.u32 	%r532, [%r531];
	// inline asm
	bfe.u32 %r131, %r141, %r16, 8;
	// inline asm
	shl.b32 	%r533, %r131, 2;
	add.s32 	%r534, %r19, %r533;
	ld.shared.u32 	%r535, [%r534];
	// inline asm
	bfe.u32 %r134, %r144, %r436, 8;
	// inline asm
	shl.b32 	%r536, %r134, 2;
	add.s32 	%r537, %r20, %r536;
	ld.shared.u32 	%r538, [%r537];
	// inline asm
	bfe.u32 %r137, %r147, %r17, 8;
	// inline asm
	shl.b32 	%r539, %r137, 2;
	add.s32 	%r540, %r21, %r539;
	xor.b32  	%r541, %r532, %r10;
	xor.b32  	%r542, %r541, %r535;
	xor.b32  	%r543, %r542, %r538;
	ld.shared.u32 	%r544, [%r540];
	// inline asm
	bfe.u32 %r140, %r141, %r38, 8;
	// inline asm
	shl.b32 	%r545, %r140, 2;
	add.s32 	%r546, %r18, %r545;
	ld.shared.u32 	%r547, [%r546];
	// inline asm
	bfe.u32 %r143, %r144, %r16, 8;
	// inline asm
	shl.b32 	%r548, %r143, 2;
	add.s32 	%r549, %r19, %r548;
	ld.shared.u32 	%r550, [%r549];
	// inline asm
	bfe.u32 %r146, %r147, %r436, 8;
	// inline asm
	shl.b32 	%r551, %r146, 2;
	add.s32 	%r552, %r20, %r551;
	ld.shared.u32 	%r553, [%r552];
	// inline asm
	bfe.u32 %r149, %r150, %r17, 8;
	// inline asm
	shl.b32 	%r554, %r149, 2;
	add.s32 	%r555, %r21, %r554;
	xor.b32  	%r556, %r547, %r11;
	xor.b32  	%r557, %r556, %r550;
	xor.b32  	%r558, %r557, %r553;
	ld.shared.u32 	%r559, [%r555];
	xor.b32  	%r189, %r558, %r559;
	xor.b32  	%r198, %r543, %r544;
	xor.b32  	%r195, %r528, %r529;
	xor.b32  	%r192, %r513, %r514;
	add.s64 	%rd25, %rd32, %rd3;
	st.global.v4.u32 	[%rd25], {%r192, %r195, %r198, %r189};
	// inline asm
	bfe.u32 %r152, %r192, %r38, 8;
	// inline asm
	shl.b32 	%r560, %r152, 2;
	add.s32 	%r561, %r18, %r560;
	ld.shared.u32 	%r562, [%r561];
	// inline asm
	bfe.u32 %r155, %r195, %r16, 8;
	// inline asm
	shl.b32 	%r563, %r155, 2;
	add.s32 	%r564, %r19, %r563;
	ld.shared.u32 	%r565, [%r564];
	// inline asm
	bfe.u32 %r158, %r198, %r436, 8;
	// inline asm
	shl.b32 	%r566, %r158, 2;
	add.s32 	%r567, %r20, %r566;
	ld.shared.u32 	%r568, [%r567];
	// inline asm
	bfe.u32 %r161, %r189, %r17, 8;
	// inline asm
	shl.b32 	%r569, %r161, 2;
	add.s32 	%r570, %r21, %r569;
	xor.b32  	%r571, %r562, %r8;
	xor.b32  	%r572, %r571, %r565;
	xor.b32  	%r573, %r572, %r568;
	ld.shared.u32 	%r574, [%r570];
	// inline asm
	bfe.u32 %r164, %r195, %r38, 8;
	// inline asm
	shl.b32 	%r575, %r164, 2;
	add.s32 	%r576, %r18, %r575;
	ld.shared.u32 	%r577, [%r576];
	// inline asm
	bfe.u32 %r167, %r198, %r16, 8;
	// inline asm
	shl.b32 	%r578, %r167, 2;
	add.s32 	%r579, %r19, %r578;
	ld.shared.u32 	%r580, [%r579];
	// inline asm
	bfe.u32 %r170, %r189, %r436, 8;
	// inline asm
	shl.b32 	%r581, %r170, 2;
	add.s32 	%r582, %r20, %r581;
	ld.shared.u32 	%r583, [%r582];
	// inline asm
	bfe.u32 %r173, %r192, %r17, 8;
	// inline asm
	shl.b32 	%r584, %r173, 2;
	add.s32 	%r585, %r21, %r584;
	xor.b32  	%r586, %r577, %r9;
	xor.b32  	%r587, %r586, %r580;
	xor.b32  	%r588, %r587, %r583;
	ld.shared.u32 	%r589, [%r585];
	// inline asm
	bfe.u32 %r176, %r198, %r38, 8;
	// inline asm
	shl.b32 	%r590, %r176, 2;
	add.s32 	%r591, %r18, %r590;
	ld.shared.u32 	%r592, [%r591];
	// inline asm
	bfe.u32 %r179, %r189, %r16, 8;
	// inline asm
	shl.b32 	%r593, %r179, 2;
	add.s32 	%r594, %r19, %r593;
	ld.shared.u32 	%r595, [%r594];
	// inline asm
	bfe.u32 %r182, %r192, %r436, 8;
	// inline asm
	shl.b32 	%r596, %r182, 2;
	add.s32 	%r597, %r20, %r596;
	ld.shared.u32 	%r598, [%r597];
	// inline asm
	bfe.u32 %r185, %r195, %r17, 8;
	// inline asm
	shl.b32 	%r599, %r185, 2;
	add.s32 	%r600, %r21, %r599;
	xor.b32  	%r601, %r592, %r10;
	xor.b32  	%r602, %r601, %r595;
	xor.b32  	%r603, %r602, %r598;
	ld.shared.u32 	%r604, [%r600];
	// inline asm
	bfe.u32 %r188, %r189, %r38, 8;
	// inline asm
	shl.b32 	%r605, %r188, 2;
	add.s32 	%r606, %r18, %r605;
	ld.shared.u32 	%r607, [%r606];
	// inline asm
	bfe.u32 %r191, %r192, %r16, 8;
	// inline asm
	shl.b32 	%r608, %r191, 2;
	add.s32 	%r609, %r19, %r608;
	ld.shared.u32 	%r610, [%r609];
	// inline asm
	bfe.u32 %r194, %r195, %r436, 8;
	// inline asm
	shl.b32 	%r611, %r194, 2;
	add.s32 	%r612, %r20, %r611;
	ld.shared.u32 	%r613, [%r612];
	// inline asm
	bfe.u32 %r197, %r198, %r17, 8;
	// inline asm
	shl.b32 	%r614, %r197, 2;
	add.s32 	%r615, %r21, %r614;
	xor.b32  	%r616, %r607, %r11;
	xor.b32  	%r617, %r616, %r610;
	xor.b32  	%r618, %r617, %r613;
	ld.shared.u32 	%r619, [%r615];
	xor.b32  	%r237, %r618, %r619;
	xor.b32  	%r246, %r603, %r604;
	xor.b32  	%r243, %r588, %r589;
	xor.b32  	%r240, %r573, %r574;
	add.s64 	%rd26, %rd25, %rd3;
	st.global.v4.u32 	[%rd26], {%r240, %r243, %r246, %r237};
	// inline asm
	bfe.u32 %r200, %r240, %r38, 8;
	// inline asm
	shl.b32 	%r620, %r200, 2;
	add.s32 	%r621, %r18, %r620;
	ld.shared.u32 	%r622, [%r621];
	// inline asm
	bfe.u32 %r203, %r243, %r16, 8;
	// inline asm
	shl.b32 	%r623, %r203, 2;
	add.s32 	%r624, %r19, %r623;
	ld.shared.u32 	%r625, [%r624];
	// inline asm
	bfe.u32 %r206, %r246, %r436, 8;
	// inline asm
	shl.b32 	%r626, %r206, 2;
	add.s32 	%r627, %r20, %r626;
	ld.shared.u32 	%r628, [%r627];
	// inline asm
	bfe.u32 %r209, %r237, %r17, 8;
	// inline asm
	shl.b32 	%r629, %r209, 2;
	add.s32 	%r630, %r21, %r629;
	xor.b32  	%r631, %r622, %r8;
	xor.b32  	%r632, %r631, %r625;
	xor.b32  	%r633, %r632, %r628;
	ld.shared.u32 	%r634, [%r630];
	// inline asm
	bfe.u32 %r212, %r243, %r38, 8;
	// inline asm
	shl.b32 	%r635, %r212, 2;
	add.s32 	%r636, %r18, %r635;
	ld.shared.u32 	%r637, [%r636];
	// inline asm
	bfe.u32 %r215, %r246, %r16, 8;
	// inline asm
	shl.b32 	%r638, %r215, 2;
	add.s32 	%r639, %r19, %r638;
	ld.shared.u32 	%r640, [%r639];
	// inline asm
	bfe.u32 %r218, %r237, %r436, 8;
	// inline asm
	shl.b32 	%r641, %r218, 2;
	add.s32 	%r642, %r20, %r641;
	ld.shared.u32 	%r643, [%r642];
	// inline asm
	bfe.u32 %r221, %r240, %r17, 8;
	// inline asm
	shl.b32 	%r644, %r221, 2;
	add.s32 	%r645, %r21, %r644;
	xor.b32  	%r646, %r637, %r9;
	xor.b32  	%r647, %r646, %r640;
	xor.b32  	%r648, %r647, %r643;
	ld.shared.u32 	%r649, [%r645];
	// inline asm
	bfe.u32 %r224, %r246, %r38, 8;
	// inline asm
	shl.b32 	%r650, %r224, 2;
	add.s32 	%r651, %r18, %r650;
	ld.shared.u32 	%r652, [%r651];
	// inline asm
	bfe.u32 %r227, %r237, %r16, 8;
	// inline asm
	shl.b32 	%r653, %r227, 2;
	add.s32 	%r654, %r19, %r653;
	ld.shared.u32 	%r655, [%r654];
	// inline asm
	bfe.u32 %r230, %r240, %r436, 8;
	// inline asm
	shl.b32 	%r656, %r230, 2;
	add.s32 	%r657, %r20, %r656;
	ld.shared.u32 	%r658, [%r657];
	// inline asm
	bfe.u32 %r233, %r243, %r17, 8;
	// inline asm
	shl.b32 	%r659, %r233, 2;
	add.s32 	%r660, %r21, %r659;
	xor.b32  	%r661, %r652, %r10;
	xor.b32  	%r662, %r661, %r655;
	xor.b32  	%r663, %r662, %r658;
	ld.shared.u32 	%r664, [%r660];
	// inline asm
	bfe.u32 %r236, %r237, %r38, 8;
	// inline asm
	shl.b32 	%r665, %r236, 2;
	add.s32 	%r666, %r18, %r665;
	ld.shared.u32 	%r667, [%r666];
	// inline asm
	bfe.u32 %r239, %r240, %r16, 8;
	// inline asm
	shl.b32 	%r668, %r239, 2;
	add.s32 	%r669, %r19, %r668;
	ld.shared.u32 	%r670, [%r669];
	// inline asm
	bfe.u32 %r242, %r243, %r436, 8;
	// inline asm
	shl.b32 	%r671, %r242, 2;
	add.s32 	%r672, %r20, %r671;
	ld.shared.u32 	%r673, [%r672];
	// inline asm
	bfe.u32 %r245, %r246, %r17, 8;
	// inline asm
	shl.b32 	%r674, %r245, 2;
	add.s32 	%r675, %r21, %r674;
	xor.b32  	%r676, %r667, %r11;
	xor.b32  	%r677, %r676, %r670;
	xor.b32  	%r678, %r677, %r673;
	ld.shared.u32 	%r679, [%r675];
	xor.b32  	%r285, %r678, %r679;
	xor.b32  	%r294, %r663, %r664;
	xor.b32  	%r291, %r648, %r649;
	xor.b32  	%r288, %r633, %r634;
	add.s64 	%rd27, %rd26, %rd3;
	st.global.v4.u32 	[%rd27], {%r288, %r291, %r294, %r285};
	// inline asm
	bfe.u32 %r248, %r288, %r38, 8;
	// inline asm
	shl.b32 	%r680, %r248, 2;
	add.s32 	%r681, %r18, %r680;
	ld.shared.u32 	%r682, [%r681];
	// inline asm
	bfe.u32 %r251, %r291, %r16, 8;
	// inline asm
	shl.b32 	%r683, %r251, 2;
	add.s32 	%r684, %r19, %r683;
	ld.shared.u32 	%r685, [%r684];
	// inline asm
	bfe.u32 %r254, %r294, %r436, 8;
	// inline asm
	shl.b32 	%r686, %r254, 2;
	add.s32 	%r687, %r20, %r686;
	ld.shared.u32 	%r688, [%r687];
	// inline asm
	bfe.u32 %r257, %r285, %r17, 8;
	// inline asm
	shl.b32 	%r689, %r257, 2;
	add.s32 	%r690, %r21, %r689;
	xor.b32  	%r691, %r682, %r8;
	xor.b32  	%r692, %r691, %r685;
	xor.b32  	%r693, %r692, %r688;
	ld.shared.u32 	%r694, [%r690];
	// inline asm
	bfe.u32 %r260, %r291, %r38, 8;
	// inline asm
	shl.b32 	%r695, %r260, 2;
	add.s32 	%r696, %r18, %r695;
	ld.shared.u32 	%r697, [%r696];
	// inline asm
	bfe.u32 %r263, %r294, %r16, 8;
	// inline asm
	shl.b32 	%r698, %r263, 2;
	add.s32 	%r699, %r19, %r698;
	ld.shared.u32 	%r700, [%r699];
	// inline asm
	bfe.u32 %r266, %r285, %r436, 8;
	// inline asm
	shl.b32 	%r701, %r266, 2;
	add.s32 	%r702, %r20, %r701;
	ld.shared.u32 	%r703, [%r702];
	// inline asm
	bfe.u32 %r269, %r288, %r17, 8;
	// inline asm
	shl.b32 	%r704, %r269, 2;
	add.s32 	%r705, %r21, %r704;
	xor.b32  	%r706, %r697, %r9;
	xor.b32  	%r707, %r706, %r700;
	xor.b32  	%r708, %r707, %r703;
	ld.shared.u32 	%r709, [%r705];
	// inline asm
	bfe.u32 %r272, %r294, %r38, 8;
	// inline asm
	shl.b32 	%r710, %r272, 2;
	add.s32 	%r711, %r18, %r710;
	ld.shared.u32 	%r712, [%r711];
	// inline asm
	bfe.u32 %r275, %r285, %r16, 8;
	// inline asm
	shl.b32 	%r713, %r275, 2;
	add.s32 	%r714, %r19, %r713;
	ld.shared.u32 	%r715, [%r714];
	// inline asm
	bfe.u32 %r278, %r288, %r436, 8;
	// inline asm
	shl.b32 	%r716, %r278, 2;
	add.s32 	%r717, %r20, %r716;
	ld.shared.u32 	%r718, [%r717];
	// inline asm
	bfe.u32 %r281, %r291, %r17, 8;
	// inline asm
	shl.b32 	%r719, %r281, 2;
	add.s32 	%r720, %r21, %r719;
	xor.b32  	%r721, %r712, %r10;
	xor.b32  	%r722, %r721, %r715;
	xor.b32  	%r723, %r722, %r718;
	ld.shared.u32 	%r724, [%r720];
	// inline asm
	bfe.u32 %r284, %r285, %r38, 8;
	// inline asm
	shl.b32 	%r725, %r284, 2;
	add.s32 	%r726, %r18, %r725;
	ld.shared.u32 	%r727, [%r726];
	// inline asm
	bfe.u32 %r287, %r288, %r16, 8;
	// inline asm
	shl.b32 	%r728, %r287, 2;
	add.s32 	%r729, %r19, %r728;
	ld.shared.u32 	%r730, [%r729];
	// inline asm
	bfe.u32 %r290, %r291, %r436, 8;
	// inline asm
	shl.b32 	%r731, %r290, 2;
	add.s32 	%r732, %r20, %r731;
	ld.shared.u32 	%r733, [%r732];
	// inline asm
	bfe.u32 %r293, %r294, %r17, 8;
	// inline asm
	shl.b32 	%r734, %r293, 2;
	add.s32 	%r735, %r21, %r734;
	xor.b32  	%r736, %r727, %r11;
	xor.b32  	%r737, %r736, %r730;
	xor.b32  	%r738, %r737, %r733;
	ld.shared.u32 	%r739, [%r735];
	xor.b32  	%r333, %r738, %r739;
	xor.b32  	%r342, %r723, %r724;
	xor.b32  	%r339, %r708, %r709;
	xor.b32  	%r336, %r693, %r694;
	add.s64 	%rd28, %rd27, %rd3;
	st.global.v4.u32 	[%rd28], {%r336, %r339, %r342, %r333};
	// inline asm
	bfe.u32 %r296, %r336, %r38, 8;
	// inline asm
	shl.b32 	%r740, %r296, 2;
	add.s32 	%r741, %r18, %r740;
	ld.shared.u32 	%r742, [%r741];
	// inline asm
	bfe.u32 %r299, %r339, %r16, 8;
	// inline asm
	shl.b32 	%r743, %r299, 2;
	add.s32 	%r744, %r19, %r743;
	ld.shared.u32 	%r745, [%r744];
	// inline asm
	bfe.u32 %r302, %r342, %r436, 8;
	// inline asm
	shl.b32 	%r746, %r302, 2;
	add.s32 	%r747, %r20, %r746;
	ld.shared.u32 	%r748, [%r747];
	// inline asm
	bfe.u32 %r305, %r333, %r17, 8;
	// inline asm
	shl.b32 	%r749, %r305, 2;
	add.s32 	%r750, %r21, %r749;
	xor.b32  	%r751, %r742, %r8;
	xor.b32  	%r752, %r751, %r745;
	xor.b32  	%r753, %r752, %r748;
	ld.shared.u32 	%r754, [%r750];
	// inline asm
	bfe.u32 %r308, %r339, %r38, 8;
	// inline asm
	shl.b32 	%r755, %r308, 2;
	add.s32 	%r756, %r18, %r755;
	ld.shared.u32 	%r757, [%r756];
	// inline asm
	bfe.u32 %r311, %r342, %r16, 8;
	// inline asm
	shl.b32 	%r758, %r311, 2;
	add.s32 	%r759, %r19, %r758;
	ld.shared.u32 	%r760, [%r759];
	// inline asm
	bfe.u32 %r314, %r333, %r436, 8;
	// inline asm
	shl.b32 	%r761, %r314, 2;
	add.s32 	%r762, %r20, %r761;
	ld.shared.u32 	%r763, [%r762];
	// inline asm
	bfe.u32 %r317, %r336, %r17, 8;
	// inline asm
	shl.b32 	%r764, %r317, 2;
	add.s32 	%r765, %r21, %r764;
	xor.b32  	%r766, %r757, %r9;
	xor.b32  	%r767, %r766, %r760;
	xor.b32  	%r768, %r767, %r763;
	ld.shared.u32 	%r769, [%r765];
	// inline asm
	bfe.u32 %r320, %r342, %r38, 8;
	// inline asm
	shl.b32 	%r770, %r320, 2;
	add.s32 	%r771, %r18, %r770;
	ld.shared.u32 	%r772, [%r771];
	// inline asm
	bfe.u32 %r323, %r333, %r16, 8;
	// inline asm
	shl.b32 	%r773, %r323, 2;
	add.s32 	%r774, %r19, %r773;
	ld.shared.u32 	%r775, [%r774];
	// inline asm
	bfe.u32 %r326, %r336, %r436, 8;
	// inline asm
	shl.b32 	%r776, %r326, 2;
	add.s32 	%r777, %r20, %r776;
	ld.shared.u32 	%r778, [%r777];
	// inline asm
	bfe.u32 %r329, %r339, %r17, 8;
	// inline asm
	shl.b32 	%r779, %r329, 2;
	add.s32 	%r780, %r21, %r779;
	xor.b32  	%r781, %r772, %r10;
	xor.b32  	%r782, %r781, %r775;
	xor.b32  	%r783, %r782, %r778;
	ld.shared.u32 	%r784, [%r780];
	// inline asm
	bfe.u32 %r332, %r333, %r38, 8;
	// inline asm
	shl.b32 	%r785, %r332, 2;
	add.s32 	%r786, %r18, %r785;
	ld.shared.u32 	%r787, [%r786];
	// inline asm
	bfe.u32 %r335, %r336, %r16, 8;
	// inline asm
	shl.b32 	%r788, %r335, 2;
	add.s32 	%r789, %r19, %r788;
	ld.shared.u32 	%r790, [%r789];
	// inline asm
	bfe.u32 %r338, %r339, %r436, 8;
	// inline asm
	shl.b32 	%r791, %r338, 2;
	add.s32 	%r792, %r20, %r791;
	ld.shared.u32 	%r793, [%r792];
	// inline asm
	bfe.u32 %r341, %r342, %r17, 8;
	// inline asm
	shl.b32 	%r794, %r341, 2;
	add.s32 	%r795, %r21, %r794;
	xor.b32  	%r796, %r787, %r11;
	xor.b32  	%r797, %r796, %r790;
	xor.b32  	%r798, %r797, %r793;
	ld.shared.u32 	%r799, [%r795];
	xor.b32  	%r381, %r798, %r799;
	xor.b32  	%r390, %r783, %r784;
	xor.b32  	%r387, %r768, %r769;
	xor.b32  	%r384, %r753, %r754;
	add.s64 	%rd29, %rd28, %rd3;
	st.global.v4.u32 	[%rd29], {%r384, %r387, %r390, %r381};
	// inline asm
	bfe.u32 %r344, %r384, %r38, 8;
	// inline asm
	shl.b32 	%r800, %r344, 2;
	add.s32 	%r801, %r18, %r800;
	ld.shared.u32 	%r802, [%r801];
	// inline asm
	bfe.u32 %r347, %r387, %r16, 8;
	// inline asm
	shl.b32 	%r803, %r347, 2;
	add.s32 	%r804, %r19, %r803;
	ld.shared.u32 	%r805, [%r804];
	// inline asm
	bfe.u32 %r350, %r390, %r436, 8;
	// inline asm
	shl.b32 	%r806, %r350, 2;
	add.s32 	%r807, %r20, %r806;
	ld.shared.u32 	%r808, [%r807];
	// inline asm
	bfe.u32 %r353, %r381, %r17, 8;
	// inline asm
	shl.b32 	%r809, %r353, 2;
	add.s32 	%r810, %r21, %r809;
	xor.b32  	%r811, %r802, %r8;
	xor.b32  	%r812, %r811, %r805;
	xor.b32  	%r813, %r812, %r808;
	ld.shared.u32 	%r814, [%r810];
	// inline asm
	bfe.u32 %r356, %r387, %r38, 8;
	// inline asm
	shl.b32 	%r815, %r356, 2;
	add.s32 	%r816, %r18, %r815;
	ld.shared.u32 	%r817, [%r816];
	// inline asm
	bfe.u32 %r359, %r390, %r16, 8;
	// inline asm
	shl.b32 	%r818, %r359, 2;
	add.s32 	%r819, %r19, %r818;
	ld.shared.u32 	%r820, [%r819];
	// inline asm
	bfe.u32 %r362, %r381, %r436, 8;
	// inline asm
	shl.b32 	%r821, %r362, 2;
	add.s32 	%r822, %r20, %r821;
	ld.shared.u32 	%r823, [%r822];
	// inline asm
	bfe.u32 %r365, %r384, %r17, 8;
	// inline asm
	shl.b32 	%r824, %r365, 2;
	add.s32 	%r825, %r21, %r824;
	xor.b32  	%r826, %r817, %r9;
	xor.b32  	%r827, %r826, %r820;
	xor.b32  	%r828, %r827, %r823;
	ld.shared.u32 	%r829, [%r825];
	// inline asm
	bfe.u32 %r368, %r390, %r38, 8;
	// inline asm
	shl.b32 	%r830, %r368, 2;
	add.s32 	%r831, %r18, %r830;
	ld.shared.u32 	%r832, [%r831];
	// inline asm
	bfe.u32 %r371, %r381, %r16, 8;
	// inline asm
	shl.b32 	%r833, %r371, 2;
	add.s32 	%r834, %r19, %r833;
	ld.shared.u32 	%r835, [%r834];
	// inline asm
	bfe.u32 %r374, %r384, %r436, 8;
	// inline asm
	shl.b32 	%r836, %r374, 2;
	add.s32 	%r837, %r20, %r836;
	ld.shared.u32 	%r838, [%r837];
	// inline asm
	bfe.u32 %r377, %r387, %r17, 8;
	// inline asm
	shl.b32 	%r839, %r377, 2;
	add.s32 	%r840, %r21, %r839;
	xor.b32  	%r841, %r832, %r10;
	xor.b32  	%r842, %r841, %r835;
	xor.b32  	%r843, %r842, %r838;
	ld.shared.u32 	%r844, [%r840];
	// inline asm
	bfe.u32 %r380, %r381, %r38, 8;
	// inline asm
	shl.b32 	%r845, %r380, 2;
	add.s32 	%r846, %r18, %r845;
	ld.shared.u32 	%r847, [%r846];
	// inline asm
	bfe.u32 %r383, %r384, %r16, 8;
	// inline asm
	shl.b32 	%r848, %r383, 2;
	add.s32 	%r849, %r19, %r848;
	ld.shared.u32 	%r850, [%r849];
	// inline asm
	bfe.u32 %r386, %r387, %r436, 8;
	// inline asm
	shl.b32 	%r851, %r386, 2;
	add.s32 	%r852, %r20, %r851;
	ld.shared.u32 	%r853, [%r852];
	// inline asm
	bfe.u32 %r389, %r390, %r17, 8;
	// inline asm
	shl.b32 	%r854, %r389, 2;
	add.s32 	%r855, %r21, %r854;
	xor.b32  	%r856, %r847, %r11;
	xor.b32  	%r857, %r856, %r850;
	xor.b32  	%r858, %r857, %r853;
	ld.shared.u32 	%r859, [%r855];
	xor.b32  	%r429, %r858, %r859;
	xor.b32  	%r438, %r843, %r844;
	xor.b32  	%r435, %r828, %r829;
	xor.b32  	%r432, %r813, %r814;
	add.s64 	%rd30, %rd29, %rd3;
	st.global.v4.u32 	[%rd30], {%r432, %r435, %r438, %r429};
	// inline asm
	bfe.u32 %r392, %r432, %r38, 8;
	// inline asm
	shl.b32 	%r860, %r392, 2;
	add.s32 	%r861, %r18, %r860;
	ld.shared.u32 	%r862, [%r861];
	// inline asm
	bfe.u32 %r395, %r435, %r16, 8;
	// inline asm
	shl.b32 	%r863, %r395, 2;
	add.s32 	%r864, %r19, %r863;
	ld.shared.u32 	%r865, [%r864];
	// inline asm
	bfe.u32 %r398, %r438, %r436, 8;
	// inline asm
	shl.b32 	%r866, %r398, 2;
	add.s32 	%r867, %r20, %r866;
	ld.shared.u32 	%r868, [%r867];
	// inline asm
	bfe.u32 %r401, %r429, %r17, 8;
	// inline asm
	shl.b32 	%r869, %r401, 2;
	add.s32 	%r870, %r21, %r869;
	xor.b32  	%r871, %r862, %r8;
	xor.b32  	%r872, %r871, %r865;
	xor.b32  	%r873, %r872, %r868;
	ld.shared.u32 	%r874, [%r870];
	xor.b32  	%r925, %r873, %r874;
	// inline asm
	bfe.u32 %r404, %r435, %r38, 8;
	// inline asm
	shl.b32 	%r875, %r404, 2;
	add.s32 	%r876, %r18, %r875;
	ld.shared.u32 	%r877, [%r876];
	// inline asm
	bfe.u32 %r407, %r438, %r16, 8;
	// inline asm
	shl.b32 	%r878, %r407, 2;
	add.s32 	%r879, %r19, %r878;
	ld.shared.u32 	%r880, [%r879];
	// inline asm
	bfe.u32 %r410, %r429, %r436, 8;
	// inline asm
	shl.b32 	%r881, %r410, 2;
	add.s32 	%r882, %r20, %r881;
	ld.shared.u32 	%r883, [%r882];
	// inline asm
	bfe.u32 %r413, %r432, %r17, 8;
	// inline asm
	shl.b32 	%r884, %r413, 2;
	add.s32 	%r885, %r21, %r884;
	xor.b32  	%r886, %r877, %r9;
	xor.b32  	%r887, %r886, %r880;
	xor.b32  	%r888, %r887, %r883;
	ld.shared.u32 	%r889, [%r885];
	xor.b32  	%r924, %r888, %r889;
	// inline asm
	bfe.u32 %r416, %r438, %r38, 8;
	// inline asm
	shl.b32 	%r890, %r416, 2;
	add.s32 	%r891, %r18, %r890;
	ld.shared.u32 	%r892, [%r891];
	// inline asm
	bfe.u32 %r419, %r429, %r16, 8;
	// inline asm
	shl.b32 	%r893, %r419, 2;
	add.s32 	%r894, %r19, %r893;
	ld.shared.u32 	%r895, [%r894];
	// inline asm
	bfe.u32 %r422, %r432, %r436, 8;
	// inline asm
	shl.b32 	%r896, %r422, 2;
	add.s32 	%r897, %r20, %r896;
	ld.shared.u32 	%r898, [%r897];
	// inline asm
	bfe.u32 %r425, %r435, %r17, 8;
	// inline asm
	shl.b32 	%r899, %r425, 2;
	add.s32 	%r900, %r21, %r899;
	xor.b32  	%r901, %r892, %r10;
	xor.b32  	%r902, %r901, %r895;
	xor.b32  	%r903, %r902, %r898;
	ld.shared.u32 	%r904, [%r900];
	xor.b32  	%r923, %r903, %r904;
	// inline asm
	bfe.u32 %r428, %r429, %r38, 8;
	// inline asm
	shl.b32 	%r905, %r428, 2;
	add.s32 	%r906, %r18, %r905;
	ld.shared.u32 	%r907, [%r906];
	// inline asm
	bfe.u32 %r431, %r432, %r16, 8;
	// inline asm
	shl.b32 	%r908, %r431, 2;
	add.s32 	%r909, %r19, %r908;
	ld.shared.u32 	%r910, [%r909];
	// inline asm
	bfe.u32 %r434, %r435, %r436, 8;
	// inline asm
	shl.b32 	%r911, %r434, 2;
	add.s32 	%r912, %r20, %r911;
	ld.shared.u32 	%r913, [%r912];
	// inline asm
	bfe.u32 %r437, %r438, %r17, 8;
	// inline asm
	shl.b32 	%r914, %r437, 2;
	add.s32 	%r915, %r21, %r914;
	xor.b32  	%r916, %r907, %r11;
	xor.b32  	%r917, %r916, %r910;
	xor.b32  	%r918, %r917, %r913;
	ld.shared.u32 	%r919, [%r915];
	xor.b32  	%r922, %r918, %r919;
	add.s64 	%rd31, %rd30, %rd3;
	add.s64 	%rd32, %rd31, %rd3;
	st.global.v4.u32 	[%rd31], {%r925, %r924, %r923, %r922};
	add.s32 	%r921, %r921, 32;
	setp.lt.u32	%p5, %r921, 131072;
	@%p5 bra 	BB3_4;

	st.global.v2.u32 	[%rd1], {%r925, %r924};
	st.global.v2.u32 	[%rd1+8], {%r923, %r922};

BB3_6:
	ret;
}

	// .globl	_Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j
.visible .entry _Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j(
	.param .u64 _Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j_param_0,
	.param .u64 _Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j_param_1,
	.param .u32 _Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<277>;
	.reg .b64 	%rd<25>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11fillAes1Rx4ILy2176ELb0EEvPvS0_jE1T[8192];

	ld.param.u64 	%rd5, [_Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j_param_0];
	ld.param.u64 	%rd6, [_Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j_param_1];
	ld.param.u32 	%r32, [_Z11fillAes1Rx4ILy2176ELb0EEvPvS0_j_param_2];
	shl.b32 	%r33, %r32, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r271, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r271;
	setp.ge.u32	%p1, %r4, %r33;
	@%p1 bra 	BB4_6;

	setp.gt.s32	%p2, %r271, 2047;
	@%p2 bra 	BB4_3;

BB4_2:
	mul.wide.s32 	%rd7, %r271, 4;
	mov.u64 	%rd8, AES_TABLE;
	add.s64 	%rd9, %rd8, %rd7;
	ld.const.u32 	%r34, [%rd9];
	shl.b32 	%r35, %r271, 2;
	mov.u32 	%r36, _ZZ11fillAes1Rx4ILy2176ELb0EEvPvS0_jE1T;
	add.s32 	%r37, %r36, %r35;
	st.shared.u32 	[%r37], %r34;
	add.s32 	%r271, %r271, %r1;
	setp.lt.s32	%p3, %r271, 2048;
	@%p3 bra 	BB4_2;

BB4_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r39, %r7, 2;
	mul.wide.u32 	%rd10, %r39, 4;
	mov.u64 	%rd11, AES_KEY_FILL;
	add.s64 	%rd12, %rd11, %rd10;
	ld.const.u32 	%r8, [%rd12];
	ld.const.u32 	%r9, [%rd12+4];
	ld.const.u32 	%r10, [%rd12+8];
	ld.const.u32 	%r11, [%rd12+12];
	shr.u32 	%r40, %r4, 2;
	mul.wide.u32 	%rd13, %r40, 16;
	mul.wide.u32 	%rd14, %r7, 4;
	add.s64 	%rd15, %rd13, %rd14;
	cvta.to.global.u64 	%rd16, %rd5;
	shl.b64 	%rd17, %rd15, 2;
	add.s64 	%rd1, %rd16, %rd17;
	ld.global.u32 	%r276, [%rd1];
	ld.global.u32 	%r275, [%rd1+4];
	ld.global.u32 	%r274, [%rd1+8];
	ld.global.u32 	%r273, [%rd1+12];
	and.b32  	%r41, %r4, 1;
	shl.b32 	%r42, %r4, 4;
	and.b32  	%r43, %r42, 16;
	xor.b32  	%r44, %r43, 16;
	add.s32 	%r16, %r44, 8;
	add.s32 	%r17, %r43, 8;
	setp.eq.s32	%p4, %r41, 0;
	mov.u32 	%r38, 0;
	mov.u32 	%r45, _ZZ11fillAes1Rx4ILy2176ELb0EEvPvS0_jE1T;
	add.s32 	%r46, %r45, 4096;
	selp.b32	%r18, %r46, %r45, %p4;
	add.s32 	%r47, %r45, 1024;
	add.s32 	%r48, %r45, 7168;
	selp.b32	%r19, %r48, %r47, %p4;
	add.s32 	%r49, %r45, 2048;
	add.s32 	%r50, %r45, 6144;
	selp.b32	%r20, %r50, %r49, %p4;
	add.s32 	%r51, %r45, 3072;
	add.s32 	%r52, %r45, 5120;
	selp.b32	%r21, %r52, %r51, %p4;
	mul.wide.u32 	%rd18, %r40, 136;
	cvt.u64.u32	%rd19, %r4;
	and.b64  	%rd20, %rd19, 3;
	or.b64  	%rd21, %rd18, %rd20;
	cvta.to.global.u64 	%rd22, %rd6;
	shl.b64 	%rd23, %rd21, 4;
	add.s64 	%rd24, %rd22, %rd23;
	mov.u32 	%r272, %r38;

BB4_4:
	.pragma "nounroll";
	// inline asm
	bfe.u32 %r55, %r276, %r38, 8;
	// inline asm
	shl.b32 	%r151, %r55, 2;
	add.s32 	%r152, %r18, %r151;
	ld.shared.u32 	%r153, [%r152];
	// inline asm
	bfe.u32 %r58, %r275, %r16, 8;
	// inline asm
	shl.b32 	%r154, %r58, 2;
	add.s32 	%r155, %r19, %r154;
	ld.shared.u32 	%r156, [%r155];
	mov.u32 	%r147, 16;
	// inline asm
	bfe.u32 %r61, %r274, %r147, 8;
	// inline asm
	shl.b32 	%r157, %r61, 2;
	add.s32 	%r158, %r20, %r157;
	ld.shared.u32 	%r159, [%r158];
	// inline asm
	bfe.u32 %r64, %r273, %r17, 8;
	// inline asm
	shl.b32 	%r160, %r64, 2;
	add.s32 	%r161, %r21, %r160;
	xor.b32  	%r162, %r153, %r8;
	xor.b32  	%r163, %r162, %r156;
	xor.b32  	%r164, %r163, %r159;
	ld.shared.u32 	%r165, [%r161];
	// inline asm
	bfe.u32 %r67, %r275, %r38, 8;
	// inline asm
	shl.b32 	%r166, %r67, 2;
	add.s32 	%r167, %r18, %r166;
	ld.shared.u32 	%r168, [%r167];
	// inline asm
	bfe.u32 %r70, %r274, %r16, 8;
	// inline asm
	shl.b32 	%r169, %r70, 2;
	add.s32 	%r170, %r19, %r169;
	ld.shared.u32 	%r171, [%r170];
	// inline asm
	bfe.u32 %r73, %r273, %r147, 8;
	// inline asm
	shl.b32 	%r172, %r73, 2;
	add.s32 	%r173, %r20, %r172;
	ld.shared.u32 	%r174, [%r173];
	// inline asm
	bfe.u32 %r76, %r276, %r17, 8;
	// inline asm
	shl.b32 	%r175, %r76, 2;
	add.s32 	%r176, %r21, %r175;
	xor.b32  	%r177, %r168, %r9;
	xor.b32  	%r178, %r177, %r171;
	xor.b32  	%r179, %r178, %r174;
	ld.shared.u32 	%r180, [%r176];
	// inline asm
	bfe.u32 %r79, %r274, %r38, 8;
	// inline asm
	shl.b32 	%r181, %r79, 2;
	add.s32 	%r182, %r18, %r181;
	ld.shared.u32 	%r183, [%r182];
	// inline asm
	bfe.u32 %r82, %r273, %r16, 8;
	// inline asm
	shl.b32 	%r184, %r82, 2;
	add.s32 	%r185, %r19, %r184;
	ld.shared.u32 	%r186, [%r185];
	// inline asm
	bfe.u32 %r85, %r276, %r147, 8;
	// inline asm
	shl.b32 	%r187, %r85, 2;
	add.s32 	%r188, %r20, %r187;
	ld.shared.u32 	%r189, [%r188];
	// inline asm
	bfe.u32 %r88, %r275, %r17, 8;
	// inline asm
	shl.b32 	%r190, %r88, 2;
	add.s32 	%r191, %r21, %r190;
	xor.b32  	%r192, %r183, %r10;
	xor.b32  	%r193, %r192, %r186;
	xor.b32  	%r194, %r193, %r189;
	ld.shared.u32 	%r195, [%r191];
	// inline asm
	bfe.u32 %r91, %r273, %r38, 8;
	// inline asm
	shl.b32 	%r196, %r91, 2;
	add.s32 	%r197, %r18, %r196;
	ld.shared.u32 	%r198, [%r197];
	// inline asm
	bfe.u32 %r94, %r276, %r16, 8;
	// inline asm
	shl.b32 	%r199, %r94, 2;
	add.s32 	%r200, %r19, %r199;
	ld.shared.u32 	%r201, [%r200];
	// inline asm
	bfe.u32 %r97, %r275, %r147, 8;
	// inline asm
	shl.b32 	%r202, %r97, 2;
	add.s32 	%r203, %r20, %r202;
	ld.shared.u32 	%r204, [%r203];
	// inline asm
	bfe.u32 %r100, %r274, %r17, 8;
	// inline asm
	shl.b32 	%r205, %r100, 2;
	add.s32 	%r206, %r21, %r205;
	xor.b32  	%r207, %r198, %r11;
	xor.b32  	%r208, %r207, %r201;
	xor.b32  	%r209, %r208, %r204;
	ld.shared.u32 	%r210, [%r206];
	xor.b32  	%r140, %r209, %r210;
	xor.b32  	%r149, %r194, %r195;
	xor.b32  	%r146, %r179, %r180;
	xor.b32  	%r143, %r164, %r165;
	st.global.v4.u32 	[%rd24], {%r143, %r146, %r149, %r140};
	// inline asm
	bfe.u32 %r103, %r143, %r38, 8;
	// inline asm
	shl.b32 	%r211, %r103, 2;
	add.s32 	%r212, %r18, %r211;
	ld.shared.u32 	%r213, [%r212];
	// inline asm
	bfe.u32 %r106, %r146, %r16, 8;
	// inline asm
	shl.b32 	%r214, %r106, 2;
	add.s32 	%r215, %r19, %r214;
	ld.shared.u32 	%r216, [%r215];
	// inline asm
	bfe.u32 %r109, %r149, %r147, 8;
	// inline asm
	shl.b32 	%r217, %r109, 2;
	add.s32 	%r218, %r20, %r217;
	ld.shared.u32 	%r219, [%r218];
	// inline asm
	bfe.u32 %r112, %r140, %r17, 8;
	// inline asm
	shl.b32 	%r220, %r112, 2;
	add.s32 	%r221, %r21, %r220;
	xor.b32  	%r222, %r213, %r8;
	xor.b32  	%r223, %r222, %r216;
	xor.b32  	%r224, %r223, %r219;
	ld.shared.u32 	%r225, [%r221];
	xor.b32  	%r276, %r224, %r225;
	// inline asm
	bfe.u32 %r115, %r146, %r38, 8;
	// inline asm
	shl.b32 	%r226, %r115, 2;
	add.s32 	%r227, %r18, %r226;
	ld.shared.u32 	%r228, [%r227];
	// inline asm
	bfe.u32 %r118, %r149, %r16, 8;
	// inline asm
	shl.b32 	%r229, %r118, 2;
	add.s32 	%r230, %r19, %r229;
	ld.shared.u32 	%r231, [%r230];
	// inline asm
	bfe.u32 %r121, %r140, %r147, 8;
	// inline asm
	shl.b32 	%r232, %r121, 2;
	add.s32 	%r233, %r20, %r232;
	ld.shared.u32 	%r234, [%r233];
	// inline asm
	bfe.u32 %r124, %r143, %r17, 8;
	// inline asm
	shl.b32 	%r235, %r124, 2;
	add.s32 	%r236, %r21, %r235;
	xor.b32  	%r237, %r228, %r9;
	xor.b32  	%r238, %r237, %r231;
	xor.b32  	%r239, %r238, %r234;
	ld.shared.u32 	%r240, [%r236];
	xor.b32  	%r275, %r239, %r240;
	// inline asm
	bfe.u32 %r127, %r149, %r38, 8;
	// inline asm
	shl.b32 	%r241, %r127, 2;
	add.s32 	%r242, %r18, %r241;
	ld.shared.u32 	%r243, [%r242];
	// inline asm
	bfe.u32 %r130, %r140, %r16, 8;
	// inline asm
	shl.b32 	%r244, %r130, 2;
	add.s32 	%r245, %r19, %r244;
	ld.shared.u32 	%r246, [%r245];
	// inline asm
	bfe.u32 %r133, %r143, %r147, 8;
	// inline asm
	shl.b32 	%r247, %r133, 2;
	add.s32 	%r248, %r20, %r247;
	ld.shared.u32 	%r249, [%r248];
	// inline asm
	bfe.u32 %r136, %r146, %r17, 8;
	// inline asm
	shl.b32 	%r250, %r136, 2;
	add.s32 	%r251, %r21, %r250;
	xor.b32  	%r252, %r243, %r10;
	xor.b32  	%r253, %r252, %r246;
	xor.b32  	%r254, %r253, %r249;
	ld.shared.u32 	%r255, [%r251];
	xor.b32  	%r274, %r254, %r255;
	// inline asm
	bfe.u32 %r139, %r140, %r38, 8;
	// inline asm
	shl.b32 	%r256, %r139, 2;
	add.s32 	%r257, %r18, %r256;
	ld.shared.u32 	%r258, [%r257];
	// inline asm
	bfe.u32 %r142, %r143, %r16, 8;
	// inline asm
	shl.b32 	%r259, %r142, 2;
	add.s32 	%r260, %r19, %r259;
	ld.shared.u32 	%r261, [%r260];
	// inline asm
	bfe.u32 %r145, %r146, %r147, 8;
	// inline asm
	shl.b32 	%r262, %r145, 2;
	add.s32 	%r263, %r20, %r262;
	ld.shared.u32 	%r264, [%r263];
	// inline asm
	bfe.u32 %r148, %r149, %r17, 8;
	// inline asm
	shl.b32 	%r265, %r148, 2;
	add.s32 	%r266, %r21, %r265;
	xor.b32  	%r267, %r258, %r11;
	xor.b32  	%r268, %r267, %r261;
	xor.b32  	%r269, %r268, %r264;
	ld.shared.u32 	%r270, [%r266];
	xor.b32  	%r273, %r269, %r270;
	st.global.v4.u32 	[%rd24+64], {%r276, %r275, %r274, %r273};
	add.s32 	%r272, %r272, 8;
	setp.lt.u32	%p5, %r272, 136;
	add.s64 	%rd24, %rd24, 128;
	@%p5 bra 	BB4_4;

	st.global.v2.u32 	[%rd1], {%r276, %r275};
	st.global.v2.u32 	[%rd1+8], {%r274, %r273};

BB4_6:
	ret;
}

	// .globl	_Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj
.visible .entry _Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj(
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj_param_0,
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj_param_1,
	.param .u32 _Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<1133>;
	.reg .b64 	%rd<40>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvjE1T[8192];

	ld.param.u64 	%rd13, [_Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj_param_0];
	ld.param.u64 	%rd14, [_Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj_param_1];
	ld.param.u32 	%r45, [_Z11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvj_param_2];
	shl.b32 	%r46, %r45, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r1127, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r1127;
	setp.ge.u32	%p1, %r4, %r46;
	@%p1 bra 	BB5_6;

	setp.gt.s32	%p2, %r1127, 2047;
	@%p2 bra 	BB5_3;

BB5_2:
	mul.wide.s32 	%rd15, %r1127, 4;
	mov.u64 	%rd16, AES_TABLE;
	add.s64 	%rd17, %rd16, %rd15;
	ld.const.u32 	%r47, [%rd17];
	shl.b32 	%r48, %r1127, 2;
	mov.u32 	%r49, _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvjE1T;
	add.s32 	%r50, %r49, %r48;
	st.shared.u32 	[%r50], %r47;
	add.s32 	%r1127, %r1127, %r1;
	setp.lt.s32	%p3, %r1127, 2048;
	@%p3 bra 	BB5_2;

BB5_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r52, %r7, 2;
	mul.wide.u32 	%rd18, %r52, 4;
	mov.u64 	%rd19, AES_STATE_HASH;
	add.s64 	%rd20, %rd19, %rd18;
	ld.const.u32 	%r1129, [%rd20];
	ld.const.u32 	%r1130, [%rd20+4];
	ld.const.u32 	%r1131, [%rd20+8];
	ld.const.u32 	%r1132, [%rd20+12];
	and.b32  	%r53, %r4, 1;
	shl.b32 	%r54, %r4, 4;
	and.b32  	%r55, %r54, 16;
	add.s32 	%r12, %r55, 8;
	xor.b32  	%r56, %r55, 16;
	add.s32 	%r13, %r56, 8;
	cvt.u64.u32	%rd1, %r7;
	setp.eq.s32	%p4, %r53, 0;
	mov.u32 	%r57, _ZZ11hashAes1Rx4ILy2097152ELj192ELj2048EEvPKvPvjE1T;
	add.s32 	%r58, %r57, 4096;
	selp.b32	%r14, %r57, %r58, %p4;
	add.s32 	%r59, %r57, 7168;
	add.s32 	%r60, %r57, 1024;
	selp.b32	%r15, %r60, %r59, %p4;
	add.s32 	%r61, %r57, 6144;
	add.s32 	%r62, %r57, 2048;
	selp.b32	%r16, %r62, %r61, %p4;
	add.s32 	%r63, %r57, 5120;
	add.s32 	%r64, %r57, 3072;
	selp.b32	%r17, %r64, %r63, %p4;
	and.b32  	%r66, %r4, -4;
	cvt.u64.u32	%rd21, %r66;
	cvt.u64.u32	%rd22, %r4;
	and.b64  	%rd23, %rd22, 3;
	or.b64  	%rd24, %rd21, %rd23;
	cvta.to.global.u64 	%rd25, %rd13;
	shl.b64 	%rd26, %rd24, 4;
	add.s64 	%rd39, %rd25, %rd26;
	shl.b32 	%r67, %r45, 5;
	mul.wide.u32 	%rd3, %r67, 16;
	mul.lo.s32 	%r68, %r45, 28;
	mul.wide.u32 	%rd4, %r68, 16;
	mul.lo.s32 	%r69, %r45, 24;
	mul.wide.u32 	%rd5, %r69, 16;
	mul.lo.s32 	%r70, %r45, 20;
	mul.wide.u32 	%rd6, %r70, 16;
	shl.b32 	%r71, %r45, 4;
	mul.wide.u32 	%rd7, %r71, 16;
	mul.lo.s32 	%r72, %r45, 12;
	mul.wide.u32 	%rd8, %r72, 16;
	shl.b32 	%r73, %r45, 3;
	mul.wide.u32 	%rd9, %r73, 16;
	mul.wide.u32 	%rd10, %r46, 16;
	shr.u32 	%r18, %r4, 2;
	mov.u32 	%r1128, -32;
	bra.uni 	BB5_4;

BB5_7:
	add.s64 	%rd12, %rd39, %rd3;
	ld.global.v4.u32 	{%r615, %r616, %r617, %r618}, [%rd39];
	ld.shared.u32 	%r623, [%r24];
	xor.b32  	%r624, %r615, %r623;
	ld.shared.u32 	%r625, [%r25];
	xor.b32  	%r626, %r624, %r625;
	ld.shared.u32 	%r627, [%r26];
	xor.b32  	%r628, %r626, %r627;
	ld.shared.u32 	%r629, [%r27];
	xor.b32  	%r319, %r628, %r629;
	ld.shared.u32 	%r630, [%r28];
	xor.b32  	%r631, %r630, %r616;
	ld.shared.u32 	%r632, [%r29];
	xor.b32  	%r633, %r631, %r632;
	ld.shared.u32 	%r634, [%r30];
	xor.b32  	%r635, %r633, %r634;
	ld.shared.u32 	%r636, [%r31];
	xor.b32  	%r322, %r635, %r636;
	ld.shared.u32 	%r637, [%r32];
	xor.b32  	%r638, %r637, %r617;
	ld.shared.u32 	%r639, [%r33];
	xor.b32  	%r640, %r638, %r639;
	ld.shared.u32 	%r641, [%r34];
	xor.b32  	%r642, %r640, %r641;
	ld.shared.u32 	%r643, [%r35];
	xor.b32  	%r325, %r642, %r643;
	ld.shared.u32 	%r644, [%r36];
	xor.b32  	%r645, %r644, %r618;
	ld.shared.u32 	%r646, [%r37];
	xor.b32  	%r647, %r645, %r646;
	ld.shared.u32 	%r648, [%r38];
	xor.b32  	%r649, %r647, %r648;
	ld.shared.u32 	%r650, [%r39];
	xor.b32  	%r316, %r649, %r650;
	add.s64 	%rd32, %rd39, %rd10;
	ld.global.v4.u32 	{%r651, %r652, %r653, %r654}, [%rd32];
	// inline asm
	bfe.u32 %r279, %r319, %r113, 8;
	// inline asm
	shl.b32 	%r659, %r279, 2;
	add.s32 	%r660, %r14, %r659;
	ld.shared.u32 	%r661, [%r660];
	// inline asm
	bfe.u32 %r282, %r322, %r12, 8;
	// inline asm
	shl.b32 	%r662, %r282, 2;
	add.s32 	%r663, %r15, %r662;
	ld.shared.u32 	%r664, [%r663];
	// inline asm
	bfe.u32 %r285, %r325, %r119, 8;
	// inline asm
	shl.b32 	%r665, %r285, 2;
	add.s32 	%r666, %r16, %r665;
	ld.shared.u32 	%r667, [%r666];
	// inline asm
	bfe.u32 %r288, %r316, %r13, 8;
	// inline asm
	shl.b32 	%r668, %r288, 2;
	add.s32 	%r669, %r17, %r668;
	xor.b32  	%r670, %r661, %r651;
	xor.b32  	%r671, %r670, %r664;
	xor.b32  	%r672, %r671, %r667;
	ld.shared.u32 	%r673, [%r669];
	xor.b32  	%r367, %r672, %r673;
	// inline asm
	bfe.u32 %r291, %r322, %r113, 8;
	// inline asm
	shl.b32 	%r674, %r291, 2;
	add.s32 	%r675, %r14, %r674;
	ld.shared.u32 	%r676, [%r675];
	// inline asm
	bfe.u32 %r294, %r325, %r12, 8;
	// inline asm
	shl.b32 	%r677, %r294, 2;
	add.s32 	%r678, %r15, %r677;
	ld.shared.u32 	%r679, [%r678];
	// inline asm
	bfe.u32 %r297, %r316, %r119, 8;
	// inline asm
	shl.b32 	%r680, %r297, 2;
	add.s32 	%r681, %r16, %r680;
	ld.shared.u32 	%r682, [%r681];
	// inline asm
	bfe.u32 %r300, %r319, %r13, 8;
	// inline asm
	shl.b32 	%r683, %r300, 2;
	add.s32 	%r684, %r17, %r683;
	xor.b32  	%r685, %r676, %r652;
	xor.b32  	%r686, %r685, %r679;
	xor.b32  	%r687, %r686, %r682;
	ld.shared.u32 	%r688, [%r684];
	xor.b32  	%r370, %r687, %r688;
	// inline asm
	bfe.u32 %r303, %r325, %r113, 8;
	// inline asm
	shl.b32 	%r689, %r303, 2;
	add.s32 	%r690, %r14, %r689;
	ld.shared.u32 	%r691, [%r690];
	// inline asm
	bfe.u32 %r306, %r316, %r12, 8;
	// inline asm
	shl.b32 	%r692, %r306, 2;
	add.s32 	%r693, %r15, %r692;
	ld.shared.u32 	%r694, [%r693];
	// inline asm
	bfe.u32 %r309, %r319, %r119, 8;
	// inline asm
	shl.b32 	%r695, %r309, 2;
	add.s32 	%r696, %r16, %r695;
	ld.shared.u32 	%r697, [%r696];
	// inline asm
	bfe.u32 %r312, %r322, %r13, 8;
	// inline asm
	shl.b32 	%r698, %r312, 2;
	add.s32 	%r699, %r17, %r698;
	xor.b32  	%r700, %r691, %r653;
	xor.b32  	%r701, %r700, %r694;
	xor.b32  	%r702, %r701, %r697;
	ld.shared.u32 	%r703, [%r699];
	xor.b32  	%r373, %r702, %r703;
	// inline asm
	bfe.u32 %r315, %r316, %r113, 8;
	// inline asm
	shl.b32 	%r704, %r315, 2;
	add.s32 	%r705, %r14, %r704;
	ld.shared.u32 	%r706, [%r705];
	// inline asm
	bfe.u32 %r318, %r319, %r12, 8;
	// inline asm
	shl.b32 	%r707, %r318, 2;
	add.s32 	%r708, %r15, %r707;
	ld.shared.u32 	%r709, [%r708];
	// inline asm
	bfe.u32 %r321, %r322, %r119, 8;
	// inline asm
	shl.b32 	%r710, %r321, 2;
	add.s32 	%r711, %r16, %r710;
	ld.shared.u32 	%r712, [%r711];
	// inline asm
	bfe.u32 %r324, %r325, %r13, 8;
	// inline asm
	shl.b32 	%r713, %r324, 2;
	add.s32 	%r714, %r17, %r713;
	xor.b32  	%r715, %r706, %r654;
	xor.b32  	%r716, %r715, %r709;
	xor.b32  	%r717, %r716, %r712;
	ld.shared.u32 	%r718, [%r714];
	xor.b32  	%r364, %r717, %r718;
	add.s64 	%rd33, %rd39, %rd9;
	ld.global.v4.u32 	{%r719, %r720, %r721, %r722}, [%rd33];
	// inline asm
	bfe.u32 %r327, %r367, %r113, 8;
	// inline asm
	shl.b32 	%r727, %r327, 2;
	add.s32 	%r728, %r14, %r727;
	ld.shared.u32 	%r729, [%r728];
	// inline asm
	bfe.u32 %r330, %r370, %r12, 8;
	// inline asm
	shl.b32 	%r730, %r330, 2;
	add.s32 	%r731, %r15, %r730;
	ld.shared.u32 	%r732, [%r731];
	// inline asm
	bfe.u32 %r333, %r373, %r119, 8;
	// inline asm
	shl.b32 	%r733, %r333, 2;
	add.s32 	%r734, %r16, %r733;
	ld.shared.u32 	%r735, [%r734];
	// inline asm
	bfe.u32 %r336, %r364, %r13, 8;
	// inline asm
	shl.b32 	%r736, %r336, 2;
	add.s32 	%r737, %r17, %r736;
	xor.b32  	%r738, %r729, %r719;
	xor.b32  	%r739, %r738, %r732;
	xor.b32  	%r740, %r739, %r735;
	ld.shared.u32 	%r741, [%r737];
	xor.b32  	%r415, %r740, %r741;
	// inline asm
	bfe.u32 %r339, %r370, %r113, 8;
	// inline asm
	shl.b32 	%r742, %r339, 2;
	add.s32 	%r743, %r14, %r742;
	ld.shared.u32 	%r744, [%r743];
	// inline asm
	bfe.u32 %r342, %r373, %r12, 8;
	// inline asm
	shl.b32 	%r745, %r342, 2;
	add.s32 	%r746, %r15, %r745;
	ld.shared.u32 	%r747, [%r746];
	// inline asm
	bfe.u32 %r345, %r364, %r119, 8;
	// inline asm
	shl.b32 	%r748, %r345, 2;
	add.s32 	%r749, %r16, %r748;
	ld.shared.u32 	%r750, [%r749];
	// inline asm
	bfe.u32 %r348, %r367, %r13, 8;
	// inline asm
	shl.b32 	%r751, %r348, 2;
	add.s32 	%r752, %r17, %r751;
	xor.b32  	%r753, %r744, %r720;
	xor.b32  	%r754, %r753, %r747;
	xor.b32  	%r755, %r754, %r750;
	ld.shared.u32 	%r756, [%r752];
	xor.b32  	%r418, %r755, %r756;
	// inline asm
	bfe.u32 %r351, %r373, %r113, 8;
	// inline asm
	shl.b32 	%r757, %r351, 2;
	add.s32 	%r758, %r14, %r757;
	ld.shared.u32 	%r759, [%r758];
	// inline asm
	bfe.u32 %r354, %r364, %r12, 8;
	// inline asm
	shl.b32 	%r760, %r354, 2;
	add.s32 	%r761, %r15, %r760;
	ld.shared.u32 	%r762, [%r761];
	// inline asm
	bfe.u32 %r357, %r367, %r119, 8;
	// inline asm
	shl.b32 	%r763, %r357, 2;
	add.s32 	%r764, %r16, %r763;
	ld.shared.u32 	%r765, [%r764];
	// inline asm
	bfe.u32 %r360, %r370, %r13, 8;
	// inline asm
	shl.b32 	%r766, %r360, 2;
	add.s32 	%r767, %r17, %r766;
	xor.b32  	%r768, %r759, %r721;
	xor.b32  	%r769, %r768, %r762;
	xor.b32  	%r770, %r769, %r765;
	ld.shared.u32 	%r771, [%r767];
	xor.b32  	%r421, %r770, %r771;
	// inline asm
	bfe.u32 %r363, %r364, %r113, 8;
	// inline asm
	shl.b32 	%r772, %r363, 2;
	add.s32 	%r773, %r14, %r772;
	ld.shared.u32 	%r774, [%r773];
	// inline asm
	bfe.u32 %r366, %r367, %r12, 8;
	// inline asm
	shl.b32 	%r775, %r366, 2;
	add.s32 	%r776, %r15, %r775;
	ld.shared.u32 	%r777, [%r776];
	// inline asm
	bfe.u32 %r369, %r370, %r119, 8;
	// inline asm
	shl.b32 	%r778, %r369, 2;
	add.s32 	%r779, %r16, %r778;
	ld.shared.u32 	%r780, [%r779];
	// inline asm
	bfe.u32 %r372, %r373, %r13, 8;
	// inline asm
	shl.b32 	%r781, %r372, 2;
	add.s32 	%r782, %r17, %r781;
	xor.b32  	%r783, %r774, %r722;
	xor.b32  	%r784, %r783, %r777;
	xor.b32  	%r785, %r784, %r780;
	ld.shared.u32 	%r786, [%r782];
	xor.b32  	%r412, %r785, %r786;
	add.s64 	%rd34, %rd39, %rd8;
	ld.global.v4.u32 	{%r787, %r788, %r789, %r790}, [%rd34];
	// inline asm
	bfe.u32 %r375, %r415, %r113, 8;
	// inline asm
	shl.b32 	%r795, %r375, 2;
	add.s32 	%r796, %r14, %r795;
	ld.shared.u32 	%r797, [%r796];
	// inline asm
	bfe.u32 %r378, %r418, %r12, 8;
	// inline asm
	shl.b32 	%r798, %r378, 2;
	add.s32 	%r799, %r15, %r798;
	ld.shared.u32 	%r800, [%r799];
	// inline asm
	bfe.u32 %r381, %r421, %r119, 8;
	// inline asm
	shl.b32 	%r801, %r381, 2;
	add.s32 	%r802, %r16, %r801;
	ld.shared.u32 	%r803, [%r802];
	// inline asm
	bfe.u32 %r384, %r412, %r13, 8;
	// inline asm
	shl.b32 	%r804, %r384, 2;
	add.s32 	%r805, %r17, %r804;
	xor.b32  	%r806, %r797, %r787;
	xor.b32  	%r807, %r806, %r800;
	xor.b32  	%r808, %r807, %r803;
	ld.shared.u32 	%r809, [%r805];
	xor.b32  	%r463, %r808, %r809;
	// inline asm
	bfe.u32 %r387, %r418, %r113, 8;
	// inline asm
	shl.b32 	%r810, %r387, 2;
	add.s32 	%r811, %r14, %r810;
	ld.shared.u32 	%r812, [%r811];
	// inline asm
	bfe.u32 %r390, %r421, %r12, 8;
	// inline asm
	shl.b32 	%r813, %r390, 2;
	add.s32 	%r814, %r15, %r813;
	ld.shared.u32 	%r815, [%r814];
	// inline asm
	bfe.u32 %r393, %r412, %r119, 8;
	// inline asm
	shl.b32 	%r816, %r393, 2;
	add.s32 	%r817, %r16, %r816;
	ld.shared.u32 	%r818, [%r817];
	// inline asm
	bfe.u32 %r396, %r415, %r13, 8;
	// inline asm
	shl.b32 	%r819, %r396, 2;
	add.s32 	%r820, %r17, %r819;
	xor.b32  	%r821, %r812, %r788;
	xor.b32  	%r822, %r821, %r815;
	xor.b32  	%r823, %r822, %r818;
	ld.shared.u32 	%r824, [%r820];
	xor.b32  	%r466, %r823, %r824;
	// inline asm
	bfe.u32 %r399, %r421, %r113, 8;
	// inline asm
	shl.b32 	%r825, %r399, 2;
	add.s32 	%r826, %r14, %r825;
	ld.shared.u32 	%r827, [%r826];
	// inline asm
	bfe.u32 %r402, %r412, %r12, 8;
	// inline asm
	shl.b32 	%r828, %r402, 2;
	add.s32 	%r829, %r15, %r828;
	ld.shared.u32 	%r830, [%r829];
	// inline asm
	bfe.u32 %r405, %r415, %r119, 8;
	// inline asm
	shl.b32 	%r831, %r405, 2;
	add.s32 	%r832, %r16, %r831;
	ld.shared.u32 	%r833, [%r832];
	// inline asm
	bfe.u32 %r408, %r418, %r13, 8;
	// inline asm
	shl.b32 	%r834, %r408, 2;
	add.s32 	%r835, %r17, %r834;
	xor.b32  	%r836, %r827, %r789;
	xor.b32  	%r837, %r836, %r830;
	xor.b32  	%r838, %r837, %r833;
	ld.shared.u32 	%r839, [%r835];
	xor.b32  	%r469, %r838, %r839;
	// inline asm
	bfe.u32 %r411, %r412, %r113, 8;
	// inline asm
	shl.b32 	%r840, %r411, 2;
	add.s32 	%r841, %r14, %r840;
	ld.shared.u32 	%r842, [%r841];
	// inline asm
	bfe.u32 %r414, %r415, %r12, 8;
	// inline asm
	shl.b32 	%r843, %r414, 2;
	add.s32 	%r844, %r15, %r843;
	ld.shared.u32 	%r845, [%r844];
	// inline asm
	bfe.u32 %r417, %r418, %r119, 8;
	// inline asm
	shl.b32 	%r846, %r417, 2;
	add.s32 	%r847, %r16, %r846;
	ld.shared.u32 	%r848, [%r847];
	// inline asm
	bfe.u32 %r420, %r421, %r13, 8;
	// inline asm
	shl.b32 	%r849, %r420, 2;
	add.s32 	%r850, %r17, %r849;
	xor.b32  	%r851, %r842, %r790;
	xor.b32  	%r852, %r851, %r845;
	xor.b32  	%r853, %r852, %r848;
	ld.shared.u32 	%r854, [%r850];
	xor.b32  	%r460, %r853, %r854;
	add.s64 	%rd35, %rd39, %rd7;
	ld.global.v4.u32 	{%r855, %r856, %r857, %r858}, [%rd35];
	// inline asm
	bfe.u32 %r423, %r463, %r113, 8;
	// inline asm
	shl.b32 	%r863, %r423, 2;
	add.s32 	%r864, %r14, %r863;
	ld.shared.u32 	%r865, [%r864];
	// inline asm
	bfe.u32 %r426, %r466, %r12, 8;
	// inline asm
	shl.b32 	%r866, %r426, 2;
	add.s32 	%r867, %r15, %r866;
	ld.shared.u32 	%r868, [%r867];
	// inline asm
	bfe.u32 %r429, %r469, %r119, 8;
	// inline asm
	shl.b32 	%r869, %r429, 2;
	add.s32 	%r870, %r16, %r869;
	ld.shared.u32 	%r871, [%r870];
	// inline asm
	bfe.u32 %r432, %r460, %r13, 8;
	// inline asm
	shl.b32 	%r872, %r432, 2;
	add.s32 	%r873, %r17, %r872;
	xor.b32  	%r874, %r865, %r855;
	xor.b32  	%r875, %r874, %r868;
	xor.b32  	%r876, %r875, %r871;
	ld.shared.u32 	%r877, [%r873];
	xor.b32  	%r511, %r876, %r877;
	// inline asm
	bfe.u32 %r435, %r466, %r113, 8;
	// inline asm
	shl.b32 	%r878, %r435, 2;
	add.s32 	%r879, %r14, %r878;
	ld.shared.u32 	%r880, [%r879];
	// inline asm
	bfe.u32 %r438, %r469, %r12, 8;
	// inline asm
	shl.b32 	%r881, %r438, 2;
	add.s32 	%r882, %r15, %r881;
	ld.shared.u32 	%r883, [%r882];
	// inline asm
	bfe.u32 %r441, %r460, %r119, 8;
	// inline asm
	shl.b32 	%r884, %r441, 2;
	add.s32 	%r885, %r16, %r884;
	ld.shared.u32 	%r886, [%r885];
	// inline asm
	bfe.u32 %r444, %r463, %r13, 8;
	// inline asm
	shl.b32 	%r887, %r444, 2;
	add.s32 	%r888, %r17, %r887;
	xor.b32  	%r889, %r880, %r856;
	xor.b32  	%r890, %r889, %r883;
	xor.b32  	%r891, %r890, %r886;
	ld.shared.u32 	%r892, [%r888];
	xor.b32  	%r514, %r891, %r892;
	// inline asm
	bfe.u32 %r447, %r469, %r113, 8;
	// inline asm
	shl.b32 	%r893, %r447, 2;
	add.s32 	%r894, %r14, %r893;
	ld.shared.u32 	%r895, [%r894];
	// inline asm
	bfe.u32 %r450, %r460, %r12, 8;
	// inline asm
	shl.b32 	%r896, %r450, 2;
	add.s32 	%r897, %r15, %r896;
	ld.shared.u32 	%r898, [%r897];
	// inline asm
	bfe.u32 %r453, %r463, %r119, 8;
	// inline asm
	shl.b32 	%r899, %r453, 2;
	add.s32 	%r900, %r16, %r899;
	ld.shared.u32 	%r901, [%r900];
	// inline asm
	bfe.u32 %r456, %r466, %r13, 8;
	// inline asm
	shl.b32 	%r902, %r456, 2;
	add.s32 	%r903, %r17, %r902;
	xor.b32  	%r904, %r895, %r857;
	xor.b32  	%r905, %r904, %r898;
	xor.b32  	%r906, %r905, %r901;
	ld.shared.u32 	%r907, [%r903];
	xor.b32  	%r517, %r906, %r907;
	// inline asm
	bfe.u32 %r459, %r460, %r113, 8;
	// inline asm
	shl.b32 	%r908, %r459, 2;
	add.s32 	%r909, %r14, %r908;
	ld.shared.u32 	%r910, [%r909];
	// inline asm
	bfe.u32 %r462, %r463, %r12, 8;
	// inline asm
	shl.b32 	%r911, %r462, 2;
	add.s32 	%r912, %r15, %r911;
	ld.shared.u32 	%r913, [%r912];
	// inline asm
	bfe.u32 %r465, %r466, %r119, 8;
	// inline asm
	shl.b32 	%r914, %r465, 2;
	add.s32 	%r915, %r16, %r914;
	ld.shared.u32 	%r916, [%r915];
	// inline asm
	bfe.u32 %r468, %r469, %r13, 8;
	// inline asm
	shl.b32 	%r917, %r468, 2;
	add.s32 	%r918, %r17, %r917;
	xor.b32  	%r919, %r910, %r858;
	xor.b32  	%r920, %r919, %r913;
	xor.b32  	%r921, %r920, %r916;
	ld.shared.u32 	%r922, [%r918];
	xor.b32  	%r508, %r921, %r922;
	add.s64 	%rd36, %rd39, %rd6;
	ld.global.v4.u32 	{%r923, %r924, %r925, %r926}, [%rd36];
	// inline asm
	bfe.u32 %r471, %r511, %r113, 8;
	// inline asm
	shl.b32 	%r931, %r471, 2;
	add.s32 	%r932, %r14, %r931;
	ld.shared.u32 	%r933, [%r932];
	// inline asm
	bfe.u32 %r474, %r514, %r12, 8;
	// inline asm
	shl.b32 	%r934, %r474, 2;
	add.s32 	%r935, %r15, %r934;
	ld.shared.u32 	%r936, [%r935];
	// inline asm
	bfe.u32 %r477, %r517, %r119, 8;
	// inline asm
	shl.b32 	%r937, %r477, 2;
	add.s32 	%r938, %r16, %r937;
	ld.shared.u32 	%r939, [%r938];
	// inline asm
	bfe.u32 %r480, %r508, %r13, 8;
	// inline asm
	shl.b32 	%r940, %r480, 2;
	add.s32 	%r941, %r17, %r940;
	xor.b32  	%r942, %r933, %r923;
	xor.b32  	%r943, %r942, %r936;
	xor.b32  	%r944, %r943, %r939;
	ld.shared.u32 	%r945, [%r941];
	xor.b32  	%r559, %r944, %r945;
	// inline asm
	bfe.u32 %r483, %r514, %r113, 8;
	// inline asm
	shl.b32 	%r946, %r483, 2;
	add.s32 	%r947, %r14, %r946;
	ld.shared.u32 	%r948, [%r947];
	// inline asm
	bfe.u32 %r486, %r517, %r12, 8;
	// inline asm
	shl.b32 	%r949, %r486, 2;
	add.s32 	%r950, %r15, %r949;
	ld.shared.u32 	%r951, [%r950];
	// inline asm
	bfe.u32 %r489, %r508, %r119, 8;
	// inline asm
	shl.b32 	%r952, %r489, 2;
	add.s32 	%r953, %r16, %r952;
	ld.shared.u32 	%r954, [%r953];
	// inline asm
	bfe.u32 %r492, %r511, %r13, 8;
	// inline asm
	shl.b32 	%r955, %r492, 2;
	add.s32 	%r956, %r17, %r955;
	xor.b32  	%r957, %r948, %r924;
	xor.b32  	%r958, %r957, %r951;
	xor.b32  	%r959, %r958, %r954;
	ld.shared.u32 	%r960, [%r956];
	xor.b32  	%r562, %r959, %r960;
	// inline asm
	bfe.u32 %r495, %r517, %r113, 8;
	// inline asm
	shl.b32 	%r961, %r495, 2;
	add.s32 	%r962, %r14, %r961;
	ld.shared.u32 	%r963, [%r962];
	// inline asm
	bfe.u32 %r498, %r508, %r12, 8;
	// inline asm
	shl.b32 	%r964, %r498, 2;
	add.s32 	%r965, %r15, %r964;
	ld.shared.u32 	%r966, [%r965];
	// inline asm
	bfe.u32 %r501, %r511, %r119, 8;
	// inline asm
	shl.b32 	%r967, %r501, 2;
	add.s32 	%r968, %r16, %r967;
	ld.shared.u32 	%r969, [%r968];
	// inline asm
	bfe.u32 %r504, %r514, %r13, 8;
	// inline asm
	shl.b32 	%r970, %r504, 2;
	add.s32 	%r971, %r17, %r970;
	xor.b32  	%r972, %r963, %r925;
	xor.b32  	%r973, %r972, %r966;
	xor.b32  	%r974, %r973, %r969;
	ld.shared.u32 	%r975, [%r971];
	xor.b32  	%r565, %r974, %r975;
	// inline asm
	bfe.u32 %r507, %r508, %r113, 8;
	// inline asm
	shl.b32 	%r976, %r507, 2;
	add.s32 	%r977, %r14, %r976;
	ld.shared.u32 	%r978, [%r977];
	// inline asm
	bfe.u32 %r510, %r511, %r12, 8;
	// inline asm
	shl.b32 	%r979, %r510, 2;
	add.s32 	%r980, %r15, %r979;
	ld.shared.u32 	%r981, [%r980];
	// inline asm
	bfe.u32 %r513, %r514, %r119, 8;
	// inline asm
	shl.b32 	%r982, %r513, 2;
	add.s32 	%r983, %r16, %r982;
	ld.shared.u32 	%r984, [%r983];
	// inline asm
	bfe.u32 %r516, %r517, %r13, 8;
	// inline asm
	shl.b32 	%r985, %r516, 2;
	add.s32 	%r986, %r17, %r985;
	xor.b32  	%r987, %r978, %r926;
	xor.b32  	%r988, %r987, %r981;
	xor.b32  	%r989, %r988, %r984;
	ld.shared.u32 	%r990, [%r986];
	xor.b32  	%r556, %r989, %r990;
	add.s64 	%rd37, %rd39, %rd5;
	ld.global.v4.u32 	{%r991, %r992, %r993, %r994}, [%rd37];
	// inline asm
	bfe.u32 %r519, %r559, %r113, 8;
	// inline asm
	shl.b32 	%r999, %r519, 2;
	add.s32 	%r1000, %r14, %r999;
	ld.shared.u32 	%r1001, [%r1000];
	// inline asm
	bfe.u32 %r522, %r562, %r12, 8;
	// inline asm
	shl.b32 	%r1002, %r522, 2;
	add.s32 	%r1003, %r15, %r1002;
	ld.shared.u32 	%r1004, [%r1003];
	// inline asm
	bfe.u32 %r525, %r565, %r119, 8;
	// inline asm
	shl.b32 	%r1005, %r525, 2;
	add.s32 	%r1006, %r16, %r1005;
	ld.shared.u32 	%r1007, [%r1006];
	// inline asm
	bfe.u32 %r528, %r556, %r13, 8;
	// inline asm
	shl.b32 	%r1008, %r528, 2;
	add.s32 	%r1009, %r17, %r1008;
	xor.b32  	%r1010, %r1001, %r991;
	xor.b32  	%r1011, %r1010, %r1004;
	xor.b32  	%r1012, %r1011, %r1007;
	ld.shared.u32 	%r1013, [%r1009];
	xor.b32  	%r607, %r1012, %r1013;
	// inline asm
	bfe.u32 %r531, %r562, %r113, 8;
	// inline asm
	shl.b32 	%r1014, %r531, 2;
	add.s32 	%r1015, %r14, %r1014;
	ld.shared.u32 	%r1016, [%r1015];
	// inline asm
	bfe.u32 %r534, %r565, %r12, 8;
	// inline asm
	shl.b32 	%r1017, %r534, 2;
	add.s32 	%r1018, %r15, %r1017;
	ld.shared.u32 	%r1019, [%r1018];
	// inline asm
	bfe.u32 %r537, %r556, %r119, 8;
	// inline asm
	shl.b32 	%r1020, %r537, 2;
	add.s32 	%r1021, %r16, %r1020;
	ld.shared.u32 	%r1022, [%r1021];
	// inline asm
	bfe.u32 %r540, %r559, %r13, 8;
	// inline asm
	shl.b32 	%r1023, %r540, 2;
	add.s32 	%r1024, %r17, %r1023;
	xor.b32  	%r1025, %r1016, %r992;
	xor.b32  	%r1026, %r1025, %r1019;
	xor.b32  	%r1027, %r1026, %r1022;
	ld.shared.u32 	%r1028, [%r1024];
	xor.b32  	%r610, %r1027, %r1028;
	// inline asm
	bfe.u32 %r543, %r565, %r113, 8;
	// inline asm
	shl.b32 	%r1029, %r543, 2;
	add.s32 	%r1030, %r14, %r1029;
	ld.shared.u32 	%r1031, [%r1030];
	// inline asm
	bfe.u32 %r546, %r556, %r12, 8;
	// inline asm
	shl.b32 	%r1032, %r546, 2;
	add.s32 	%r1033, %r15, %r1032;
	ld.shared.u32 	%r1034, [%r1033];
	// inline asm
	bfe.u32 %r549, %r559, %r119, 8;
	// inline asm
	shl.b32 	%r1035, %r549, 2;
	add.s32 	%r1036, %r16, %r1035;
	ld.shared.u32 	%r1037, [%r1036];
	// inline asm
	bfe.u32 %r552, %r562, %r13, 8;
	// inline asm
	shl.b32 	%r1038, %r552, 2;
	add.s32 	%r1039, %r17, %r1038;
	xor.b32  	%r1040, %r1031, %r993;
	xor.b32  	%r1041, %r1040, %r1034;
	xor.b32  	%r1042, %r1041, %r1037;
	ld.shared.u32 	%r1043, [%r1039];
	xor.b32  	%r613, %r1042, %r1043;
	// inline asm
	bfe.u32 %r555, %r556, %r113, 8;
	// inline asm
	shl.b32 	%r1044, %r555, 2;
	add.s32 	%r1045, %r14, %r1044;
	ld.shared.u32 	%r1046, [%r1045];
	// inline asm
	bfe.u32 %r558, %r559, %r12, 8;
	// inline asm
	shl.b32 	%r1047, %r558, 2;
	add.s32 	%r1048, %r15, %r1047;
	ld.shared.u32 	%r1049, [%r1048];
	// inline asm
	bfe.u32 %r561, %r562, %r119, 8;
	// inline asm
	shl.b32 	%r1050, %r561, 2;
	add.s32 	%r1051, %r16, %r1050;
	ld.shared.u32 	%r1052, [%r1051];
	// inline asm
	bfe.u32 %r564, %r565, %r13, 8;
	// inline asm
	shl.b32 	%r1053, %r564, 2;
	add.s32 	%r1054, %r17, %r1053;
	xor.b32  	%r1055, %r1046, %r994;
	xor.b32  	%r1056, %r1055, %r1049;
	xor.b32  	%r1057, %r1056, %r1052;
	ld.shared.u32 	%r1058, [%r1054];
	xor.b32  	%r604, %r1057, %r1058;
	add.s64 	%rd38, %rd39, %rd4;
	ld.global.v4.u32 	{%r1059, %r1060, %r1061, %r1062}, [%rd38];
	// inline asm
	bfe.u32 %r567, %r607, %r113, 8;
	// inline asm
	shl.b32 	%r1067, %r567, 2;
	add.s32 	%r1068, %r14, %r1067;
	ld.shared.u32 	%r1069, [%r1068];
	// inline asm
	bfe.u32 %r570, %r610, %r12, 8;
	// inline asm
	shl.b32 	%r1070, %r570, 2;
	add.s32 	%r1071, %r15, %r1070;
	ld.shared.u32 	%r1072, [%r1071];
	// inline asm
	bfe.u32 %r573, %r613, %r119, 8;
	// inline asm
	shl.b32 	%r1073, %r573, 2;
	add.s32 	%r1074, %r16, %r1073;
	ld.shared.u32 	%r1075, [%r1074];
	// inline asm
	bfe.u32 %r576, %r604, %r13, 8;
	// inline asm
	shl.b32 	%r1076, %r576, 2;
	add.s32 	%r1077, %r17, %r1076;
	xor.b32  	%r1078, %r1069, %r1059;
	xor.b32  	%r1079, %r1078, %r1072;
	xor.b32  	%r1080, %r1079, %r1075;
	ld.shared.u32 	%r1081, [%r1077];
	xor.b32  	%r1129, %r1080, %r1081;
	// inline asm
	bfe.u32 %r579, %r610, %r113, 8;
	// inline asm
	shl.b32 	%r1082, %r579, 2;
	add.s32 	%r1083, %r14, %r1082;
	ld.shared.u32 	%r1084, [%r1083];
	// inline asm
	bfe.u32 %r582, %r613, %r12, 8;
	// inline asm
	shl.b32 	%r1085, %r582, 2;
	add.s32 	%r1086, %r15, %r1085;
	ld.shared.u32 	%r1087, [%r1086];
	// inline asm
	bfe.u32 %r585, %r604, %r119, 8;
	// inline asm
	shl.b32 	%r1088, %r585, 2;
	add.s32 	%r1089, %r16, %r1088;
	ld.shared.u32 	%r1090, [%r1089];
	// inline asm
	bfe.u32 %r588, %r607, %r13, 8;
	// inline asm
	shl.b32 	%r1091, %r588, 2;
	add.s32 	%r1092, %r17, %r1091;
	xor.b32  	%r1093, %r1084, %r1060;
	xor.b32  	%r1094, %r1093, %r1087;
	xor.b32  	%r1095, %r1094, %r1090;
	ld.shared.u32 	%r1096, [%r1092];
	xor.b32  	%r1130, %r1095, %r1096;
	// inline asm
	bfe.u32 %r591, %r613, %r113, 8;
	// inline asm
	shl.b32 	%r1097, %r591, 2;
	add.s32 	%r1098, %r14, %r1097;
	ld.shared.u32 	%r1099, [%r1098];
	// inline asm
	bfe.u32 %r594, %r604, %r12, 8;
	// inline asm
	shl.b32 	%r1100, %r594, 2;
	add.s32 	%r1101, %r15, %r1100;
	ld.shared.u32 	%r1102, [%r1101];
	// inline asm
	bfe.u32 %r597, %r607, %r119, 8;
	// inline asm
	shl.b32 	%r1103, %r597, 2;
	add.s32 	%r1104, %r16, %r1103;
	ld.shared.u32 	%r1105, [%r1104];
	// inline asm
	bfe.u32 %r600, %r610, %r13, 8;
	// inline asm
	shl.b32 	%r1106, %r600, 2;
	add.s32 	%r1107, %r17, %r1106;
	xor.b32  	%r1108, %r1099, %r1061;
	xor.b32  	%r1109, %r1108, %r1102;
	xor.b32  	%r1110, %r1109, %r1105;
	ld.shared.u32 	%r1111, [%r1107];
	xor.b32  	%r1131, %r1110, %r1111;
	// inline asm
	bfe.u32 %r603, %r604, %r113, 8;
	// inline asm
	shl.b32 	%r1112, %r603, 2;
	add.s32 	%r1113, %r14, %r1112;
	ld.shared.u32 	%r1114, [%r1113];
	// inline asm
	bfe.u32 %r606, %r607, %r12, 8;
	// inline asm
	shl.b32 	%r1115, %r606, 2;
	add.s32 	%r1116, %r15, %r1115;
	ld.shared.u32 	%r1117, [%r1116];
	// inline asm
	bfe.u32 %r609, %r610, %r119, 8;
	// inline asm
	shl.b32 	%r1118, %r609, 2;
	add.s32 	%r1119, %r16, %r1118;
	ld.shared.u32 	%r1120, [%r1119];
	// inline asm
	bfe.u32 %r612, %r613, %r13, 8;
	// inline asm
	shl.b32 	%r1121, %r612, 2;
	add.s32 	%r1122, %r17, %r1121;
	xor.b32  	%r1123, %r1114, %r1062;
	xor.b32  	%r1124, %r1123, %r1117;
	xor.b32  	%r1125, %r1124, %r1120;
	ld.shared.u32 	%r1126, [%r1122];
	xor.b32  	%r1132, %r1125, %r1126;
	mov.u64 	%rd39, %rd12;

BB5_4:
	mov.u32 	%r113, 0;
	// inline asm
	bfe.u32 %r75, %r1129, %r113, 8;
	// inline asm
	shl.b32 	%r123, %r75, 2;
	add.s32 	%r24, %r14, %r123;
	// inline asm
	bfe.u32 %r78, %r1130, %r12, 8;
	// inline asm
	shl.b32 	%r124, %r78, 2;
	add.s32 	%r25, %r15, %r124;
	mov.u32 	%r119, 16;
	// inline asm
	bfe.u32 %r81, %r1131, %r119, 8;
	// inline asm
	shl.b32 	%r125, %r81, 2;
	add.s32 	%r26, %r16, %r125;
	// inline asm
	bfe.u32 %r84, %r1132, %r13, 8;
	// inline asm
	shl.b32 	%r126, %r84, 2;
	add.s32 	%r27, %r17, %r126;
	// inline asm
	bfe.u32 %r87, %r1130, %r113, 8;
	// inline asm
	shl.b32 	%r127, %r87, 2;
	add.s32 	%r28, %r14, %r127;
	// inline asm
	bfe.u32 %r90, %r1131, %r12, 8;
	// inline asm
	shl.b32 	%r128, %r90, 2;
	add.s32 	%r29, %r15, %r128;
	// inline asm
	bfe.u32 %r93, %r1132, %r119, 8;
	// inline asm
	shl.b32 	%r129, %r93, 2;
	add.s32 	%r30, %r16, %r129;
	// inline asm
	bfe.u32 %r96, %r1129, %r13, 8;
	// inline asm
	shl.b32 	%r130, %r96, 2;
	add.s32 	%r31, %r17, %r130;
	// inline asm
	bfe.u32 %r99, %r1131, %r113, 8;
	// inline asm
	shl.b32 	%r131, %r99, 2;
	add.s32 	%r32, %r14, %r131;
	// inline asm
	bfe.u32 %r102, %r1132, %r12, 8;
	// inline asm
	shl.b32 	%r132, %r102, 2;
	add.s32 	%r33, %r15, %r132;
	// inline asm
	bfe.u32 %r105, %r1129, %r119, 8;
	// inline asm
	shl.b32 	%r133, %r105, 2;
	add.s32 	%r34, %r16, %r133;
	// inline asm
	bfe.u32 %r108, %r1130, %r13, 8;
	// inline asm
	shl.b32 	%r134, %r108, 2;
	add.s32 	%r35, %r17, %r134;
	// inline asm
	bfe.u32 %r111, %r1132, %r113, 8;
	// inline asm
	shl.b32 	%r135, %r111, 2;
	add.s32 	%r36, %r14, %r135;
	// inline asm
	bfe.u32 %r114, %r1129, %r12, 8;
	// inline asm
	shl.b32 	%r136, %r114, 2;
	add.s32 	%r37, %r15, %r136;
	// inline asm
	bfe.u32 %r117, %r1130, %r119, 8;
	// inline asm
	shl.b32 	%r137, %r117, 2;
	add.s32 	%r38, %r16, %r137;
	// inline asm
	bfe.u32 %r120, %r1131, %r13, 8;
	// inline asm
	shl.b32 	%r138, %r120, 2;
	add.s32 	%r39, %r17, %r138;
	add.s32 	%r1128, %r1128, 32;
	setp.lt.u32	%p5, %r1128, 131072;
	@%p5 bra 	BB5_7;

	ld.shared.u32 	%r187, [%r24];
	ld.shared.u32 	%r188, [%r25];
	xor.b32  	%r189, %r187, %r188;
	ld.shared.u32 	%r190, [%r26];
	xor.b32  	%r191, %r189, %r190;
	ld.shared.u32 	%r192, [%r27];
	xor.b32  	%r193, %r191, %r192;
	xor.b32  	%r140, %r193, 298578503;
	ld.shared.u32 	%r194, [%r28];
	ld.shared.u32 	%r195, [%r29];
	xor.b32  	%r196, %r194, %r195;
	ld.shared.u32 	%r197, [%r30];
	xor.b32  	%r198, %r196, %r197;
	ld.shared.u32 	%r199, [%r31];
	xor.b32  	%r200, %r198, %r199;
	xor.b32  	%r143, %r200, 710578844;
	ld.shared.u32 	%r201, [%r32];
	ld.shared.u32 	%r202, [%r33];
	xor.b32  	%r203, %r201, %r202;
	ld.shared.u32 	%r204, [%r34];
	xor.b32  	%r205, %r203, %r204;
	ld.shared.u32 	%r206, [%r35];
	xor.b32  	%r207, %r205, %r206;
	xor.b32  	%r146, %r207, -456828611;
	ld.shared.u32 	%r208, [%r36];
	ld.shared.u32 	%r209, [%r37];
	xor.b32  	%r210, %r208, %r209;
	ld.shared.u32 	%r211, [%r38];
	xor.b32  	%r212, %r210, %r211;
	ld.shared.u32 	%r213, [%r39];
	xor.b32  	%r214, %r212, %r213;
	xor.b32  	%r149, %r214, -2087382397;
	// inline asm
	bfe.u32 %r139, %r140, %r113, 8;
	// inline asm
	shl.b32 	%r215, %r139, 2;
	add.s32 	%r216, %r14, %r215;
	ld.shared.u32 	%r217, [%r216];
	// inline asm
	bfe.u32 %r142, %r143, %r12, 8;
	// inline asm
	shl.b32 	%r218, %r142, 2;
	add.s32 	%r219, %r15, %r218;
	ld.shared.u32 	%r220, [%r219];
	// inline asm
	bfe.u32 %r145, %r146, %r119, 8;
	// inline asm
	shl.b32 	%r221, %r145, 2;
	add.s32 	%r222, %r16, %r221;
	ld.shared.u32 	%r223, [%r222];
	// inline asm
	bfe.u32 %r148, %r149, %r13, 8;
	// inline asm
	shl.b32 	%r224, %r148, 2;
	add.s32 	%r225, %r17, %r224;
	xor.b32  	%r226, %r217, %r220;
	xor.b32  	%r227, %r226, %r223;
	ld.shared.u32 	%r228, [%r225];
	xor.b32  	%r229, %r227, %r228;
	// inline asm
	bfe.u32 %r151, %r143, %r113, 8;
	// inline asm
	shl.b32 	%r230, %r151, 2;
	add.s32 	%r231, %r14, %r230;
	ld.shared.u32 	%r232, [%r231];
	// inline asm
	bfe.u32 %r154, %r146, %r12, 8;
	// inline asm
	shl.b32 	%r233, %r154, 2;
	add.s32 	%r234, %r15, %r233;
	ld.shared.u32 	%r235, [%r234];
	// inline asm
	bfe.u32 %r157, %r149, %r119, 8;
	// inline asm
	shl.b32 	%r236, %r157, 2;
	add.s32 	%r237, %r16, %r236;
	ld.shared.u32 	%r238, [%r237];
	// inline asm
	bfe.u32 %r160, %r140, %r13, 8;
	// inline asm
	shl.b32 	%r239, %r160, 2;
	add.s32 	%r240, %r17, %r239;
	xor.b32  	%r241, %r232, %r235;
	xor.b32  	%r242, %r241, %r238;
	ld.shared.u32 	%r243, [%r240];
	xor.b32  	%r244, %r242, %r243;
	// inline asm
	bfe.u32 %r163, %r146, %r113, 8;
	// inline asm
	shl.b32 	%r245, %r163, 2;
	add.s32 	%r246, %r14, %r245;
	ld.shared.u32 	%r247, [%r246];
	// inline asm
	bfe.u32 %r166, %r149, %r12, 8;
	// inline asm
	shl.b32 	%r248, %r166, 2;
	add.s32 	%r249, %r15, %r248;
	ld.shared.u32 	%r250, [%r249];
	// inline asm
	bfe.u32 %r169, %r140, %r119, 8;
	// inline asm
	shl.b32 	%r251, %r169, 2;
	add.s32 	%r252, %r16, %r251;
	ld.shared.u32 	%r253, [%r252];
	// inline asm
	bfe.u32 %r172, %r143, %r13, 8;
	// inline asm
	shl.b32 	%r254, %r172, 2;
	add.s32 	%r255, %r17, %r254;
	xor.b32  	%r256, %r247, %r250;
	xor.b32  	%r257, %r256, %r253;
	ld.shared.u32 	%r258, [%r255];
	xor.b32  	%r259, %r257, %r258;
	// inline asm
	bfe.u32 %r175, %r149, %r113, 8;
	// inline asm
	shl.b32 	%r260, %r175, 2;
	add.s32 	%r261, %r14, %r260;
	ld.shared.u32 	%r262, [%r261];
	// inline asm
	bfe.u32 %r178, %r140, %r12, 8;
	// inline asm
	shl.b32 	%r263, %r178, 2;
	add.s32 	%r264, %r15, %r263;
	ld.shared.u32 	%r265, [%r264];
	// inline asm
	bfe.u32 %r181, %r143, %r119, 8;
	// inline asm
	shl.b32 	%r266, %r181, 2;
	add.s32 	%r267, %r16, %r266;
	ld.shared.u32 	%r268, [%r267];
	// inline asm
	bfe.u32 %r184, %r146, %r13, 8;
	// inline asm
	shl.b32 	%r269, %r184, 2;
	add.s32 	%r270, %r17, %r269;
	xor.b32  	%r271, %r262, %r265;
	xor.b32  	%r272, %r271, %r268;
	ld.shared.u32 	%r273, [%r270];
	xor.b32  	%r274, %r272, %r273;
	mul.wide.u32 	%rd27, %r18, 128;
	add.s64 	%rd28, %rd1, %rd27;
	cvta.to.global.u64 	%rd29, %rd14;
	shl.b64 	%rd30, %rd28, 4;
	add.s64 	%rd31, %rd29, %rd30;
	xor.b32  	%r275, %r274, -14591054;
	xor.b32  	%r276, %r259, -1413733085;
	xor.b32  	%r277, %r244, 1199304459;
	xor.b32  	%r278, %r229, -830378859;
	st.global.v4.u32 	[%rd31+192], {%r278, %r277, %r276, %r275};

BB5_6:
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2745>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj2048ELj32EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 2048;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 32;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249095;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301160;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	add.s64 	%rd2698, %rd2651, %rd1381;
	add.s64 	%rd2699, %rd2698, %rd2670;
	xor.b64  	%rd2700, %rd2699, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2700;
	mov.b64	%rd2701, {%r3436, %r3435};
	add.s64 	%rd2702, %rd2701, %rd2682;
	xor.b64  	%rd2703, %rd2702, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2703;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2704, {%r3440, %r3439};
	add.s64 	%rd2705, %rd2699, %rd1383;
	add.s64 	%rd2706, %rd2705, %rd2704;
	xor.b64  	%rd2707, %rd2706, %rd2701;
	mov.b64	{%r3441, %r3442}, %rd2707;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2708, {%r3444, %r3443};
	add.s64 	%rd2709, %rd2708, %rd2702;
	xor.b64  	%rd2710, %rd2709, %rd2704;
	mov.b64	{%r1518, %r1519}, %rd2710;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	add.s64 	%rd2711, %rd2665, %rd1392;
	add.s64 	%rd2712, %rd2711, %rd2684;
	xor.b64  	%rd2713, %rd2712, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2713;
	mov.b64	%rd2714, {%r3446, %r3445};
	add.s64 	%rd2715, %rd2714, %rd2640;
	xor.b64  	%rd2716, %rd2715, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2716;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2717, {%r3450, %r3449};
	add.s64 	%rd2718, %rd2712, %rd1388;
	add.s64 	%rd2719, %rd2718, %rd2717;
	xor.b64  	%rd2720, %rd2719, %rd2714;
	mov.b64	{%r3451, %r3452}, %rd2720;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2721, {%r3454, %r3453};
	add.s64 	%rd2722, %rd2721, %rd2715;
	xor.b64  	%rd2723, %rd2722, %rd2717;
	mov.b64	{%r1526, %r1527}, %rd2723;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	add.s64 	%rd2724, %rd2642, %rd1386;
	add.s64 	%rd2725, %rd2724, %rd2679;
	xor.b64  	%rd2726, %rd2725, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2726;
	mov.b64	%rd2727, {%r3456, %r3455};
	add.s64 	%rd2728, %rd2727, %rd2654;
	xor.b64  	%rd2729, %rd2728, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2729;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2730, {%r3460, %r3459};
	add.s64 	%rd2731, %rd2725, %rd1384;
	add.s64 	%rd2732, %rd2731, %rd2730;
	xor.b64  	%rd2733, %rd2732, %rd2727;
	mov.b64	{%r3461, %r3462}, %rd2733;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2734, {%r3464, %r3463};
	add.s64 	%rd2735, %rd2734, %rd2728;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r1534, %r1535}, %rd2736;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	xor.b64  	%rd2737, %rd2693, %rd1366;
	xor.b64  	%rd2738, %rd2737, %rd2722;
	st.global.u64 	[%rd8], %rd2738;
	xor.b64  	%rd2739, %rd2706, %rd1368;
	xor.b64  	%rd2740, %rd2739, %rd2735;
	st.global.u64 	[%rd8+8], %rd2740;
	xor.b64  	%rd2741, %rd2696, %rd1370;
	xor.b64  	%rd2742, %rd2741, %rd2719;
	st.global.u64 	[%rd8+16], %rd2742;
	xor.b64  	%rd2743, %rd2709, %rd1372;
	xor.b64  	%rd2744, %rd2743, %rd2732;
	st.global.u64 	[%rd8+24], %rd2744;
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2757>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj2048ELj64EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 2048;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 64;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249063;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301192;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	mov.b64	%rd2698, {%r1505, %r1509};
	add.s64 	%rd2699, %rd2651, %rd1381;
	add.s64 	%rd2700, %rd2699, %rd2670;
	xor.b64  	%rd2701, %rd2700, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2701;
	mov.b64	%rd2702, {%r3436, %r3435};
	add.s64 	%rd2703, %rd2702, %rd2682;
	xor.b64  	%rd2704, %rd2703, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2704;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2705, {%r3440, %r3439};
	add.s64 	%rd2706, %rd2700, %rd1383;
	add.s64 	%rd2707, %rd2706, %rd2705;
	xor.b64  	%rd2708, %rd2707, %rd2702;
	mov.b64	{%r3441, %r3442}, %rd2708;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2709, {%r3444, %r3443};
	add.s64 	%rd2710, %rd2709, %rd2703;
	xor.b64  	%rd2711, %rd2710, %rd2705;
	mov.b64	{%r1518, %r1519}, %rd2711;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	mov.b64	%rd2712, {%r1513, %r1517};
	add.s64 	%rd2713, %rd2665, %rd1392;
	add.s64 	%rd2714, %rd2713, %rd2684;
	xor.b64  	%rd2715, %rd2714, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2715;
	mov.b64	%rd2716, {%r3446, %r3445};
	add.s64 	%rd2717, %rd2716, %rd2640;
	xor.b64  	%rd2718, %rd2717, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2718;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2719, {%r3450, %r3449};
	add.s64 	%rd2720, %rd2714, %rd1388;
	add.s64 	%rd2721, %rd2720, %rd2719;
	xor.b64  	%rd2722, %rd2721, %rd2716;
	mov.b64	{%r3451, %r3452}, %rd2722;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2723, {%r3454, %r3453};
	add.s64 	%rd2724, %rd2723, %rd2717;
	xor.b64  	%rd2725, %rd2724, %rd2719;
	mov.b64	{%r1526, %r1527}, %rd2725;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	mov.b64	%rd2726, {%r1521, %r1525};
	add.s64 	%rd2727, %rd2642, %rd1386;
	add.s64 	%rd2728, %rd2727, %rd2679;
	xor.b64  	%rd2729, %rd2728, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2729;
	mov.b64	%rd2730, {%r3456, %r3455};
	add.s64 	%rd2731, %rd2730, %rd2654;
	xor.b64  	%rd2732, %rd2731, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2732;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2733, {%r3460, %r3459};
	add.s64 	%rd2734, %rd2728, %rd1384;
	add.s64 	%rd2735, %rd2734, %rd2733;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r3461, %r3462}, %rd2736;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2737, {%r3464, %r3463};
	add.s64 	%rd2738, %rd2737, %rd2731;
	xor.b64  	%rd2739, %rd2738, %rd2733;
	mov.b64	{%r1534, %r1535}, %rd2739;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	mov.b64	%rd2740, {%r1529, %r1533};
	xor.b64  	%rd2741, %rd2693, %rd1366;
	xor.b64  	%rd2742, %rd2741, %rd2724;
	st.global.u64 	[%rd8], %rd2742;
	xor.b64  	%rd2743, %rd2707, %rd1368;
	xor.b64  	%rd2744, %rd2743, %rd2738;
	st.global.u64 	[%rd8+8], %rd2744;
	xor.b64  	%rd2745, %rd2696, %rd1370;
	xor.b64  	%rd2746, %rd2745, %rd2721;
	st.global.u64 	[%rd8+16], %rd2746;
	xor.b64  	%rd2747, %rd2710, %rd1372;
	xor.b64  	%rd2748, %rd2747, %rd2735;
	st.global.u64 	[%rd8+24], %rd2748;
	xor.b64  	%rd2749, %rd2709, %rd1374;
	xor.b64  	%rd2750, %rd2749, %rd2740;
	st.global.u64 	[%rd8+32], %rd2750;
	xor.b64  	%rd2751, %rd2698, %rd1376;
	xor.b64  	%rd2752, %rd2751, %rd2723;
	st.global.u64 	[%rd8+40], %rd2752;
	xor.b64  	%rd2753, %rd2712, %rd1378;
	xor.b64  	%rd2754, %rd2753, %rd2737;
	st.global.u64 	[%rd8+48], %rd2754;
	xor.b64  	%rd2755, %rd2695, %rd1380;
	xor.b64  	%rd2756, %rd2755, %rd2726;
	st.global.u64 	[%rd8+56], %rd2756;
	ret;
}

	// .globl	_Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj
.visible .entry _Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj(
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj_param_0,
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj_param_1,
	.param .u32 _Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<1133>;
	.reg .b64 	%rd<40>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvjE1T[8192];

	ld.param.u64 	%rd13, [_Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj_param_0];
	ld.param.u64 	%rd14, [_Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj_param_1];
	ld.param.u32 	%r45, [_Z11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvj_param_2];
	shl.b32 	%r46, %r45, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r1127, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r1127;
	setp.ge.u32	%p1, %r4, %r46;
	@%p1 bra 	BB8_6;

	setp.gt.s32	%p2, %r1127, 2047;
	@%p2 bra 	BB8_3;

BB8_2:
	mul.wide.s32 	%rd15, %r1127, 4;
	mov.u64 	%rd16, AES_TABLE;
	add.s64 	%rd17, %rd16, %rd15;
	ld.const.u32 	%r47, [%rd17];
	shl.b32 	%r48, %r1127, 2;
	mov.u32 	%r49, _ZZ11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvjE1T;
	add.s32 	%r50, %r49, %r48;
	st.shared.u32 	[%r50], %r47;
	add.s32 	%r1127, %r1127, %r1;
	setp.lt.s32	%p3, %r1127, 2048;
	@%p3 bra 	BB8_2;

BB8_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r52, %r7, 2;
	mul.wide.u32 	%rd18, %r52, 4;
	mov.u64 	%rd19, AES_STATE_HASH;
	add.s64 	%rd20, %rd19, %rd18;
	ld.const.u32 	%r1129, [%rd20];
	ld.const.u32 	%r1130, [%rd20+4];
	ld.const.u32 	%r1131, [%rd20+8];
	ld.const.u32 	%r1132, [%rd20+12];
	and.b32  	%r53, %r4, 1;
	shl.b32 	%r54, %r4, 4;
	and.b32  	%r55, %r54, 16;
	add.s32 	%r12, %r55, 8;
	xor.b32  	%r56, %r55, 16;
	add.s32 	%r13, %r56, 8;
	cvt.u64.u32	%rd1, %r7;
	setp.eq.s32	%p4, %r53, 0;
	mov.u32 	%r57, _ZZ11hashAes1Rx4ILy2097152ELj192ELj256EEvPKvPvjE1T;
	add.s32 	%r58, %r57, 4096;
	selp.b32	%r14, %r57, %r58, %p4;
	add.s32 	%r59, %r57, 7168;
	add.s32 	%r60, %r57, 1024;
	selp.b32	%r15, %r60, %r59, %p4;
	add.s32 	%r61, %r57, 6144;
	add.s32 	%r62, %r57, 2048;
	selp.b32	%r16, %r62, %r61, %p4;
	add.s32 	%r63, %r57, 5120;
	add.s32 	%r64, %r57, 3072;
	selp.b32	%r17, %r64, %r63, %p4;
	and.b32  	%r66, %r4, -4;
	cvt.u64.u32	%rd21, %r66;
	cvt.u64.u32	%rd22, %r4;
	and.b64  	%rd23, %rd22, 3;
	or.b64  	%rd24, %rd21, %rd23;
	cvta.to.global.u64 	%rd25, %rd13;
	shl.b64 	%rd26, %rd24, 4;
	add.s64 	%rd39, %rd25, %rd26;
	shl.b32 	%r67, %r45, 5;
	mul.wide.u32 	%rd3, %r67, 16;
	mul.lo.s32 	%r68, %r45, 28;
	mul.wide.u32 	%rd4, %r68, 16;
	mul.lo.s32 	%r69, %r45, 24;
	mul.wide.u32 	%rd5, %r69, 16;
	mul.lo.s32 	%r70, %r45, 20;
	mul.wide.u32 	%rd6, %r70, 16;
	shl.b32 	%r71, %r45, 4;
	mul.wide.u32 	%rd7, %r71, 16;
	mul.lo.s32 	%r72, %r45, 12;
	mul.wide.u32 	%rd8, %r72, 16;
	shl.b32 	%r73, %r45, 3;
	mul.wide.u32 	%rd9, %r73, 16;
	mul.wide.u32 	%rd10, %r46, 16;
	shr.u32 	%r18, %r4, 2;
	mov.u32 	%r1128, -32;
	bra.uni 	BB8_4;

BB8_7:
	add.s64 	%rd12, %rd39, %rd3;
	ld.global.v4.u32 	{%r615, %r616, %r617, %r618}, [%rd39];
	ld.shared.u32 	%r623, [%r24];
	xor.b32  	%r624, %r615, %r623;
	ld.shared.u32 	%r625, [%r25];
	xor.b32  	%r626, %r624, %r625;
	ld.shared.u32 	%r627, [%r26];
	xor.b32  	%r628, %r626, %r627;
	ld.shared.u32 	%r629, [%r27];
	xor.b32  	%r319, %r628, %r629;
	ld.shared.u32 	%r630, [%r28];
	xor.b32  	%r631, %r630, %r616;
	ld.shared.u32 	%r632, [%r29];
	xor.b32  	%r633, %r631, %r632;
	ld.shared.u32 	%r634, [%r30];
	xor.b32  	%r635, %r633, %r634;
	ld.shared.u32 	%r636, [%r31];
	xor.b32  	%r322, %r635, %r636;
	ld.shared.u32 	%r637, [%r32];
	xor.b32  	%r638, %r637, %r617;
	ld.shared.u32 	%r639, [%r33];
	xor.b32  	%r640, %r638, %r639;
	ld.shared.u32 	%r641, [%r34];
	xor.b32  	%r642, %r640, %r641;
	ld.shared.u32 	%r643, [%r35];
	xor.b32  	%r325, %r642, %r643;
	ld.shared.u32 	%r644, [%r36];
	xor.b32  	%r645, %r644, %r618;
	ld.shared.u32 	%r646, [%r37];
	xor.b32  	%r647, %r645, %r646;
	ld.shared.u32 	%r648, [%r38];
	xor.b32  	%r649, %r647, %r648;
	ld.shared.u32 	%r650, [%r39];
	xor.b32  	%r316, %r649, %r650;
	add.s64 	%rd32, %rd39, %rd10;
	ld.global.v4.u32 	{%r651, %r652, %r653, %r654}, [%rd32];
	// inline asm
	bfe.u32 %r279, %r319, %r113, 8;
	// inline asm
	shl.b32 	%r659, %r279, 2;
	add.s32 	%r660, %r14, %r659;
	ld.shared.u32 	%r661, [%r660];
	// inline asm
	bfe.u32 %r282, %r322, %r12, 8;
	// inline asm
	shl.b32 	%r662, %r282, 2;
	add.s32 	%r663, %r15, %r662;
	ld.shared.u32 	%r664, [%r663];
	// inline asm
	bfe.u32 %r285, %r325, %r119, 8;
	// inline asm
	shl.b32 	%r665, %r285, 2;
	add.s32 	%r666, %r16, %r665;
	ld.shared.u32 	%r667, [%r666];
	// inline asm
	bfe.u32 %r288, %r316, %r13, 8;
	// inline asm
	shl.b32 	%r668, %r288, 2;
	add.s32 	%r669, %r17, %r668;
	xor.b32  	%r670, %r661, %r651;
	xor.b32  	%r671, %r670, %r664;
	xor.b32  	%r672, %r671, %r667;
	ld.shared.u32 	%r673, [%r669];
	xor.b32  	%r367, %r672, %r673;
	// inline asm
	bfe.u32 %r291, %r322, %r113, 8;
	// inline asm
	shl.b32 	%r674, %r291, 2;
	add.s32 	%r675, %r14, %r674;
	ld.shared.u32 	%r676, [%r675];
	// inline asm
	bfe.u32 %r294, %r325, %r12, 8;
	// inline asm
	shl.b32 	%r677, %r294, 2;
	add.s32 	%r678, %r15, %r677;
	ld.shared.u32 	%r679, [%r678];
	// inline asm
	bfe.u32 %r297, %r316, %r119, 8;
	// inline asm
	shl.b32 	%r680, %r297, 2;
	add.s32 	%r681, %r16, %r680;
	ld.shared.u32 	%r682, [%r681];
	// inline asm
	bfe.u32 %r300, %r319, %r13, 8;
	// inline asm
	shl.b32 	%r683, %r300, 2;
	add.s32 	%r684, %r17, %r683;
	xor.b32  	%r685, %r676, %r652;
	xor.b32  	%r686, %r685, %r679;
	xor.b32  	%r687, %r686, %r682;
	ld.shared.u32 	%r688, [%r684];
	xor.b32  	%r370, %r687, %r688;
	// inline asm
	bfe.u32 %r303, %r325, %r113, 8;
	// inline asm
	shl.b32 	%r689, %r303, 2;
	add.s32 	%r690, %r14, %r689;
	ld.shared.u32 	%r691, [%r690];
	// inline asm
	bfe.u32 %r306, %r316, %r12, 8;
	// inline asm
	shl.b32 	%r692, %r306, 2;
	add.s32 	%r693, %r15, %r692;
	ld.shared.u32 	%r694, [%r693];
	// inline asm
	bfe.u32 %r309, %r319, %r119, 8;
	// inline asm
	shl.b32 	%r695, %r309, 2;
	add.s32 	%r696, %r16, %r695;
	ld.shared.u32 	%r697, [%r696];
	// inline asm
	bfe.u32 %r312, %r322, %r13, 8;
	// inline asm
	shl.b32 	%r698, %r312, 2;
	add.s32 	%r699, %r17, %r698;
	xor.b32  	%r700, %r691, %r653;
	xor.b32  	%r701, %r700, %r694;
	xor.b32  	%r702, %r701, %r697;
	ld.shared.u32 	%r703, [%r699];
	xor.b32  	%r373, %r702, %r703;
	// inline asm
	bfe.u32 %r315, %r316, %r113, 8;
	// inline asm
	shl.b32 	%r704, %r315, 2;
	add.s32 	%r705, %r14, %r704;
	ld.shared.u32 	%r706, [%r705];
	// inline asm
	bfe.u32 %r318, %r319, %r12, 8;
	// inline asm
	shl.b32 	%r707, %r318, 2;
	add.s32 	%r708, %r15, %r707;
	ld.shared.u32 	%r709, [%r708];
	// inline asm
	bfe.u32 %r321, %r322, %r119, 8;
	// inline asm
	shl.b32 	%r710, %r321, 2;
	add.s32 	%r711, %r16, %r710;
	ld.shared.u32 	%r712, [%r711];
	// inline asm
	bfe.u32 %r324, %r325, %r13, 8;
	// inline asm
	shl.b32 	%r713, %r324, 2;
	add.s32 	%r714, %r17, %r713;
	xor.b32  	%r715, %r706, %r654;
	xor.b32  	%r716, %r715, %r709;
	xor.b32  	%r717, %r716, %r712;
	ld.shared.u32 	%r718, [%r714];
	xor.b32  	%r364, %r717, %r718;
	add.s64 	%rd33, %rd39, %rd9;
	ld.global.v4.u32 	{%r719, %r720, %r721, %r722}, [%rd33];
	// inline asm
	bfe.u32 %r327, %r367, %r113, 8;
	// inline asm
	shl.b32 	%r727, %r327, 2;
	add.s32 	%r728, %r14, %r727;
	ld.shared.u32 	%r729, [%r728];
	// inline asm
	bfe.u32 %r330, %r370, %r12, 8;
	// inline asm
	shl.b32 	%r730, %r330, 2;
	add.s32 	%r731, %r15, %r730;
	ld.shared.u32 	%r732, [%r731];
	// inline asm
	bfe.u32 %r333, %r373, %r119, 8;
	// inline asm
	shl.b32 	%r733, %r333, 2;
	add.s32 	%r734, %r16, %r733;
	ld.shared.u32 	%r735, [%r734];
	// inline asm
	bfe.u32 %r336, %r364, %r13, 8;
	// inline asm
	shl.b32 	%r736, %r336, 2;
	add.s32 	%r737, %r17, %r736;
	xor.b32  	%r738, %r729, %r719;
	xor.b32  	%r739, %r738, %r732;
	xor.b32  	%r740, %r739, %r735;
	ld.shared.u32 	%r741, [%r737];
	xor.b32  	%r415, %r740, %r741;
	// inline asm
	bfe.u32 %r339, %r370, %r113, 8;
	// inline asm
	shl.b32 	%r742, %r339, 2;
	add.s32 	%r743, %r14, %r742;
	ld.shared.u32 	%r744, [%r743];
	// inline asm
	bfe.u32 %r342, %r373, %r12, 8;
	// inline asm
	shl.b32 	%r745, %r342, 2;
	add.s32 	%r746, %r15, %r745;
	ld.shared.u32 	%r747, [%r746];
	// inline asm
	bfe.u32 %r345, %r364, %r119, 8;
	// inline asm
	shl.b32 	%r748, %r345, 2;
	add.s32 	%r749, %r16, %r748;
	ld.shared.u32 	%r750, [%r749];
	// inline asm
	bfe.u32 %r348, %r367, %r13, 8;
	// inline asm
	shl.b32 	%r751, %r348, 2;
	add.s32 	%r752, %r17, %r751;
	xor.b32  	%r753, %r744, %r720;
	xor.b32  	%r754, %r753, %r747;
	xor.b32  	%r755, %r754, %r750;
	ld.shared.u32 	%r756, [%r752];
	xor.b32  	%r418, %r755, %r756;
	// inline asm
	bfe.u32 %r351, %r373, %r113, 8;
	// inline asm
	shl.b32 	%r757, %r351, 2;
	add.s32 	%r758, %r14, %r757;
	ld.shared.u32 	%r759, [%r758];
	// inline asm
	bfe.u32 %r354, %r364, %r12, 8;
	// inline asm
	shl.b32 	%r760, %r354, 2;
	add.s32 	%r761, %r15, %r760;
	ld.shared.u32 	%r762, [%r761];
	// inline asm
	bfe.u32 %r357, %r367, %r119, 8;
	// inline asm
	shl.b32 	%r763, %r357, 2;
	add.s32 	%r764, %r16, %r763;
	ld.shared.u32 	%r765, [%r764];
	// inline asm
	bfe.u32 %r360, %r370, %r13, 8;
	// inline asm
	shl.b32 	%r766, %r360, 2;
	add.s32 	%r767, %r17, %r766;
	xor.b32  	%r768, %r759, %r721;
	xor.b32  	%r769, %r768, %r762;
	xor.b32  	%r770, %r769, %r765;
	ld.shared.u32 	%r771, [%r767];
	xor.b32  	%r421, %r770, %r771;
	// inline asm
	bfe.u32 %r363, %r364, %r113, 8;
	// inline asm
	shl.b32 	%r772, %r363, 2;
	add.s32 	%r773, %r14, %r772;
	ld.shared.u32 	%r774, [%r773];
	// inline asm
	bfe.u32 %r366, %r367, %r12, 8;
	// inline asm
	shl.b32 	%r775, %r366, 2;
	add.s32 	%r776, %r15, %r775;
	ld.shared.u32 	%r777, [%r776];
	// inline asm
	bfe.u32 %r369, %r370, %r119, 8;
	// inline asm
	shl.b32 	%r778, %r369, 2;
	add.s32 	%r779, %r16, %r778;
	ld.shared.u32 	%r780, [%r779];
	// inline asm
	bfe.u32 %r372, %r373, %r13, 8;
	// inline asm
	shl.b32 	%r781, %r372, 2;
	add.s32 	%r782, %r17, %r781;
	xor.b32  	%r783, %r774, %r722;
	xor.b32  	%r784, %r783, %r777;
	xor.b32  	%r785, %r784, %r780;
	ld.shared.u32 	%r786, [%r782];
	xor.b32  	%r412, %r785, %r786;
	add.s64 	%rd34, %rd39, %rd8;
	ld.global.v4.u32 	{%r787, %r788, %r789, %r790}, [%rd34];
	// inline asm
	bfe.u32 %r375, %r415, %r113, 8;
	// inline asm
	shl.b32 	%r795, %r375, 2;
	add.s32 	%r796, %r14, %r795;
	ld.shared.u32 	%r797, [%r796];
	// inline asm
	bfe.u32 %r378, %r418, %r12, 8;
	// inline asm
	shl.b32 	%r798, %r378, 2;
	add.s32 	%r799, %r15, %r798;
	ld.shared.u32 	%r800, [%r799];
	// inline asm
	bfe.u32 %r381, %r421, %r119, 8;
	// inline asm
	shl.b32 	%r801, %r381, 2;
	add.s32 	%r802, %r16, %r801;
	ld.shared.u32 	%r803, [%r802];
	// inline asm
	bfe.u32 %r384, %r412, %r13, 8;
	// inline asm
	shl.b32 	%r804, %r384, 2;
	add.s32 	%r805, %r17, %r804;
	xor.b32  	%r806, %r797, %r787;
	xor.b32  	%r807, %r806, %r800;
	xor.b32  	%r808, %r807, %r803;
	ld.shared.u32 	%r809, [%r805];
	xor.b32  	%r463, %r808, %r809;
	// inline asm
	bfe.u32 %r387, %r418, %r113, 8;
	// inline asm
	shl.b32 	%r810, %r387, 2;
	add.s32 	%r811, %r14, %r810;
	ld.shared.u32 	%r812, [%r811];
	// inline asm
	bfe.u32 %r390, %r421, %r12, 8;
	// inline asm
	shl.b32 	%r813, %r390, 2;
	add.s32 	%r814, %r15, %r813;
	ld.shared.u32 	%r815, [%r814];
	// inline asm
	bfe.u32 %r393, %r412, %r119, 8;
	// inline asm
	shl.b32 	%r816, %r393, 2;
	add.s32 	%r817, %r16, %r816;
	ld.shared.u32 	%r818, [%r817];
	// inline asm
	bfe.u32 %r396, %r415, %r13, 8;
	// inline asm
	shl.b32 	%r819, %r396, 2;
	add.s32 	%r820, %r17, %r819;
	xor.b32  	%r821, %r812, %r788;
	xor.b32  	%r822, %r821, %r815;
	xor.b32  	%r823, %r822, %r818;
	ld.shared.u32 	%r824, [%r820];
	xor.b32  	%r466, %r823, %r824;
	// inline asm
	bfe.u32 %r399, %r421, %r113, 8;
	// inline asm
	shl.b32 	%r825, %r399, 2;
	add.s32 	%r826, %r14, %r825;
	ld.shared.u32 	%r827, [%r826];
	// inline asm
	bfe.u32 %r402, %r412, %r12, 8;
	// inline asm
	shl.b32 	%r828, %r402, 2;
	add.s32 	%r829, %r15, %r828;
	ld.shared.u32 	%r830, [%r829];
	// inline asm
	bfe.u32 %r405, %r415, %r119, 8;
	// inline asm
	shl.b32 	%r831, %r405, 2;
	add.s32 	%r832, %r16, %r831;
	ld.shared.u32 	%r833, [%r832];
	// inline asm
	bfe.u32 %r408, %r418, %r13, 8;
	// inline asm
	shl.b32 	%r834, %r408, 2;
	add.s32 	%r835, %r17, %r834;
	xor.b32  	%r836, %r827, %r789;
	xor.b32  	%r837, %r836, %r830;
	xor.b32  	%r838, %r837, %r833;
	ld.shared.u32 	%r839, [%r835];
	xor.b32  	%r469, %r838, %r839;
	// inline asm
	bfe.u32 %r411, %r412, %r113, 8;
	// inline asm
	shl.b32 	%r840, %r411, 2;
	add.s32 	%r841, %r14, %r840;
	ld.shared.u32 	%r842, [%r841];
	// inline asm
	bfe.u32 %r414, %r415, %r12, 8;
	// inline asm
	shl.b32 	%r843, %r414, 2;
	add.s32 	%r844, %r15, %r843;
	ld.shared.u32 	%r845, [%r844];
	// inline asm
	bfe.u32 %r417, %r418, %r119, 8;
	// inline asm
	shl.b32 	%r846, %r417, 2;
	add.s32 	%r847, %r16, %r846;
	ld.shared.u32 	%r848, [%r847];
	// inline asm
	bfe.u32 %r420, %r421, %r13, 8;
	// inline asm
	shl.b32 	%r849, %r420, 2;
	add.s32 	%r850, %r17, %r849;
	xor.b32  	%r851, %r842, %r790;
	xor.b32  	%r852, %r851, %r845;
	xor.b32  	%r853, %r852, %r848;
	ld.shared.u32 	%r854, [%r850];
	xor.b32  	%r460, %r853, %r854;
	add.s64 	%rd35, %rd39, %rd7;
	ld.global.v4.u32 	{%r855, %r856, %r857, %r858}, [%rd35];
	// inline asm
	bfe.u32 %r423, %r463, %r113, 8;
	// inline asm
	shl.b32 	%r863, %r423, 2;
	add.s32 	%r864, %r14, %r863;
	ld.shared.u32 	%r865, [%r864];
	// inline asm
	bfe.u32 %r426, %r466, %r12, 8;
	// inline asm
	shl.b32 	%r866, %r426, 2;
	add.s32 	%r867, %r15, %r866;
	ld.shared.u32 	%r868, [%r867];
	// inline asm
	bfe.u32 %r429, %r469, %r119, 8;
	// inline asm
	shl.b32 	%r869, %r429, 2;
	add.s32 	%r870, %r16, %r869;
	ld.shared.u32 	%r871, [%r870];
	// inline asm
	bfe.u32 %r432, %r460, %r13, 8;
	// inline asm
	shl.b32 	%r872, %r432, 2;
	add.s32 	%r873, %r17, %r872;
	xor.b32  	%r874, %r865, %r855;
	xor.b32  	%r875, %r874, %r868;
	xor.b32  	%r876, %r875, %r871;
	ld.shared.u32 	%r877, [%r873];
	xor.b32  	%r511, %r876, %r877;
	// inline asm
	bfe.u32 %r435, %r466, %r113, 8;
	// inline asm
	shl.b32 	%r878, %r435, 2;
	add.s32 	%r879, %r14, %r878;
	ld.shared.u32 	%r880, [%r879];
	// inline asm
	bfe.u32 %r438, %r469, %r12, 8;
	// inline asm
	shl.b32 	%r881, %r438, 2;
	add.s32 	%r882, %r15, %r881;
	ld.shared.u32 	%r883, [%r882];
	// inline asm
	bfe.u32 %r441, %r460, %r119, 8;
	// inline asm
	shl.b32 	%r884, %r441, 2;
	add.s32 	%r885, %r16, %r884;
	ld.shared.u32 	%r886, [%r885];
	// inline asm
	bfe.u32 %r444, %r463, %r13, 8;
	// inline asm
	shl.b32 	%r887, %r444, 2;
	add.s32 	%r888, %r17, %r887;
	xor.b32  	%r889, %r880, %r856;
	xor.b32  	%r890, %r889, %r883;
	xor.b32  	%r891, %r890, %r886;
	ld.shared.u32 	%r892, [%r888];
	xor.b32  	%r514, %r891, %r892;
	// inline asm
	bfe.u32 %r447, %r469, %r113, 8;
	// inline asm
	shl.b32 	%r893, %r447, 2;
	add.s32 	%r894, %r14, %r893;
	ld.shared.u32 	%r895, [%r894];
	// inline asm
	bfe.u32 %r450, %r460, %r12, 8;
	// inline asm
	shl.b32 	%r896, %r450, 2;
	add.s32 	%r897, %r15, %r896;
	ld.shared.u32 	%r898, [%r897];
	// inline asm
	bfe.u32 %r453, %r463, %r119, 8;
	// inline asm
	shl.b32 	%r899, %r453, 2;
	add.s32 	%r900, %r16, %r899;
	ld.shared.u32 	%r901, [%r900];
	// inline asm
	bfe.u32 %r456, %r466, %r13, 8;
	// inline asm
	shl.b32 	%r902, %r456, 2;
	add.s32 	%r903, %r17, %r902;
	xor.b32  	%r904, %r895, %r857;
	xor.b32  	%r905, %r904, %r898;
	xor.b32  	%r906, %r905, %r901;
	ld.shared.u32 	%r907, [%r903];
	xor.b32  	%r517, %r906, %r907;
	// inline asm
	bfe.u32 %r459, %r460, %r113, 8;
	// inline asm
	shl.b32 	%r908, %r459, 2;
	add.s32 	%r909, %r14, %r908;
	ld.shared.u32 	%r910, [%r909];
	// inline asm
	bfe.u32 %r462, %r463, %r12, 8;
	// inline asm
	shl.b32 	%r911, %r462, 2;
	add.s32 	%r912, %r15, %r911;
	ld.shared.u32 	%r913, [%r912];
	// inline asm
	bfe.u32 %r465, %r466, %r119, 8;
	// inline asm
	shl.b32 	%r914, %r465, 2;
	add.s32 	%r915, %r16, %r914;
	ld.shared.u32 	%r916, [%r915];
	// inline asm
	bfe.u32 %r468, %r469, %r13, 8;
	// inline asm
	shl.b32 	%r917, %r468, 2;
	add.s32 	%r918, %r17, %r917;
	xor.b32  	%r919, %r910, %r858;
	xor.b32  	%r920, %r919, %r913;
	xor.b32  	%r921, %r920, %r916;
	ld.shared.u32 	%r922, [%r918];
	xor.b32  	%r508, %r921, %r922;
	add.s64 	%rd36, %rd39, %rd6;
	ld.global.v4.u32 	{%r923, %r924, %r925, %r926}, [%rd36];
	// inline asm
	bfe.u32 %r471, %r511, %r113, 8;
	// inline asm
	shl.b32 	%r931, %r471, 2;
	add.s32 	%r932, %r14, %r931;
	ld.shared.u32 	%r933, [%r932];
	// inline asm
	bfe.u32 %r474, %r514, %r12, 8;
	// inline asm
	shl.b32 	%r934, %r474, 2;
	add.s32 	%r935, %r15, %r934;
	ld.shared.u32 	%r936, [%r935];
	// inline asm
	bfe.u32 %r477, %r517, %r119, 8;
	// inline asm
	shl.b32 	%r937, %r477, 2;
	add.s32 	%r938, %r16, %r937;
	ld.shared.u32 	%r939, [%r938];
	// inline asm
	bfe.u32 %r480, %r508, %r13, 8;
	// inline asm
	shl.b32 	%r940, %r480, 2;
	add.s32 	%r941, %r17, %r940;
	xor.b32  	%r942, %r933, %r923;
	xor.b32  	%r943, %r942, %r936;
	xor.b32  	%r944, %r943, %r939;
	ld.shared.u32 	%r945, [%r941];
	xor.b32  	%r559, %r944, %r945;
	// inline asm
	bfe.u32 %r483, %r514, %r113, 8;
	// inline asm
	shl.b32 	%r946, %r483, 2;
	add.s32 	%r947, %r14, %r946;
	ld.shared.u32 	%r948, [%r947];
	// inline asm
	bfe.u32 %r486, %r517, %r12, 8;
	// inline asm
	shl.b32 	%r949, %r486, 2;
	add.s32 	%r950, %r15, %r949;
	ld.shared.u32 	%r951, [%r950];
	// inline asm
	bfe.u32 %r489, %r508, %r119, 8;
	// inline asm
	shl.b32 	%r952, %r489, 2;
	add.s32 	%r953, %r16, %r952;
	ld.shared.u32 	%r954, [%r953];
	// inline asm
	bfe.u32 %r492, %r511, %r13, 8;
	// inline asm
	shl.b32 	%r955, %r492, 2;
	add.s32 	%r956, %r17, %r955;
	xor.b32  	%r957, %r948, %r924;
	xor.b32  	%r958, %r957, %r951;
	xor.b32  	%r959, %r958, %r954;
	ld.shared.u32 	%r960, [%r956];
	xor.b32  	%r562, %r959, %r960;
	// inline asm
	bfe.u32 %r495, %r517, %r113, 8;
	// inline asm
	shl.b32 	%r961, %r495, 2;
	add.s32 	%r962, %r14, %r961;
	ld.shared.u32 	%r963, [%r962];
	// inline asm
	bfe.u32 %r498, %r508, %r12, 8;
	// inline asm
	shl.b32 	%r964, %r498, 2;
	add.s32 	%r965, %r15, %r964;
	ld.shared.u32 	%r966, [%r965];
	// inline asm
	bfe.u32 %r501, %r511, %r119, 8;
	// inline asm
	shl.b32 	%r967, %r501, 2;
	add.s32 	%r968, %r16, %r967;
	ld.shared.u32 	%r969, [%r968];
	// inline asm
	bfe.u32 %r504, %r514, %r13, 8;
	// inline asm
	shl.b32 	%r970, %r504, 2;
	add.s32 	%r971, %r17, %r970;
	xor.b32  	%r972, %r963, %r925;
	xor.b32  	%r973, %r972, %r966;
	xor.b32  	%r974, %r973, %r969;
	ld.shared.u32 	%r975, [%r971];
	xor.b32  	%r565, %r974, %r975;
	// inline asm
	bfe.u32 %r507, %r508, %r113, 8;
	// inline asm
	shl.b32 	%r976, %r507, 2;
	add.s32 	%r977, %r14, %r976;
	ld.shared.u32 	%r978, [%r977];
	// inline asm
	bfe.u32 %r510, %r511, %r12, 8;
	// inline asm
	shl.b32 	%r979, %r510, 2;
	add.s32 	%r980, %r15, %r979;
	ld.shared.u32 	%r981, [%r980];
	// inline asm
	bfe.u32 %r513, %r514, %r119, 8;
	// inline asm
	shl.b32 	%r982, %r513, 2;
	add.s32 	%r983, %r16, %r982;
	ld.shared.u32 	%r984, [%r983];
	// inline asm
	bfe.u32 %r516, %r517, %r13, 8;
	// inline asm
	shl.b32 	%r985, %r516, 2;
	add.s32 	%r986, %r17, %r985;
	xor.b32  	%r987, %r978, %r926;
	xor.b32  	%r988, %r987, %r981;
	xor.b32  	%r989, %r988, %r984;
	ld.shared.u32 	%r990, [%r986];
	xor.b32  	%r556, %r989, %r990;
	add.s64 	%rd37, %rd39, %rd5;
	ld.global.v4.u32 	{%r991, %r992, %r993, %r994}, [%rd37];
	// inline asm
	bfe.u32 %r519, %r559, %r113, 8;
	// inline asm
	shl.b32 	%r999, %r519, 2;
	add.s32 	%r1000, %r14, %r999;
	ld.shared.u32 	%r1001, [%r1000];
	// inline asm
	bfe.u32 %r522, %r562, %r12, 8;
	// inline asm
	shl.b32 	%r1002, %r522, 2;
	add.s32 	%r1003, %r15, %r1002;
	ld.shared.u32 	%r1004, [%r1003];
	// inline asm
	bfe.u32 %r525, %r565, %r119, 8;
	// inline asm
	shl.b32 	%r1005, %r525, 2;
	add.s32 	%r1006, %r16, %r1005;
	ld.shared.u32 	%r1007, [%r1006];
	// inline asm
	bfe.u32 %r528, %r556, %r13, 8;
	// inline asm
	shl.b32 	%r1008, %r528, 2;
	add.s32 	%r1009, %r17, %r1008;
	xor.b32  	%r1010, %r1001, %r991;
	xor.b32  	%r1011, %r1010, %r1004;
	xor.b32  	%r1012, %r1011, %r1007;
	ld.shared.u32 	%r1013, [%r1009];
	xor.b32  	%r607, %r1012, %r1013;
	// inline asm
	bfe.u32 %r531, %r562, %r113, 8;
	// inline asm
	shl.b32 	%r1014, %r531, 2;
	add.s32 	%r1015, %r14, %r1014;
	ld.shared.u32 	%r1016, [%r1015];
	// inline asm
	bfe.u32 %r534, %r565, %r12, 8;
	// inline asm
	shl.b32 	%r1017, %r534, 2;
	add.s32 	%r1018, %r15, %r1017;
	ld.shared.u32 	%r1019, [%r1018];
	// inline asm
	bfe.u32 %r537, %r556, %r119, 8;
	// inline asm
	shl.b32 	%r1020, %r537, 2;
	add.s32 	%r1021, %r16, %r1020;
	ld.shared.u32 	%r1022, [%r1021];
	// inline asm
	bfe.u32 %r540, %r559, %r13, 8;
	// inline asm
	shl.b32 	%r1023, %r540, 2;
	add.s32 	%r1024, %r17, %r1023;
	xor.b32  	%r1025, %r1016, %r992;
	xor.b32  	%r1026, %r1025, %r1019;
	xor.b32  	%r1027, %r1026, %r1022;
	ld.shared.u32 	%r1028, [%r1024];
	xor.b32  	%r610, %r1027, %r1028;
	// inline asm
	bfe.u32 %r543, %r565, %r113, 8;
	// inline asm
	shl.b32 	%r1029, %r543, 2;
	add.s32 	%r1030, %r14, %r1029;
	ld.shared.u32 	%r1031, [%r1030];
	// inline asm
	bfe.u32 %r546, %r556, %r12, 8;
	// inline asm
	shl.b32 	%r1032, %r546, 2;
	add.s32 	%r1033, %r15, %r1032;
	ld.shared.u32 	%r1034, [%r1033];
	// inline asm
	bfe.u32 %r549, %r559, %r119, 8;
	// inline asm
	shl.b32 	%r1035, %r549, 2;
	add.s32 	%r1036, %r16, %r1035;
	ld.shared.u32 	%r1037, [%r1036];
	// inline asm
	bfe.u32 %r552, %r562, %r13, 8;
	// inline asm
	shl.b32 	%r1038, %r552, 2;
	add.s32 	%r1039, %r17, %r1038;
	xor.b32  	%r1040, %r1031, %r993;
	xor.b32  	%r1041, %r1040, %r1034;
	xor.b32  	%r1042, %r1041, %r1037;
	ld.shared.u32 	%r1043, [%r1039];
	xor.b32  	%r613, %r1042, %r1043;
	// inline asm
	bfe.u32 %r555, %r556, %r113, 8;
	// inline asm
	shl.b32 	%r1044, %r555, 2;
	add.s32 	%r1045, %r14, %r1044;
	ld.shared.u32 	%r1046, [%r1045];
	// inline asm
	bfe.u32 %r558, %r559, %r12, 8;
	// inline asm
	shl.b32 	%r1047, %r558, 2;
	add.s32 	%r1048, %r15, %r1047;
	ld.shared.u32 	%r1049, [%r1048];
	// inline asm
	bfe.u32 %r561, %r562, %r119, 8;
	// inline asm
	shl.b32 	%r1050, %r561, 2;
	add.s32 	%r1051, %r16, %r1050;
	ld.shared.u32 	%r1052, [%r1051];
	// inline asm
	bfe.u32 %r564, %r565, %r13, 8;
	// inline asm
	shl.b32 	%r1053, %r564, 2;
	add.s32 	%r1054, %r17, %r1053;
	xor.b32  	%r1055, %r1046, %r994;
	xor.b32  	%r1056, %r1055, %r1049;
	xor.b32  	%r1057, %r1056, %r1052;
	ld.shared.u32 	%r1058, [%r1054];
	xor.b32  	%r604, %r1057, %r1058;
	add.s64 	%rd38, %rd39, %rd4;
	ld.global.v4.u32 	{%r1059, %r1060, %r1061, %r1062}, [%rd38];
	// inline asm
	bfe.u32 %r567, %r607, %r113, 8;
	// inline asm
	shl.b32 	%r1067, %r567, 2;
	add.s32 	%r1068, %r14, %r1067;
	ld.shared.u32 	%r1069, [%r1068];
	// inline asm
	bfe.u32 %r570, %r610, %r12, 8;
	// inline asm
	shl.b32 	%r1070, %r570, 2;
	add.s32 	%r1071, %r15, %r1070;
	ld.shared.u32 	%r1072, [%r1071];
	// inline asm
	bfe.u32 %r573, %r613, %r119, 8;
	// inline asm
	shl.b32 	%r1073, %r573, 2;
	add.s32 	%r1074, %r16, %r1073;
	ld.shared.u32 	%r1075, [%r1074];
	// inline asm
	bfe.u32 %r576, %r604, %r13, 8;
	// inline asm
	shl.b32 	%r1076, %r576, 2;
	add.s32 	%r1077, %r17, %r1076;
	xor.b32  	%r1078, %r1069, %r1059;
	xor.b32  	%r1079, %r1078, %r1072;
	xor.b32  	%r1080, %r1079, %r1075;
	ld.shared.u32 	%r1081, [%r1077];
	xor.b32  	%r1129, %r1080, %r1081;
	// inline asm
	bfe.u32 %r579, %r610, %r113, 8;
	// inline asm
	shl.b32 	%r1082, %r579, 2;
	add.s32 	%r1083, %r14, %r1082;
	ld.shared.u32 	%r1084, [%r1083];
	// inline asm
	bfe.u32 %r582, %r613, %r12, 8;
	// inline asm
	shl.b32 	%r1085, %r582, 2;
	add.s32 	%r1086, %r15, %r1085;
	ld.shared.u32 	%r1087, [%r1086];
	// inline asm
	bfe.u32 %r585, %r604, %r119, 8;
	// inline asm
	shl.b32 	%r1088, %r585, 2;
	add.s32 	%r1089, %r16, %r1088;
	ld.shared.u32 	%r1090, [%r1089];
	// inline asm
	bfe.u32 %r588, %r607, %r13, 8;
	// inline asm
	shl.b32 	%r1091, %r588, 2;
	add.s32 	%r1092, %r17, %r1091;
	xor.b32  	%r1093, %r1084, %r1060;
	xor.b32  	%r1094, %r1093, %r1087;
	xor.b32  	%r1095, %r1094, %r1090;
	ld.shared.u32 	%r1096, [%r1092];
	xor.b32  	%r1130, %r1095, %r1096;
	// inline asm
	bfe.u32 %r591, %r613, %r113, 8;
	// inline asm
	shl.b32 	%r1097, %r591, 2;
	add.s32 	%r1098, %r14, %r1097;
	ld.shared.u32 	%r1099, [%r1098];
	// inline asm
	bfe.u32 %r594, %r604, %r12, 8;
	// inline asm
	shl.b32 	%r1100, %r594, 2;
	add.s32 	%r1101, %r15, %r1100;
	ld.shared.u32 	%r1102, [%r1101];
	// inline asm
	bfe.u32 %r597, %r607, %r119, 8;
	// inline asm
	shl.b32 	%r1103, %r597, 2;
	add.s32 	%r1104, %r16, %r1103;
	ld.shared.u32 	%r1105, [%r1104];
	// inline asm
	bfe.u32 %r600, %r610, %r13, 8;
	// inline asm
	shl.b32 	%r1106, %r600, 2;
	add.s32 	%r1107, %r17, %r1106;
	xor.b32  	%r1108, %r1099, %r1061;
	xor.b32  	%r1109, %r1108, %r1102;
	xor.b32  	%r1110, %r1109, %r1105;
	ld.shared.u32 	%r1111, [%r1107];
	xor.b32  	%r1131, %r1110, %r1111;
	// inline asm
	bfe.u32 %r603, %r604, %r113, 8;
	// inline asm
	shl.b32 	%r1112, %r603, 2;
	add.s32 	%r1113, %r14, %r1112;
	ld.shared.u32 	%r1114, [%r1113];
	// inline asm
	bfe.u32 %r606, %r607, %r12, 8;
	// inline asm
	shl.b32 	%r1115, %r606, 2;
	add.s32 	%r1116, %r15, %r1115;
	ld.shared.u32 	%r1117, [%r1116];
	// inline asm
	bfe.u32 %r609, %r610, %r119, 8;
	// inline asm
	shl.b32 	%r1118, %r609, 2;
	add.s32 	%r1119, %r16, %r1118;
	ld.shared.u32 	%r1120, [%r1119];
	// inline asm
	bfe.u32 %r612, %r613, %r13, 8;
	// inline asm
	shl.b32 	%r1121, %r612, 2;
	add.s32 	%r1122, %r17, %r1121;
	xor.b32  	%r1123, %r1114, %r1062;
	xor.b32  	%r1124, %r1123, %r1117;
	xor.b32  	%r1125, %r1124, %r1120;
	ld.shared.u32 	%r1126, [%r1122];
	xor.b32  	%r1132, %r1125, %r1126;
	mov.u64 	%rd39, %rd12;

BB8_4:
	mov.u32 	%r113, 0;
	// inline asm
	bfe.u32 %r75, %r1129, %r113, 8;
	// inline asm
	shl.b32 	%r123, %r75, 2;
	add.s32 	%r24, %r14, %r123;
	// inline asm
	bfe.u32 %r78, %r1130, %r12, 8;
	// inline asm
	shl.b32 	%r124, %r78, 2;
	add.s32 	%r25, %r15, %r124;
	mov.u32 	%r119, 16;
	// inline asm
	bfe.u32 %r81, %r1131, %r119, 8;
	// inline asm
	shl.b32 	%r125, %r81, 2;
	add.s32 	%r26, %r16, %r125;
	// inline asm
	bfe.u32 %r84, %r1132, %r13, 8;
	// inline asm
	shl.b32 	%r126, %r84, 2;
	add.s32 	%r27, %r17, %r126;
	// inline asm
	bfe.u32 %r87, %r1130, %r113, 8;
	// inline asm
	shl.b32 	%r127, %r87, 2;
	add.s32 	%r28, %r14, %r127;
	// inline asm
	bfe.u32 %r90, %r1131, %r12, 8;
	// inline asm
	shl.b32 	%r128, %r90, 2;
	add.s32 	%r29, %r15, %r128;
	// inline asm
	bfe.u32 %r93, %r1132, %r119, 8;
	// inline asm
	shl.b32 	%r129, %r93, 2;
	add.s32 	%r30, %r16, %r129;
	// inline asm
	bfe.u32 %r96, %r1129, %r13, 8;
	// inline asm
	shl.b32 	%r130, %r96, 2;
	add.s32 	%r31, %r17, %r130;
	// inline asm
	bfe.u32 %r99, %r1131, %r113, 8;
	// inline asm
	shl.b32 	%r131, %r99, 2;
	add.s32 	%r32, %r14, %r131;
	// inline asm
	bfe.u32 %r102, %r1132, %r12, 8;
	// inline asm
	shl.b32 	%r132, %r102, 2;
	add.s32 	%r33, %r15, %r132;
	// inline asm
	bfe.u32 %r105, %r1129, %r119, 8;
	// inline asm
	shl.b32 	%r133, %r105, 2;
	add.s32 	%r34, %r16, %r133;
	// inline asm
	bfe.u32 %r108, %r1130, %r13, 8;
	// inline asm
	shl.b32 	%r134, %r108, 2;
	add.s32 	%r35, %r17, %r134;
	// inline asm
	bfe.u32 %r111, %r1132, %r113, 8;
	// inline asm
	shl.b32 	%r135, %r111, 2;
	add.s32 	%r36, %r14, %r135;
	// inline asm
	bfe.u32 %r114, %r1129, %r12, 8;
	// inline asm
	shl.b32 	%r136, %r114, 2;
	add.s32 	%r37, %r15, %r136;
	// inline asm
	bfe.u32 %r117, %r1130, %r119, 8;
	// inline asm
	shl.b32 	%r137, %r117, 2;
	add.s32 	%r38, %r16, %r137;
	// inline asm
	bfe.u32 %r120, %r1131, %r13, 8;
	// inline asm
	shl.b32 	%r138, %r120, 2;
	add.s32 	%r39, %r17, %r138;
	add.s32 	%r1128, %r1128, 32;
	setp.lt.u32	%p5, %r1128, 131072;
	@%p5 bra 	BB8_7;

	ld.shared.u32 	%r187, [%r24];
	ld.shared.u32 	%r188, [%r25];
	xor.b32  	%r189, %r187, %r188;
	ld.shared.u32 	%r190, [%r26];
	xor.b32  	%r191, %r189, %r190;
	ld.shared.u32 	%r192, [%r27];
	xor.b32  	%r193, %r191, %r192;
	xor.b32  	%r140, %r193, 298578503;
	ld.shared.u32 	%r194, [%r28];
	ld.shared.u32 	%r195, [%r29];
	xor.b32  	%r196, %r194, %r195;
	ld.shared.u32 	%r197, [%r30];
	xor.b32  	%r198, %r196, %r197;
	ld.shared.u32 	%r199, [%r31];
	xor.b32  	%r200, %r198, %r199;
	xor.b32  	%r143, %r200, 710578844;
	ld.shared.u32 	%r201, [%r32];
	ld.shared.u32 	%r202, [%r33];
	xor.b32  	%r203, %r201, %r202;
	ld.shared.u32 	%r204, [%r34];
	xor.b32  	%r205, %r203, %r204;
	ld.shared.u32 	%r206, [%r35];
	xor.b32  	%r207, %r205, %r206;
	xor.b32  	%r146, %r207, -456828611;
	ld.shared.u32 	%r208, [%r36];
	ld.shared.u32 	%r209, [%r37];
	xor.b32  	%r210, %r208, %r209;
	ld.shared.u32 	%r211, [%r38];
	xor.b32  	%r212, %r210, %r211;
	ld.shared.u32 	%r213, [%r39];
	xor.b32  	%r214, %r212, %r213;
	xor.b32  	%r149, %r214, -2087382397;
	// inline asm
	bfe.u32 %r139, %r140, %r113, 8;
	// inline asm
	shl.b32 	%r215, %r139, 2;
	add.s32 	%r216, %r14, %r215;
	ld.shared.u32 	%r217, [%r216];
	// inline asm
	bfe.u32 %r142, %r143, %r12, 8;
	// inline asm
	shl.b32 	%r218, %r142, 2;
	add.s32 	%r219, %r15, %r218;
	ld.shared.u32 	%r220, [%r219];
	// inline asm
	bfe.u32 %r145, %r146, %r119, 8;
	// inline asm
	shl.b32 	%r221, %r145, 2;
	add.s32 	%r222, %r16, %r221;
	ld.shared.u32 	%r223, [%r222];
	// inline asm
	bfe.u32 %r148, %r149, %r13, 8;
	// inline asm
	shl.b32 	%r224, %r148, 2;
	add.s32 	%r225, %r17, %r224;
	xor.b32  	%r226, %r217, %r220;
	xor.b32  	%r227, %r226, %r223;
	ld.shared.u32 	%r228, [%r225];
	xor.b32  	%r229, %r227, %r228;
	// inline asm
	bfe.u32 %r151, %r143, %r113, 8;
	// inline asm
	shl.b32 	%r230, %r151, 2;
	add.s32 	%r231, %r14, %r230;
	ld.shared.u32 	%r232, [%r231];
	// inline asm
	bfe.u32 %r154, %r146, %r12, 8;
	// inline asm
	shl.b32 	%r233, %r154, 2;
	add.s32 	%r234, %r15, %r233;
	ld.shared.u32 	%r235, [%r234];
	// inline asm
	bfe.u32 %r157, %r149, %r119, 8;
	// inline asm
	shl.b32 	%r236, %r157, 2;
	add.s32 	%r237, %r16, %r236;
	ld.shared.u32 	%r238, [%r237];
	// inline asm
	bfe.u32 %r160, %r140, %r13, 8;
	// inline asm
	shl.b32 	%r239, %r160, 2;
	add.s32 	%r240, %r17, %r239;
	xor.b32  	%r241, %r232, %r235;
	xor.b32  	%r242, %r241, %r238;
	ld.shared.u32 	%r243, [%r240];
	xor.b32  	%r244, %r242, %r243;
	// inline asm
	bfe.u32 %r163, %r146, %r113, 8;
	// inline asm
	shl.b32 	%r245, %r163, 2;
	add.s32 	%r246, %r14, %r245;
	ld.shared.u32 	%r247, [%r246];
	// inline asm
	bfe.u32 %r166, %r149, %r12, 8;
	// inline asm
	shl.b32 	%r248, %r166, 2;
	add.s32 	%r249, %r15, %r248;
	ld.shared.u32 	%r250, [%r249];
	// inline asm
	bfe.u32 %r169, %r140, %r119, 8;
	// inline asm
	shl.b32 	%r251, %r169, 2;
	add.s32 	%r252, %r16, %r251;
	ld.shared.u32 	%r253, [%r252];
	// inline asm
	bfe.u32 %r172, %r143, %r13, 8;
	// inline asm
	shl.b32 	%r254, %r172, 2;
	add.s32 	%r255, %r17, %r254;
	xor.b32  	%r256, %r247, %r250;
	xor.b32  	%r257, %r256, %r253;
	ld.shared.u32 	%r258, [%r255];
	xor.b32  	%r259, %r257, %r258;
	// inline asm
	bfe.u32 %r175, %r149, %r113, 8;
	// inline asm
	shl.b32 	%r260, %r175, 2;
	add.s32 	%r261, %r14, %r260;
	ld.shared.u32 	%r262, [%r261];
	// inline asm
	bfe.u32 %r178, %r140, %r12, 8;
	// inline asm
	shl.b32 	%r263, %r178, 2;
	add.s32 	%r264, %r15, %r263;
	ld.shared.u32 	%r265, [%r264];
	// inline asm
	bfe.u32 %r181, %r143, %r119, 8;
	// inline asm
	shl.b32 	%r266, %r181, 2;
	add.s32 	%r267, %r16, %r266;
	ld.shared.u32 	%r268, [%r267];
	// inline asm
	bfe.u32 %r184, %r146, %r13, 8;
	// inline asm
	shl.b32 	%r269, %r184, 2;
	add.s32 	%r270, %r17, %r269;
	xor.b32  	%r271, %r262, %r265;
	xor.b32  	%r272, %r271, %r268;
	ld.shared.u32 	%r273, [%r270];
	xor.b32  	%r274, %r272, %r273;
	mul.wide.u32 	%rd27, %r18, 16;
	add.s64 	%rd28, %rd1, %rd27;
	cvta.to.global.u64 	%rd29, %rd14;
	shl.b64 	%rd30, %rd28, 4;
	add.s64 	%rd31, %rd29, %rd30;
	xor.b32  	%r275, %r274, -14591054;
	xor.b32  	%r276, %r259, -1413733085;
	xor.b32  	%r277, %r244, 1199304459;
	xor.b32  	%r278, %r229, -830378859;
	st.global.v4.u32 	[%rd31+192], {%r278, %r277, %r276, %r275};

BB8_6:
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2745>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj256ELj32EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 256;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 32;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249095;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301160;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	add.s64 	%rd2698, %rd2651, %rd1381;
	add.s64 	%rd2699, %rd2698, %rd2670;
	xor.b64  	%rd2700, %rd2699, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2700;
	mov.b64	%rd2701, {%r3436, %r3435};
	add.s64 	%rd2702, %rd2701, %rd2682;
	xor.b64  	%rd2703, %rd2702, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2703;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2704, {%r3440, %r3439};
	add.s64 	%rd2705, %rd2699, %rd1383;
	add.s64 	%rd2706, %rd2705, %rd2704;
	xor.b64  	%rd2707, %rd2706, %rd2701;
	mov.b64	{%r3441, %r3442}, %rd2707;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2708, {%r3444, %r3443};
	add.s64 	%rd2709, %rd2708, %rd2702;
	xor.b64  	%rd2710, %rd2709, %rd2704;
	mov.b64	{%r1518, %r1519}, %rd2710;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	add.s64 	%rd2711, %rd2665, %rd1392;
	add.s64 	%rd2712, %rd2711, %rd2684;
	xor.b64  	%rd2713, %rd2712, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2713;
	mov.b64	%rd2714, {%r3446, %r3445};
	add.s64 	%rd2715, %rd2714, %rd2640;
	xor.b64  	%rd2716, %rd2715, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2716;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2717, {%r3450, %r3449};
	add.s64 	%rd2718, %rd2712, %rd1388;
	add.s64 	%rd2719, %rd2718, %rd2717;
	xor.b64  	%rd2720, %rd2719, %rd2714;
	mov.b64	{%r3451, %r3452}, %rd2720;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2721, {%r3454, %r3453};
	add.s64 	%rd2722, %rd2721, %rd2715;
	xor.b64  	%rd2723, %rd2722, %rd2717;
	mov.b64	{%r1526, %r1527}, %rd2723;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	add.s64 	%rd2724, %rd2642, %rd1386;
	add.s64 	%rd2725, %rd2724, %rd2679;
	xor.b64  	%rd2726, %rd2725, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2726;
	mov.b64	%rd2727, {%r3456, %r3455};
	add.s64 	%rd2728, %rd2727, %rd2654;
	xor.b64  	%rd2729, %rd2728, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2729;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2730, {%r3460, %r3459};
	add.s64 	%rd2731, %rd2725, %rd1384;
	add.s64 	%rd2732, %rd2731, %rd2730;
	xor.b64  	%rd2733, %rd2732, %rd2727;
	mov.b64	{%r3461, %r3462}, %rd2733;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2734, {%r3464, %r3463};
	add.s64 	%rd2735, %rd2734, %rd2728;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r1534, %r1535}, %rd2736;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	xor.b64  	%rd2737, %rd2693, %rd1366;
	xor.b64  	%rd2738, %rd2737, %rd2722;
	st.global.u64 	[%rd8], %rd2738;
	xor.b64  	%rd2739, %rd2706, %rd1368;
	xor.b64  	%rd2740, %rd2739, %rd2735;
	st.global.u64 	[%rd8+8], %rd2740;
	xor.b64  	%rd2741, %rd2696, %rd1370;
	xor.b64  	%rd2742, %rd2741, %rd2719;
	st.global.u64 	[%rd8+16], %rd2742;
	xor.b64  	%rd2743, %rd2709, %rd1372;
	xor.b64  	%rd2744, %rd2743, %rd2732;
	st.global.u64 	[%rd8+24], %rd2744;
	ret;
}

	// .globl	_Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv
.visible .entry _Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv(
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_0,
	.param .u64 _Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_1
)
{
	.reg .b32 	%r<3465>;
	.reg .b64 	%rd<2757>;


	ld.param.u64 	%rd1, [_Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_0];
	ld.param.u64 	%rd2, [_Z22blake2b_hash_registersILj256ELj256ELj64EEvPvPKv_param_1];
	mov.u32 	%r1537, %ctaid.x;
	mov.u32 	%r1538, %ntid.x;
	mov.u32 	%r1539, %tid.x;
	mad.lo.s32 	%r1540, %r1538, %r1537, %r1539;
	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r1540, 256;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1540, 64;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd9, [%rd5];
	ld.global.u64 	%rd10, [%rd5+8];
	ld.global.u64 	%rd11, [%rd5+16];
	ld.global.u64 	%rd12, [%rd5+24];
	ld.global.u64 	%rd13, [%rd5+32];
	ld.global.u64 	%rd14, [%rd5+40];
	ld.global.u64 	%rd15, [%rd5+48];
	ld.global.u64 	%rd16, [%rd5+56];
	ld.global.u64 	%rd17, [%rd5+64];
	ld.global.u64 	%rd18, [%rd5+72];
	ld.global.u64 	%rd19, [%rd5+80];
	ld.global.u64 	%rd20, [%rd5+88];
	ld.global.u64 	%rd21, [%rd5+96];
	ld.global.u64 	%rd22, [%rd5+104];
	ld.global.u64 	%rd23, [%rd5+112];
	ld.global.u64 	%rd24, [%rd5+120];
	add.s64 	%rd25, %rd9, -4965156021692249063;
	xor.b64  	%rd26, %rd25, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd26;
	mov.b64	%rd27, {%r1542, %r1541};
	add.s64 	%rd28, %rd27, 7640891576956012808;
	xor.b64  	%rd29, %rd28, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd29;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd30, {%r1548, %r1547};
	add.s64 	%rd31, %rd10, %rd25;
	add.s64 	%rd32, %rd31, %rd30;
	xor.b64  	%rd33, %rd32, %rd27;
	mov.b64	{%r1549, %r1550}, %rd33;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd34, {%r1554, %r1553};
	add.s64 	%rd35, %rd34, %rd28;
	xor.b64  	%rd36, %rd35, %rd30;
	mov.b64	{%r6, %r7}, %rd36;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd37, {%r1, %r5};
	add.s64 	%rd38, %rd11, 6227659224458531674;
	xor.b64  	%rd39, %rd38, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd39;
	mov.b64	%rd40, {%r1556, %r1555};
	add.s64 	%rd41, %rd40, -4942790177534073029;
	xor.b64  	%rd42, %rd41, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd42;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd43, {%r1560, %r1559};
	add.s64 	%rd44, %rd12, %rd38;
	add.s64 	%rd45, %rd44, %rd43;
	xor.b64  	%rd46, %rd45, %rd40;
	mov.b64	{%r1561, %r1562}, %rd46;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd47, {%r1564, %r1563};
	add.s64 	%rd48, %rd47, %rd41;
	xor.b64  	%rd49, %rd48, %rd43;
	mov.b64	{%r14, %r15}, %rd49;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd50, {%r9, %r13};
	add.s64 	%rd51, %rd13, 6625583534739731862;
	xor.b64  	%rd52, %rd51, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd52;
	mov.b64	%rd53, {%r1566, %r1565};
	add.s64 	%rd54, %rd53, 4354685564936845355;
	xor.b64  	%rd55, %rd54, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd55;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd56, {%r1570, %r1569};
	add.s64 	%rd57, %rd14, %rd51;
	add.s64 	%rd58, %rd57, %rd56;
	xor.b64  	%rd59, %rd58, %rd53;
	mov.b64	{%r1571, %r1572}, %rd59;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd60, {%r1574, %r1573};
	add.s64 	%rd61, %rd60, %rd54;
	xor.b64  	%rd62, %rd61, %rd56;
	mov.b64	{%r22, %r23}, %rd62;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd63, {%r17, %r21};
	add.s64 	%rd64, %rd15, 85782056580896874;
	xor.b64  	%rd65, %rd64, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd65;
	mov.b64	%rd66, {%r1576, %r1575};
	add.s64 	%rd67, %rd66, -6534734903238641935;
	xor.b64  	%rd68, %rd67, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd68;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd69, {%r1580, %r1579};
	add.s64 	%rd70, %rd16, %rd64;
	add.s64 	%rd71, %rd70, %rd69;
	xor.b64  	%rd72, %rd71, %rd66;
	mov.b64	{%r1581, %r1582}, %rd72;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd73, {%r1584, %r1583};
	add.s64 	%rd74, %rd73, %rd67;
	xor.b64  	%rd75, %rd74, %rd69;
	mov.b64	{%r30, %r31}, %rd75;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd76, {%r25, %r29};
	add.s64 	%rd77, %rd32, %rd17;
	add.s64 	%rd78, %rd77, %rd50;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r1585, %r1586}, %rd79;
	mov.b64	%rd80, {%r1586, %r1585};
	add.s64 	%rd81, %rd80, %rd61;
	xor.b64  	%rd82, %rd81, %rd50;
	mov.b64	{%r1587, %r1588}, %rd82;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd83, {%r1590, %r1589};
	add.s64 	%rd84, %rd78, %rd18;
	add.s64 	%rd85, %rd84, %rd83;
	xor.b64  	%rd86, %rd80, %rd85;
	mov.b64	{%r1591, %r1592}, %rd86;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd87, {%r1594, %r1593};
	add.s64 	%rd88, %rd87, %rd81;
	xor.b64  	%rd89, %rd88, %rd83;
	mov.b64	{%r38, %r39}, %rd89;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd90, {%r33, %r37};
	add.s64 	%rd91, %rd45, %rd19;
	add.s64 	%rd92, %rd91, %rd63;
	xor.b64  	%rd93, %rd92, %rd34;
	mov.b64	{%r1595, %r1596}, %rd93;
	mov.b64	%rd94, {%r1596, %r1595};
	add.s64 	%rd95, %rd94, %rd74;
	xor.b64  	%rd96, %rd95, %rd63;
	mov.b64	{%r1597, %r1598}, %rd96;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd97, {%r1600, %r1599};
	add.s64 	%rd98, %rd92, %rd20;
	add.s64 	%rd99, %rd98, %rd97;
	xor.b64  	%rd100, %rd99, %rd94;
	mov.b64	{%r1601, %r1602}, %rd100;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd101, {%r1604, %r1603};
	add.s64 	%rd102, %rd101, %rd95;
	xor.b64  	%rd103, %rd102, %rd97;
	mov.b64	{%r46, %r47}, %rd103;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd104, {%r41, %r45};
	add.s64 	%rd105, %rd58, %rd21;
	add.s64 	%rd106, %rd105, %rd76;
	xor.b64  	%rd107, %rd106, %rd47;
	mov.b64	{%r1605, %r1606}, %rd107;
	mov.b64	%rd108, {%r1606, %r1605};
	add.s64 	%rd109, %rd108, %rd35;
	xor.b64  	%rd110, %rd109, %rd76;
	mov.b64	{%r1607, %r1608}, %rd110;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd111, {%r1610, %r1609};
	add.s64 	%rd112, %rd106, %rd22;
	add.s64 	%rd113, %rd112, %rd111;
	xor.b64  	%rd114, %rd113, %rd108;
	mov.b64	{%r1611, %r1612}, %rd114;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd115, {%r1614, %r1613};
	add.s64 	%rd116, %rd115, %rd109;
	xor.b64  	%rd117, %rd116, %rd111;
	mov.b64	{%r54, %r55}, %rd117;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd118, {%r49, %r53};
	add.s64 	%rd119, %rd37, %rd23;
	add.s64 	%rd120, %rd119, %rd71;
	xor.b64  	%rd121, %rd120, %rd60;
	mov.b64	{%r1615, %r1616}, %rd121;
	mov.b64	%rd122, {%r1616, %r1615};
	add.s64 	%rd123, %rd122, %rd48;
	xor.b64  	%rd124, %rd123, %rd37;
	mov.b64	{%r1617, %r1618}, %rd124;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd125, {%r1620, %r1619};
	add.s64 	%rd126, %rd120, %rd24;
	add.s64 	%rd127, %rd126, %rd125;
	xor.b64  	%rd128, %rd127, %rd122;
	mov.b64	{%r1621, %r1622}, %rd128;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd129, {%r1624, %r1623};
	add.s64 	%rd130, %rd129, %rd123;
	xor.b64  	%rd131, %rd130, %rd125;
	mov.b64	{%r62, %r63}, %rd131;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd132, {%r57, %r61};
	add.s64 	%rd133, %rd85, %rd23;
	add.s64 	%rd134, %rd133, %rd132;
	xor.b64  	%rd135, %rd134, %rd101;
	mov.b64	{%r1625, %r1626}, %rd135;
	mov.b64	%rd136, {%r1626, %r1625};
	add.s64 	%rd137, %rd136, %rd116;
	xor.b64  	%rd138, %rd137, %rd132;
	mov.b64	{%r1627, %r1628}, %rd138;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd139, {%r1630, %r1629};
	add.s64 	%rd140, %rd134, %rd19;
	add.s64 	%rd141, %rd140, %rd139;
	xor.b64  	%rd142, %rd136, %rd141;
	mov.b64	{%r1631, %r1632}, %rd142;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd143, {%r1634, %r1633};
	add.s64 	%rd144, %rd137, %rd143;
	xor.b64  	%rd145, %rd144, %rd139;
	mov.b64	{%r70, %r71}, %rd145;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd146, {%r65, %r69};
	add.s64 	%rd147, %rd90, %rd13;
	add.s64 	%rd148, %rd147, %rd99;
	xor.b64  	%rd149, %rd115, %rd148;
	mov.b64	{%r1635, %r1636}, %rd149;
	mov.b64	%rd150, {%r1636, %r1635};
	add.s64 	%rd151, %rd130, %rd150;
	xor.b64  	%rd152, %rd151, %rd90;
	mov.b64	{%r1637, %r1638}, %rd152;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd153, {%r1640, %r1639};
	add.s64 	%rd154, %rd148, %rd17;
	add.s64 	%rd155, %rd154, %rd153;
	xor.b64  	%rd156, %rd155, %rd150;
	mov.b64	{%r1641, %r1642}, %rd156;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd157, {%r1644, %r1643};
	add.s64 	%rd158, %rd157, %rd151;
	xor.b64  	%rd159, %rd158, %rd153;
	mov.b64	{%r78, %r79}, %rd159;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd160, {%r73, %r77};
	add.s64 	%rd161, %rd104, %rd18;
	add.s64 	%rd162, %rd161, %rd113;
	xor.b64  	%rd163, %rd129, %rd162;
	mov.b64	{%r1645, %r1646}, %rd163;
	mov.b64	%rd164, {%r1646, %r1645};
	add.s64 	%rd165, %rd164, %rd88;
	xor.b64  	%rd166, %rd165, %rd104;
	mov.b64	{%r1647, %r1648}, %rd166;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd167, {%r1650, %r1649};
	add.s64 	%rd168, %rd162, %rd24;
	add.s64 	%rd169, %rd168, %rd167;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r1651, %r1652}, %rd170;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd171, {%r1654, %r1653};
	add.s64 	%rd172, %rd171, %rd165;
	xor.b64  	%rd173, %rd172, %rd167;
	mov.b64	{%r86, %r87}, %rd173;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd174, {%r81, %r85};
	add.s64 	%rd175, %rd118, %rd22;
	add.s64 	%rd176, %rd175, %rd127;
	xor.b64  	%rd177, %rd176, %rd87;
	mov.b64	{%r1655, %r1656}, %rd177;
	mov.b64	%rd178, {%r1656, %r1655};
	add.s64 	%rd179, %rd178, %rd102;
	xor.b64  	%rd180, %rd179, %rd118;
	mov.b64	{%r1657, %r1658}, %rd180;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd181, {%r1660, %r1659};
	add.s64 	%rd182, %rd176, %rd15;
	add.s64 	%rd183, %rd182, %rd181;
	xor.b64  	%rd184, %rd183, %rd178;
	mov.b64	{%r1661, %r1662}, %rd184;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd185, {%r1664, %r1663};
	add.s64 	%rd186, %rd185, %rd179;
	xor.b64  	%rd187, %rd186, %rd181;
	mov.b64	{%r94, %r95}, %rd187;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd188, {%r89, %r93};
	add.s64 	%rd189, %rd141, %rd10;
	add.s64 	%rd190, %rd189, %rd160;
	xor.b64  	%rd191, %rd185, %rd190;
	mov.b64	{%r1665, %r1666}, %rd191;
	mov.b64	%rd192, {%r1666, %r1665};
	add.s64 	%rd193, %rd192, %rd172;
	xor.b64  	%rd194, %rd193, %rd160;
	mov.b64	{%r1667, %r1668}, %rd194;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd195, {%r1670, %r1669};
	add.s64 	%rd196, %rd190, %rd21;
	add.s64 	%rd197, %rd196, %rd195;
	xor.b64  	%rd198, %rd192, %rd197;
	mov.b64	{%r1671, %r1672}, %rd198;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd199, {%r1674, %r1673};
	add.s64 	%rd200, %rd199, %rd193;
	xor.b64  	%rd201, %rd200, %rd195;
	mov.b64	{%r102, %r103}, %rd201;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd202, {%r97, %r101};
	add.s64 	%rd203, %rd155, %rd9;
	add.s64 	%rd204, %rd203, %rd174;
	xor.b64  	%rd205, %rd204, %rd143;
	mov.b64	{%r1675, %r1676}, %rd205;
	mov.b64	%rd206, {%r1676, %r1675};
	add.s64 	%rd207, %rd206, %rd186;
	xor.b64  	%rd208, %rd207, %rd174;
	mov.b64	{%r1677, %r1678}, %rd208;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd209, {%r1680, %r1679};
	add.s64 	%rd210, %rd204, %rd11;
	add.s64 	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd206;
	mov.b64	{%r1681, %r1682}, %rd212;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd213, {%r1684, %r1683};
	add.s64 	%rd214, %rd213, %rd207;
	xor.b64  	%rd215, %rd214, %rd209;
	mov.b64	{%r110, %r111}, %rd215;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd216, {%r105, %r109};
	add.s64 	%rd217, %rd169, %rd20;
	add.s64 	%rd218, %rd217, %rd188;
	xor.b64  	%rd219, %rd218, %rd157;
	mov.b64	{%r1685, %r1686}, %rd219;
	mov.b64	%rd220, {%r1686, %r1685};
	add.s64 	%rd221, %rd220, %rd144;
	xor.b64  	%rd222, %rd221, %rd188;
	mov.b64	{%r1687, %r1688}, %rd222;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd223, {%r1690, %r1689};
	add.s64 	%rd224, %rd218, %rd16;
	add.s64 	%rd225, %rd224, %rd223;
	xor.b64  	%rd226, %rd225, %rd220;
	mov.b64	{%r1691, %r1692}, %rd226;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd227, {%r1694, %r1693};
	add.s64 	%rd228, %rd227, %rd221;
	xor.b64  	%rd229, %rd228, %rd223;
	mov.b64	{%r118, %r119}, %rd229;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd230, {%r113, %r117};
	add.s64 	%rd231, %rd146, %rd14;
	add.s64 	%rd232, %rd231, %rd183;
	xor.b64  	%rd233, %rd232, %rd171;
	mov.b64	{%r1695, %r1696}, %rd233;
	mov.b64	%rd234, {%r1696, %r1695};
	add.s64 	%rd235, %rd234, %rd158;
	xor.b64  	%rd236, %rd235, %rd146;
	mov.b64	{%r1697, %r1698}, %rd236;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd237, {%r1700, %r1699};
	add.s64 	%rd238, %rd232, %rd12;
	add.s64 	%rd239, %rd238, %rd237;
	xor.b64  	%rd240, %rd239, %rd234;
	mov.b64	{%r1701, %r1702}, %rd240;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd241, {%r1704, %r1703};
	add.s64 	%rd242, %rd241, %rd235;
	xor.b64  	%rd243, %rd242, %rd237;
	mov.b64	{%r126, %r127}, %rd243;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd244, {%r121, %r125};
	add.s64 	%rd245, %rd197, %rd20;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd213;
	mov.b64	{%r1705, %r1706}, %rd247;
	mov.b64	%rd248, {%r1706, %r1705};
	add.s64 	%rd249, %rd248, %rd228;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r1707, %r1708}, %rd250;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd251, {%r1710, %r1709};
	add.s64 	%rd252, %rd246, %rd17;
	add.s64 	%rd253, %rd252, %rd251;
	xor.b64  	%rd254, %rd248, %rd253;
	mov.b64	{%r1711, %r1712}, %rd254;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd255, {%r1714, %r1713};
	add.s64 	%rd256, %rd249, %rd255;
	xor.b64  	%rd257, %rd256, %rd251;
	mov.b64	{%r134, %r135}, %rd257;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd258, {%r129, %r133};
	add.s64 	%rd259, %rd202, %rd21;
	add.s64 	%rd260, %rd259, %rd211;
	xor.b64  	%rd261, %rd227, %rd260;
	mov.b64	{%r1715, %r1716}, %rd261;
	mov.b64	%rd262, {%r1716, %r1715};
	add.s64 	%rd263, %rd242, %rd262;
	xor.b64  	%rd264, %rd263, %rd202;
	mov.b64	{%r1717, %r1718}, %rd264;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd265, {%r1720, %r1719};
	add.s64 	%rd266, %rd260, %rd9;
	add.s64 	%rd267, %rd266, %rd265;
	xor.b64  	%rd268, %rd267, %rd262;
	mov.b64	{%r1721, %r1722}, %rd268;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd269, {%r1724, %r1723};
	add.s64 	%rd270, %rd269, %rd263;
	xor.b64  	%rd271, %rd270, %rd265;
	mov.b64	{%r142, %r143}, %rd271;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd272, {%r137, %r141};
	add.s64 	%rd273, %rd216, %rd14;
	add.s64 	%rd274, %rd273, %rd225;
	xor.b64  	%rd275, %rd241, %rd274;
	mov.b64	{%r1725, %r1726}, %rd275;
	mov.b64	%rd276, {%r1726, %r1725};
	add.s64 	%rd277, %rd276, %rd200;
	xor.b64  	%rd278, %rd277, %rd216;
	mov.b64	{%r1727, %r1728}, %rd278;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd279, {%r1730, %r1729};
	add.s64 	%rd280, %rd274, %rd11;
	add.s64 	%rd281, %rd280, %rd279;
	xor.b64  	%rd282, %rd281, %rd276;
	mov.b64	{%r1731, %r1732}, %rd282;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd283, {%r1734, %r1733};
	add.s64 	%rd284, %rd283, %rd277;
	xor.b64  	%rd285, %rd284, %rd279;
	mov.b64	{%r150, %r151}, %rd285;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd286, {%r145, %r149};
	add.s64 	%rd287, %rd230, %rd24;
	add.s64 	%rd288, %rd287, %rd239;
	xor.b64  	%rd289, %rd288, %rd199;
	mov.b64	{%r1735, %r1736}, %rd289;
	mov.b64	%rd290, {%r1736, %r1735};
	add.s64 	%rd291, %rd290, %rd214;
	xor.b64  	%rd292, %rd291, %rd230;
	mov.b64	{%r1737, %r1738}, %rd292;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd293, {%r1740, %r1739};
	add.s64 	%rd294, %rd288, %rd22;
	add.s64 	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd290;
	mov.b64	{%r1741, %r1742}, %rd296;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd297, {%r1744, %r1743};
	add.s64 	%rd298, %rd297, %rd291;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r158, %r159}, %rd299;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd300, {%r153, %r157};
	add.s64 	%rd301, %rd253, %rd19;
	add.s64 	%rd302, %rd301, %rd272;
	xor.b64  	%rd303, %rd297, %rd302;
	mov.b64	{%r1745, %r1746}, %rd303;
	mov.b64	%rd304, {%r1746, %r1745};
	add.s64 	%rd305, %rd304, %rd284;
	xor.b64  	%rd306, %rd305, %rd272;
	mov.b64	{%r1747, %r1748}, %rd306;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd307, {%r1750, %r1749};
	add.s64 	%rd308, %rd302, %rd23;
	add.s64 	%rd309, %rd308, %rd307;
	xor.b64  	%rd310, %rd304, %rd309;
	mov.b64	{%r1751, %r1752}, %rd310;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd311, {%r1754, %r1753};
	add.s64 	%rd312, %rd311, %rd305;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r166, %r167}, %rd313;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd314, {%r161, %r165};
	add.s64 	%rd315, %rd267, %rd12;
	add.s64 	%rd316, %rd315, %rd286;
	xor.b64  	%rd317, %rd316, %rd255;
	mov.b64	{%r1755, %r1756}, %rd317;
	mov.b64	%rd318, {%r1756, %r1755};
	add.s64 	%rd319, %rd318, %rd298;
	xor.b64  	%rd320, %rd319, %rd286;
	mov.b64	{%r1757, %r1758}, %rd320;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd321, {%r1760, %r1759};
	add.s64 	%rd322, %rd316, %rd15;
	add.s64 	%rd323, %rd322, %rd321;
	xor.b64  	%rd324, %rd323, %rd318;
	mov.b64	{%r1761, %r1762}, %rd324;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd325, {%r1764, %r1763};
	add.s64 	%rd326, %rd325, %rd319;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r174, %r175}, %rd327;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd328, {%r169, %r173};
	add.s64 	%rd329, %rd281, %rd16;
	add.s64 	%rd330, %rd329, %rd300;
	xor.b64  	%rd331, %rd330, %rd269;
	mov.b64	{%r1765, %r1766}, %rd331;
	mov.b64	%rd332, {%r1766, %r1765};
	add.s64 	%rd333, %rd332, %rd256;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1767, %r1768}, %rd334;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd335, {%r1770, %r1769};
	add.s64 	%rd336, %rd330, %rd10;
	add.s64 	%rd337, %rd336, %rd335;
	xor.b64  	%rd338, %rd337, %rd332;
	mov.b64	{%r1771, %r1772}, %rd338;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd339, {%r1774, %r1773};
	add.s64 	%rd340, %rd339, %rd333;
	xor.b64  	%rd341, %rd340, %rd335;
	mov.b64	{%r182, %r183}, %rd341;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd342, {%r177, %r181};
	add.s64 	%rd343, %rd258, %rd18;
	add.s64 	%rd344, %rd343, %rd295;
	xor.b64  	%rd345, %rd344, %rd283;
	mov.b64	{%r1775, %r1776}, %rd345;
	mov.b64	%rd346, {%r1776, %r1775};
	add.s64 	%rd347, %rd346, %rd270;
	xor.b64  	%rd348, %rd347, %rd258;
	mov.b64	{%r1777, %r1778}, %rd348;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd349, {%r1780, %r1779};
	add.s64 	%rd350, %rd344, %rd13;
	add.s64 	%rd351, %rd350, %rd349;
	xor.b64  	%rd352, %rd351, %rd346;
	mov.b64	{%r1781, %r1782}, %rd352;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd353, {%r1784, %r1783};
	add.s64 	%rd354, %rd353, %rd347;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r190, %r191}, %rd355;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd356, {%r185, %r189};
	add.s64 	%rd357, %rd309, %rd16;
	add.s64 	%rd358, %rd357, %rd356;
	xor.b64  	%rd359, %rd358, %rd325;
	mov.b64	{%r1785, %r1786}, %rd359;
	mov.b64	%rd360, {%r1786, %r1785};
	add.s64 	%rd361, %rd360, %rd340;
	xor.b64  	%rd362, %rd361, %rd356;
	mov.b64	{%r1787, %r1788}, %rd362;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd363, {%r1790, %r1789};
	add.s64 	%rd364, %rd358, %rd18;
	add.s64 	%rd365, %rd364, %rd363;
	xor.b64  	%rd366, %rd360, %rd365;
	mov.b64	{%r1791, %r1792}, %rd366;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd367, {%r1794, %r1793};
	add.s64 	%rd368, %rd361, %rd367;
	xor.b64  	%rd369, %rd368, %rd363;
	mov.b64	{%r198, %r199}, %rd369;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd370, {%r193, %r197};
	add.s64 	%rd371, %rd314, %rd12;
	add.s64 	%rd372, %rd371, %rd323;
	xor.b64  	%rd373, %rd339, %rd372;
	mov.b64	{%r1795, %r1796}, %rd373;
	mov.b64	%rd374, {%r1796, %r1795};
	add.s64 	%rd375, %rd354, %rd374;
	xor.b64  	%rd376, %rd375, %rd314;
	mov.b64	{%r1797, %r1798}, %rd376;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd377, {%r1800, %r1799};
	add.s64 	%rd378, %rd372, %rd10;
	add.s64 	%rd379, %rd378, %rd377;
	xor.b64  	%rd380, %rd379, %rd374;
	mov.b64	{%r1801, %r1802}, %rd380;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd381, {%r1804, %r1803};
	add.s64 	%rd382, %rd381, %rd375;
	xor.b64  	%rd383, %rd382, %rd377;
	mov.b64	{%r206, %r207}, %rd383;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd384, {%r201, %r205};
	add.s64 	%rd385, %rd328, %rd22;
	add.s64 	%rd386, %rd385, %rd337;
	xor.b64  	%rd387, %rd353, %rd386;
	mov.b64	{%r1805, %r1806}, %rd387;
	mov.b64	%rd388, {%r1806, %r1805};
	add.s64 	%rd389, %rd388, %rd312;
	xor.b64  	%rd390, %rd389, %rd328;
	mov.b64	{%r1807, %r1808}, %rd390;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd391, {%r1810, %r1809};
	add.s64 	%rd392, %rd386, %rd21;
	add.s64 	%rd393, %rd392, %rd391;
	xor.b64  	%rd394, %rd393, %rd388;
	mov.b64	{%r1811, %r1812}, %rd394;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd395, {%r1814, %r1813};
	add.s64 	%rd396, %rd395, %rd389;
	xor.b64  	%rd397, %rd396, %rd391;
	mov.b64	{%r214, %r215}, %rd397;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd398, {%r209, %r213};
	add.s64 	%rd399, %rd342, %rd20;
	add.s64 	%rd400, %rd399, %rd351;
	xor.b64  	%rd401, %rd400, %rd311;
	mov.b64	{%r1815, %r1816}, %rd401;
	mov.b64	%rd402, {%r1816, %r1815};
	add.s64 	%rd403, %rd402, %rd326;
	xor.b64  	%rd404, %rd403, %rd342;
	mov.b64	{%r1817, %r1818}, %rd404;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd405, {%r1820, %r1819};
	add.s64 	%rd406, %rd400, %rd23;
	add.s64 	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd402;
	mov.b64	{%r1821, %r1822}, %rd408;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd409, {%r1824, %r1823};
	add.s64 	%rd410, %rd409, %rd403;
	xor.b64  	%rd411, %rd410, %rd405;
	mov.b64	{%r222, %r223}, %rd411;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd412, {%r217, %r221};
	add.s64 	%rd413, %rd365, %rd11;
	add.s64 	%rd414, %rd413, %rd384;
	xor.b64  	%rd415, %rd409, %rd414;
	mov.b64	{%r1825, %r1826}, %rd415;
	mov.b64	%rd416, {%r1826, %r1825};
	add.s64 	%rd417, %rd416, %rd396;
	xor.b64  	%rd418, %rd417, %rd384;
	mov.b64	{%r1827, %r1828}, %rd418;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd419, {%r1830, %r1829};
	add.s64 	%rd420, %rd414, %rd15;
	add.s64 	%rd421, %rd420, %rd419;
	xor.b64  	%rd422, %rd416, %rd421;
	mov.b64	{%r1831, %r1832}, %rd422;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd423, {%r1834, %r1833};
	add.s64 	%rd424, %rd423, %rd417;
	xor.b64  	%rd425, %rd424, %rd419;
	mov.b64	{%r230, %r231}, %rd425;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd426, {%r225, %r229};
	add.s64 	%rd427, %rd379, %rd14;
	add.s64 	%rd428, %rd427, %rd398;
	xor.b64  	%rd429, %rd428, %rd367;
	mov.b64	{%r1835, %r1836}, %rd429;
	mov.b64	%rd430, {%r1836, %r1835};
	add.s64 	%rd431, %rd430, %rd410;
	xor.b64  	%rd432, %rd431, %rd398;
	mov.b64	{%r1837, %r1838}, %rd432;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd433, {%r1840, %r1839};
	add.s64 	%rd434, %rd428, %rd19;
	add.s64 	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r1841, %r1842}, %rd436;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd437, {%r1844, %r1843};
	add.s64 	%rd438, %rd437, %rd431;
	xor.b64  	%rd439, %rd438, %rd433;
	mov.b64	{%r238, %r239}, %rd439;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd440, {%r233, %r237};
	add.s64 	%rd441, %rd393, %rd13;
	add.s64 	%rd442, %rd441, %rd412;
	xor.b64  	%rd443, %rd442, %rd381;
	mov.b64	{%r1845, %r1846}, %rd443;
	mov.b64	%rd444, {%r1846, %r1845};
	add.s64 	%rd445, %rd444, %rd368;
	xor.b64  	%rd446, %rd445, %rd412;
	mov.b64	{%r1847, %r1848}, %rd446;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd447, {%r1850, %r1849};
	add.s64 	%rd448, %rd442, %rd9;
	add.s64 	%rd449, %rd448, %rd447;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r1851, %r1852}, %rd450;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd451, {%r1854, %r1853};
	add.s64 	%rd452, %rd451, %rd445;
	xor.b64  	%rd453, %rd452, %rd447;
	mov.b64	{%r246, %r247}, %rd453;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd454, {%r241, %r245};
	add.s64 	%rd455, %rd370, %rd24;
	add.s64 	%rd456, %rd455, %rd407;
	xor.b64  	%rd457, %rd456, %rd395;
	mov.b64	{%r1855, %r1856}, %rd457;
	mov.b64	%rd458, {%r1856, %r1855};
	add.s64 	%rd459, %rd458, %rd382;
	xor.b64  	%rd460, %rd459, %rd370;
	mov.b64	{%r1857, %r1858}, %rd460;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd461, {%r1860, %r1859};
	add.s64 	%rd462, %rd456, %rd17;
	add.s64 	%rd463, %rd462, %rd461;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r1861, %r1862}, %rd464;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd465, {%r1864, %r1863};
	add.s64 	%rd466, %rd465, %rd459;
	xor.b64  	%rd467, %rd466, %rd461;
	mov.b64	{%r254, %r255}, %rd467;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd468, {%r249, %r253};
	add.s64 	%rd469, %rd421, %rd18;
	add.s64 	%rd470, %rd469, %rd468;
	xor.b64  	%rd471, %rd470, %rd437;
	mov.b64	{%r1865, %r1866}, %rd471;
	mov.b64	%rd472, {%r1866, %r1865};
	add.s64 	%rd473, %rd472, %rd452;
	xor.b64  	%rd474, %rd473, %rd468;
	mov.b64	{%r1867, %r1868}, %rd474;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd475, {%r1870, %r1869};
	add.s64 	%rd476, %rd470, %rd9;
	add.s64 	%rd477, %rd476, %rd475;
	xor.b64  	%rd478, %rd472, %rd477;
	mov.b64	{%r1871, %r1872}, %rd478;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd479, {%r1874, %r1873};
	add.s64 	%rd480, %rd473, %rd479;
	xor.b64  	%rd481, %rd480, %rd475;
	mov.b64	{%r262, %r263}, %rd481;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd482, {%r257, %r261};
	add.s64 	%rd483, %rd426, %rd14;
	add.s64 	%rd484, %rd483, %rd435;
	xor.b64  	%rd485, %rd451, %rd484;
	mov.b64	{%r1875, %r1876}, %rd485;
	mov.b64	%rd486, {%r1876, %r1875};
	add.s64 	%rd487, %rd466, %rd486;
	xor.b64  	%rd488, %rd487, %rd426;
	mov.b64	{%r1877, %r1878}, %rd488;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd489, {%r1880, %r1879};
	add.s64 	%rd490, %rd484, %rd16;
	add.s64 	%rd491, %rd490, %rd489;
	xor.b64  	%rd492, %rd491, %rd486;
	mov.b64	{%r1881, %r1882}, %rd492;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd493, {%r1884, %r1883};
	add.s64 	%rd494, %rd493, %rd487;
	xor.b64  	%rd495, %rd494, %rd489;
	mov.b64	{%r270, %r271}, %rd495;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd496, {%r265, %r269};
	add.s64 	%rd497, %rd440, %rd11;
	add.s64 	%rd498, %rd497, %rd449;
	xor.b64  	%rd499, %rd465, %rd498;
	mov.b64	{%r1885, %r1886}, %rd499;
	mov.b64	%rd500, {%r1886, %r1885};
	add.s64 	%rd501, %rd500, %rd424;
	xor.b64  	%rd502, %rd501, %rd440;
	mov.b64	{%r1887, %r1888}, %rd502;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd503, {%r1890, %r1889};
	add.s64 	%rd504, %rd498, %rd13;
	add.s64 	%rd505, %rd504, %rd503;
	xor.b64  	%rd506, %rd505, %rd500;
	mov.b64	{%r1891, %r1892}, %rd506;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd507, {%r1894, %r1893};
	add.s64 	%rd508, %rd507, %rd501;
	xor.b64  	%rd509, %rd508, %rd503;
	mov.b64	{%r278, %r279}, %rd509;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd510, {%r273, %r277};
	add.s64 	%rd511, %rd454, %rd19;
	add.s64 	%rd512, %rd511, %rd463;
	xor.b64  	%rd513, %rd512, %rd423;
	mov.b64	{%r1895, %r1896}, %rd513;
	mov.b64	%rd514, {%r1896, %r1895};
	add.s64 	%rd515, %rd514, %rd438;
	xor.b64  	%rd516, %rd515, %rd454;
	mov.b64	{%r1897, %r1898}, %rd516;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd517, {%r1900, %r1899};
	add.s64 	%rd518, %rd512, %rd24;
	add.s64 	%rd519, %rd518, %rd517;
	xor.b64  	%rd520, %rd519, %rd514;
	mov.b64	{%r1901, %r1902}, %rd520;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd521, {%r1904, %r1903};
	add.s64 	%rd522, %rd521, %rd515;
	xor.b64  	%rd523, %rd522, %rd517;
	mov.b64	{%r286, %r287}, %rd523;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd524, {%r281, %r285};
	add.s64 	%rd525, %rd477, %rd23;
	add.s64 	%rd526, %rd525, %rd496;
	xor.b64  	%rd527, %rd521, %rd526;
	mov.b64	{%r1905, %r1906}, %rd527;
	mov.b64	%rd528, {%r1906, %r1905};
	add.s64 	%rd529, %rd528, %rd508;
	xor.b64  	%rd530, %rd529, %rd496;
	mov.b64	{%r1907, %r1908}, %rd530;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd531, {%r1910, %r1909};
	add.s64 	%rd532, %rd526, %rd10;
	add.s64 	%rd533, %rd532, %rd531;
	xor.b64  	%rd534, %rd528, %rd533;
	mov.b64	{%r1911, %r1912}, %rd534;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd535, {%r1914, %r1913};
	add.s64 	%rd536, %rd535, %rd529;
	xor.b64  	%rd537, %rd536, %rd531;
	mov.b64	{%r294, %r295}, %rd537;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd538, {%r289, %r293};
	add.s64 	%rd539, %rd491, %rd20;
	add.s64 	%rd540, %rd539, %rd510;
	xor.b64  	%rd541, %rd540, %rd479;
	mov.b64	{%r1915, %r1916}, %rd541;
	mov.b64	%rd542, {%r1916, %r1915};
	add.s64 	%rd543, %rd542, %rd522;
	xor.b64  	%rd544, %rd543, %rd510;
	mov.b64	{%r1917, %r1918}, %rd544;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd545, {%r1920, %r1919};
	add.s64 	%rd546, %rd540, %rd21;
	add.s64 	%rd547, %rd546, %rd545;
	xor.b64  	%rd548, %rd547, %rd542;
	mov.b64	{%r1921, %r1922}, %rd548;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd549, {%r1924, %r1923};
	add.s64 	%rd550, %rd549, %rd543;
	xor.b64  	%rd551, %rd550, %rd545;
	mov.b64	{%r302, %r303}, %rd551;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd552, {%r297, %r301};
	add.s64 	%rd553, %rd505, %rd15;
	add.s64 	%rd554, %rd553, %rd524;
	xor.b64  	%rd555, %rd554, %rd493;
	mov.b64	{%r1925, %r1926}, %rd555;
	mov.b64	%rd556, {%r1926, %r1925};
	add.s64 	%rd557, %rd556, %rd480;
	xor.b64  	%rd558, %rd557, %rd524;
	mov.b64	{%r1927, %r1928}, %rd558;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd559, {%r1930, %r1929};
	add.s64 	%rd560, %rd554, %rd17;
	add.s64 	%rd561, %rd560, %rd559;
	xor.b64  	%rd562, %rd561, %rd556;
	mov.b64	{%r1931, %r1932}, %rd562;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd563, {%r1934, %r1933};
	add.s64 	%rd564, %rd563, %rd557;
	xor.b64  	%rd565, %rd564, %rd559;
	mov.b64	{%r310, %r311}, %rd565;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd566, {%r305, %r309};
	add.s64 	%rd567, %rd482, %rd12;
	add.s64 	%rd568, %rd567, %rd519;
	xor.b64  	%rd569, %rd568, %rd507;
	mov.b64	{%r1935, %r1936}, %rd569;
	mov.b64	%rd570, {%r1936, %r1935};
	add.s64 	%rd571, %rd570, %rd494;
	xor.b64  	%rd572, %rd571, %rd482;
	mov.b64	{%r1937, %r1938}, %rd572;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd573, {%r1940, %r1939};
	add.s64 	%rd574, %rd568, %rd22;
	add.s64 	%rd575, %rd574, %rd573;
	xor.b64  	%rd576, %rd575, %rd570;
	mov.b64	{%r1941, %r1942}, %rd576;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd577, {%r1944, %r1943};
	add.s64 	%rd578, %rd577, %rd571;
	xor.b64  	%rd579, %rd578, %rd573;
	mov.b64	{%r318, %r319}, %rd579;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd580, {%r313, %r317};
	add.s64 	%rd581, %rd533, %rd11;
	add.s64 	%rd582, %rd581, %rd580;
	xor.b64  	%rd583, %rd582, %rd549;
	mov.b64	{%r1945, %r1946}, %rd583;
	mov.b64	%rd584, {%r1946, %r1945};
	add.s64 	%rd585, %rd584, %rd564;
	xor.b64  	%rd586, %rd585, %rd580;
	mov.b64	{%r1947, %r1948}, %rd586;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd587, {%r1950, %r1949};
	add.s64 	%rd588, %rd582, %rd21;
	add.s64 	%rd589, %rd588, %rd587;
	xor.b64  	%rd590, %rd584, %rd589;
	mov.b64	{%r1951, %r1952}, %rd590;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd591, {%r1954, %r1953};
	add.s64 	%rd592, %rd585, %rd591;
	xor.b64  	%rd593, %rd592, %rd587;
	mov.b64	{%r326, %r327}, %rd593;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd594, {%r321, %r325};
	add.s64 	%rd595, %rd538, %rd15;
	add.s64 	%rd596, %rd595, %rd547;
	xor.b64  	%rd597, %rd563, %rd596;
	mov.b64	{%r1955, %r1956}, %rd597;
	mov.b64	%rd598, {%r1956, %r1955};
	add.s64 	%rd599, %rd578, %rd598;
	xor.b64  	%rd600, %rd599, %rd538;
	mov.b64	{%r1957, %r1958}, %rd600;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd601, {%r1960, %r1959};
	add.s64 	%rd602, %rd596, %rd19;
	add.s64 	%rd603, %rd602, %rd601;
	xor.b64  	%rd604, %rd603, %rd598;
	mov.b64	{%r1961, %r1962}, %rd604;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd605, {%r1964, %r1963};
	add.s64 	%rd606, %rd605, %rd599;
	xor.b64  	%rd607, %rd606, %rd601;
	mov.b64	{%r334, %r335}, %rd607;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd608, {%r329, %r333};
	add.s64 	%rd609, %rd552, %rd9;
	add.s64 	%rd610, %rd609, %rd561;
	xor.b64  	%rd611, %rd577, %rd610;
	mov.b64	{%r1965, %r1966}, %rd611;
	mov.b64	%rd612, {%r1966, %r1965};
	add.s64 	%rd613, %rd612, %rd536;
	xor.b64  	%rd614, %rd613, %rd552;
	mov.b64	{%r1967, %r1968}, %rd614;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd615, {%r1970, %r1969};
	add.s64 	%rd616, %rd610, %rd20;
	add.s64 	%rd617, %rd616, %rd615;
	xor.b64  	%rd618, %rd617, %rd612;
	mov.b64	{%r1971, %r1972}, %rd618;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd619, {%r1974, %r1973};
	add.s64 	%rd620, %rd619, %rd613;
	xor.b64  	%rd621, %rd620, %rd615;
	mov.b64	{%r342, %r343}, %rd621;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd622, {%r337, %r341};
	add.s64 	%rd623, %rd566, %rd17;
	add.s64 	%rd624, %rd623, %rd575;
	xor.b64  	%rd625, %rd624, %rd535;
	mov.b64	{%r1975, %r1976}, %rd625;
	mov.b64	%rd626, {%r1976, %r1975};
	add.s64 	%rd627, %rd626, %rd550;
	xor.b64  	%rd628, %rd627, %rd566;
	mov.b64	{%r1977, %r1978}, %rd628;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd629, {%r1980, %r1979};
	add.s64 	%rd630, %rd624, %rd12;
	add.s64 	%rd631, %rd630, %rd629;
	xor.b64  	%rd632, %rd631, %rd626;
	mov.b64	{%r1981, %r1982}, %rd632;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd633, {%r1984, %r1983};
	add.s64 	%rd634, %rd633, %rd627;
	xor.b64  	%rd635, %rd634, %rd629;
	mov.b64	{%r350, %r351}, %rd635;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd636, {%r345, %r349};
	add.s64 	%rd637, %rd589, %rd13;
	add.s64 	%rd638, %rd637, %rd608;
	xor.b64  	%rd639, %rd633, %rd638;
	mov.b64	{%r1985, %r1986}, %rd639;
	mov.b64	%rd640, {%r1986, %r1985};
	add.s64 	%rd641, %rd640, %rd620;
	xor.b64  	%rd642, %rd641, %rd608;
	mov.b64	{%r1987, %r1988}, %rd642;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd643, {%r1990, %r1989};
	add.s64 	%rd644, %rd638, %rd22;
	add.s64 	%rd645, %rd644, %rd643;
	xor.b64  	%rd646, %rd640, %rd645;
	mov.b64	{%r1991, %r1992}, %rd646;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd647, {%r1994, %r1993};
	add.s64 	%rd648, %rd647, %rd641;
	xor.b64  	%rd649, %rd648, %rd643;
	mov.b64	{%r358, %r359}, %rd649;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd650, {%r353, %r357};
	add.s64 	%rd651, %rd603, %rd16;
	add.s64 	%rd652, %rd651, %rd622;
	xor.b64  	%rd653, %rd652, %rd591;
	mov.b64	{%r1995, %r1996}, %rd653;
	mov.b64	%rd654, {%r1996, %r1995};
	add.s64 	%rd655, %rd654, %rd634;
	xor.b64  	%rd656, %rd655, %rd622;
	mov.b64	{%r1997, %r1998}, %rd656;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd657, {%r2000, %r1999};
	add.s64 	%rd658, %rd652, %rd14;
	add.s64 	%rd659, %rd658, %rd657;
	xor.b64  	%rd660, %rd659, %rd654;
	mov.b64	{%r2001, %r2002}, %rd660;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd661, {%r2004, %r2003};
	add.s64 	%rd662, %rd661, %rd655;
	xor.b64  	%rd663, %rd662, %rd657;
	mov.b64	{%r366, %r367}, %rd663;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd664, {%r361, %r365};
	add.s64 	%rd665, %rd617, %rd24;
	add.s64 	%rd666, %rd665, %rd636;
	xor.b64  	%rd667, %rd666, %rd605;
	mov.b64	{%r2005, %r2006}, %rd667;
	mov.b64	%rd668, {%r2006, %r2005};
	add.s64 	%rd669, %rd668, %rd592;
	xor.b64  	%rd670, %rd669, %rd636;
	mov.b64	{%r2007, %r2008}, %rd670;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd671, {%r2010, %r2009};
	add.s64 	%rd672, %rd666, %rd23;
	add.s64 	%rd673, %rd672, %rd671;
	xor.b64  	%rd674, %rd673, %rd668;
	mov.b64	{%r2011, %r2012}, %rd674;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd675, {%r2014, %r2013};
	add.s64 	%rd676, %rd675, %rd669;
	xor.b64  	%rd677, %rd676, %rd671;
	mov.b64	{%r374, %r375}, %rd677;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd678, {%r369, %r373};
	add.s64 	%rd679, %rd594, %rd10;
	add.s64 	%rd680, %rd679, %rd631;
	xor.b64  	%rd681, %rd680, %rd619;
	mov.b64	{%r2015, %r2016}, %rd681;
	mov.b64	%rd682, {%r2016, %r2015};
	add.s64 	%rd683, %rd682, %rd606;
	xor.b64  	%rd684, %rd683, %rd594;
	mov.b64	{%r2017, %r2018}, %rd684;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd685, {%r2020, %r2019};
	add.s64 	%rd686, %rd680, %rd18;
	add.s64 	%rd687, %rd686, %rd685;
	xor.b64  	%rd688, %rd687, %rd682;
	mov.b64	{%r2021, %r2022}, %rd688;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd689, {%r2024, %r2023};
	add.s64 	%rd690, %rd689, %rd683;
	xor.b64  	%rd691, %rd690, %rd685;
	mov.b64	{%r382, %r383}, %rd691;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd692, {%r377, %r381};
	add.s64 	%rd693, %rd645, %rd21;
	add.s64 	%rd694, %rd693, %rd692;
	xor.b64  	%rd695, %rd694, %rd661;
	mov.b64	{%r2025, %r2026}, %rd695;
	mov.b64	%rd696, {%r2026, %r2025};
	add.s64 	%rd697, %rd696, %rd676;
	xor.b64  	%rd698, %rd697, %rd692;
	mov.b64	{%r2027, %r2028}, %rd698;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd699, {%r2030, %r2029};
	add.s64 	%rd700, %rd694, %rd14;
	add.s64 	%rd701, %rd700, %rd699;
	xor.b64  	%rd702, %rd696, %rd701;
	mov.b64	{%r2031, %r2032}, %rd702;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd703, {%r2034, %r2033};
	add.s64 	%rd704, %rd697, %rd703;
	xor.b64  	%rd705, %rd704, %rd699;
	mov.b64	{%r390, %r391}, %rd705;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd706, {%r385, %r389};
	add.s64 	%rd707, %rd650, %rd10;
	add.s64 	%rd708, %rd707, %rd659;
	xor.b64  	%rd709, %rd675, %rd708;
	mov.b64	{%r2035, %r2036}, %rd709;
	mov.b64	%rd710, {%r2036, %r2035};
	add.s64 	%rd711, %rd690, %rd710;
	xor.b64  	%rd712, %rd711, %rd650;
	mov.b64	{%r2037, %r2038}, %rd712;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd713, {%r2040, %r2039};
	add.s64 	%rd714, %rd708, %rd24;
	add.s64 	%rd715, %rd714, %rd713;
	xor.b64  	%rd716, %rd715, %rd710;
	mov.b64	{%r2041, %r2042}, %rd716;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd717, {%r2044, %r2043};
	add.s64 	%rd718, %rd717, %rd711;
	xor.b64  	%rd719, %rd718, %rd713;
	mov.b64	{%r398, %r399}, %rd719;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd720, {%r393, %r397};
	add.s64 	%rd721, %rd664, %rd23;
	add.s64 	%rd722, %rd721, %rd673;
	xor.b64  	%rd723, %rd689, %rd722;
	mov.b64	{%r2045, %r2046}, %rd723;
	mov.b64	%rd724, {%r2046, %r2045};
	add.s64 	%rd725, %rd724, %rd648;
	xor.b64  	%rd726, %rd725, %rd664;
	mov.b64	{%r2047, %r2048}, %rd726;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd727, {%r2050, %r2049};
	add.s64 	%rd728, %rd722, %rd22;
	add.s64 	%rd729, %rd728, %rd727;
	xor.b64  	%rd730, %rd729, %rd724;
	mov.b64	{%r2051, %r2052}, %rd730;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd731, {%r2054, %r2053};
	add.s64 	%rd732, %rd731, %rd725;
	xor.b64  	%rd733, %rd732, %rd727;
	mov.b64	{%r406, %r407}, %rd733;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd734, {%r401, %r405};
	add.s64 	%rd735, %rd678, %rd13;
	add.s64 	%rd736, %rd735, %rd687;
	xor.b64  	%rd737, %rd736, %rd647;
	mov.b64	{%r2055, %r2056}, %rd737;
	mov.b64	%rd738, {%r2056, %r2055};
	add.s64 	%rd739, %rd738, %rd662;
	xor.b64  	%rd740, %rd739, %rd678;
	mov.b64	{%r2057, %r2058}, %rd740;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd741, {%r2060, %r2059};
	add.s64 	%rd742, %rd736, %rd19;
	add.s64 	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd738;
	mov.b64	{%r2061, %r2062}, %rd744;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd745, {%r2064, %r2063};
	add.s64 	%rd746, %rd745, %rd739;
	xor.b64  	%rd747, %rd746, %rd741;
	mov.b64	{%r414, %r415}, %rd747;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd748, {%r409, %r413};
	add.s64 	%rd749, %rd701, %rd9;
	add.s64 	%rd750, %rd749, %rd720;
	xor.b64  	%rd751, %rd745, %rd750;
	mov.b64	{%r2065, %r2066}, %rd751;
	mov.b64	%rd752, {%r2066, %r2065};
	add.s64 	%rd753, %rd752, %rd732;
	xor.b64  	%rd754, %rd753, %rd720;
	mov.b64	{%r2067, %r2068}, %rd754;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd755, {%r2070, %r2069};
	add.s64 	%rd756, %rd750, %rd16;
	add.s64 	%rd757, %rd756, %rd755;
	xor.b64  	%rd758, %rd752, %rd757;
	mov.b64	{%r2071, %r2072}, %rd758;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd759, {%r2074, %r2073};
	add.s64 	%rd760, %rd759, %rd753;
	xor.b64  	%rd761, %rd760, %rd755;
	mov.b64	{%r422, %r423}, %rd761;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd762, {%r417, %r421};
	add.s64 	%rd763, %rd715, %rd15;
	add.s64 	%rd764, %rd763, %rd734;
	xor.b64  	%rd765, %rd764, %rd703;
	mov.b64	{%r2075, %r2076}, %rd765;
	mov.b64	%rd766, {%r2076, %r2075};
	add.s64 	%rd767, %rd766, %rd746;
	xor.b64  	%rd768, %rd767, %rd734;
	mov.b64	{%r2077, %r2078}, %rd768;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd769, {%r2080, %r2079};
	add.s64 	%rd770, %rd764, %rd12;
	add.s64 	%rd771, %rd770, %rd769;
	xor.b64  	%rd772, %rd771, %rd766;
	mov.b64	{%r2081, %r2082}, %rd772;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd773, {%r2084, %r2083};
	add.s64 	%rd774, %rd773, %rd767;
	xor.b64  	%rd775, %rd774, %rd769;
	mov.b64	{%r430, %r431}, %rd775;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd776, {%r425, %r429};
	add.s64 	%rd777, %rd729, %rd18;
	add.s64 	%rd778, %rd777, %rd748;
	xor.b64  	%rd779, %rd778, %rd717;
	mov.b64	{%r2085, %r2086}, %rd779;
	mov.b64	%rd780, {%r2086, %r2085};
	add.s64 	%rd781, %rd780, %rd704;
	xor.b64  	%rd782, %rd781, %rd748;
	mov.b64	{%r2087, %r2088}, %rd782;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd783, {%r2090, %r2089};
	add.s64 	%rd784, %rd778, %rd11;
	add.s64 	%rd785, %rd784, %rd783;
	xor.b64  	%rd786, %rd785, %rd780;
	mov.b64	{%r2091, %r2092}, %rd786;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd787, {%r2094, %r2093};
	add.s64 	%rd788, %rd787, %rd781;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r438, %r439}, %rd789;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd790, {%r433, %r437};
	add.s64 	%rd791, %rd706, %rd17;
	add.s64 	%rd792, %rd791, %rd743;
	xor.b64  	%rd793, %rd792, %rd731;
	mov.b64	{%r2095, %r2096}, %rd793;
	mov.b64	%rd794, {%r2096, %r2095};
	add.s64 	%rd795, %rd794, %rd718;
	xor.b64  	%rd796, %rd795, %rd706;
	mov.b64	{%r2097, %r2098}, %rd796;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd797, {%r2100, %r2099};
	add.s64 	%rd798, %rd792, %rd20;
	add.s64 	%rd799, %rd798, %rd797;
	xor.b64  	%rd800, %rd799, %rd794;
	mov.b64	{%r2101, %r2102}, %rd800;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd801, {%r2104, %r2103};
	add.s64 	%rd802, %rd801, %rd795;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r446, %r447}, %rd803;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd804, {%r441, %r445};
	add.s64 	%rd805, %rd757, %rd22;
	add.s64 	%rd806, %rd805, %rd804;
	xor.b64  	%rd807, %rd806, %rd773;
	mov.b64	{%r2105, %r2106}, %rd807;
	mov.b64	%rd808, {%r2106, %r2105};
	add.s64 	%rd809, %rd808, %rd788;
	xor.b64  	%rd810, %rd809, %rd804;
	mov.b64	{%r2107, %r2108}, %rd810;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd811, {%r2110, %r2109};
	add.s64 	%rd812, %rd806, %rd20;
	add.s64 	%rd813, %rd812, %rd811;
	xor.b64  	%rd814, %rd808, %rd813;
	mov.b64	{%r2111, %r2112}, %rd814;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd815, {%r2114, %r2113};
	add.s64 	%rd816, %rd809, %rd815;
	xor.b64  	%rd817, %rd816, %rd811;
	mov.b64	{%r454, %r455}, %rd817;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd818, {%r449, %r453};
	add.s64 	%rd819, %rd762, %rd16;
	add.s64 	%rd820, %rd819, %rd771;
	xor.b64  	%rd821, %rd787, %rd820;
	mov.b64	{%r2115, %r2116}, %rd821;
	mov.b64	%rd822, {%r2116, %r2115};
	add.s64 	%rd823, %rd802, %rd822;
	xor.b64  	%rd824, %rd823, %rd762;
	mov.b64	{%r2117, %r2118}, %rd824;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd825, {%r2120, %r2119};
	add.s64 	%rd826, %rd820, %rd23;
	add.s64 	%rd827, %rd826, %rd825;
	xor.b64  	%rd828, %rd827, %rd822;
	mov.b64	{%r2121, %r2122}, %rd828;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd829, {%r2124, %r2123};
	add.s64 	%rd830, %rd829, %rd823;
	xor.b64  	%rd831, %rd830, %rd825;
	mov.b64	{%r462, %r463}, %rd831;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd832, {%r457, %r461};
	add.s64 	%rd833, %rd776, %rd21;
	add.s64 	%rd834, %rd833, %rd785;
	xor.b64  	%rd835, %rd801, %rd834;
	mov.b64	{%r2125, %r2126}, %rd835;
	mov.b64	%rd836, {%r2126, %r2125};
	add.s64 	%rd837, %rd836, %rd760;
	xor.b64  	%rd838, %rd837, %rd776;
	mov.b64	{%r2127, %r2128}, %rd838;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd839, {%r2130, %r2129};
	add.s64 	%rd840, %rd834, %rd10;
	add.s64 	%rd841, %rd840, %rd839;
	xor.b64  	%rd842, %rd841, %rd836;
	mov.b64	{%r2131, %r2132}, %rd842;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd843, {%r2134, %r2133};
	add.s64 	%rd844, %rd843, %rd837;
	xor.b64  	%rd845, %rd844, %rd839;
	mov.b64	{%r470, %r471}, %rd845;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd846, {%r465, %r469};
	add.s64 	%rd847, %rd790, %rd12;
	add.s64 	%rd848, %rd847, %rd799;
	xor.b64  	%rd849, %rd848, %rd759;
	mov.b64	{%r2135, %r2136}, %rd849;
	mov.b64	%rd850, {%r2136, %r2135};
	add.s64 	%rd851, %rd850, %rd774;
	xor.b64  	%rd852, %rd851, %rd790;
	mov.b64	{%r2137, %r2138}, %rd852;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd853, {%r2140, %r2139};
	add.s64 	%rd854, %rd848, %rd18;
	add.s64 	%rd855, %rd854, %rd853;
	xor.b64  	%rd856, %rd855, %rd850;
	mov.b64	{%r2141, %r2142}, %rd856;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd857, {%r2144, %r2143};
	add.s64 	%rd858, %rd857, %rd851;
	xor.b64  	%rd859, %rd858, %rd853;
	mov.b64	{%r478, %r479}, %rd859;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd860, {%r473, %r477};
	add.s64 	%rd861, %rd813, %rd14;
	add.s64 	%rd862, %rd861, %rd832;
	xor.b64  	%rd863, %rd857, %rd862;
	mov.b64	{%r2145, %r2146}, %rd863;
	mov.b64	%rd864, {%r2146, %r2145};
	add.s64 	%rd865, %rd864, %rd844;
	xor.b64  	%rd866, %rd865, %rd832;
	mov.b64	{%r2147, %r2148}, %rd866;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd867, {%r2150, %r2149};
	add.s64 	%rd868, %rd862, %rd9;
	add.s64 	%rd869, %rd868, %rd867;
	xor.b64  	%rd870, %rd864, %rd869;
	mov.b64	{%r2151, %r2152}, %rd870;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd871, {%r2154, %r2153};
	add.s64 	%rd872, %rd871, %rd865;
	xor.b64  	%rd873, %rd872, %rd867;
	mov.b64	{%r486, %r487}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd874, {%r481, %r485};
	add.s64 	%rd875, %rd827, %rd24;
	add.s64 	%rd876, %rd875, %rd846;
	xor.b64  	%rd877, %rd876, %rd815;
	mov.b64	{%r2155, %r2156}, %rd877;
	mov.b64	%rd878, {%r2156, %r2155};
	add.s64 	%rd879, %rd878, %rd858;
	xor.b64  	%rd880, %rd879, %rd846;
	mov.b64	{%r2157, %r2158}, %rd880;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd881, {%r2160, %r2159};
	add.s64 	%rd882, %rd876, %rd13;
	add.s64 	%rd883, %rd882, %rd881;
	xor.b64  	%rd884, %rd883, %rd878;
	mov.b64	{%r2161, %r2162}, %rd884;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd885, {%r2164, %r2163};
	add.s64 	%rd886, %rd885, %rd879;
	xor.b64  	%rd887, %rd886, %rd881;
	mov.b64	{%r494, %r495}, %rd887;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd888, {%r489, %r493};
	add.s64 	%rd889, %rd841, %rd17;
	add.s64 	%rd890, %rd889, %rd860;
	xor.b64  	%rd891, %rd890, %rd829;
	mov.b64	{%r2165, %r2166}, %rd891;
	mov.b64	%rd892, {%r2166, %r2165};
	add.s64 	%rd893, %rd892, %rd816;
	xor.b64  	%rd894, %rd893, %rd860;
	mov.b64	{%r2167, %r2168}, %rd894;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd895, {%r2170, %r2169};
	add.s64 	%rd896, %rd890, %rd15;
	add.s64 	%rd897, %rd896, %rd895;
	xor.b64  	%rd898, %rd897, %rd892;
	mov.b64	{%r2171, %r2172}, %rd898;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd899, {%r2174, %r2173};
	add.s64 	%rd900, %rd899, %rd893;
	xor.b64  	%rd901, %rd900, %rd895;
	mov.b64	{%r502, %r503}, %rd901;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd902, {%r497, %r501};
	add.s64 	%rd903, %rd818, %rd11;
	add.s64 	%rd904, %rd903, %rd855;
	xor.b64  	%rd905, %rd904, %rd843;
	mov.b64	{%r2175, %r2176}, %rd905;
	mov.b64	%rd906, {%r2176, %r2175};
	add.s64 	%rd907, %rd906, %rd830;
	xor.b64  	%rd908, %rd907, %rd818;
	mov.b64	{%r2177, %r2178}, %rd908;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd909, {%r2180, %r2179};
	add.s64 	%rd910, %rd904, %rd19;
	add.s64 	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd906;
	mov.b64	{%r2181, %r2182}, %rd912;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd913, {%r2184, %r2183};
	add.s64 	%rd914, %rd913, %rd907;
	xor.b64  	%rd915, %rd914, %rd909;
	mov.b64	{%r510, %r511}, %rd915;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd916, {%r505, %r509};
	add.s64 	%rd917, %rd869, %rd15;
	add.s64 	%rd918, %rd917, %rd916;
	xor.b64  	%rd919, %rd918, %rd885;
	mov.b64	{%r2185, %r2186}, %rd919;
	mov.b64	%rd920, {%r2186, %r2185};
	add.s64 	%rd921, %rd920, %rd900;
	xor.b64  	%rd922, %rd921, %rd916;
	mov.b64	{%r2187, %r2188}, %rd922;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd923, {%r2190, %r2189};
	add.s64 	%rd924, %rd918, %rd24;
	add.s64 	%rd925, %rd924, %rd923;
	xor.b64  	%rd926, %rd920, %rd925;
	mov.b64	{%r2191, %r2192}, %rd926;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd927, {%r2194, %r2193};
	add.s64 	%rd928, %rd921, %rd927;
	xor.b64  	%rd929, %rd928, %rd923;
	mov.b64	{%r518, %r519}, %rd929;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd930, {%r513, %r517};
	add.s64 	%rd931, %rd874, %rd23;
	add.s64 	%rd932, %rd931, %rd883;
	xor.b64  	%rd933, %rd899, %rd932;
	mov.b64	{%r2195, %r2196}, %rd933;
	mov.b64	%rd934, {%r2196, %r2195};
	add.s64 	%rd935, %rd914, %rd934;
	xor.b64  	%rd936, %rd935, %rd874;
	mov.b64	{%r2197, %r2198}, %rd936;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd937, {%r2200, %r2199};
	add.s64 	%rd938, %rd932, %rd18;
	add.s64 	%rd939, %rd938, %rd937;
	xor.b64  	%rd940, %rd939, %rd934;
	mov.b64	{%r2201, %r2202}, %rd940;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd941, {%r2204, %r2203};
	add.s64 	%rd942, %rd941, %rd935;
	xor.b64  	%rd943, %rd942, %rd937;
	mov.b64	{%r526, %r527}, %rd943;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd944, {%r521, %r525};
	add.s64 	%rd945, %rd888, %rd20;
	add.s64 	%rd946, %rd945, %rd897;
	xor.b64  	%rd947, %rd913, %rd946;
	mov.b64	{%r2205, %r2206}, %rd947;
	mov.b64	%rd948, {%r2206, %r2205};
	add.s64 	%rd949, %rd948, %rd872;
	xor.b64  	%rd950, %rd949, %rd888;
	mov.b64	{%r2207, %r2208}, %rd950;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd951, {%r2210, %r2209};
	add.s64 	%rd952, %rd946, %rd12;
	add.s64 	%rd953, %rd952, %rd951;
	xor.b64  	%rd954, %rd953, %rd948;
	mov.b64	{%r2211, %r2212}, %rd954;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd955, {%r2214, %r2213};
	add.s64 	%rd956, %rd955, %rd949;
	xor.b64  	%rd957, %rd956, %rd951;
	mov.b64	{%r534, %r535}, %rd957;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd958, {%r529, %r533};
	add.s64 	%rd959, %rd902, %rd9;
	add.s64 	%rd960, %rd959, %rd911;
	xor.b64  	%rd961, %rd960, %rd871;
	mov.b64	{%r2215, %r2216}, %rd961;
	mov.b64	%rd962, {%r2216, %r2215};
	add.s64 	%rd963, %rd962, %rd886;
	xor.b64  	%rd964, %rd963, %rd902;
	mov.b64	{%r2217, %r2218}, %rd964;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd965, {%r2220, %r2219};
	add.s64 	%rd966, %rd960, %rd17;
	add.s64 	%rd967, %rd966, %rd965;
	xor.b64  	%rd968, %rd967, %rd962;
	mov.b64	{%r2221, %r2222}, %rd968;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd969, {%r2224, %r2223};
	add.s64 	%rd970, %rd969, %rd963;
	xor.b64  	%rd971, %rd970, %rd965;
	mov.b64	{%r542, %r543}, %rd971;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd972, {%r537, %r541};
	add.s64 	%rd973, %rd925, %rd21;
	add.s64 	%rd974, %rd973, %rd944;
	xor.b64  	%rd975, %rd969, %rd974;
	mov.b64	{%r2225, %r2226}, %rd975;
	mov.b64	%rd976, {%r2226, %r2225};
	add.s64 	%rd977, %rd976, %rd956;
	xor.b64  	%rd978, %rd977, %rd944;
	mov.b64	{%r2227, %r2228}, %rd978;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd979, {%r2230, %r2229};
	add.s64 	%rd980, %rd974, %rd11;
	add.s64 	%rd981, %rd980, %rd979;
	xor.b64  	%rd982, %rd976, %rd981;
	mov.b64	{%r2231, %r2232}, %rd982;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd983, {%r2234, %r2233};
	add.s64 	%rd984, %rd983, %rd977;
	xor.b64  	%rd985, %rd984, %rd979;
	mov.b64	{%r550, %r551}, %rd985;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd986, {%r545, %r549};
	add.s64 	%rd987, %rd939, %rd22;
	add.s64 	%rd988, %rd987, %rd958;
	xor.b64  	%rd989, %rd988, %rd927;
	mov.b64	{%r2235, %r2236}, %rd989;
	mov.b64	%rd990, {%r2236, %r2235};
	add.s64 	%rd991, %rd990, %rd970;
	xor.b64  	%rd992, %rd991, %rd958;
	mov.b64	{%r2237, %r2238}, %rd992;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd993, {%r2240, %r2239};
	add.s64 	%rd994, %rd988, %rd16;
	add.s64 	%rd995, %rd994, %rd993;
	xor.b64  	%rd996, %rd995, %rd990;
	mov.b64	{%r2241, %r2242}, %rd996;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd997, {%r2244, %r2243};
	add.s64 	%rd998, %rd997, %rd991;
	xor.b64  	%rd999, %rd998, %rd993;
	mov.b64	{%r558, %r559}, %rd999;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd1000, {%r553, %r557};
	add.s64 	%rd1001, %rd953, %rd10;
	add.s64 	%rd1002, %rd1001, %rd972;
	xor.b64  	%rd1003, %rd1002, %rd941;
	mov.b64	{%r2245, %r2246}, %rd1003;
	mov.b64	%rd1004, {%r2246, %r2245};
	add.s64 	%rd1005, %rd1004, %rd928;
	xor.b64  	%rd1006, %rd1005, %rd972;
	mov.b64	{%r2247, %r2248}, %rd1006;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1007, {%r2250, %r2249};
	add.s64 	%rd1008, %rd1002, %rd13;
	add.s64 	%rd1009, %rd1008, %rd1007;
	xor.b64  	%rd1010, %rd1009, %rd1004;
	mov.b64	{%r2251, %r2252}, %rd1010;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1011, {%r2254, %r2253};
	add.s64 	%rd1012, %rd1011, %rd1005;
	xor.b64  	%rd1013, %rd1012, %rd1007;
	mov.b64	{%r566, %r567}, %rd1013;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1014, {%r561, %r565};
	add.s64 	%rd1015, %rd930, %rd19;
	add.s64 	%rd1016, %rd1015, %rd967;
	xor.b64  	%rd1017, %rd1016, %rd955;
	mov.b64	{%r2255, %r2256}, %rd1017;
	mov.b64	%rd1018, {%r2256, %r2255};
	add.s64 	%rd1019, %rd1018, %rd942;
	xor.b64  	%rd1020, %rd1019, %rd930;
	mov.b64	{%r2257, %r2258}, %rd1020;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1021, {%r2260, %r2259};
	add.s64 	%rd1022, %rd1016, %rd14;
	add.s64 	%rd1023, %rd1022, %rd1021;
	xor.b64  	%rd1024, %rd1023, %rd1018;
	mov.b64	{%r2261, %r2262}, %rd1024;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1025, {%r2264, %r2263};
	add.s64 	%rd1026, %rd1025, %rd1019;
	xor.b64  	%rd1027, %rd1026, %rd1021;
	mov.b64	{%r574, %r575}, %rd1027;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1028, {%r569, %r573};
	add.s64 	%rd1029, %rd981, %rd19;
	add.s64 	%rd1030, %rd1029, %rd1028;
	xor.b64  	%rd1031, %rd1030, %rd997;
	mov.b64	{%r2265, %r2266}, %rd1031;
	mov.b64	%rd1032, {%r2266, %r2265};
	add.s64 	%rd1033, %rd1032, %rd1012;
	xor.b64  	%rd1034, %rd1033, %rd1028;
	mov.b64	{%r2267, %r2268}, %rd1034;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1035, {%r2270, %r2269};
	add.s64 	%rd1036, %rd1030, %rd11;
	add.s64 	%rd1037, %rd1036, %rd1035;
	xor.b64  	%rd1038, %rd1032, %rd1037;
	mov.b64	{%r2271, %r2272}, %rd1038;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1039, {%r2274, %r2273};
	add.s64 	%rd1040, %rd1033, %rd1039;
	xor.b64  	%rd1041, %rd1040, %rd1035;
	mov.b64	{%r582, %r583}, %rd1041;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1042, {%r577, %r581};
	add.s64 	%rd1043, %rd986, %rd17;
	add.s64 	%rd1044, %rd1043, %rd995;
	xor.b64  	%rd1045, %rd1011, %rd1044;
	mov.b64	{%r2275, %r2276}, %rd1045;
	mov.b64	%rd1046, {%r2276, %r2275};
	add.s64 	%rd1047, %rd1026, %rd1046;
	xor.b64  	%rd1048, %rd1047, %rd986;
	mov.b64	{%r2277, %r2278}, %rd1048;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1049, {%r2280, %r2279};
	add.s64 	%rd1050, %rd1044, %rd13;
	add.s64 	%rd1051, %rd1050, %rd1049;
	xor.b64  	%rd1052, %rd1051, %rd1046;
	mov.b64	{%r2281, %r2282}, %rd1052;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1053, {%r2284, %r2283};
	add.s64 	%rd1054, %rd1053, %rd1047;
	xor.b64  	%rd1055, %rd1054, %rd1049;
	mov.b64	{%r590, %r591}, %rd1055;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1056, {%r585, %r589};
	add.s64 	%rd1057, %rd1000, %rd16;
	add.s64 	%rd1058, %rd1057, %rd1009;
	xor.b64  	%rd1059, %rd1025, %rd1058;
	mov.b64	{%r2285, %r2286}, %rd1059;
	mov.b64	%rd1060, {%r2286, %r2285};
	add.s64 	%rd1061, %rd1060, %rd984;
	xor.b64  	%rd1062, %rd1061, %rd1000;
	mov.b64	{%r2287, %r2288}, %rd1062;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1063, {%r2290, %r2289};
	add.s64 	%rd1064, %rd1058, %rd15;
	add.s64 	%rd1065, %rd1064, %rd1063;
	xor.b64  	%rd1066, %rd1065, %rd1060;
	mov.b64	{%r2291, %r2292}, %rd1066;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1067, {%r2294, %r2293};
	add.s64 	%rd1068, %rd1067, %rd1061;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r598, %r599}, %rd1069;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1070, {%r593, %r597};
	add.s64 	%rd1071, %rd1014, %rd10;
	add.s64 	%rd1072, %rd1071, %rd1023;
	xor.b64  	%rd1073, %rd1072, %rd983;
	mov.b64	{%r2295, %r2296}, %rd1073;
	mov.b64	%rd1074, {%r2296, %r2295};
	add.s64 	%rd1075, %rd1074, %rd998;
	xor.b64  	%rd1076, %rd1075, %rd1014;
	mov.b64	{%r2297, %r2298}, %rd1076;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1077, {%r2300, %r2299};
	add.s64 	%rd1078, %rd1072, %rd14;
	add.s64 	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1074;
	mov.b64	{%r2301, %r2302}, %rd1080;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1081, {%r2304, %r2303};
	add.s64 	%rd1082, %rd1081, %rd1075;
	xor.b64  	%rd1083, %rd1082, %rd1077;
	mov.b64	{%r606, %r607}, %rd1083;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1084, {%r601, %r605};
	add.s64 	%rd1085, %rd1037, %rd24;
	add.s64 	%rd1086, %rd1085, %rd1056;
	xor.b64  	%rd1087, %rd1081, %rd1086;
	mov.b64	{%r2305, %r2306}, %rd1087;
	mov.b64	%rd1088, {%r2306, %r2305};
	add.s64 	%rd1089, %rd1088, %rd1068;
	xor.b64  	%rd1090, %rd1089, %rd1056;
	mov.b64	{%r2307, %r2308}, %rd1090;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1091, {%r2310, %r2309};
	add.s64 	%rd1092, %rd1086, %rd20;
	add.s64 	%rd1093, %rd1092, %rd1091;
	xor.b64  	%rd1094, %rd1088, %rd1093;
	mov.b64	{%r2311, %r2312}, %rd1094;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1095, {%r2314, %r2313};
	add.s64 	%rd1096, %rd1095, %rd1089;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r614, %r615}, %rd1097;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1098, {%r609, %r613};
	add.s64 	%rd1099, %rd1051, %rd18;
	add.s64 	%rd1100, %rd1099, %rd1070;
	xor.b64  	%rd1101, %rd1100, %rd1039;
	mov.b64	{%r2315, %r2316}, %rd1101;
	mov.b64	%rd1102, {%r2316, %r2315};
	add.s64 	%rd1103, %rd1102, %rd1082;
	xor.b64  	%rd1104, %rd1103, %rd1070;
	mov.b64	{%r2317, %r2318}, %rd1104;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1105, {%r2320, %r2319};
	add.s64 	%rd1106, %rd1100, %rd23;
	add.s64 	%rd1107, %rd1106, %rd1105;
	xor.b64  	%rd1108, %rd1107, %rd1102;
	mov.b64	{%r2321, %r2322}, %rd1108;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1109, {%r2324, %r2323};
	add.s64 	%rd1110, %rd1109, %rd1103;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r622, %r623}, %rd1111;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1112, {%r617, %r621};
	add.s64 	%rd1113, %rd1065, %rd12;
	add.s64 	%rd1114, %rd1113, %rd1084;
	xor.b64  	%rd1115, %rd1114, %rd1053;
	mov.b64	{%r2325, %r2326}, %rd1115;
	mov.b64	%rd1116, {%r2326, %r2325};
	add.s64 	%rd1117, %rd1116, %rd1040;
	xor.b64  	%rd1118, %rd1117, %rd1084;
	mov.b64	{%r2327, %r2328}, %rd1118;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1119, {%r2330, %r2329};
	add.s64 	%rd1120, %rd1114, %rd21;
	add.s64 	%rd1121, %rd1120, %rd1119;
	xor.b64  	%rd1122, %rd1121, %rd1116;
	mov.b64	{%r2331, %r2332}, %rd1122;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1123, {%r2334, %r2333};
	add.s64 	%rd1124, %rd1123, %rd1117;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r630, %r631}, %rd1125;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1126, {%r625, %r629};
	add.s64 	%rd1127, %rd1042, %rd22;
	add.s64 	%rd1128, %rd1127, %rd1079;
	xor.b64  	%rd1129, %rd1128, %rd1067;
	mov.b64	{%r2335, %r2336}, %rd1129;
	mov.b64	%rd1130, {%r2336, %r2335};
	add.s64 	%rd1131, %rd1130, %rd1054;
	xor.b64  	%rd1132, %rd1131, %rd1042;
	mov.b64	{%r2337, %r2338}, %rd1132;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1133, {%r2340, %r2339};
	add.s64 	%rd1134, %rd1128, %rd9;
	add.s64 	%rd1135, %rd1134, %rd1133;
	xor.b64  	%rd1136, %rd1135, %rd1130;
	mov.b64	{%r2341, %r2342}, %rd1136;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1137, {%r2344, %r2343};
	add.s64 	%rd1138, %rd1137, %rd1131;
	xor.b64  	%rd1139, %rd1138, %rd1133;
	mov.b64	{%r638, %r639}, %rd1139;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1140, {%r633, %r637};
	add.s64 	%rd1141, %rd1093, %rd9;
	add.s64 	%rd1142, %rd1141, %rd1140;
	xor.b64  	%rd1143, %rd1142, %rd1109;
	mov.b64	{%r2345, %r2346}, %rd1143;
	mov.b64	%rd1144, {%r2346, %r2345};
	add.s64 	%rd1145, %rd1144, %rd1124;
	xor.b64  	%rd1146, %rd1145, %rd1140;
	mov.b64	{%r2347, %r2348}, %rd1146;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1147, {%r2350, %r2349};
	add.s64 	%rd1148, %rd1142, %rd10;
	add.s64 	%rd1149, %rd1148, %rd1147;
	xor.b64  	%rd1150, %rd1144, %rd1149;
	mov.b64	{%r2351, %r2352}, %rd1150;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1151, {%r2354, %r2353};
	add.s64 	%rd1152, %rd1145, %rd1151;
	xor.b64  	%rd1153, %rd1152, %rd1147;
	mov.b64	{%r646, %r647}, %rd1153;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1154, {%r641, %r645};
	add.s64 	%rd1155, %rd1098, %rd11;
	add.s64 	%rd1156, %rd1155, %rd1107;
	xor.b64  	%rd1157, %rd1123, %rd1156;
	mov.b64	{%r2355, %r2356}, %rd1157;
	mov.b64	%rd1158, {%r2356, %r2355};
	add.s64 	%rd1159, %rd1138, %rd1158;
	xor.b64  	%rd1160, %rd1159, %rd1098;
	mov.b64	{%r2357, %r2358}, %rd1160;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1161, {%r2360, %r2359};
	add.s64 	%rd1162, %rd1156, %rd12;
	add.s64 	%rd1163, %rd1162, %rd1161;
	xor.b64  	%rd1164, %rd1163, %rd1158;
	mov.b64	{%r2361, %r2362}, %rd1164;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1165, {%r2364, %r2363};
	add.s64 	%rd1166, %rd1165, %rd1159;
	xor.b64  	%rd1167, %rd1166, %rd1161;
	mov.b64	{%r654, %r655}, %rd1167;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1168, {%r649, %r653};
	add.s64 	%rd1169, %rd1112, %rd13;
	add.s64 	%rd1170, %rd1169, %rd1121;
	xor.b64  	%rd1171, %rd1137, %rd1170;
	mov.b64	{%r2365, %r2366}, %rd1171;
	mov.b64	%rd1172, {%r2366, %r2365};
	add.s64 	%rd1173, %rd1172, %rd1096;
	xor.b64  	%rd1174, %rd1173, %rd1112;
	mov.b64	{%r2367, %r2368}, %rd1174;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1175, {%r2370, %r2369};
	add.s64 	%rd1176, %rd1170, %rd14;
	add.s64 	%rd1177, %rd1176, %rd1175;
	xor.b64  	%rd1178, %rd1177, %rd1172;
	mov.b64	{%r2371, %r2372}, %rd1178;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1179, {%r2374, %r2373};
	add.s64 	%rd1180, %rd1179, %rd1173;
	xor.b64  	%rd1181, %rd1180, %rd1175;
	mov.b64	{%r662, %r663}, %rd1181;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1182, {%r657, %r661};
	add.s64 	%rd1183, %rd1126, %rd15;
	add.s64 	%rd1184, %rd1183, %rd1135;
	xor.b64  	%rd1185, %rd1184, %rd1095;
	mov.b64	{%r2375, %r2376}, %rd1185;
	mov.b64	%rd1186, {%r2376, %r2375};
	add.s64 	%rd1187, %rd1186, %rd1110;
	xor.b64  	%rd1188, %rd1187, %rd1126;
	mov.b64	{%r2377, %r2378}, %rd1188;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1189, {%r2380, %r2379};
	add.s64 	%rd1190, %rd1184, %rd16;
	add.s64 	%rd1191, %rd1190, %rd1189;
	xor.b64  	%rd1192, %rd1191, %rd1186;
	mov.b64	{%r2381, %r2382}, %rd1192;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1193, {%r2384, %r2383};
	add.s64 	%rd1194, %rd1193, %rd1187;
	xor.b64  	%rd1195, %rd1194, %rd1189;
	mov.b64	{%r670, %r671}, %rd1195;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1196, {%r665, %r669};
	add.s64 	%rd1197, %rd1149, %rd17;
	add.s64 	%rd1198, %rd1197, %rd1168;
	xor.b64  	%rd1199, %rd1193, %rd1198;
	mov.b64	{%r2385, %r2386}, %rd1199;
	mov.b64	%rd1200, {%r2386, %r2385};
	add.s64 	%rd1201, %rd1200, %rd1180;
	xor.b64  	%rd1202, %rd1201, %rd1168;
	mov.b64	{%r2387, %r2388}, %rd1202;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1203, {%r2390, %r2389};
	add.s64 	%rd1204, %rd1198, %rd18;
	add.s64 	%rd1205, %rd1204, %rd1203;
	xor.b64  	%rd1206, %rd1200, %rd1205;
	mov.b64	{%r2391, %r2392}, %rd1206;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1207, {%r2394, %r2393};
	add.s64 	%rd1208, %rd1207, %rd1201;
	xor.b64  	%rd1209, %rd1208, %rd1203;
	mov.b64	{%r678, %r679}, %rd1209;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1210, {%r673, %r677};
	add.s64 	%rd1211, %rd1163, %rd19;
	add.s64 	%rd1212, %rd1211, %rd1182;
	xor.b64  	%rd1213, %rd1212, %rd1151;
	mov.b64	{%r2395, %r2396}, %rd1213;
	mov.b64	%rd1214, {%r2396, %r2395};
	add.s64 	%rd1215, %rd1214, %rd1194;
	xor.b64  	%rd1216, %rd1215, %rd1182;
	mov.b64	{%r2397, %r2398}, %rd1216;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1217, {%r2400, %r2399};
	add.s64 	%rd1218, %rd1212, %rd20;
	add.s64 	%rd1219, %rd1218, %rd1217;
	xor.b64  	%rd1220, %rd1219, %rd1214;
	mov.b64	{%r2401, %r2402}, %rd1220;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1221, {%r2404, %r2403};
	add.s64 	%rd1222, %rd1221, %rd1215;
	xor.b64  	%rd1223, %rd1222, %rd1217;
	mov.b64	{%r686, %r687}, %rd1223;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1224, {%r681, %r685};
	add.s64 	%rd1225, %rd1177, %rd21;
	add.s64 	%rd1226, %rd1225, %rd1196;
	xor.b64  	%rd1227, %rd1226, %rd1165;
	mov.b64	{%r2405, %r2406}, %rd1227;
	mov.b64	%rd1228, {%r2406, %r2405};
	add.s64 	%rd1229, %rd1228, %rd1152;
	xor.b64  	%rd1230, %rd1229, %rd1196;
	mov.b64	{%r2407, %r2408}, %rd1230;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1231, {%r2410, %r2409};
	add.s64 	%rd1232, %rd1226, %rd22;
	add.s64 	%rd1233, %rd1232, %rd1231;
	xor.b64  	%rd1234, %rd1233, %rd1228;
	mov.b64	{%r2411, %r2412}, %rd1234;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1235, {%r2414, %r2413};
	add.s64 	%rd1236, %rd1235, %rd1229;
	xor.b64  	%rd1237, %rd1236, %rd1231;
	mov.b64	{%r694, %r695}, %rd1237;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1238, {%r689, %r693};
	add.s64 	%rd1239, %rd1154, %rd23;
	add.s64 	%rd1240, %rd1239, %rd1191;
	xor.b64  	%rd1241, %rd1240, %rd1179;
	mov.b64	{%r2415, %r2416}, %rd1241;
	mov.b64	%rd1242, {%r2416, %r2415};
	add.s64 	%rd1243, %rd1242, %rd1166;
	xor.b64  	%rd1244, %rd1243, %rd1154;
	mov.b64	{%r2417, %r2418}, %rd1244;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1245, {%r2420, %r2419};
	add.s64 	%rd1246, %rd1240, %rd24;
	add.s64 	%rd1247, %rd1246, %rd1245;
	xor.b64  	%rd1248, %rd1247, %rd1242;
	mov.b64	{%r2421, %r2422}, %rd1248;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1249, {%r2424, %r2423};
	add.s64 	%rd1250, %rd1249, %rd1243;
	xor.b64  	%rd1251, %rd1250, %rd1245;
	mov.b64	{%r702, %r703}, %rd1251;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1252, {%r697, %r701};
	add.s64 	%rd1253, %rd1205, %rd23;
	add.s64 	%rd1254, %rd1253, %rd1252;
	xor.b64  	%rd1255, %rd1254, %rd1221;
	mov.b64	{%r2425, %r2426}, %rd1255;
	mov.b64	%rd1256, {%r2426, %r2425};
	add.s64 	%rd1257, %rd1256, %rd1236;
	xor.b64  	%rd1258, %rd1257, %rd1252;
	mov.b64	{%r2427, %r2428}, %rd1258;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1259, {%r2430, %r2429};
	add.s64 	%rd1260, %rd1254, %rd19;
	add.s64 	%rd1261, %rd1260, %rd1259;
	xor.b64  	%rd1262, %rd1256, %rd1261;
	mov.b64	{%r2431, %r2432}, %rd1262;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1263, {%r2434, %r2433};
	add.s64 	%rd1264, %rd1257, %rd1263;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r710, %r711}, %rd1265;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1266, {%r705, %r709};
	add.s64 	%rd1267, %rd1210, %rd13;
	add.s64 	%rd1268, %rd1267, %rd1219;
	xor.b64  	%rd1269, %rd1235, %rd1268;
	mov.b64	{%r2435, %r2436}, %rd1269;
	mov.b64	%rd1270, {%r2436, %r2435};
	add.s64 	%rd1271, %rd1250, %rd1270;
	xor.b64  	%rd1272, %rd1271, %rd1210;
	mov.b64	{%r2437, %r2438}, %rd1272;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1273, {%r2440, %r2439};
	add.s64 	%rd1274, %rd1268, %rd17;
	add.s64 	%rd1275, %rd1274, %rd1273;
	xor.b64  	%rd1276, %rd1275, %rd1270;
	mov.b64	{%r2441, %r2442}, %rd1276;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1277, {%r2444, %r2443};
	add.s64 	%rd1278, %rd1277, %rd1271;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r718, %r719}, %rd1279;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1280, {%r713, %r717};
	add.s64 	%rd1281, %rd1224, %rd18;
	add.s64 	%rd1282, %rd1281, %rd1233;
	xor.b64  	%rd1283, %rd1249, %rd1282;
	mov.b64	{%r2445, %r2446}, %rd1283;
	mov.b64	%rd1284, {%r2446, %r2445};
	add.s64 	%rd1285, %rd1284, %rd1208;
	xor.b64  	%rd1286, %rd1285, %rd1224;
	mov.b64	{%r2447, %r2448}, %rd1286;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1287, {%r2450, %r2449};
	add.s64 	%rd1288, %rd1282, %rd24;
	add.s64 	%rd1289, %rd1288, %rd1287;
	xor.b64  	%rd1290, %rd1289, %rd1284;
	mov.b64	{%r2451, %r2452}, %rd1290;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1291, {%r2454, %r2453};
	add.s64 	%rd1292, %rd1291, %rd1285;
	xor.b64  	%rd1293, %rd1292, %rd1287;
	mov.b64	{%r726, %r727}, %rd1293;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1294, {%r721, %r725};
	add.s64 	%rd1295, %rd1238, %rd22;
	add.s64 	%rd1296, %rd1295, %rd1247;
	xor.b64  	%rd1297, %rd1296, %rd1207;
	mov.b64	{%r2455, %r2456}, %rd1297;
	mov.b64	%rd1298, {%r2456, %r2455};
	add.s64 	%rd1299, %rd1298, %rd1222;
	xor.b64  	%rd1300, %rd1299, %rd1238;
	mov.b64	{%r2457, %r2458}, %rd1300;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1301, {%r2460, %r2459};
	add.s64 	%rd1302, %rd1296, %rd15;
	add.s64 	%rd1303, %rd1302, %rd1301;
	xor.b64  	%rd1304, %rd1303, %rd1298;
	mov.b64	{%r2461, %r2462}, %rd1304;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1305, {%r2464, %r2463};
	add.s64 	%rd1306, %rd1305, %rd1299;
	xor.b64  	%rd1307, %rd1306, %rd1301;
	mov.b64	{%r734, %r735}, %rd1307;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1308, {%r729, %r733};
	add.s64 	%rd1309, %rd1261, %rd10;
	add.s64 	%rd1310, %rd1309, %rd1280;
	xor.b64  	%rd1311, %rd1305, %rd1310;
	mov.b64	{%r2465, %r2466}, %rd1311;
	mov.b64	%rd1312, {%r2466, %r2465};
	add.s64 	%rd1313, %rd1312, %rd1292;
	xor.b64  	%rd1314, %rd1313, %rd1280;
	mov.b64	{%r2467, %r2468}, %rd1314;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1315, {%r2470, %r2469};
	add.s64 	%rd1316, %rd1310, %rd21;
	add.s64 	%rd1317, %rd1316, %rd1315;
	xor.b64  	%rd1318, %rd1312, %rd1317;
	mov.b64	{%r2471, %r2472}, %rd1318;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1319, {%r2474, %r2473};
	add.s64 	%rd1320, %rd1319, %rd1313;
	xor.b64  	%rd1321, %rd1320, %rd1315;
	mov.b64	{%r742, %r743}, %rd1321;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1322, {%r737, %r741};
	add.s64 	%rd1323, %rd1275, %rd9;
	add.s64 	%rd1324, %rd1323, %rd1294;
	xor.b64  	%rd1325, %rd1324, %rd1263;
	mov.b64	{%r2475, %r2476}, %rd1325;
	mov.b64	%rd1326, {%r2476, %r2475};
	add.s64 	%rd1327, %rd1326, %rd1306;
	xor.b64  	%rd1328, %rd1327, %rd1294;
	mov.b64	{%r2477, %r2478}, %rd1328;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1329, {%r2480, %r2479};
	add.s64 	%rd1330, %rd1324, %rd11;
	add.s64 	%rd1331, %rd1330, %rd1329;
	xor.b64  	%rd1332, %rd1331, %rd1326;
	mov.b64	{%r2481, %r2482}, %rd1332;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1333, {%r2484, %r2483};
	add.s64 	%rd1334, %rd1333, %rd1327;
	xor.b64  	%rd1335, %rd1334, %rd1329;
	mov.b64	{%r750, %r751}, %rd1335;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1336, {%r745, %r749};
	add.s64 	%rd1337, %rd1289, %rd20;
	add.s64 	%rd1338, %rd1337, %rd1308;
	xor.b64  	%rd1339, %rd1338, %rd1277;
	mov.b64	{%r2485, %r2486}, %rd1339;
	mov.b64	%rd1340, {%r2486, %r2485};
	add.s64 	%rd1341, %rd1340, %rd1264;
	xor.b64  	%rd1342, %rd1341, %rd1308;
	mov.b64	{%r2487, %r2488}, %rd1342;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1343, {%r2490, %r2489};
	add.s64 	%rd1344, %rd1338, %rd16;
	add.s64 	%rd1345, %rd1344, %rd1343;
	xor.b64  	%rd1346, %rd1345, %rd1340;
	mov.b64	{%r2491, %r2492}, %rd1346;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1347, {%r2494, %r2493};
	add.s64 	%rd1348, %rd1347, %rd1341;
	xor.b64  	%rd1349, %rd1348, %rd1343;
	mov.b64	{%r758, %r759}, %rd1349;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1350, {%r753, %r757};
	add.s64 	%rd1351, %rd1266, %rd14;
	add.s64 	%rd1352, %rd1351, %rd1303;
	xor.b64  	%rd1353, %rd1352, %rd1291;
	mov.b64	{%r2495, %r2496}, %rd1353;
	mov.b64	%rd1354, {%r2496, %r2495};
	add.s64 	%rd1355, %rd1354, %rd1278;
	xor.b64  	%rd1356, %rd1355, %rd1266;
	mov.b64	{%r2497, %r2498}, %rd1356;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1357, {%r2500, %r2499};
	add.s64 	%rd1358, %rd1352, %rd12;
	add.s64 	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1354;
	mov.b64	{%r2501, %r2502}, %rd1360;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1361, {%r2504, %r2503};
	add.s64 	%rd1362, %rd1361, %rd1355;
	xor.b64  	%rd1363, %rd1362, %rd1357;
	mov.b64	{%r766, %r767}, %rd1363;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1364, {%r761, %r765};
	xor.b64  	%rd1365, %rd1317, %rd1348;
	xor.b64  	%rd1366, %rd1365, 7640891576939301192;
	xor.b64  	%rd1367, %rd1331, %rd1362;
	xor.b64  	%rd1368, %rd1367, -4942790177534073029;
	xor.b64  	%rd1369, %rd1320, %rd1345;
	xor.b64  	%rd1370, %rd1369, 4354685564936845355;
	xor.b64  	%rd1371, %rd1334, %rd1359;
	xor.b64  	%rd1372, %rd1371, -6534734903238641935;
	xor.b64  	%rd1373, %rd1333, %rd1364;
	xor.b64  	%rd1374, %rd1373, 5840696475078001361;
	xor.b64  	%rd1375, %rd1322, %rd1347;
	xor.b64  	%rd1376, %rd1375, -7276294671716946913;
	xor.b64  	%rd1377, %rd1336, %rd1361;
	xor.b64  	%rd1378, %rd1377, 2270897969802886507;
	xor.b64  	%rd1379, %rd1319, %rd1350;
	xor.b64  	%rd1380, %rd1379, 6620516959819538809;
	ld.global.u64 	%rd1381, [%rd5+128];
	ld.global.u64 	%rd1382, [%rd5+136];
	ld.global.u64 	%rd1383, [%rd5+144];
	ld.global.u64 	%rd1384, [%rd5+152];
	ld.global.u64 	%rd1385, [%rd5+160];
	ld.global.u64 	%rd1386, [%rd5+168];
	ld.global.u64 	%rd1387, [%rd5+176];
	ld.global.u64 	%rd1388, [%rd5+184];
	ld.global.u64 	%rd1389, [%rd5+192];
	ld.global.u64 	%rd1390, [%rd5+200];
	ld.global.u64 	%rd1391, [%rd5+208];
	ld.global.u64 	%rd1392, [%rd5+216];
	ld.global.u64 	%rd1393, [%rd5+224];
	ld.global.u64 	%rd1394, [%rd5+232];
	ld.global.u64 	%rd1395, [%rd5+240];
	ld.global.u64 	%rd1396, [%rd5+248];
	add.s64 	%rd1397, %rd1381, %rd1366;
	add.s64 	%rd1398, %rd1397, %rd1374;
	xor.b64  	%rd1399, %rd1398, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1399;
	mov.b64	%rd1400, {%r2506, %r2505};
	add.s64 	%rd1401, %rd1400, 7640891576956012808;
	xor.b64  	%rd1402, %rd1401, %rd1374;
	mov.b64	{%r2507, %r2508}, %rd1402;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1403, {%r2510, %r2509};
	add.s64 	%rd1404, %rd1398, %rd1382;
	add.s64 	%rd1405, %rd1404, %rd1403;
	xor.b64  	%rd1406, %rd1405, %rd1400;
	mov.b64	{%r2511, %r2512}, %rd1406;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1407, {%r2514, %r2513};
	add.s64 	%rd1408, %rd1407, %rd1401;
	xor.b64  	%rd1409, %rd1408, %rd1403;
	mov.b64	{%r774, %r775}, %rd1409;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1410, {%r769, %r773};
	add.s64 	%rd1411, %rd1368, %rd1376;
	add.s64 	%rd1412, %rd1411, %rd1383;
	xor.b64  	%rd1413, %rd1412, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1413;
	mov.b64	%rd1414, {%r2516, %r2515};
	add.s64 	%rd1415, %rd1414, -4942790177534073029;
	xor.b64  	%rd1416, %rd1415, %rd1376;
	mov.b64	{%r2517, %r2518}, %rd1416;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1417, {%r2520, %r2519};
	add.s64 	%rd1418, %rd1412, %rd1384;
	add.s64 	%rd1419, %rd1418, %rd1417;
	xor.b64  	%rd1420, %rd1419, %rd1414;
	mov.b64	{%r2521, %r2522}, %rd1420;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1421, {%r2524, %r2523};
	add.s64 	%rd1422, %rd1421, %rd1415;
	xor.b64  	%rd1423, %rd1422, %rd1417;
	mov.b64	{%r782, %r783}, %rd1423;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1424, {%r777, %r781};
	add.s64 	%rd1425, %rd1378, %rd1370;
	add.s64 	%rd1426, %rd1425, %rd1385;
	xor.b64  	%rd1427, %rd1426, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1427;
	mov.b64	%rd1428, {%r2526, %r2525};
	add.s64 	%rd1429, %rd1428, 4354685564936845355;
	xor.b64  	%rd1430, %rd1429, %rd1378;
	mov.b64	{%r2527, %r2528}, %rd1430;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1431, {%r2530, %r2529};
	add.s64 	%rd1432, %rd1386, %rd1426;
	add.s64 	%rd1433, %rd1432, %rd1431;
	xor.b64  	%rd1434, %rd1433, %rd1428;
	mov.b64	{%r2531, %r2532}, %rd1434;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1435, {%r2534, %r2533};
	add.s64 	%rd1436, %rd1435, %rd1429;
	xor.b64  	%rd1437, %rd1436, %rd1431;
	mov.b64	{%r790, %r791}, %rd1437;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1438, {%r785, %r789};
	add.s64 	%rd1439, %rd1372, %rd1380;
	add.s64 	%rd1440, %rd1439, %rd1387;
	xor.b64  	%rd1441, %rd1440, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1441;
	mov.b64	%rd1442, {%r2536, %r2535};
	add.s64 	%rd1443, %rd1442, -6534734903238641935;
	xor.b64  	%rd1444, %rd1443, %rd1380;
	mov.b64	{%r2537, %r2538}, %rd1444;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1445, {%r2540, %r2539};
	add.s64 	%rd1446, %rd1388, %rd1440;
	add.s64 	%rd1447, %rd1446, %rd1445;
	xor.b64  	%rd1448, %rd1447, %rd1442;
	mov.b64	{%r2541, %r2542}, %rd1448;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1449, {%r2544, %r2543};
	add.s64 	%rd1450, %rd1449, %rd1443;
	xor.b64  	%rd1451, %rd1450, %rd1445;
	mov.b64	{%r798, %r799}, %rd1451;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1452, {%r793, %r797};
	add.s64 	%rd1453, %rd1405, %rd1389;
	add.s64 	%rd1454, %rd1453, %rd1424;
	xor.b64  	%rd1455, %rd1449, %rd1454;
	mov.b64	{%r2545, %r2546}, %rd1455;
	mov.b64	%rd1456, {%r2546, %r2545};
	add.s64 	%rd1457, %rd1456, %rd1436;
	xor.b64  	%rd1458, %rd1457, %rd1424;
	mov.b64	{%r2547, %r2548}, %rd1458;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1459, {%r2550, %r2549};
	add.s64 	%rd1460, %rd1454, %rd1390;
	add.s64 	%rd1461, %rd1460, %rd1459;
	xor.b64  	%rd1462, %rd1456, %rd1461;
	mov.b64	{%r2551, %r2552}, %rd1462;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1463, {%r2554, %r2553};
	add.s64 	%rd1464, %rd1463, %rd1457;
	xor.b64  	%rd1465, %rd1464, %rd1459;
	mov.b64	{%r806, %r807}, %rd1465;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1466, {%r801, %r805};
	add.s64 	%rd1467, %rd1419, %rd1391;
	add.s64 	%rd1468, %rd1467, %rd1438;
	xor.b64  	%rd1469, %rd1468, %rd1407;
	mov.b64	{%r2555, %r2556}, %rd1469;
	mov.b64	%rd1470, {%r2556, %r2555};
	add.s64 	%rd1471, %rd1470, %rd1450;
	xor.b64  	%rd1472, %rd1471, %rd1438;
	mov.b64	{%r2557, %r2558}, %rd1472;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1473, {%r2560, %r2559};
	add.s64 	%rd1474, %rd1468, %rd1392;
	add.s64 	%rd1475, %rd1474, %rd1473;
	xor.b64  	%rd1476, %rd1475, %rd1470;
	mov.b64	{%r2561, %r2562}, %rd1476;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1477, {%r2564, %r2563};
	add.s64 	%rd1478, %rd1477, %rd1471;
	xor.b64  	%rd1479, %rd1478, %rd1473;
	mov.b64	{%r814, %r815}, %rd1479;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1480, {%r809, %r813};
	add.s64 	%rd1481, %rd1433, %rd1393;
	add.s64 	%rd1482, %rd1481, %rd1452;
	xor.b64  	%rd1483, %rd1482, %rd1421;
	mov.b64	{%r2565, %r2566}, %rd1483;
	mov.b64	%rd1484, {%r2566, %r2565};
	add.s64 	%rd1485, %rd1484, %rd1408;
	xor.b64  	%rd1486, %rd1485, %rd1452;
	mov.b64	{%r2567, %r2568}, %rd1486;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1487, {%r2570, %r2569};
	add.s64 	%rd1488, %rd1482, %rd1394;
	add.s64 	%rd1489, %rd1488, %rd1487;
	xor.b64  	%rd1490, %rd1489, %rd1484;
	mov.b64	{%r2571, %r2572}, %rd1490;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1491, {%r2574, %r2573};
	add.s64 	%rd1492, %rd1491, %rd1485;
	xor.b64  	%rd1493, %rd1492, %rd1487;
	mov.b64	{%r822, %r823}, %rd1493;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1494, {%r817, %r821};
	add.s64 	%rd1495, %rd1410, %rd1395;
	add.s64 	%rd1496, %rd1495, %rd1447;
	xor.b64  	%rd1497, %rd1496, %rd1435;
	mov.b64	{%r2575, %r2576}, %rd1497;
	mov.b64	%rd1498, {%r2576, %r2575};
	add.s64 	%rd1499, %rd1498, %rd1422;
	xor.b64  	%rd1500, %rd1499, %rd1410;
	mov.b64	{%r2577, %r2578}, %rd1500;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1501, {%r2580, %r2579};
	add.s64 	%rd1502, %rd1496, %rd1396;
	add.s64 	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1498;
	mov.b64	{%r2581, %r2582}, %rd1504;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1505, {%r2584, %r2583};
	add.s64 	%rd1506, %rd1505, %rd1499;
	xor.b64  	%rd1507, %rd1506, %rd1501;
	mov.b64	{%r830, %r831}, %rd1507;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1508, {%r825, %r829};
	add.s64 	%rd1509, %rd1461, %rd1395;
	add.s64 	%rd1510, %rd1509, %rd1508;
	xor.b64  	%rd1511, %rd1510, %rd1477;
	mov.b64	{%r2585, %r2586}, %rd1511;
	mov.b64	%rd1512, {%r2586, %r2585};
	add.s64 	%rd1513, %rd1512, %rd1492;
	xor.b64  	%rd1514, %rd1513, %rd1508;
	mov.b64	{%r2587, %r2588}, %rd1514;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1515, {%r2590, %r2589};
	add.s64 	%rd1516, %rd1510, %rd1391;
	add.s64 	%rd1517, %rd1516, %rd1515;
	xor.b64  	%rd1518, %rd1512, %rd1517;
	mov.b64	{%r2591, %r2592}, %rd1518;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1519, {%r2594, %r2593};
	add.s64 	%rd1520, %rd1513, %rd1519;
	xor.b64  	%rd1521, %rd1520, %rd1515;
	mov.b64	{%r838, %r839}, %rd1521;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1522, {%r833, %r837};
	add.s64 	%rd1523, %rd1466, %rd1385;
	add.s64 	%rd1524, %rd1523, %rd1475;
	xor.b64  	%rd1525, %rd1491, %rd1524;
	mov.b64	{%r2595, %r2596}, %rd1525;
	mov.b64	%rd1526, {%r2596, %r2595};
	add.s64 	%rd1527, %rd1506, %rd1526;
	xor.b64  	%rd1528, %rd1527, %rd1466;
	mov.b64	{%r2597, %r2598}, %rd1528;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1529, {%r2600, %r2599};
	add.s64 	%rd1530, %rd1524, %rd1389;
	add.s64 	%rd1531, %rd1530, %rd1529;
	xor.b64  	%rd1532, %rd1531, %rd1526;
	mov.b64	{%r2601, %r2602}, %rd1532;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1533, {%r2604, %r2603};
	add.s64 	%rd1534, %rd1533, %rd1527;
	xor.b64  	%rd1535, %rd1534, %rd1529;
	mov.b64	{%r846, %r847}, %rd1535;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1536, {%r841, %r845};
	add.s64 	%rd1537, %rd1480, %rd1390;
	add.s64 	%rd1538, %rd1537, %rd1489;
	xor.b64  	%rd1539, %rd1505, %rd1538;
	mov.b64	{%r2605, %r2606}, %rd1539;
	mov.b64	%rd1540, {%r2606, %r2605};
	add.s64 	%rd1541, %rd1540, %rd1464;
	xor.b64  	%rd1542, %rd1541, %rd1480;
	mov.b64	{%r2607, %r2608}, %rd1542;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1543, {%r2610, %r2609};
	add.s64 	%rd1544, %rd1538, %rd1396;
	add.s64 	%rd1545, %rd1544, %rd1543;
	xor.b64  	%rd1546, %rd1545, %rd1540;
	mov.b64	{%r2611, %r2612}, %rd1546;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1547, {%r2614, %r2613};
	add.s64 	%rd1548, %rd1547, %rd1541;
	xor.b64  	%rd1549, %rd1548, %rd1543;
	mov.b64	{%r854, %r855}, %rd1549;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1550, {%r849, %r853};
	add.s64 	%rd1551, %rd1494, %rd1394;
	add.s64 	%rd1552, %rd1551, %rd1503;
	xor.b64  	%rd1553, %rd1552, %rd1463;
	mov.b64	{%r2615, %r2616}, %rd1553;
	mov.b64	%rd1554, {%r2616, %r2615};
	add.s64 	%rd1555, %rd1554, %rd1478;
	xor.b64  	%rd1556, %rd1555, %rd1494;
	mov.b64	{%r2617, %r2618}, %rd1556;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1557, {%r2620, %r2619};
	add.s64 	%rd1558, %rd1552, %rd1387;
	add.s64 	%rd1559, %rd1558, %rd1557;
	xor.b64  	%rd1560, %rd1559, %rd1554;
	mov.b64	{%r2621, %r2622}, %rd1560;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1561, {%r2624, %r2623};
	add.s64 	%rd1562, %rd1561, %rd1555;
	xor.b64  	%rd1563, %rd1562, %rd1557;
	mov.b64	{%r862, %r863}, %rd1563;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1564, {%r857, %r861};
	add.s64 	%rd1565, %rd1517, %rd1382;
	add.s64 	%rd1566, %rd1565, %rd1536;
	xor.b64  	%rd1567, %rd1561, %rd1566;
	mov.b64	{%r2625, %r2626}, %rd1567;
	mov.b64	%rd1568, {%r2626, %r2625};
	add.s64 	%rd1569, %rd1568, %rd1548;
	xor.b64  	%rd1570, %rd1569, %rd1536;
	mov.b64	{%r2627, %r2628}, %rd1570;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1571, {%r2630, %r2629};
	add.s64 	%rd1572, %rd1566, %rd1393;
	add.s64 	%rd1573, %rd1572, %rd1571;
	xor.b64  	%rd1574, %rd1568, %rd1573;
	mov.b64	{%r2631, %r2632}, %rd1574;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1575, {%r2634, %r2633};
	add.s64 	%rd1576, %rd1575, %rd1569;
	xor.b64  	%rd1577, %rd1576, %rd1571;
	mov.b64	{%r870, %r871}, %rd1577;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1578, {%r865, %r869};
	add.s64 	%rd1579, %rd1531, %rd1381;
	add.s64 	%rd1580, %rd1579, %rd1550;
	xor.b64  	%rd1581, %rd1580, %rd1519;
	mov.b64	{%r2635, %r2636}, %rd1581;
	mov.b64	%rd1582, {%r2636, %r2635};
	add.s64 	%rd1583, %rd1582, %rd1562;
	xor.b64  	%rd1584, %rd1583, %rd1550;
	mov.b64	{%r2637, %r2638}, %rd1584;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1585, {%r2640, %r2639};
	add.s64 	%rd1586, %rd1580, %rd1383;
	add.s64 	%rd1587, %rd1586, %rd1585;
	xor.b64  	%rd1588, %rd1587, %rd1582;
	mov.b64	{%r2641, %r2642}, %rd1588;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1589, {%r2644, %r2643};
	add.s64 	%rd1590, %rd1589, %rd1583;
	xor.b64  	%rd1591, %rd1590, %rd1585;
	mov.b64	{%r878, %r879}, %rd1591;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1592, {%r873, %r877};
	add.s64 	%rd1593, %rd1545, %rd1392;
	add.s64 	%rd1594, %rd1593, %rd1564;
	xor.b64  	%rd1595, %rd1594, %rd1533;
	mov.b64	{%r2645, %r2646}, %rd1595;
	mov.b64	%rd1596, {%r2646, %r2645};
	add.s64 	%rd1597, %rd1596, %rd1520;
	xor.b64  	%rd1598, %rd1597, %rd1564;
	mov.b64	{%r2647, %r2648}, %rd1598;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1599, {%r2650, %r2649};
	add.s64 	%rd1600, %rd1594, %rd1388;
	add.s64 	%rd1601, %rd1600, %rd1599;
	xor.b64  	%rd1602, %rd1601, %rd1596;
	mov.b64	{%r2651, %r2652}, %rd1602;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1603, {%r2654, %r2653};
	add.s64 	%rd1604, %rd1603, %rd1597;
	xor.b64  	%rd1605, %rd1604, %rd1599;
	mov.b64	{%r886, %r887}, %rd1605;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1606, {%r881, %r885};
	add.s64 	%rd1607, %rd1522, %rd1386;
	add.s64 	%rd1608, %rd1607, %rd1559;
	xor.b64  	%rd1609, %rd1608, %rd1547;
	mov.b64	{%r2655, %r2656}, %rd1609;
	mov.b64	%rd1610, {%r2656, %r2655};
	add.s64 	%rd1611, %rd1610, %rd1534;
	xor.b64  	%rd1612, %rd1611, %rd1522;
	mov.b64	{%r2657, %r2658}, %rd1612;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1613, {%r2660, %r2659};
	add.s64 	%rd1614, %rd1608, %rd1384;
	add.s64 	%rd1615, %rd1614, %rd1613;
	xor.b64  	%rd1616, %rd1615, %rd1610;
	mov.b64	{%r2661, %r2662}, %rd1616;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1617, {%r2664, %r2663};
	add.s64 	%rd1618, %rd1617, %rd1611;
	xor.b64  	%rd1619, %rd1618, %rd1613;
	mov.b64	{%r894, %r895}, %rd1619;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1620, {%r889, %r893};
	add.s64 	%rd1621, %rd1573, %rd1392;
	add.s64 	%rd1622, %rd1621, %rd1620;
	xor.b64  	%rd1623, %rd1622, %rd1589;
	mov.b64	{%r2665, %r2666}, %rd1623;
	mov.b64	%rd1624, {%r2666, %r2665};
	add.s64 	%rd1625, %rd1624, %rd1604;
	xor.b64  	%rd1626, %rd1625, %rd1620;
	mov.b64	{%r2667, %r2668}, %rd1626;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1627, {%r2670, %r2669};
	add.s64 	%rd1628, %rd1622, %rd1389;
	add.s64 	%rd1629, %rd1628, %rd1627;
	xor.b64  	%rd1630, %rd1624, %rd1629;
	mov.b64	{%r2671, %r2672}, %rd1630;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1631, {%r2674, %r2673};
	add.s64 	%rd1632, %rd1625, %rd1631;
	xor.b64  	%rd1633, %rd1632, %rd1627;
	mov.b64	{%r902, %r903}, %rd1633;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1634, {%r897, %r901};
	add.s64 	%rd1635, %rd1578, %rd1393;
	add.s64 	%rd1636, %rd1635, %rd1587;
	xor.b64  	%rd1637, %rd1603, %rd1636;
	mov.b64	{%r2675, %r2676}, %rd1637;
	mov.b64	%rd1638, {%r2676, %r2675};
	add.s64 	%rd1639, %rd1618, %rd1638;
	xor.b64  	%rd1640, %rd1639, %rd1578;
	mov.b64	{%r2677, %r2678}, %rd1640;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1641, {%r2680, %r2679};
	add.s64 	%rd1642, %rd1636, %rd1381;
	add.s64 	%rd1643, %rd1642, %rd1641;
	xor.b64  	%rd1644, %rd1643, %rd1638;
	mov.b64	{%r2681, %r2682}, %rd1644;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1645, {%r2684, %r2683};
	add.s64 	%rd1646, %rd1645, %rd1639;
	xor.b64  	%rd1647, %rd1646, %rd1641;
	mov.b64	{%r910, %r911}, %rd1647;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1648, {%r905, %r909};
	add.s64 	%rd1649, %rd1592, %rd1386;
	add.s64 	%rd1650, %rd1649, %rd1601;
	xor.b64  	%rd1651, %rd1617, %rd1650;
	mov.b64	{%r2685, %r2686}, %rd1651;
	mov.b64	%rd1652, {%r2686, %r2685};
	add.s64 	%rd1653, %rd1652, %rd1576;
	xor.b64  	%rd1654, %rd1653, %rd1592;
	mov.b64	{%r2687, %r2688}, %rd1654;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1655, {%r2690, %r2689};
	add.s64 	%rd1656, %rd1650, %rd1383;
	add.s64 	%rd1657, %rd1656, %rd1655;
	xor.b64  	%rd1658, %rd1657, %rd1652;
	mov.b64	{%r2691, %r2692}, %rd1658;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1659, {%r2694, %r2693};
	add.s64 	%rd1660, %rd1659, %rd1653;
	xor.b64  	%rd1661, %rd1660, %rd1655;
	mov.b64	{%r918, %r919}, %rd1661;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1662, {%r913, %r917};
	add.s64 	%rd1663, %rd1606, %rd1396;
	add.s64 	%rd1664, %rd1663, %rd1615;
	xor.b64  	%rd1665, %rd1664, %rd1575;
	mov.b64	{%r2695, %r2696}, %rd1665;
	mov.b64	%rd1666, {%r2696, %r2695};
	add.s64 	%rd1667, %rd1666, %rd1590;
	xor.b64  	%rd1668, %rd1667, %rd1606;
	mov.b64	{%r2697, %r2698}, %rd1668;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1669, {%r2700, %r2699};
	add.s64 	%rd1670, %rd1664, %rd1394;
	add.s64 	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1666;
	mov.b64	{%r2701, %r2702}, %rd1672;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1673, {%r2704, %r2703};
	add.s64 	%rd1674, %rd1673, %rd1667;
	xor.b64  	%rd1675, %rd1674, %rd1669;
	mov.b64	{%r926, %r927}, %rd1675;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1676, {%r921, %r925};
	add.s64 	%rd1677, %rd1629, %rd1391;
	add.s64 	%rd1678, %rd1677, %rd1648;
	xor.b64  	%rd1679, %rd1673, %rd1678;
	mov.b64	{%r2705, %r2706}, %rd1679;
	mov.b64	%rd1680, {%r2706, %r2705};
	add.s64 	%rd1681, %rd1680, %rd1660;
	xor.b64  	%rd1682, %rd1681, %rd1648;
	mov.b64	{%r2707, %r2708}, %rd1682;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1683, {%r2710, %r2709};
	add.s64 	%rd1684, %rd1678, %rd1395;
	add.s64 	%rd1685, %rd1684, %rd1683;
	xor.b64  	%rd1686, %rd1680, %rd1685;
	mov.b64	{%r2711, %r2712}, %rd1686;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1687, {%r2714, %r2713};
	add.s64 	%rd1688, %rd1687, %rd1681;
	xor.b64  	%rd1689, %rd1688, %rd1683;
	mov.b64	{%r934, %r935}, %rd1689;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1690, {%r929, %r933};
	add.s64 	%rd1691, %rd1643, %rd1384;
	add.s64 	%rd1692, %rd1691, %rd1662;
	xor.b64  	%rd1693, %rd1692, %rd1631;
	mov.b64	{%r2715, %r2716}, %rd1693;
	mov.b64	%rd1694, {%r2716, %r2715};
	add.s64 	%rd1695, %rd1694, %rd1674;
	xor.b64  	%rd1696, %rd1695, %rd1662;
	mov.b64	{%r2717, %r2718}, %rd1696;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1697, {%r2720, %r2719};
	add.s64 	%rd1698, %rd1692, %rd1387;
	add.s64 	%rd1699, %rd1698, %rd1697;
	xor.b64  	%rd1700, %rd1699, %rd1694;
	mov.b64	{%r2721, %r2722}, %rd1700;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1701, {%r2724, %r2723};
	add.s64 	%rd1702, %rd1701, %rd1695;
	xor.b64  	%rd1703, %rd1702, %rd1697;
	mov.b64	{%r942, %r943}, %rd1703;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1704, {%r937, %r941};
	add.s64 	%rd1705, %rd1657, %rd1388;
	add.s64 	%rd1706, %rd1705, %rd1676;
	xor.b64  	%rd1707, %rd1706, %rd1645;
	mov.b64	{%r2725, %r2726}, %rd1707;
	mov.b64	%rd1708, {%r2726, %r2725};
	add.s64 	%rd1709, %rd1708, %rd1632;
	xor.b64  	%rd1710, %rd1709, %rd1676;
	mov.b64	{%r2727, %r2728}, %rd1710;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1711, {%r2730, %r2729};
	add.s64 	%rd1712, %rd1706, %rd1382;
	add.s64 	%rd1713, %rd1712, %rd1711;
	xor.b64  	%rd1714, %rd1713, %rd1708;
	mov.b64	{%r2731, %r2732}, %rd1714;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1715, {%r2734, %r2733};
	add.s64 	%rd1716, %rd1715, %rd1709;
	xor.b64  	%rd1717, %rd1716, %rd1711;
	mov.b64	{%r950, %r951}, %rd1717;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1718, {%r945, %r949};
	add.s64 	%rd1719, %rd1634, %rd1390;
	add.s64 	%rd1720, %rd1719, %rd1671;
	xor.b64  	%rd1721, %rd1720, %rd1659;
	mov.b64	{%r2735, %r2736}, %rd1721;
	mov.b64	%rd1722, {%r2736, %r2735};
	add.s64 	%rd1723, %rd1722, %rd1646;
	xor.b64  	%rd1724, %rd1723, %rd1634;
	mov.b64	{%r2737, %r2738}, %rd1724;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1725, {%r2740, %r2739};
	add.s64 	%rd1726, %rd1720, %rd1385;
	add.s64 	%rd1727, %rd1726, %rd1725;
	xor.b64  	%rd1728, %rd1727, %rd1722;
	mov.b64	{%r2741, %r2742}, %rd1728;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1729, {%r2744, %r2743};
	add.s64 	%rd1730, %rd1729, %rd1723;
	xor.b64  	%rd1731, %rd1730, %rd1725;
	mov.b64	{%r958, %r959}, %rd1731;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1732, {%r953, %r957};
	add.s64 	%rd1733, %rd1685, %rd1388;
	add.s64 	%rd1734, %rd1733, %rd1732;
	xor.b64  	%rd1735, %rd1734, %rd1701;
	mov.b64	{%r2745, %r2746}, %rd1735;
	mov.b64	%rd1736, {%r2746, %r2745};
	add.s64 	%rd1737, %rd1736, %rd1716;
	xor.b64  	%rd1738, %rd1737, %rd1732;
	mov.b64	{%r2747, %r2748}, %rd1738;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1739, {%r2750, %r2749};
	add.s64 	%rd1740, %rd1734, %rd1390;
	add.s64 	%rd1741, %rd1740, %rd1739;
	xor.b64  	%rd1742, %rd1736, %rd1741;
	mov.b64	{%r2751, %r2752}, %rd1742;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1743, {%r2754, %r2753};
	add.s64 	%rd1744, %rd1737, %rd1743;
	xor.b64  	%rd1745, %rd1744, %rd1739;
	mov.b64	{%r966, %r967}, %rd1745;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1746, {%r961, %r965};
	add.s64 	%rd1747, %rd1690, %rd1384;
	add.s64 	%rd1748, %rd1747, %rd1699;
	xor.b64  	%rd1749, %rd1715, %rd1748;
	mov.b64	{%r2755, %r2756}, %rd1749;
	mov.b64	%rd1750, {%r2756, %r2755};
	add.s64 	%rd1751, %rd1730, %rd1750;
	xor.b64  	%rd1752, %rd1751, %rd1690;
	mov.b64	{%r2757, %r2758}, %rd1752;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1753, {%r2760, %r2759};
	add.s64 	%rd1754, %rd1748, %rd1382;
	add.s64 	%rd1755, %rd1754, %rd1753;
	xor.b64  	%rd1756, %rd1755, %rd1750;
	mov.b64	{%r2761, %r2762}, %rd1756;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1757, {%r2764, %r2763};
	add.s64 	%rd1758, %rd1757, %rd1751;
	xor.b64  	%rd1759, %rd1758, %rd1753;
	mov.b64	{%r974, %r975}, %rd1759;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1760, {%r969, %r973};
	add.s64 	%rd1761, %rd1704, %rd1394;
	add.s64 	%rd1762, %rd1761, %rd1713;
	xor.b64  	%rd1763, %rd1729, %rd1762;
	mov.b64	{%r2765, %r2766}, %rd1763;
	mov.b64	%rd1764, {%r2766, %r2765};
	add.s64 	%rd1765, %rd1764, %rd1688;
	xor.b64  	%rd1766, %rd1765, %rd1704;
	mov.b64	{%r2767, %r2768}, %rd1766;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1767, {%r2770, %r2769};
	add.s64 	%rd1768, %rd1762, %rd1393;
	add.s64 	%rd1769, %rd1768, %rd1767;
	xor.b64  	%rd1770, %rd1769, %rd1764;
	mov.b64	{%r2771, %r2772}, %rd1770;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1771, {%r2774, %r2773};
	add.s64 	%rd1772, %rd1771, %rd1765;
	xor.b64  	%rd1773, %rd1772, %rd1767;
	mov.b64	{%r982, %r983}, %rd1773;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1774, {%r977, %r981};
	add.s64 	%rd1775, %rd1718, %rd1392;
	add.s64 	%rd1776, %rd1775, %rd1727;
	xor.b64  	%rd1777, %rd1776, %rd1687;
	mov.b64	{%r2775, %r2776}, %rd1777;
	mov.b64	%rd1778, {%r2776, %r2775};
	add.s64 	%rd1779, %rd1778, %rd1702;
	xor.b64  	%rd1780, %rd1779, %rd1718;
	mov.b64	{%r2777, %r2778}, %rd1780;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1781, {%r2780, %r2779};
	add.s64 	%rd1782, %rd1776, %rd1395;
	add.s64 	%rd1783, %rd1782, %rd1781;
	xor.b64  	%rd1784, %rd1783, %rd1778;
	mov.b64	{%r2781, %r2782}, %rd1784;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1785, {%r2784, %r2783};
	add.s64 	%rd1786, %rd1785, %rd1779;
	xor.b64  	%rd1787, %rd1786, %rd1781;
	mov.b64	{%r990, %r991}, %rd1787;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1788, {%r985, %r989};
	add.s64 	%rd1789, %rd1741, %rd1383;
	add.s64 	%rd1790, %rd1789, %rd1760;
	xor.b64  	%rd1791, %rd1785, %rd1790;
	mov.b64	{%r2785, %r2786}, %rd1791;
	mov.b64	%rd1792, {%r2786, %r2785};
	add.s64 	%rd1793, %rd1792, %rd1772;
	xor.b64  	%rd1794, %rd1793, %rd1760;
	mov.b64	{%r2787, %r2788}, %rd1794;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1795, {%r2790, %r2789};
	add.s64 	%rd1796, %rd1790, %rd1387;
	add.s64 	%rd1797, %rd1796, %rd1795;
	xor.b64  	%rd1798, %rd1792, %rd1797;
	mov.b64	{%r2791, %r2792}, %rd1798;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1799, {%r2794, %r2793};
	add.s64 	%rd1800, %rd1799, %rd1793;
	xor.b64  	%rd1801, %rd1800, %rd1795;
	mov.b64	{%r998, %r999}, %rd1801;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1802, {%r993, %r997};
	add.s64 	%rd1803, %rd1755, %rd1386;
	add.s64 	%rd1804, %rd1803, %rd1774;
	xor.b64  	%rd1805, %rd1804, %rd1743;
	mov.b64	{%r2795, %r2796}, %rd1805;
	mov.b64	%rd1806, {%r2796, %r2795};
	add.s64 	%rd1807, %rd1806, %rd1786;
	xor.b64  	%rd1808, %rd1807, %rd1774;
	mov.b64	{%r2797, %r2798}, %rd1808;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1809, {%r2800, %r2799};
	add.s64 	%rd1810, %rd1804, %rd1391;
	add.s64 	%rd1811, %rd1810, %rd1809;
	xor.b64  	%rd1812, %rd1811, %rd1806;
	mov.b64	{%r2801, %r2802}, %rd1812;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1813, {%r2804, %r2803};
	add.s64 	%rd1814, %rd1813, %rd1807;
	xor.b64  	%rd1815, %rd1814, %rd1809;
	mov.b64	{%r1006, %r1007}, %rd1815;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1816, {%r1001, %r1005};
	add.s64 	%rd1817, %rd1769, %rd1385;
	add.s64 	%rd1818, %rd1817, %rd1788;
	xor.b64  	%rd1819, %rd1818, %rd1757;
	mov.b64	{%r2805, %r2806}, %rd1819;
	mov.b64	%rd1820, {%r2806, %r2805};
	add.s64 	%rd1821, %rd1820, %rd1744;
	xor.b64  	%rd1822, %rd1821, %rd1788;
	mov.b64	{%r2807, %r2808}, %rd1822;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1823, {%r2810, %r2809};
	add.s64 	%rd1824, %rd1818, %rd1381;
	add.s64 	%rd1825, %rd1824, %rd1823;
	xor.b64  	%rd1826, %rd1825, %rd1820;
	mov.b64	{%r2811, %r2812}, %rd1826;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1827, {%r2814, %r2813};
	add.s64 	%rd1828, %rd1827, %rd1821;
	xor.b64  	%rd1829, %rd1828, %rd1823;
	mov.b64	{%r1014, %r1015}, %rd1829;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1830, {%r1009, %r1013};
	add.s64 	%rd1831, %rd1746, %rd1396;
	add.s64 	%rd1832, %rd1831, %rd1783;
	xor.b64  	%rd1833, %rd1832, %rd1771;
	mov.b64	{%r2815, %r2816}, %rd1833;
	mov.b64	%rd1834, {%r2816, %r2815};
	add.s64 	%rd1835, %rd1834, %rd1758;
	xor.b64  	%rd1836, %rd1835, %rd1746;
	mov.b64	{%r2817, %r2818}, %rd1836;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1837, {%r2820, %r2819};
	add.s64 	%rd1838, %rd1832, %rd1389;
	add.s64 	%rd1839, %rd1838, %rd1837;
	xor.b64  	%rd1840, %rd1839, %rd1834;
	mov.b64	{%r2821, %r2822}, %rd1840;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1841, {%r2824, %r2823};
	add.s64 	%rd1842, %rd1841, %rd1835;
	xor.b64  	%rd1843, %rd1842, %rd1837;
	mov.b64	{%r1022, %r1023}, %rd1843;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1844, {%r1017, %r1021};
	add.s64 	%rd1845, %rd1797, %rd1390;
	add.s64 	%rd1846, %rd1845, %rd1844;
	xor.b64  	%rd1847, %rd1846, %rd1813;
	mov.b64	{%r2825, %r2826}, %rd1847;
	mov.b64	%rd1848, {%r2826, %r2825};
	add.s64 	%rd1849, %rd1848, %rd1828;
	xor.b64  	%rd1850, %rd1849, %rd1844;
	mov.b64	{%r2827, %r2828}, %rd1850;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1851, {%r2830, %r2829};
	add.s64 	%rd1852, %rd1846, %rd1381;
	add.s64 	%rd1853, %rd1852, %rd1851;
	xor.b64  	%rd1854, %rd1848, %rd1853;
	mov.b64	{%r2831, %r2832}, %rd1854;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1855, {%r2834, %r2833};
	add.s64 	%rd1856, %rd1849, %rd1855;
	xor.b64  	%rd1857, %rd1856, %rd1851;
	mov.b64	{%r1030, %r1031}, %rd1857;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1858, {%r1025, %r1029};
	add.s64 	%rd1859, %rd1802, %rd1386;
	add.s64 	%rd1860, %rd1859, %rd1811;
	xor.b64  	%rd1861, %rd1827, %rd1860;
	mov.b64	{%r2835, %r2836}, %rd1861;
	mov.b64	%rd1862, {%r2836, %r2835};
	add.s64 	%rd1863, %rd1842, %rd1862;
	xor.b64  	%rd1864, %rd1863, %rd1802;
	mov.b64	{%r2837, %r2838}, %rd1864;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1865, {%r2840, %r2839};
	add.s64 	%rd1866, %rd1860, %rd1388;
	add.s64 	%rd1867, %rd1866, %rd1865;
	xor.b64  	%rd1868, %rd1867, %rd1862;
	mov.b64	{%r2841, %r2842}, %rd1868;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1869, {%r2844, %r2843};
	add.s64 	%rd1870, %rd1869, %rd1863;
	xor.b64  	%rd1871, %rd1870, %rd1865;
	mov.b64	{%r1038, %r1039}, %rd1871;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1872, {%r1033, %r1037};
	add.s64 	%rd1873, %rd1816, %rd1383;
	add.s64 	%rd1874, %rd1873, %rd1825;
	xor.b64  	%rd1875, %rd1841, %rd1874;
	mov.b64	{%r2845, %r2846}, %rd1875;
	mov.b64	%rd1876, {%r2846, %r2845};
	add.s64 	%rd1877, %rd1876, %rd1800;
	xor.b64  	%rd1878, %rd1877, %rd1816;
	mov.b64	{%r2847, %r2848}, %rd1878;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1879, {%r2850, %r2849};
	add.s64 	%rd1880, %rd1874, %rd1385;
	add.s64 	%rd1881, %rd1880, %rd1879;
	xor.b64  	%rd1882, %rd1881, %rd1876;
	mov.b64	{%r2851, %r2852}, %rd1882;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1883, {%r2854, %r2853};
	add.s64 	%rd1884, %rd1883, %rd1877;
	xor.b64  	%rd1885, %rd1884, %rd1879;
	mov.b64	{%r1046, %r1047}, %rd1885;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1886, {%r1041, %r1045};
	add.s64 	%rd1887, %rd1830, %rd1391;
	add.s64 	%rd1888, %rd1887, %rd1839;
	xor.b64  	%rd1889, %rd1888, %rd1799;
	mov.b64	{%r2855, %r2856}, %rd1889;
	mov.b64	%rd1890, {%r2856, %r2855};
	add.s64 	%rd1891, %rd1890, %rd1814;
	xor.b64  	%rd1892, %rd1891, %rd1830;
	mov.b64	{%r2857, %r2858}, %rd1892;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1893, {%r2860, %r2859};
	add.s64 	%rd1894, %rd1888, %rd1396;
	add.s64 	%rd1895, %rd1894, %rd1893;
	xor.b64  	%rd1896, %rd1895, %rd1890;
	mov.b64	{%r2861, %r2862}, %rd1896;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1897, {%r2864, %r2863};
	add.s64 	%rd1898, %rd1897, %rd1891;
	xor.b64  	%rd1899, %rd1898, %rd1893;
	mov.b64	{%r1054, %r1055}, %rd1899;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1900, {%r1049, %r1053};
	add.s64 	%rd1901, %rd1853, %rd1395;
	add.s64 	%rd1902, %rd1901, %rd1872;
	xor.b64  	%rd1903, %rd1897, %rd1902;
	mov.b64	{%r2865, %r2866}, %rd1903;
	mov.b64	%rd1904, {%r2866, %r2865};
	add.s64 	%rd1905, %rd1904, %rd1884;
	xor.b64  	%rd1906, %rd1905, %rd1872;
	mov.b64	{%r2867, %r2868}, %rd1906;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1907, {%r2870, %r2869};
	add.s64 	%rd1908, %rd1902, %rd1382;
	add.s64 	%rd1909, %rd1908, %rd1907;
	xor.b64  	%rd1910, %rd1904, %rd1909;
	mov.b64	{%r2871, %r2872}, %rd1910;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1911, {%r2874, %r2873};
	add.s64 	%rd1912, %rd1911, %rd1905;
	xor.b64  	%rd1913, %rd1912, %rd1907;
	mov.b64	{%r1062, %r1063}, %rd1913;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1914, {%r1057, %r1061};
	add.s64 	%rd1915, %rd1867, %rd1392;
	add.s64 	%rd1916, %rd1915, %rd1886;
	xor.b64  	%rd1917, %rd1916, %rd1855;
	mov.b64	{%r2875, %r2876}, %rd1917;
	mov.b64	%rd1918, {%r2876, %r2875};
	add.s64 	%rd1919, %rd1918, %rd1898;
	xor.b64  	%rd1920, %rd1919, %rd1886;
	mov.b64	{%r2877, %r2878}, %rd1920;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1921, {%r2880, %r2879};
	add.s64 	%rd1922, %rd1916, %rd1393;
	add.s64 	%rd1923, %rd1922, %rd1921;
	xor.b64  	%rd1924, %rd1923, %rd1918;
	mov.b64	{%r2881, %r2882}, %rd1924;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1925, {%r2884, %r2883};
	add.s64 	%rd1926, %rd1925, %rd1919;
	xor.b64  	%rd1927, %rd1926, %rd1921;
	mov.b64	{%r1070, %r1071}, %rd1927;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1928, {%r1065, %r1069};
	add.s64 	%rd1929, %rd1881, %rd1387;
	add.s64 	%rd1930, %rd1929, %rd1900;
	xor.b64  	%rd1931, %rd1930, %rd1869;
	mov.b64	{%r2885, %r2886}, %rd1931;
	mov.b64	%rd1932, {%r2886, %r2885};
	add.s64 	%rd1933, %rd1932, %rd1856;
	xor.b64  	%rd1934, %rd1933, %rd1900;
	mov.b64	{%r2887, %r2888}, %rd1934;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1935, {%r2890, %r2889};
	add.s64 	%rd1936, %rd1930, %rd1389;
	add.s64 	%rd1937, %rd1936, %rd1935;
	xor.b64  	%rd1938, %rd1937, %rd1932;
	mov.b64	{%r2891, %r2892}, %rd1938;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1939, {%r2894, %r2893};
	add.s64 	%rd1940, %rd1939, %rd1933;
	xor.b64  	%rd1941, %rd1940, %rd1935;
	mov.b64	{%r1078, %r1079}, %rd1941;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1942, {%r1073, %r1077};
	add.s64 	%rd1943, %rd1858, %rd1384;
	add.s64 	%rd1944, %rd1943, %rd1895;
	xor.b64  	%rd1945, %rd1944, %rd1883;
	mov.b64	{%r2895, %r2896}, %rd1945;
	mov.b64	%rd1946, {%r2896, %r2895};
	add.s64 	%rd1947, %rd1946, %rd1870;
	xor.b64  	%rd1948, %rd1947, %rd1858;
	mov.b64	{%r2897, %r2898}, %rd1948;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1949, {%r2900, %r2899};
	add.s64 	%rd1950, %rd1944, %rd1394;
	add.s64 	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1946;
	mov.b64	{%r2901, %r2902}, %rd1952;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1953, {%r2904, %r2903};
	add.s64 	%rd1954, %rd1953, %rd1947;
	xor.b64  	%rd1955, %rd1954, %rd1949;
	mov.b64	{%r1086, %r1087}, %rd1955;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1956, {%r1081, %r1085};
	add.s64 	%rd1957, %rd1909, %rd1383;
	add.s64 	%rd1958, %rd1957, %rd1956;
	xor.b64  	%rd1959, %rd1958, %rd1925;
	mov.b64	{%r2905, %r2906}, %rd1959;
	mov.b64	%rd1960, {%r2906, %r2905};
	add.s64 	%rd1961, %rd1960, %rd1940;
	xor.b64  	%rd1962, %rd1961, %rd1956;
	mov.b64	{%r2907, %r2908}, %rd1962;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1963, {%r2910, %r2909};
	add.s64 	%rd1964, %rd1958, %rd1393;
	add.s64 	%rd1965, %rd1964, %rd1963;
	xor.b64  	%rd1966, %rd1960, %rd1965;
	mov.b64	{%r2911, %r2912}, %rd1966;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1967, {%r2914, %r2913};
	add.s64 	%rd1968, %rd1961, %rd1967;
	xor.b64  	%rd1969, %rd1968, %rd1963;
	mov.b64	{%r1094, %r1095}, %rd1969;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1970, {%r1089, %r1093};
	add.s64 	%rd1971, %rd1914, %rd1387;
	add.s64 	%rd1972, %rd1971, %rd1923;
	xor.b64  	%rd1973, %rd1939, %rd1972;
	mov.b64	{%r2915, %r2916}, %rd1973;
	mov.b64	%rd1974, {%r2916, %r2915};
	add.s64 	%rd1975, %rd1954, %rd1974;
	xor.b64  	%rd1976, %rd1975, %rd1914;
	mov.b64	{%r2917, %r2918}, %rd1976;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1977, {%r2920, %r2919};
	add.s64 	%rd1978, %rd1972, %rd1391;
	add.s64 	%rd1979, %rd1978, %rd1977;
	xor.b64  	%rd1980, %rd1979, %rd1974;
	mov.b64	{%r2921, %r2922}, %rd1980;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1981, {%r2924, %r2923};
	add.s64 	%rd1982, %rd1981, %rd1975;
	xor.b64  	%rd1983, %rd1982, %rd1977;
	mov.b64	{%r1102, %r1103}, %rd1983;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1984, {%r1097, %r1101};
	add.s64 	%rd1985, %rd1928, %rd1381;
	add.s64 	%rd1986, %rd1985, %rd1937;
	xor.b64  	%rd1987, %rd1953, %rd1986;
	mov.b64	{%r2925, %r2926}, %rd1987;
	mov.b64	%rd1988, {%r2926, %r2925};
	add.s64 	%rd1989, %rd1988, %rd1912;
	xor.b64  	%rd1990, %rd1989, %rd1928;
	mov.b64	{%r2927, %r2928}, %rd1990;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1991, {%r2930, %r2929};
	add.s64 	%rd1992, %rd1986, %rd1392;
	add.s64 	%rd1993, %rd1992, %rd1991;
	xor.b64  	%rd1994, %rd1993, %rd1988;
	mov.b64	{%r2931, %r2932}, %rd1994;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1995, {%r2934, %r2933};
	add.s64 	%rd1996, %rd1995, %rd1989;
	xor.b64  	%rd1997, %rd1996, %rd1991;
	mov.b64	{%r1110, %r1111}, %rd1997;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1998, {%r1105, %r1109};
	add.s64 	%rd1999, %rd1942, %rd1389;
	add.s64 	%rd2000, %rd1999, %rd1951;
	xor.b64  	%rd2001, %rd2000, %rd1911;
	mov.b64	{%r2935, %r2936}, %rd2001;
	mov.b64	%rd2002, {%r2936, %r2935};
	add.s64 	%rd2003, %rd2002, %rd1926;
	xor.b64  	%rd2004, %rd2003, %rd1942;
	mov.b64	{%r2937, %r2938}, %rd2004;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2005, {%r2940, %r2939};
	add.s64 	%rd2006, %rd2000, %rd1384;
	add.s64 	%rd2007, %rd2006, %rd2005;
	xor.b64  	%rd2008, %rd2007, %rd2002;
	mov.b64	{%r2941, %r2942}, %rd2008;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2009, {%r2944, %r2943};
	add.s64 	%rd2010, %rd2009, %rd2003;
	xor.b64  	%rd2011, %rd2010, %rd2005;
	mov.b64	{%r1118, %r1119}, %rd2011;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2012, {%r1113, %r1117};
	add.s64 	%rd2013, %rd1965, %rd1385;
	add.s64 	%rd2014, %rd2013, %rd1984;
	xor.b64  	%rd2015, %rd2009, %rd2014;
	mov.b64	{%r2945, %r2946}, %rd2015;
	mov.b64	%rd2016, {%r2946, %r2945};
	add.s64 	%rd2017, %rd2016, %rd1996;
	xor.b64  	%rd2018, %rd2017, %rd1984;
	mov.b64	{%r2947, %r2948}, %rd2018;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2019, {%r2950, %r2949};
	add.s64 	%rd2020, %rd2014, %rd1394;
	add.s64 	%rd2021, %rd2020, %rd2019;
	xor.b64  	%rd2022, %rd2016, %rd2021;
	mov.b64	{%r2951, %r2952}, %rd2022;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2023, {%r2954, %r2953};
	add.s64 	%rd2024, %rd2023, %rd2017;
	xor.b64  	%rd2025, %rd2024, %rd2019;
	mov.b64	{%r1126, %r1127}, %rd2025;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2026, {%r1121, %r1125};
	add.s64 	%rd2027, %rd1979, %rd1388;
	add.s64 	%rd2028, %rd2027, %rd1998;
	xor.b64  	%rd2029, %rd2028, %rd1967;
	mov.b64	{%r2955, %r2956}, %rd2029;
	mov.b64	%rd2030, {%r2956, %r2955};
	add.s64 	%rd2031, %rd2030, %rd2010;
	xor.b64  	%rd2032, %rd2031, %rd1998;
	mov.b64	{%r2957, %r2958}, %rd2032;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2033, {%r2960, %r2959};
	add.s64 	%rd2034, %rd2028, %rd1386;
	add.s64 	%rd2035, %rd2034, %rd2033;
	xor.b64  	%rd2036, %rd2035, %rd2030;
	mov.b64	{%r2961, %r2962}, %rd2036;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2037, {%r2964, %r2963};
	add.s64 	%rd2038, %rd2037, %rd2031;
	xor.b64  	%rd2039, %rd2038, %rd2033;
	mov.b64	{%r1134, %r1135}, %rd2039;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2040, {%r1129, %r1133};
	add.s64 	%rd2041, %rd1993, %rd1396;
	add.s64 	%rd2042, %rd2041, %rd2012;
	xor.b64  	%rd2043, %rd2042, %rd1981;
	mov.b64	{%r2965, %r2966}, %rd2043;
	mov.b64	%rd2044, {%r2966, %r2965};
	add.s64 	%rd2045, %rd2044, %rd1968;
	xor.b64  	%rd2046, %rd2045, %rd2012;
	mov.b64	{%r2967, %r2968}, %rd2046;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2047, {%r2970, %r2969};
	add.s64 	%rd2048, %rd2042, %rd1395;
	add.s64 	%rd2049, %rd2048, %rd2047;
	xor.b64  	%rd2050, %rd2049, %rd2044;
	mov.b64	{%r2971, %r2972}, %rd2050;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2051, {%r2974, %r2973};
	add.s64 	%rd2052, %rd2051, %rd2045;
	xor.b64  	%rd2053, %rd2052, %rd2047;
	mov.b64	{%r1142, %r1143}, %rd2053;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2054, {%r1137, %r1141};
	add.s64 	%rd2055, %rd1970, %rd1382;
	add.s64 	%rd2056, %rd2055, %rd2007;
	xor.b64  	%rd2057, %rd2056, %rd1995;
	mov.b64	{%r2975, %r2976}, %rd2057;
	mov.b64	%rd2058, {%r2976, %r2975};
	add.s64 	%rd2059, %rd2058, %rd1982;
	xor.b64  	%rd2060, %rd2059, %rd1970;
	mov.b64	{%r2977, %r2978}, %rd2060;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2061, {%r2980, %r2979};
	add.s64 	%rd2062, %rd2056, %rd1390;
	add.s64 	%rd2063, %rd2062, %rd2061;
	xor.b64  	%rd2064, %rd2063, %rd2058;
	mov.b64	{%r2981, %r2982}, %rd2064;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2065, {%r2984, %r2983};
	add.s64 	%rd2066, %rd2065, %rd2059;
	xor.b64  	%rd2067, %rd2066, %rd2061;
	mov.b64	{%r1150, %r1151}, %rd2067;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2068, {%r1145, %r1149};
	add.s64 	%rd2069, %rd2021, %rd1393;
	add.s64 	%rd2070, %rd2069, %rd2068;
	xor.b64  	%rd2071, %rd2070, %rd2037;
	mov.b64	{%r2985, %r2986}, %rd2071;
	mov.b64	%rd2072, {%r2986, %r2985};
	add.s64 	%rd2073, %rd2072, %rd2052;
	xor.b64  	%rd2074, %rd2073, %rd2068;
	mov.b64	{%r2987, %r2988}, %rd2074;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2075, {%r2990, %r2989};
	add.s64 	%rd2076, %rd2070, %rd1386;
	add.s64 	%rd2077, %rd2076, %rd2075;
	xor.b64  	%rd2078, %rd2072, %rd2077;
	mov.b64	{%r2991, %r2992}, %rd2078;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2079, {%r2994, %r2993};
	add.s64 	%rd2080, %rd2073, %rd2079;
	xor.b64  	%rd2081, %rd2080, %rd2075;
	mov.b64	{%r1158, %r1159}, %rd2081;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2082, {%r1153, %r1157};
	add.s64 	%rd2083, %rd2026, %rd1382;
	add.s64 	%rd2084, %rd2083, %rd2035;
	xor.b64  	%rd2085, %rd2051, %rd2084;
	mov.b64	{%r2995, %r2996}, %rd2085;
	mov.b64	%rd2086, {%r2996, %r2995};
	add.s64 	%rd2087, %rd2066, %rd2086;
	xor.b64  	%rd2088, %rd2087, %rd2026;
	mov.b64	{%r2997, %r2998}, %rd2088;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2089, {%r3000, %r2999};
	add.s64 	%rd2090, %rd2084, %rd1396;
	add.s64 	%rd2091, %rd2090, %rd2089;
	xor.b64  	%rd2092, %rd2091, %rd2086;
	mov.b64	{%r3001, %r3002}, %rd2092;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2093, {%r3004, %r3003};
	add.s64 	%rd2094, %rd2093, %rd2087;
	xor.b64  	%rd2095, %rd2094, %rd2089;
	mov.b64	{%r1166, %r1167}, %rd2095;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2096, {%r1161, %r1165};
	add.s64 	%rd2097, %rd2040, %rd1395;
	add.s64 	%rd2098, %rd2097, %rd2049;
	xor.b64  	%rd2099, %rd2065, %rd2098;
	mov.b64	{%r3005, %r3006}, %rd2099;
	mov.b64	%rd2100, {%r3006, %r3005};
	add.s64 	%rd2101, %rd2100, %rd2024;
	xor.b64  	%rd2102, %rd2101, %rd2040;
	mov.b64	{%r3007, %r3008}, %rd2102;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2103, {%r3010, %r3009};
	add.s64 	%rd2104, %rd2098, %rd1394;
	add.s64 	%rd2105, %rd2104, %rd2103;
	xor.b64  	%rd2106, %rd2105, %rd2100;
	mov.b64	{%r3011, %r3012}, %rd2106;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2107, {%r3014, %r3013};
	add.s64 	%rd2108, %rd2107, %rd2101;
	xor.b64  	%rd2109, %rd2108, %rd2103;
	mov.b64	{%r1174, %r1175}, %rd2109;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2110, {%r1169, %r1173};
	add.s64 	%rd2111, %rd2054, %rd1385;
	add.s64 	%rd2112, %rd2111, %rd2063;
	xor.b64  	%rd2113, %rd2112, %rd2023;
	mov.b64	{%r3015, %r3016}, %rd2113;
	mov.b64	%rd2114, {%r3016, %r3015};
	add.s64 	%rd2115, %rd2114, %rd2038;
	xor.b64  	%rd2116, %rd2115, %rd2054;
	mov.b64	{%r3017, %r3018}, %rd2116;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2117, {%r3020, %r3019};
	add.s64 	%rd2118, %rd2112, %rd1391;
	add.s64 	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2114;
	mov.b64	{%r3021, %r3022}, %rd2120;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2121, {%r3024, %r3023};
	add.s64 	%rd2122, %rd2121, %rd2115;
	xor.b64  	%rd2123, %rd2122, %rd2117;
	mov.b64	{%r1182, %r1183}, %rd2123;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2124, {%r1177, %r1181};
	add.s64 	%rd2125, %rd2077, %rd1381;
	add.s64 	%rd2126, %rd2125, %rd2096;
	xor.b64  	%rd2127, %rd2121, %rd2126;
	mov.b64	{%r3025, %r3026}, %rd2127;
	mov.b64	%rd2128, {%r3026, %r3025};
	add.s64 	%rd2129, %rd2128, %rd2108;
	xor.b64  	%rd2130, %rd2129, %rd2096;
	mov.b64	{%r3027, %r3028}, %rd2130;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2131, {%r3030, %r3029};
	add.s64 	%rd2132, %rd2126, %rd1388;
	add.s64 	%rd2133, %rd2132, %rd2131;
	xor.b64  	%rd2134, %rd2128, %rd2133;
	mov.b64	{%r3031, %r3032}, %rd2134;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2135, {%r3034, %r3033};
	add.s64 	%rd2136, %rd2135, %rd2129;
	xor.b64  	%rd2137, %rd2136, %rd2131;
	mov.b64	{%r1190, %r1191}, %rd2137;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2138, {%r1185, %r1189};
	add.s64 	%rd2139, %rd2091, %rd1387;
	add.s64 	%rd2140, %rd2139, %rd2110;
	xor.b64  	%rd2141, %rd2140, %rd2079;
	mov.b64	{%r3035, %r3036}, %rd2141;
	mov.b64	%rd2142, {%r3036, %r3035};
	add.s64 	%rd2143, %rd2142, %rd2122;
	xor.b64  	%rd2144, %rd2143, %rd2110;
	mov.b64	{%r3037, %r3038}, %rd2144;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2145, {%r3040, %r3039};
	add.s64 	%rd2146, %rd2140, %rd1384;
	add.s64 	%rd2147, %rd2146, %rd2145;
	xor.b64  	%rd2148, %rd2147, %rd2142;
	mov.b64	{%r3041, %r3042}, %rd2148;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2149, {%r3044, %r3043};
	add.s64 	%rd2150, %rd2149, %rd2143;
	xor.b64  	%rd2151, %rd2150, %rd2145;
	mov.b64	{%r1198, %r1199}, %rd2151;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2152, {%r1193, %r1197};
	add.s64 	%rd2153, %rd2105, %rd1390;
	add.s64 	%rd2154, %rd2153, %rd2124;
	xor.b64  	%rd2155, %rd2154, %rd2093;
	mov.b64	{%r3045, %r3046}, %rd2155;
	mov.b64	%rd2156, {%r3046, %r3045};
	add.s64 	%rd2157, %rd2156, %rd2080;
	xor.b64  	%rd2158, %rd2157, %rd2124;
	mov.b64	{%r3047, %r3048}, %rd2158;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2159, {%r3050, %r3049};
	add.s64 	%rd2160, %rd2154, %rd1383;
	add.s64 	%rd2161, %rd2160, %rd2159;
	xor.b64  	%rd2162, %rd2161, %rd2156;
	mov.b64	{%r3051, %r3052}, %rd2162;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2163, {%r3054, %r3053};
	add.s64 	%rd2164, %rd2163, %rd2157;
	xor.b64  	%rd2165, %rd2164, %rd2159;
	mov.b64	{%r1206, %r1207}, %rd2165;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2166, {%r1201, %r1205};
	add.s64 	%rd2167, %rd2082, %rd1389;
	add.s64 	%rd2168, %rd2167, %rd2119;
	xor.b64  	%rd2169, %rd2168, %rd2107;
	mov.b64	{%r3055, %r3056}, %rd2169;
	mov.b64	%rd2170, {%r3056, %r3055};
	add.s64 	%rd2171, %rd2170, %rd2094;
	xor.b64  	%rd2172, %rd2171, %rd2082;
	mov.b64	{%r3057, %r3058}, %rd2172;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2173, {%r3060, %r3059};
	add.s64 	%rd2174, %rd2168, %rd1392;
	add.s64 	%rd2175, %rd2174, %rd2173;
	xor.b64  	%rd2176, %rd2175, %rd2170;
	mov.b64	{%r3061, %r3062}, %rd2176;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2177, {%r3064, %r3063};
	add.s64 	%rd2178, %rd2177, %rd2171;
	xor.b64  	%rd2179, %rd2178, %rd2173;
	mov.b64	{%r1214, %r1215}, %rd2179;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2180, {%r1209, %r1213};
	add.s64 	%rd2181, %rd2133, %rd1394;
	add.s64 	%rd2182, %rd2181, %rd2180;
	xor.b64  	%rd2183, %rd2182, %rd2149;
	mov.b64	{%r3065, %r3066}, %rd2183;
	mov.b64	%rd2184, {%r3066, %r3065};
	add.s64 	%rd2185, %rd2184, %rd2164;
	xor.b64  	%rd2186, %rd2185, %rd2180;
	mov.b64	{%r3067, %r3068}, %rd2186;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2187, {%r3070, %r3069};
	add.s64 	%rd2188, %rd2182, %rd1392;
	add.s64 	%rd2189, %rd2188, %rd2187;
	xor.b64  	%rd2190, %rd2184, %rd2189;
	mov.b64	{%r3071, %r3072}, %rd2190;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2191, {%r3074, %r3073};
	add.s64 	%rd2192, %rd2185, %rd2191;
	xor.b64  	%rd2193, %rd2192, %rd2187;
	mov.b64	{%r1222, %r1223}, %rd2193;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2194, {%r1217, %r1221};
	add.s64 	%rd2195, %rd2138, %rd1388;
	add.s64 	%rd2196, %rd2195, %rd2147;
	xor.b64  	%rd2197, %rd2163, %rd2196;
	mov.b64	{%r3075, %r3076}, %rd2197;
	mov.b64	%rd2198, {%r3076, %r3075};
	add.s64 	%rd2199, %rd2178, %rd2198;
	xor.b64  	%rd2200, %rd2199, %rd2138;
	mov.b64	{%r3077, %r3078}, %rd2200;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2201, {%r3080, %r3079};
	add.s64 	%rd2202, %rd2196, %rd1395;
	add.s64 	%rd2203, %rd2202, %rd2201;
	xor.b64  	%rd2204, %rd2203, %rd2198;
	mov.b64	{%r3081, %r3082}, %rd2204;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2205, {%r3084, %r3083};
	add.s64 	%rd2206, %rd2205, %rd2199;
	xor.b64  	%rd2207, %rd2206, %rd2201;
	mov.b64	{%r1230, %r1231}, %rd2207;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2208, {%r1225, %r1229};
	add.s64 	%rd2209, %rd2152, %rd1393;
	add.s64 	%rd2210, %rd2209, %rd2161;
	xor.b64  	%rd2211, %rd2177, %rd2210;
	mov.b64	{%r3085, %r3086}, %rd2211;
	mov.b64	%rd2212, {%r3086, %r3085};
	add.s64 	%rd2213, %rd2212, %rd2136;
	xor.b64  	%rd2214, %rd2213, %rd2152;
	mov.b64	{%r3087, %r3088}, %rd2214;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2215, {%r3090, %r3089};
	add.s64 	%rd2216, %rd2210, %rd1382;
	add.s64 	%rd2217, %rd2216, %rd2215;
	xor.b64  	%rd2218, %rd2217, %rd2212;
	mov.b64	{%r3091, %r3092}, %rd2218;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2219, {%r3094, %r3093};
	add.s64 	%rd2220, %rd2219, %rd2213;
	xor.b64  	%rd2221, %rd2220, %rd2215;
	mov.b64	{%r1238, %r1239}, %rd2221;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2222, {%r1233, %r1237};
	add.s64 	%rd2223, %rd2166, %rd1384;
	add.s64 	%rd2224, %rd2223, %rd2175;
	xor.b64  	%rd2225, %rd2224, %rd2135;
	mov.b64	{%r3095, %r3096}, %rd2225;
	mov.b64	%rd2226, {%r3096, %r3095};
	add.s64 	%rd2227, %rd2226, %rd2150;
	xor.b64  	%rd2228, %rd2227, %rd2166;
	mov.b64	{%r3097, %r3098}, %rd2228;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2229, {%r3100, %r3099};
	add.s64 	%rd2230, %rd2224, %rd1390;
	add.s64 	%rd2231, %rd2230, %rd2229;
	xor.b64  	%rd2232, %rd2231, %rd2226;
	mov.b64	{%r3101, %r3102}, %rd2232;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2233, {%r3104, %r3103};
	add.s64 	%rd2234, %rd2233, %rd2227;
	xor.b64  	%rd2235, %rd2234, %rd2229;
	mov.b64	{%r1246, %r1247}, %rd2235;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2236, {%r1241, %r1245};
	add.s64 	%rd2237, %rd2189, %rd1386;
	add.s64 	%rd2238, %rd2237, %rd2208;
	xor.b64  	%rd2239, %rd2233, %rd2238;
	mov.b64	{%r3105, %r3106}, %rd2239;
	mov.b64	%rd2240, {%r3106, %r3105};
	add.s64 	%rd2241, %rd2240, %rd2220;
	xor.b64  	%rd2242, %rd2241, %rd2208;
	mov.b64	{%r3107, %r3108}, %rd2242;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2243, {%r3110, %r3109};
	add.s64 	%rd2244, %rd2238, %rd1381;
	add.s64 	%rd2245, %rd2244, %rd2243;
	xor.b64  	%rd2246, %rd2240, %rd2245;
	mov.b64	{%r3111, %r3112}, %rd2246;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2247, {%r3114, %r3113};
	add.s64 	%rd2248, %rd2247, %rd2241;
	xor.b64  	%rd2249, %rd2248, %rd2243;
	mov.b64	{%r1254, %r1255}, %rd2249;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2250, {%r1249, %r1253};
	add.s64 	%rd2251, %rd2203, %rd1396;
	add.s64 	%rd2252, %rd2251, %rd2222;
	xor.b64  	%rd2253, %rd2252, %rd2191;
	mov.b64	{%r3115, %r3116}, %rd2253;
	mov.b64	%rd2254, {%r3116, %r3115};
	add.s64 	%rd2255, %rd2254, %rd2234;
	xor.b64  	%rd2256, %rd2255, %rd2222;
	mov.b64	{%r3117, %r3118}, %rd2256;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2257, {%r3120, %r3119};
	add.s64 	%rd2258, %rd2252, %rd1385;
	add.s64 	%rd2259, %rd2258, %rd2257;
	xor.b64  	%rd2260, %rd2259, %rd2254;
	mov.b64	{%r3121, %r3122}, %rd2260;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2261, {%r3124, %r3123};
	add.s64 	%rd2262, %rd2261, %rd2255;
	xor.b64  	%rd2263, %rd2262, %rd2257;
	mov.b64	{%r1262, %r1263}, %rd2263;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2264, {%r1257, %r1261};
	add.s64 	%rd2265, %rd2217, %rd1389;
	add.s64 	%rd2266, %rd2265, %rd2236;
	xor.b64  	%rd2267, %rd2266, %rd2205;
	mov.b64	{%r3125, %r3126}, %rd2267;
	mov.b64	%rd2268, {%r3126, %r3125};
	add.s64 	%rd2269, %rd2268, %rd2192;
	xor.b64  	%rd2270, %rd2269, %rd2236;
	mov.b64	{%r3127, %r3128}, %rd2270;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2271, {%r3130, %r3129};
	add.s64 	%rd2272, %rd2266, %rd1387;
	add.s64 	%rd2273, %rd2272, %rd2271;
	xor.b64  	%rd2274, %rd2273, %rd2268;
	mov.b64	{%r3131, %r3132}, %rd2274;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2275, {%r3134, %r3133};
	add.s64 	%rd2276, %rd2275, %rd2269;
	xor.b64  	%rd2277, %rd2276, %rd2271;
	mov.b64	{%r1270, %r1271}, %rd2277;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2278, {%r1265, %r1269};
	add.s64 	%rd2279, %rd2194, %rd1383;
	add.s64 	%rd2280, %rd2279, %rd2231;
	xor.b64  	%rd2281, %rd2280, %rd2219;
	mov.b64	{%r3135, %r3136}, %rd2281;
	mov.b64	%rd2282, {%r3136, %r3135};
	add.s64 	%rd2283, %rd2282, %rd2206;
	xor.b64  	%rd2284, %rd2283, %rd2194;
	mov.b64	{%r3137, %r3138}, %rd2284;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2285, {%r3140, %r3139};
	add.s64 	%rd2286, %rd2280, %rd1391;
	add.s64 	%rd2287, %rd2286, %rd2285;
	xor.b64  	%rd2288, %rd2287, %rd2282;
	mov.b64	{%r3141, %r3142}, %rd2288;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2289, {%r3144, %r3143};
	add.s64 	%rd2290, %rd2289, %rd2283;
	xor.b64  	%rd2291, %rd2290, %rd2285;
	mov.b64	{%r1278, %r1279}, %rd2291;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2292, {%r1273, %r1277};
	add.s64 	%rd2293, %rd2245, %rd1387;
	add.s64 	%rd2294, %rd2293, %rd2292;
	xor.b64  	%rd2295, %rd2294, %rd2261;
	mov.b64	{%r3145, %r3146}, %rd2295;
	mov.b64	%rd2296, {%r3146, %r3145};
	add.s64 	%rd2297, %rd2296, %rd2276;
	xor.b64  	%rd2298, %rd2297, %rd2292;
	mov.b64	{%r3147, %r3148}, %rd2298;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2299, {%r3150, %r3149};
	add.s64 	%rd2300, %rd2294, %rd1396;
	add.s64 	%rd2301, %rd2300, %rd2299;
	xor.b64  	%rd2302, %rd2296, %rd2301;
	mov.b64	{%r3151, %r3152}, %rd2302;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2303, {%r3154, %r3153};
	add.s64 	%rd2304, %rd2297, %rd2303;
	xor.b64  	%rd2305, %rd2304, %rd2299;
	mov.b64	{%r1286, %r1287}, %rd2305;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2306, {%r1281, %r1285};
	add.s64 	%rd2307, %rd2250, %rd1395;
	add.s64 	%rd2308, %rd2307, %rd2259;
	xor.b64  	%rd2309, %rd2275, %rd2308;
	mov.b64	{%r3155, %r3156}, %rd2309;
	mov.b64	%rd2310, {%r3156, %r3155};
	add.s64 	%rd2311, %rd2290, %rd2310;
	xor.b64  	%rd2312, %rd2311, %rd2250;
	mov.b64	{%r3157, %r3158}, %rd2312;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2313, {%r3160, %r3159};
	add.s64 	%rd2314, %rd2308, %rd1390;
	add.s64 	%rd2315, %rd2314, %rd2313;
	xor.b64  	%rd2316, %rd2315, %rd2310;
	mov.b64	{%r3161, %r3162}, %rd2316;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2317, {%r3164, %r3163};
	add.s64 	%rd2318, %rd2317, %rd2311;
	xor.b64  	%rd2319, %rd2318, %rd2313;
	mov.b64	{%r1294, %r1295}, %rd2319;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2320, {%r1289, %r1293};
	add.s64 	%rd2321, %rd2264, %rd1392;
	add.s64 	%rd2322, %rd2321, %rd2273;
	xor.b64  	%rd2323, %rd2289, %rd2322;
	mov.b64	{%r3165, %r3166}, %rd2323;
	mov.b64	%rd2324, {%r3166, %r3165};
	add.s64 	%rd2325, %rd2324, %rd2248;
	xor.b64  	%rd2326, %rd2325, %rd2264;
	mov.b64	{%r3167, %r3168}, %rd2326;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2327, {%r3170, %r3169};
	add.s64 	%rd2328, %rd2322, %rd1384;
	add.s64 	%rd2329, %rd2328, %rd2327;
	xor.b64  	%rd2330, %rd2329, %rd2324;
	mov.b64	{%r3171, %r3172}, %rd2330;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2331, {%r3174, %r3173};
	add.s64 	%rd2332, %rd2331, %rd2325;
	xor.b64  	%rd2333, %rd2332, %rd2327;
	mov.b64	{%r1302, %r1303}, %rd2333;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2334, {%r1297, %r1301};
	add.s64 	%rd2335, %rd2278, %rd1381;
	add.s64 	%rd2336, %rd2335, %rd2287;
	xor.b64  	%rd2337, %rd2336, %rd2247;
	mov.b64	{%r3175, %r3176}, %rd2337;
	mov.b64	%rd2338, {%r3176, %r3175};
	add.s64 	%rd2339, %rd2338, %rd2262;
	xor.b64  	%rd2340, %rd2339, %rd2278;
	mov.b64	{%r3177, %r3178}, %rd2340;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2341, {%r3180, %r3179};
	add.s64 	%rd2342, %rd2336, %rd1389;
	add.s64 	%rd2343, %rd2342, %rd2341;
	xor.b64  	%rd2344, %rd2343, %rd2338;
	mov.b64	{%r3181, %r3182}, %rd2344;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2345, {%r3184, %r3183};
	add.s64 	%rd2346, %rd2345, %rd2339;
	xor.b64  	%rd2347, %rd2346, %rd2341;
	mov.b64	{%r1310, %r1311}, %rd2347;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2348, {%r1305, %r1309};
	add.s64 	%rd2349, %rd2301, %rd1393;
	add.s64 	%rd2350, %rd2349, %rd2320;
	xor.b64  	%rd2351, %rd2345, %rd2350;
	mov.b64	{%r3185, %r3186}, %rd2351;
	mov.b64	%rd2352, {%r3186, %r3185};
	add.s64 	%rd2353, %rd2352, %rd2332;
	xor.b64  	%rd2354, %rd2353, %rd2320;
	mov.b64	{%r3187, %r3188}, %rd2354;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2355, {%r3190, %r3189};
	add.s64 	%rd2356, %rd2350, %rd1383;
	add.s64 	%rd2357, %rd2356, %rd2355;
	xor.b64  	%rd2358, %rd2352, %rd2357;
	mov.b64	{%r3191, %r3192}, %rd2358;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2359, {%r3194, %r3193};
	add.s64 	%rd2360, %rd2359, %rd2353;
	xor.b64  	%rd2361, %rd2360, %rd2355;
	mov.b64	{%r1318, %r1319}, %rd2361;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2362, {%r1313, %r1317};
	add.s64 	%rd2363, %rd2315, %rd1394;
	add.s64 	%rd2364, %rd2363, %rd2334;
	xor.b64  	%rd2365, %rd2364, %rd2303;
	mov.b64	{%r3195, %r3196}, %rd2365;
	mov.b64	%rd2366, {%r3196, %r3195};
	add.s64 	%rd2367, %rd2366, %rd2346;
	xor.b64  	%rd2368, %rd2367, %rd2334;
	mov.b64	{%r3197, %r3198}, %rd2368;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2369, {%r3200, %r3199};
	add.s64 	%rd2370, %rd2364, %rd1388;
	add.s64 	%rd2371, %rd2370, %rd2369;
	xor.b64  	%rd2372, %rd2371, %rd2366;
	mov.b64	{%r3201, %r3202}, %rd2372;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2373, {%r3204, %r3203};
	add.s64 	%rd2374, %rd2373, %rd2367;
	xor.b64  	%rd2375, %rd2374, %rd2369;
	mov.b64	{%r1326, %r1327}, %rd2375;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2376, {%r1321, %r1325};
	add.s64 	%rd2377, %rd2329, %rd1382;
	add.s64 	%rd2378, %rd2377, %rd2348;
	xor.b64  	%rd2379, %rd2378, %rd2317;
	mov.b64	{%r3205, %r3206}, %rd2379;
	mov.b64	%rd2380, {%r3206, %r3205};
	add.s64 	%rd2381, %rd2380, %rd2304;
	xor.b64  	%rd2382, %rd2381, %rd2348;
	mov.b64	{%r3207, %r3208}, %rd2382;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2383, {%r3210, %r3209};
	add.s64 	%rd2384, %rd2378, %rd1385;
	add.s64 	%rd2385, %rd2384, %rd2383;
	xor.b64  	%rd2386, %rd2385, %rd2380;
	mov.b64	{%r3211, %r3212}, %rd2386;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2387, {%r3214, %r3213};
	add.s64 	%rd2388, %rd2387, %rd2381;
	xor.b64  	%rd2389, %rd2388, %rd2383;
	mov.b64	{%r1334, %r1335}, %rd2389;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2390, {%r1329, %r1333};
	add.s64 	%rd2391, %rd2306, %rd1391;
	add.s64 	%rd2392, %rd2391, %rd2343;
	xor.b64  	%rd2393, %rd2392, %rd2331;
	mov.b64	{%r3215, %r3216}, %rd2393;
	mov.b64	%rd2394, {%r3216, %r3215};
	add.s64 	%rd2395, %rd2394, %rd2318;
	xor.b64  	%rd2396, %rd2395, %rd2306;
	mov.b64	{%r3217, %r3218}, %rd2396;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2397, {%r3220, %r3219};
	add.s64 	%rd2398, %rd2392, %rd1386;
	add.s64 	%rd2399, %rd2398, %rd2397;
	xor.b64  	%rd2400, %rd2399, %rd2394;
	mov.b64	{%r3221, %r3222}, %rd2400;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2401, {%r3224, %r3223};
	add.s64 	%rd2402, %rd2401, %rd2395;
	xor.b64  	%rd2403, %rd2402, %rd2397;
	mov.b64	{%r1342, %r1343}, %rd2403;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2404, {%r1337, %r1341};
	add.s64 	%rd2405, %rd2357, %rd1391;
	add.s64 	%rd2406, %rd2405, %rd2404;
	xor.b64  	%rd2407, %rd2406, %rd2373;
	mov.b64	{%r3225, %r3226}, %rd2407;
	mov.b64	%rd2408, {%r3226, %r3225};
	add.s64 	%rd2409, %rd2408, %rd2388;
	xor.b64  	%rd2410, %rd2409, %rd2404;
	mov.b64	{%r3227, %r3228}, %rd2410;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2411, {%r3230, %r3229};
	add.s64 	%rd2412, %rd2406, %rd1383;
	add.s64 	%rd2413, %rd2412, %rd2411;
	xor.b64  	%rd2414, %rd2408, %rd2413;
	mov.b64	{%r3231, %r3232}, %rd2414;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2415, {%r3234, %r3233};
	add.s64 	%rd2416, %rd2409, %rd2415;
	xor.b64  	%rd2417, %rd2416, %rd2411;
	mov.b64	{%r1350, %r1351}, %rd2417;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2418, {%r1345, %r1349};
	add.s64 	%rd2419, %rd2362, %rd1389;
	add.s64 	%rd2420, %rd2419, %rd2371;
	xor.b64  	%rd2421, %rd2387, %rd2420;
	mov.b64	{%r3235, %r3236}, %rd2421;
	mov.b64	%rd2422, {%r3236, %r3235};
	add.s64 	%rd2423, %rd2402, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2362;
	mov.b64	{%r3237, %r3238}, %rd2424;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2425, {%r3240, %r3239};
	add.s64 	%rd2426, %rd2420, %rd1385;
	add.s64 	%rd2427, %rd2426, %rd2425;
	xor.b64  	%rd2428, %rd2427, %rd2422;
	mov.b64	{%r3241, %r3242}, %rd2428;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2429, {%r3244, %r3243};
	add.s64 	%rd2430, %rd2429, %rd2423;
	xor.b64  	%rd2431, %rd2430, %rd2425;
	mov.b64	{%r1358, %r1359}, %rd2431;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2432, {%r1353, %r1357};
	add.s64 	%rd2433, %rd2376, %rd1388;
	add.s64 	%rd2434, %rd2433, %rd2385;
	xor.b64  	%rd2435, %rd2401, %rd2434;
	mov.b64	{%r3245, %r3246}, %rd2435;
	mov.b64	%rd2436, {%r3246, %r3245};
	add.s64 	%rd2437, %rd2436, %rd2360;
	xor.b64  	%rd2438, %rd2437, %rd2376;
	mov.b64	{%r3247, %r3248}, %rd2438;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2439, {%r3250, %r3249};
	add.s64 	%rd2440, %rd2434, %rd1387;
	add.s64 	%rd2441, %rd2440, %rd2439;
	xor.b64  	%rd2442, %rd2441, %rd2436;
	mov.b64	{%r3251, %r3252}, %rd2442;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2443, {%r3254, %r3253};
	add.s64 	%rd2444, %rd2443, %rd2437;
	xor.b64  	%rd2445, %rd2444, %rd2439;
	mov.b64	{%r1366, %r1367}, %rd2445;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2446, {%r1361, %r1365};
	add.s64 	%rd2447, %rd2390, %rd1382;
	add.s64 	%rd2448, %rd2447, %rd2399;
	xor.b64  	%rd2449, %rd2448, %rd2359;
	mov.b64	{%r3255, %r3256}, %rd2449;
	mov.b64	%rd2450, {%r3256, %r3255};
	add.s64 	%rd2451, %rd2450, %rd2374;
	xor.b64  	%rd2452, %rd2451, %rd2390;
	mov.b64	{%r3257, %r3258}, %rd2452;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2453, {%r3260, %r3259};
	add.s64 	%rd2454, %rd2448, %rd1386;
	add.s64 	%rd2455, %rd2454, %rd2453;
	xor.b64  	%rd2456, %rd2455, %rd2450;
	mov.b64	{%r3261, %r3262}, %rd2456;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2457, {%r3264, %r3263};
	add.s64 	%rd2458, %rd2457, %rd2451;
	xor.b64  	%rd2459, %rd2458, %rd2453;
	mov.b64	{%r1374, %r1375}, %rd2459;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2460, {%r1369, %r1373};
	add.s64 	%rd2461, %rd2413, %rd1396;
	add.s64 	%rd2462, %rd2461, %rd2432;
	xor.b64  	%rd2463, %rd2457, %rd2462;
	mov.b64	{%r3265, %r3266}, %rd2463;
	mov.b64	%rd2464, {%r3266, %r3265};
	add.s64 	%rd2465, %rd2464, %rd2444;
	xor.b64  	%rd2466, %rd2465, %rd2432;
	mov.b64	{%r3267, %r3268}, %rd2466;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2467, {%r3270, %r3269};
	add.s64 	%rd2468, %rd2462, %rd1392;
	add.s64 	%rd2469, %rd2468, %rd2467;
	xor.b64  	%rd2470, %rd2464, %rd2469;
	mov.b64	{%r3271, %r3272}, %rd2470;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2471, {%r3274, %r3273};
	add.s64 	%rd2472, %rd2471, %rd2465;
	xor.b64  	%rd2473, %rd2472, %rd2467;
	mov.b64	{%r1382, %r1383}, %rd2473;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2474, {%r1377, %r1381};
	add.s64 	%rd2475, %rd2427, %rd1390;
	add.s64 	%rd2476, %rd2475, %rd2446;
	xor.b64  	%rd2477, %rd2476, %rd2415;
	mov.b64	{%r3275, %r3276}, %rd2477;
	mov.b64	%rd2478, {%r3276, %r3275};
	add.s64 	%rd2479, %rd2478, %rd2458;
	xor.b64  	%rd2480, %rd2479, %rd2446;
	mov.b64	{%r3277, %r3278}, %rd2480;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2481, {%r3280, %r3279};
	add.s64 	%rd2482, %rd2476, %rd1395;
	add.s64 	%rd2483, %rd2482, %rd2481;
	xor.b64  	%rd2484, %rd2483, %rd2478;
	mov.b64	{%r3281, %r3282}, %rd2484;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2485, {%r3284, %r3283};
	add.s64 	%rd2486, %rd2485, %rd2479;
	xor.b64  	%rd2487, %rd2486, %rd2481;
	mov.b64	{%r1390, %r1391}, %rd2487;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2488, {%r1385, %r1389};
	add.s64 	%rd2489, %rd2441, %rd1384;
	add.s64 	%rd2490, %rd2489, %rd2460;
	xor.b64  	%rd2491, %rd2490, %rd2429;
	mov.b64	{%r3285, %r3286}, %rd2491;
	mov.b64	%rd2492, {%r3286, %r3285};
	add.s64 	%rd2493, %rd2492, %rd2416;
	xor.b64  	%rd2494, %rd2493, %rd2460;
	mov.b64	{%r3287, %r3288}, %rd2494;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2495, {%r3290, %r3289};
	add.s64 	%rd2496, %rd2490, %rd1393;
	add.s64 	%rd2497, %rd2496, %rd2495;
	xor.b64  	%rd2498, %rd2497, %rd2492;
	mov.b64	{%r3291, %r3292}, %rd2498;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2499, {%r3294, %r3293};
	add.s64 	%rd2500, %rd2499, %rd2493;
	xor.b64  	%rd2501, %rd2500, %rd2495;
	mov.b64	{%r1398, %r1399}, %rd2501;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2502, {%r1393, %r1397};
	add.s64 	%rd2503, %rd2418, %rd1394;
	add.s64 	%rd2504, %rd2503, %rd2455;
	xor.b64  	%rd2505, %rd2504, %rd2443;
	mov.b64	{%r3295, %r3296}, %rd2505;
	mov.b64	%rd2506, {%r3296, %r3295};
	add.s64 	%rd2507, %rd2506, %rd2430;
	xor.b64  	%rd2508, %rd2507, %rd2418;
	mov.b64	{%r3297, %r3298}, %rd2508;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2509, {%r3300, %r3299};
	add.s64 	%rd2510, %rd2504, %rd1381;
	add.s64 	%rd2511, %rd2510, %rd2509;
	xor.b64  	%rd2512, %rd2511, %rd2506;
	mov.b64	{%r3301, %r3302}, %rd2512;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2513, {%r3304, %r3303};
	add.s64 	%rd2514, %rd2513, %rd2507;
	xor.b64  	%rd2515, %rd2514, %rd2509;
	mov.b64	{%r1406, %r1407}, %rd2515;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2516, {%r1401, %r1405};
	add.s64 	%rd2517, %rd2469, %rd1381;
	add.s64 	%rd2518, %rd2517, %rd2516;
	xor.b64  	%rd2519, %rd2518, %rd2485;
	mov.b64	{%r3305, %r3306}, %rd2519;
	mov.b64	%rd2520, {%r3306, %r3305};
	add.s64 	%rd2521, %rd2520, %rd2500;
	xor.b64  	%rd2522, %rd2521, %rd2516;
	mov.b64	{%r3307, %r3308}, %rd2522;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2523, {%r3310, %r3309};
	add.s64 	%rd2524, %rd2518, %rd1382;
	add.s64 	%rd2525, %rd2524, %rd2523;
	xor.b64  	%rd2526, %rd2520, %rd2525;
	mov.b64	{%r3311, %r3312}, %rd2526;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2527, {%r3314, %r3313};
	add.s64 	%rd2528, %rd2521, %rd2527;
	xor.b64  	%rd2529, %rd2528, %rd2523;
	mov.b64	{%r1414, %r1415}, %rd2529;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2530, {%r1409, %r1413};
	add.s64 	%rd2531, %rd2474, %rd1383;
	add.s64 	%rd2532, %rd2531, %rd2483;
	xor.b64  	%rd2533, %rd2499, %rd2532;
	mov.b64	{%r3315, %r3316}, %rd2533;
	mov.b64	%rd2534, {%r3316, %r3315};
	add.s64 	%rd2535, %rd2514, %rd2534;
	xor.b64  	%rd2536, %rd2535, %rd2474;
	mov.b64	{%r3317, %r3318}, %rd2536;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2537, {%r3320, %r3319};
	add.s64 	%rd2538, %rd2532, %rd1384;
	add.s64 	%rd2539, %rd2538, %rd2537;
	xor.b64  	%rd2540, %rd2539, %rd2534;
	mov.b64	{%r3321, %r3322}, %rd2540;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2541, {%r3324, %r3323};
	add.s64 	%rd2542, %rd2541, %rd2535;
	xor.b64  	%rd2543, %rd2542, %rd2537;
	mov.b64	{%r1422, %r1423}, %rd2543;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2544, {%r1417, %r1421};
	add.s64 	%rd2545, %rd2488, %rd1385;
	add.s64 	%rd2546, %rd2545, %rd2497;
	xor.b64  	%rd2547, %rd2513, %rd2546;
	mov.b64	{%r3325, %r3326}, %rd2547;
	mov.b64	%rd2548, {%r3326, %r3325};
	add.s64 	%rd2549, %rd2548, %rd2472;
	xor.b64  	%rd2550, %rd2549, %rd2488;
	mov.b64	{%r3327, %r3328}, %rd2550;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2551, {%r3330, %r3329};
	add.s64 	%rd2552, %rd2546, %rd1386;
	add.s64 	%rd2553, %rd2552, %rd2551;
	xor.b64  	%rd2554, %rd2553, %rd2548;
	mov.b64	{%r3331, %r3332}, %rd2554;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2555, {%r3334, %r3333};
	add.s64 	%rd2556, %rd2555, %rd2549;
	xor.b64  	%rd2557, %rd2556, %rd2551;
	mov.b64	{%r1430, %r1431}, %rd2557;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2558, {%r1425, %r1429};
	add.s64 	%rd2559, %rd2502, %rd1387;
	add.s64 	%rd2560, %rd2559, %rd2511;
	xor.b64  	%rd2561, %rd2560, %rd2471;
	mov.b64	{%r3335, %r3336}, %rd2561;
	mov.b64	%rd2562, {%r3336, %r3335};
	add.s64 	%rd2563, %rd2562, %rd2486;
	xor.b64  	%rd2564, %rd2563, %rd2502;
	mov.b64	{%r3337, %r3338}, %rd2564;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2565, {%r3340, %r3339};
	add.s64 	%rd2566, %rd2560, %rd1388;
	add.s64 	%rd2567, %rd2566, %rd2565;
	xor.b64  	%rd2568, %rd2567, %rd2562;
	mov.b64	{%r3341, %r3342}, %rd2568;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2569, {%r3344, %r3343};
	add.s64 	%rd2570, %rd2569, %rd2563;
	xor.b64  	%rd2571, %rd2570, %rd2565;
	mov.b64	{%r1438, %r1439}, %rd2571;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2572, {%r1433, %r1437};
	add.s64 	%rd2573, %rd2525, %rd1389;
	add.s64 	%rd2574, %rd2573, %rd2544;
	xor.b64  	%rd2575, %rd2569, %rd2574;
	mov.b64	{%r3345, %r3346}, %rd2575;
	mov.b64	%rd2576, {%r3346, %r3345};
	add.s64 	%rd2577, %rd2576, %rd2556;
	xor.b64  	%rd2578, %rd2577, %rd2544;
	mov.b64	{%r3347, %r3348}, %rd2578;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2579, {%r3350, %r3349};
	add.s64 	%rd2580, %rd2574, %rd1390;
	add.s64 	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2576, %rd2581;
	mov.b64	{%r3351, %r3352}, %rd2582;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2583, {%r3354, %r3353};
	add.s64 	%rd2584, %rd2583, %rd2577;
	xor.b64  	%rd2585, %rd2584, %rd2579;
	mov.b64	{%r1446, %r1447}, %rd2585;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2586, {%r1441, %r1445};
	add.s64 	%rd2587, %rd2539, %rd1391;
	add.s64 	%rd2588, %rd2587, %rd2558;
	xor.b64  	%rd2589, %rd2588, %rd2527;
	mov.b64	{%r3355, %r3356}, %rd2589;
	mov.b64	%rd2590, {%r3356, %r3355};
	add.s64 	%rd2591, %rd2590, %rd2570;
	xor.b64  	%rd2592, %rd2591, %rd2558;
	mov.b64	{%r3357, %r3358}, %rd2592;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2593, {%r3360, %r3359};
	add.s64 	%rd2594, %rd2588, %rd1392;
	add.s64 	%rd2595, %rd2594, %rd2593;
	xor.b64  	%rd2596, %rd2595, %rd2590;
	mov.b64	{%r3361, %r3362}, %rd2596;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2597, {%r3364, %r3363};
	add.s64 	%rd2598, %rd2597, %rd2591;
	xor.b64  	%rd2599, %rd2598, %rd2593;
	mov.b64	{%r1454, %r1455}, %rd2599;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2600, {%r1449, %r1453};
	add.s64 	%rd2601, %rd2553, %rd1393;
	add.s64 	%rd2602, %rd2601, %rd2572;
	xor.b64  	%rd2603, %rd2602, %rd2541;
	mov.b64	{%r3365, %r3366}, %rd2603;
	mov.b64	%rd2604, {%r3366, %r3365};
	add.s64 	%rd2605, %rd2604, %rd2528;
	xor.b64  	%rd2606, %rd2605, %rd2572;
	mov.b64	{%r3367, %r3368}, %rd2606;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2607, {%r3370, %r3369};
	add.s64 	%rd2608, %rd2602, %rd1394;
	add.s64 	%rd2609, %rd2608, %rd2607;
	xor.b64  	%rd2610, %rd2609, %rd2604;
	mov.b64	{%r3371, %r3372}, %rd2610;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2611, {%r3374, %r3373};
	add.s64 	%rd2612, %rd2611, %rd2605;
	xor.b64  	%rd2613, %rd2612, %rd2607;
	mov.b64	{%r1462, %r1463}, %rd2613;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2614, {%r1457, %r1461};
	add.s64 	%rd2615, %rd2530, %rd1395;
	add.s64 	%rd2616, %rd2615, %rd2567;
	xor.b64  	%rd2617, %rd2616, %rd2555;
	mov.b64	{%r3375, %r3376}, %rd2617;
	mov.b64	%rd2618, {%r3376, %r3375};
	add.s64 	%rd2619, %rd2618, %rd2542;
	xor.b64  	%rd2620, %rd2619, %rd2530;
	mov.b64	{%r3377, %r3378}, %rd2620;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2621, {%r3380, %r3379};
	add.s64 	%rd2622, %rd2616, %rd1396;
	add.s64 	%rd2623, %rd2622, %rd2621;
	xor.b64  	%rd2624, %rd2623, %rd2618;
	mov.b64	{%r3381, %r3382}, %rd2624;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2625, {%r3384, %r3383};
	add.s64 	%rd2626, %rd2625, %rd2619;
	xor.b64  	%rd2627, %rd2626, %rd2621;
	mov.b64	{%r1470, %r1471}, %rd2627;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2628, {%r1465, %r1469};
	add.s64 	%rd2629, %rd2581, %rd1395;
	add.s64 	%rd2630, %rd2629, %rd2628;
	xor.b64  	%rd2631, %rd2630, %rd2597;
	mov.b64	{%r3385, %r3386}, %rd2631;
	mov.b64	%rd2632, {%r3386, %r3385};
	add.s64 	%rd2633, %rd2632, %rd2612;
	xor.b64  	%rd2634, %rd2633, %rd2628;
	mov.b64	{%r3387, %r3388}, %rd2634;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2635, {%r3390, %r3389};
	add.s64 	%rd2636, %rd2630, %rd1391;
	add.s64 	%rd2637, %rd2636, %rd2635;
	xor.b64  	%rd2638, %rd2632, %rd2637;
	mov.b64	{%r3391, %r3392}, %rd2638;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2639, {%r3394, %r3393};
	add.s64 	%rd2640, %rd2633, %rd2639;
	xor.b64  	%rd2641, %rd2640, %rd2635;
	mov.b64	{%r1478, %r1479}, %rd2641;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2642, {%r1473, %r1477};
	add.s64 	%rd2643, %rd2586, %rd1385;
	add.s64 	%rd2644, %rd2643, %rd2595;
	xor.b64  	%rd2645, %rd2611, %rd2644;
	mov.b64	{%r3395, %r3396}, %rd2645;
	mov.b64	%rd2646, {%r3396, %r3395};
	add.s64 	%rd2647, %rd2626, %rd2646;
	xor.b64  	%rd2648, %rd2647, %rd2586;
	mov.b64	{%r3397, %r3398}, %rd2648;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2649, {%r3400, %r3399};
	add.s64 	%rd2650, %rd2644, %rd1389;
	add.s64 	%rd2651, %rd2650, %rd2649;
	xor.b64  	%rd2652, %rd2651, %rd2646;
	mov.b64	{%r3401, %r3402}, %rd2652;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2653, {%r3404, %r3403};
	add.s64 	%rd2654, %rd2653, %rd2647;
	xor.b64  	%rd2655, %rd2654, %rd2649;
	mov.b64	{%r1486, %r1487}, %rd2655;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2656, {%r1481, %r1485};
	add.s64 	%rd2657, %rd2600, %rd1390;
	add.s64 	%rd2658, %rd2657, %rd2609;
	xor.b64  	%rd2659, %rd2625, %rd2658;
	mov.b64	{%r3405, %r3406}, %rd2659;
	mov.b64	%rd2660, {%r3406, %r3405};
	add.s64 	%rd2661, %rd2660, %rd2584;
	xor.b64  	%rd2662, %rd2661, %rd2600;
	mov.b64	{%r3407, %r3408}, %rd2662;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2663, {%r3410, %r3409};
	add.s64 	%rd2664, %rd2658, %rd1396;
	add.s64 	%rd2665, %rd2664, %rd2663;
	xor.b64  	%rd2666, %rd2665, %rd2660;
	mov.b64	{%r3411, %r3412}, %rd2666;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2667, {%r3414, %r3413};
	add.s64 	%rd2668, %rd2667, %rd2661;
	xor.b64  	%rd2669, %rd2668, %rd2663;
	mov.b64	{%r1494, %r1495}, %rd2669;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2670, {%r1489, %r1493};
	add.s64 	%rd2671, %rd2614, %rd1394;
	add.s64 	%rd2672, %rd2671, %rd2623;
	xor.b64  	%rd2673, %rd2672, %rd2583;
	mov.b64	{%r3415, %r3416}, %rd2673;
	mov.b64	%rd2674, {%r3416, %r3415};
	add.s64 	%rd2675, %rd2674, %rd2598;
	xor.b64  	%rd2676, %rd2675, %rd2614;
	mov.b64	{%r3417, %r3418}, %rd2676;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2677, {%r3420, %r3419};
	add.s64 	%rd2678, %rd2672, %rd1387;
	add.s64 	%rd2679, %rd2678, %rd2677;
	xor.b64  	%rd2680, %rd2679, %rd2674;
	mov.b64	{%r3421, %r3422}, %rd2680;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2681, {%r3424, %r3423};
	add.s64 	%rd2682, %rd2681, %rd2675;
	xor.b64  	%rd2683, %rd2682, %rd2677;
	mov.b64	{%r1502, %r1503}, %rd2683;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2684, {%r1497, %r1501};
	add.s64 	%rd2685, %rd2637, %rd1382;
	add.s64 	%rd2686, %rd2685, %rd2656;
	xor.b64  	%rd2687, %rd2681, %rd2686;
	mov.b64	{%r3425, %r3426}, %rd2687;
	mov.b64	%rd2688, {%r3426, %r3425};
	add.s64 	%rd2689, %rd2688, %rd2668;
	xor.b64  	%rd2690, %rd2689, %rd2656;
	mov.b64	{%r3427, %r3428}, %rd2690;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2691, {%r3430, %r3429};
	add.s64 	%rd2692, %rd2686, %rd1393;
	add.s64 	%rd2693, %rd2692, %rd2691;
	xor.b64  	%rd2694, %rd2688, %rd2693;
	mov.b64	{%r3431, %r3432}, %rd2694;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2695, {%r3434, %r3433};
	add.s64 	%rd2696, %rd2695, %rd2689;
	xor.b64  	%rd2697, %rd2696, %rd2691;
	mov.b64	{%r1510, %r1511}, %rd2697;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	mov.b64	%rd2698, {%r1505, %r1509};
	add.s64 	%rd2699, %rd2651, %rd1381;
	add.s64 	%rd2700, %rd2699, %rd2670;
	xor.b64  	%rd2701, %rd2700, %rd2639;
	mov.b64	{%r3435, %r3436}, %rd2701;
	mov.b64	%rd2702, {%r3436, %r3435};
	add.s64 	%rd2703, %rd2702, %rd2682;
	xor.b64  	%rd2704, %rd2703, %rd2670;
	mov.b64	{%r3437, %r3438}, %rd2704;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2705, {%r3440, %r3439};
	add.s64 	%rd2706, %rd2700, %rd1383;
	add.s64 	%rd2707, %rd2706, %rd2705;
	xor.b64  	%rd2708, %rd2707, %rd2702;
	mov.b64	{%r3441, %r3442}, %rd2708;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2709, {%r3444, %r3443};
	add.s64 	%rd2710, %rd2709, %rd2703;
	xor.b64  	%rd2711, %rd2710, %rd2705;
	mov.b64	{%r1518, %r1519}, %rd2711;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	mov.b64	%rd2712, {%r1513, %r1517};
	add.s64 	%rd2713, %rd2665, %rd1392;
	add.s64 	%rd2714, %rd2713, %rd2684;
	xor.b64  	%rd2715, %rd2714, %rd2653;
	mov.b64	{%r3445, %r3446}, %rd2715;
	mov.b64	%rd2716, {%r3446, %r3445};
	add.s64 	%rd2717, %rd2716, %rd2640;
	xor.b64  	%rd2718, %rd2717, %rd2684;
	mov.b64	{%r3447, %r3448}, %rd2718;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2719, {%r3450, %r3449};
	add.s64 	%rd2720, %rd2714, %rd1388;
	add.s64 	%rd2721, %rd2720, %rd2719;
	xor.b64  	%rd2722, %rd2721, %rd2716;
	mov.b64	{%r3451, %r3452}, %rd2722;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2723, {%r3454, %r3453};
	add.s64 	%rd2724, %rd2723, %rd2717;
	xor.b64  	%rd2725, %rd2724, %rd2719;
	mov.b64	{%r1526, %r1527}, %rd2725;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	mov.b64	%rd2726, {%r1521, %r1525};
	add.s64 	%rd2727, %rd2642, %rd1386;
	add.s64 	%rd2728, %rd2727, %rd2679;
	xor.b64  	%rd2729, %rd2728, %rd2667;
	mov.b64	{%r3455, %r3456}, %rd2729;
	mov.b64	%rd2730, {%r3456, %r3455};
	add.s64 	%rd2731, %rd2730, %rd2654;
	xor.b64  	%rd2732, %rd2731, %rd2642;
	mov.b64	{%r3457, %r3458}, %rd2732;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2733, {%r3460, %r3459};
	add.s64 	%rd2734, %rd2728, %rd1384;
	add.s64 	%rd2735, %rd2734, %rd2733;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r3461, %r3462}, %rd2736;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2737, {%r3464, %r3463};
	add.s64 	%rd2738, %rd2737, %rd2731;
	xor.b64  	%rd2739, %rd2738, %rd2733;
	mov.b64	{%r1534, %r1535}, %rd2739;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	mov.b64	%rd2740, {%r1529, %r1533};
	xor.b64  	%rd2741, %rd2693, %rd1366;
	xor.b64  	%rd2742, %rd2741, %rd2724;
	st.global.u64 	[%rd8], %rd2742;
	xor.b64  	%rd2743, %rd2707, %rd1368;
	xor.b64  	%rd2744, %rd2743, %rd2738;
	st.global.u64 	[%rd8+8], %rd2744;
	xor.b64  	%rd2745, %rd2696, %rd1370;
	xor.b64  	%rd2746, %rd2745, %rd2721;
	st.global.u64 	[%rd8+16], %rd2746;
	xor.b64  	%rd2747, %rd2710, %rd1372;
	xor.b64  	%rd2748, %rd2747, %rd2735;
	st.global.u64 	[%rd8+24], %rd2748;
	xor.b64  	%rd2749, %rd2709, %rd1374;
	xor.b64  	%rd2750, %rd2749, %rd2740;
	st.global.u64 	[%rd8+32], %rd2750;
	xor.b64  	%rd2751, %rd2698, %rd1376;
	xor.b64  	%rd2752, %rd2751, %rd2723;
	st.global.u64 	[%rd8+40], %rd2752;
	xor.b64  	%rd2753, %rd2712, %rd1378;
	xor.b64  	%rd2754, %rd2753, %rd2737;
	st.global.u64 	[%rd8+48], %rd2754;
	xor.b64  	%rd2755, %rd2695, %rd1380;
	xor.b64  	%rd2756, %rd2755, %rd2726;
	st.global.u64 	[%rd8+56], %rd2756;
	ret;
}

	// .globl	_Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj
.visible .entry _Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj(
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj_param_0,
	.param .u64 _Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj_param_1,
	.param .u32 _Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<1133>;
	.reg .b64 	%rd<40>;
	// demoted variable
	.shared .align 4 .b8 _ZZ11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvjE1T[8192];

	ld.param.u64 	%rd13, [_Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj_param_0];
	ld.param.u64 	%rd14, [_Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj_param_1];
	ld.param.u32 	%r45, [_Z11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvj_param_2];
	shl.b32 	%r46, %r45, 2;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r1127, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r1127;
	setp.ge.u32	%p1, %r4, %r46;
	@%p1 bra 	BB11_6;

	setp.gt.s32	%p2, %r1127, 2047;
	@%p2 bra 	BB11_3;

BB11_2:
	mul.wide.s32 	%rd15, %r1127, 4;
	mov.u64 	%rd16, AES_TABLE;
	add.s64 	%rd17, %rd16, %rd15;
	ld.const.u32 	%r47, [%rd17];
	shl.b32 	%r48, %r1127, 2;
	mov.u32 	%r49, _ZZ11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvjE1T;
	add.s32 	%r50, %r49, %r48;
	st.shared.u32 	[%r50], %r47;
	add.s32 	%r1127, %r1127, %r1;
	setp.lt.s32	%p3, %r1127, 2048;
	@%p3 bra 	BB11_2;

BB11_3:
	and.b32  	%r7, %r4, 3;
	bar.sync 	0;
	shl.b32 	%r52, %r7, 2;
	mul.wide.u32 	%rd18, %r52, 4;
	mov.u64 	%rd19, AES_STATE_HASH;
	add.s64 	%rd20, %rd19, %rd18;
	ld.const.u32 	%r1129, [%rd20];
	ld.const.u32 	%r1130, [%rd20+4];
	ld.const.u32 	%r1131, [%rd20+8];
	ld.const.u32 	%r1132, [%rd20+12];
	and.b32  	%r53, %r4, 1;
	shl.b32 	%r54, %r4, 4;
	and.b32  	%r55, %r54, 16;
	add.s32 	%r12, %r55, 8;
	xor.b32  	%r56, %r55, 16;
	add.s32 	%r13, %r56, 8;
	cvt.u64.u32	%rd1, %r7;
	setp.eq.s32	%p4, %r53, 0;
	mov.u32 	%r57, _ZZ11hashAes1Rx4ILy2097152ELj0ELj64EEvPKvPvjE1T;
	add.s32 	%r58, %r57, 4096;
	selp.b32	%r14, %r57, %r58, %p4;
	add.s32 	%r59, %r57, 7168;
	add.s32 	%r60, %r57, 1024;
	selp.b32	%r15, %r60, %r59, %p4;
	add.s32 	%r61, %r57, 6144;
	add.s32 	%r62, %r57, 2048;
	selp.b32	%r16, %r62, %r61, %p4;
	add.s32 	%r63, %r57, 5120;
	add.s32 	%r64, %r57, 3072;
	selp.b32	%r17, %r64, %r63, %p4;
	and.b32  	%r66, %r4, -4;
	cvt.u64.u32	%rd21, %r66;
	cvt.u64.u32	%rd22, %r4;
	and.b64  	%rd23, %rd22, 3;
	or.b64  	%rd24, %rd21, %rd23;
	cvta.to.global.u64 	%rd25, %rd13;
	shl.b64 	%rd26, %rd24, 4;
	add.s64 	%rd39, %rd25, %rd26;
	shl.b32 	%r67, %r45, 5;
	mul.wide.u32 	%rd3, %r67, 16;
	mul.lo.s32 	%r68, %r45, 28;
	mul.wide.u32 	%rd4, %r68, 16;
	mul.lo.s32 	%r69, %r45, 24;
	mul.wide.u32 	%rd5, %r69, 16;
	mul.lo.s32 	%r70, %r45, 20;
	mul.wide.u32 	%rd6, %r70, 16;
	shl.b32 	%r71, %r45, 4;
	mul.wide.u32 	%rd7, %r71, 16;
	mul.lo.s32 	%r72, %r45, 12;
	mul.wide.u32 	%rd8, %r72, 16;
	shl.b32 	%r73, %r45, 3;
	mul.wide.u32 	%rd9, %r73, 16;
	mul.wide.u32 	%rd10, %r46, 16;
	shr.u32 	%r18, %r4, 2;
	mov.u32 	%r1128, -32;
	bra.uni 	BB11_4;

BB11_7:
	add.s64 	%rd12, %rd39, %rd3;
	ld.global.v4.u32 	{%r615, %r616, %r617, %r618}, [%rd39];
	ld.shared.u32 	%r623, [%r24];
	xor.b32  	%r624, %r615, %r623;
	ld.shared.u32 	%r625, [%r25];
	xor.b32  	%r626, %r624, %r625;
	ld.shared.u32 	%r627, [%r26];
	xor.b32  	%r628, %r626, %r627;
	ld.shared.u32 	%r629, [%r27];
	xor.b32  	%r319, %r628, %r629;
	ld.shared.u32 	%r630, [%r28];
	xor.b32  	%r631, %r630, %r616;
	ld.shared.u32 	%r632, [%r29];
	xor.b32  	%r633, %r631, %r632;
	ld.shared.u32 	%r634, [%r30];
	xor.b32  	%r635, %r633, %r634;
	ld.shared.u32 	%r636, [%r31];
	xor.b32  	%r322, %r635, %r636;
	ld.shared.u32 	%r637, [%r32];
	xor.b32  	%r638, %r637, %r617;
	ld.shared.u32 	%r639, [%r33];
	xor.b32  	%r640, %r638, %r639;
	ld.shared.u32 	%r641, [%r34];
	xor.b32  	%r642, %r640, %r641;
	ld.shared.u32 	%r643, [%r35];
	xor.b32  	%r325, %r642, %r643;
	ld.shared.u32 	%r644, [%r36];
	xor.b32  	%r645, %r644, %r618;
	ld.shared.u32 	%r646, [%r37];
	xor.b32  	%r647, %r645, %r646;
	ld.shared.u32 	%r648, [%r38];
	xor.b32  	%r649, %r647, %r648;
	ld.shared.u32 	%r650, [%r39];
	xor.b32  	%r316, %r649, %r650;
	add.s64 	%rd32, %rd39, %rd10;
	ld.global.v4.u32 	{%r651, %r652, %r653, %r654}, [%rd32];
	// inline asm
	bfe.u32 %r279, %r319, %r113, 8;
	// inline asm
	shl.b32 	%r659, %r279, 2;
	add.s32 	%r660, %r14, %r659;
	ld.shared.u32 	%r661, [%r660];
	// inline asm
	bfe.u32 %r282, %r322, %r12, 8;
	// inline asm
	shl.b32 	%r662, %r282, 2;
	add.s32 	%r663, %r15, %r662;
	ld.shared.u32 	%r664, [%r663];
	// inline asm
	bfe.u32 %r285, %r325, %r119, 8;
	// inline asm
	shl.b32 	%r665, %r285, 2;
	add.s32 	%r666, %r16, %r665;
	ld.shared.u32 	%r667, [%r666];
	// inline asm
	bfe.u32 %r288, %r316, %r13, 8;
	// inline asm
	shl.b32 	%r668, %r288, 2;
	add.s32 	%r669, %r17, %r668;
	xor.b32  	%r670, %r661, %r651;
	xor.b32  	%r671, %r670, %r664;
	xor.b32  	%r672, %r671, %r667;
	ld.shared.u32 	%r673, [%r669];
	xor.b32  	%r367, %r672, %r673;
	// inline asm
	bfe.u32 %r291, %r322, %r113, 8;
	// inline asm
	shl.b32 	%r674, %r291, 2;
	add.s32 	%r675, %r14, %r674;
	ld.shared.u32 	%r676, [%r675];
	// inline asm
	bfe.u32 %r294, %r325, %r12, 8;
	// inline asm
	shl.b32 	%r677, %r294, 2;
	add.s32 	%r678, %r15, %r677;
	ld.shared.u32 	%r679, [%r678];
	// inline asm
	bfe.u32 %r297, %r316, %r119, 8;
	// inline asm
	shl.b32 	%r680, %r297, 2;
	add.s32 	%r681, %r16, %r680;
	ld.shared.u32 	%r682, [%r681];
	// inline asm
	bfe.u32 %r300, %r319, %r13, 8;
	// inline asm
	shl.b32 	%r683, %r300, 2;
	add.s32 	%r684, %r17, %r683;
	xor.b32  	%r685, %r676, %r652;
	xor.b32  	%r686, %r685, %r679;
	xor.b32  	%r687, %r686, %r682;
	ld.shared.u32 	%r688, [%r684];
	xor.b32  	%r370, %r687, %r688;
	// inline asm
	bfe.u32 %r303, %r325, %r113, 8;
	// inline asm
	shl.b32 	%r689, %r303, 2;
	add.s32 	%r690, %r14, %r689;
	ld.shared.u32 	%r691, [%r690];
	// inline asm
	bfe.u32 %r306, %r316, %r12, 8;
	// inline asm
	shl.b32 	%r692, %r306, 2;
	add.s32 	%r693, %r15, %r692;
	ld.shared.u32 	%r694, [%r693];
	// inline asm
	bfe.u32 %r309, %r319, %r119, 8;
	// inline asm
	shl.b32 	%r695, %r309, 2;
	add.s32 	%r696, %r16, %r695;
	ld.shared.u32 	%r697, [%r696];
	// inline asm
	bfe.u32 %r312, %r322, %r13, 8;
	// inline asm
	shl.b32 	%r698, %r312, 2;
	add.s32 	%r699, %r17, %r698;
	xor.b32  	%r700, %r691, %r653;
	xor.b32  	%r701, %r700, %r694;
	xor.b32  	%r702, %r701, %r697;
	ld.shared.u32 	%r703, [%r699];
	xor.b32  	%r373, %r702, %r703;
	// inline asm
	bfe.u32 %r315, %r316, %r113, 8;
	// inline asm
	shl.b32 	%r704, %r315, 2;
	add.s32 	%r705, %r14, %r704;
	ld.shared.u32 	%r706, [%r705];
	// inline asm
	bfe.u32 %r318, %r319, %r12, 8;
	// inline asm
	shl.b32 	%r707, %r318, 2;
	add.s32 	%r708, %r15, %r707;
	ld.shared.u32 	%r709, [%r708];
	// inline asm
	bfe.u32 %r321, %r322, %r119, 8;
	// inline asm
	shl.b32 	%r710, %r321, 2;
	add.s32 	%r711, %r16, %r710;
	ld.shared.u32 	%r712, [%r711];
	// inline asm
	bfe.u32 %r324, %r325, %r13, 8;
	// inline asm
	shl.b32 	%r713, %r324, 2;
	add.s32 	%r714, %r17, %r713;
	xor.b32  	%r715, %r706, %r654;
	xor.b32  	%r716, %r715, %r709;
	xor.b32  	%r717, %r716, %r712;
	ld.shared.u32 	%r718, [%r714];
	xor.b32  	%r364, %r717, %r718;
	add.s64 	%rd33, %rd39, %rd9;
	ld.global.v4.u32 	{%r719, %r720, %r721, %r722}, [%rd33];
	// inline asm
	bfe.u32 %r327, %r367, %r113, 8;
	// inline asm
	shl.b32 	%r727, %r327, 2;
	add.s32 	%r728, %r14, %r727;
	ld.shared.u32 	%r729, [%r728];
	// inline asm
	bfe.u32 %r330, %r370, %r12, 8;
	// inline asm
	shl.b32 	%r730, %r330, 2;
	add.s32 	%r731, %r15, %r730;
	ld.shared.u32 	%r732, [%r731];
	// inline asm
	bfe.u32 %r333, %r373, %r119, 8;
	// inline asm
	shl.b32 	%r733, %r333, 2;
	add.s32 	%r734, %r16, %r733;
	ld.shared.u32 	%r735, [%r734];
	// inline asm
	bfe.u32 %r336, %r364, %r13, 8;
	// inline asm
	shl.b32 	%r736, %r336, 2;
	add.s32 	%r737, %r17, %r736;
	xor.b32  	%r738, %r729, %r719;
	xor.b32  	%r739, %r738, %r732;
	xor.b32  	%r740, %r739, %r735;
	ld.shared.u32 	%r741, [%r737];
	xor.b32  	%r415, %r740, %r741;
	// inline asm
	bfe.u32 %r339, %r370, %r113, 8;
	// inline asm
	shl.b32 	%r742, %r339, 2;
	add.s32 	%r743, %r14, %r742;
	ld.shared.u32 	%r744, [%r743];
	// inline asm
	bfe.u32 %r342, %r373, %r12, 8;
	// inline asm
	shl.b32 	%r745, %r342, 2;
	add.s32 	%r746, %r15, %r745;
	ld.shared.u32 	%r747, [%r746];
	// inline asm
	bfe.u32 %r345, %r364, %r119, 8;
	// inline asm
	shl.b32 	%r748, %r345, 2;
	add.s32 	%r749, %r16, %r748;
	ld.shared.u32 	%r750, [%r749];
	// inline asm
	bfe.u32 %r348, %r367, %r13, 8;
	// inline asm
	shl.b32 	%r751, %r348, 2;
	add.s32 	%r752, %r17, %r751;
	xor.b32  	%r753, %r744, %r720;
	xor.b32  	%r754, %r753, %r747;
	xor.b32  	%r755, %r754, %r750;
	ld.shared.u32 	%r756, [%r752];
	xor.b32  	%r418, %r755, %r756;
	// inline asm
	bfe.u32 %r351, %r373, %r113, 8;
	// inline asm
	shl.b32 	%r757, %r351, 2;
	add.s32 	%r758, %r14, %r757;
	ld.shared.u32 	%r759, [%r758];
	// inline asm
	bfe.u32 %r354, %r364, %r12, 8;
	// inline asm
	shl.b32 	%r760, %r354, 2;
	add.s32 	%r761, %r15, %r760;
	ld.shared.u32 	%r762, [%r761];
	// inline asm
	bfe.u32 %r357, %r367, %r119, 8;
	// inline asm
	shl.b32 	%r763, %r357, 2;
	add.s32 	%r764, %r16, %r763;
	ld.shared.u32 	%r765, [%r764];
	// inline asm
	bfe.u32 %r360, %r370, %r13, 8;
	// inline asm
	shl.b32 	%r766, %r360, 2;
	add.s32 	%r767, %r17, %r766;
	xor.b32  	%r768, %r759, %r721;
	xor.b32  	%r769, %r768, %r762;
	xor.b32  	%r770, %r769, %r765;
	ld.shared.u32 	%r771, [%r767];
	xor.b32  	%r421, %r770, %r771;
	// inline asm
	bfe.u32 %r363, %r364, %r113, 8;
	// inline asm
	shl.b32 	%r772, %r363, 2;
	add.s32 	%r773, %r14, %r772;
	ld.shared.u32 	%r774, [%r773];
	// inline asm
	bfe.u32 %r366, %r367, %r12, 8;
	// inline asm
	shl.b32 	%r775, %r366, 2;
	add.s32 	%r776, %r15, %r775;
	ld.shared.u32 	%r777, [%r776];
	// inline asm
	bfe.u32 %r369, %r370, %r119, 8;
	// inline asm
	shl.b32 	%r778, %r369, 2;
	add.s32 	%r779, %r16, %r778;
	ld.shared.u32 	%r780, [%r779];
	// inline asm
	bfe.u32 %r372, %r373, %r13, 8;
	// inline asm
	shl.b32 	%r781, %r372, 2;
	add.s32 	%r782, %r17, %r781;
	xor.b32  	%r783, %r774, %r722;
	xor.b32  	%r784, %r783, %r777;
	xor.b32  	%r785, %r784, %r780;
	ld.shared.u32 	%r786, [%r782];
	xor.b32  	%r412, %r785, %r786;
	add.s64 	%rd34, %rd39, %rd8;
	ld.global.v4.u32 	{%r787, %r788, %r789, %r790}, [%rd34];
	// inline asm
	bfe.u32 %r375, %r415, %r113, 8;
	// inline asm
	shl.b32 	%r795, %r375, 2;
	add.s32 	%r796, %r14, %r795;
	ld.shared.u32 	%r797, [%r796];
	// inline asm
	bfe.u32 %r378, %r418, %r12, 8;
	// inline asm
	shl.b32 	%r798, %r378, 2;
	add.s32 	%r799, %r15, %r798;
	ld.shared.u32 	%r800, [%r799];
	// inline asm
	bfe.u32 %r381, %r421, %r119, 8;
	// inline asm
	shl.b32 	%r801, %r381, 2;
	add.s32 	%r802, %r16, %r801;
	ld.shared.u32 	%r803, [%r802];
	// inline asm
	bfe.u32 %r384, %r412, %r13, 8;
	// inline asm
	shl.b32 	%r804, %r384, 2;
	add.s32 	%r805, %r17, %r804;
	xor.b32  	%r806, %r797, %r787;
	xor.b32  	%r807, %r806, %r800;
	xor.b32  	%r808, %r807, %r803;
	ld.shared.u32 	%r809, [%r805];
	xor.b32  	%r463, %r808, %r809;
	// inline asm
	bfe.u32 %r387, %r418, %r113, 8;
	// inline asm
	shl.b32 	%r810, %r387, 2;
	add.s32 	%r811, %r14, %r810;
	ld.shared.u32 	%r812, [%r811];
	// inline asm
	bfe.u32 %r390, %r421, %r12, 8;
	// inline asm
	shl.b32 	%r813, %r390, 2;
	add.s32 	%r814, %r15, %r813;
	ld.shared.u32 	%r815, [%r814];
	// inline asm
	bfe.u32 %r393, %r412, %r119, 8;
	// inline asm
	shl.b32 	%r816, %r393, 2;
	add.s32 	%r817, %r16, %r816;
	ld.shared.u32 	%r818, [%r817];
	// inline asm
	bfe.u32 %r396, %r415, %r13, 8;
	// inline asm
	shl.b32 	%r819, %r396, 2;
	add.s32 	%r820, %r17, %r819;
	xor.b32  	%r821, %r812, %r788;
	xor.b32  	%r822, %r821, %r815;
	xor.b32  	%r823, %r822, %r818;
	ld.shared.u32 	%r824, [%r820];
	xor.b32  	%r466, %r823, %r824;
	// inline asm
	bfe.u32 %r399, %r421, %r113, 8;
	// inline asm
	shl.b32 	%r825, %r399, 2;
	add.s32 	%r826, %r14, %r825;
	ld.shared.u32 	%r827, [%r826];
	// inline asm
	bfe.u32 %r402, %r412, %r12, 8;
	// inline asm
	shl.b32 	%r828, %r402, 2;
	add.s32 	%r829, %r15, %r828;
	ld.shared.u32 	%r830, [%r829];
	// inline asm
	bfe.u32 %r405, %r415, %r119, 8;
	// inline asm
	shl.b32 	%r831, %r405, 2;
	add.s32 	%r832, %r16, %r831;
	ld.shared.u32 	%r833, [%r832];
	// inline asm
	bfe.u32 %r408, %r418, %r13, 8;
	// inline asm
	shl.b32 	%r834, %r408, 2;
	add.s32 	%r835, %r17, %r834;
	xor.b32  	%r836, %r827, %r789;
	xor.b32  	%r837, %r836, %r830;
	xor.b32  	%r838, %r837, %r833;
	ld.shared.u32 	%r839, [%r835];
	xor.b32  	%r469, %r838, %r839;
	// inline asm
	bfe.u32 %r411, %r412, %r113, 8;
	// inline asm
	shl.b32 	%r840, %r411, 2;
	add.s32 	%r841, %r14, %r840;
	ld.shared.u32 	%r842, [%r841];
	// inline asm
	bfe.u32 %r414, %r415, %r12, 8;
	// inline asm
	shl.b32 	%r843, %r414, 2;
	add.s32 	%r844, %r15, %r843;
	ld.shared.u32 	%r845, [%r844];
	// inline asm
	bfe.u32 %r417, %r418, %r119, 8;
	// inline asm
	shl.b32 	%r846, %r417, 2;
	add.s32 	%r847, %r16, %r846;
	ld.shared.u32 	%r848, [%r847];
	// inline asm
	bfe.u32 %r420, %r421, %r13, 8;
	// inline asm
	shl.b32 	%r849, %r420, 2;
	add.s32 	%r850, %r17, %r849;
	xor.b32  	%r851, %r842, %r790;
	xor.b32  	%r852, %r851, %r845;
	xor.b32  	%r853, %r852, %r848;
	ld.shared.u32 	%r854, [%r850];
	xor.b32  	%r460, %r853, %r854;
	add.s64 	%rd35, %rd39, %rd7;
	ld.global.v4.u32 	{%r855, %r856, %r857, %r858}, [%rd35];
	// inline asm
	bfe.u32 %r423, %r463, %r113, 8;
	// inline asm
	shl.b32 	%r863, %r423, 2;
	add.s32 	%r864, %r14, %r863;
	ld.shared.u32 	%r865, [%r864];
	// inline asm
	bfe.u32 %r426, %r466, %r12, 8;
	// inline asm
	shl.b32 	%r866, %r426, 2;
	add.s32 	%r867, %r15, %r866;
	ld.shared.u32 	%r868, [%r867];
	// inline asm
	bfe.u32 %r429, %r469, %r119, 8;
	// inline asm
	shl.b32 	%r869, %r429, 2;
	add.s32 	%r870, %r16, %r869;
	ld.shared.u32 	%r871, [%r870];
	// inline asm
	bfe.u32 %r432, %r460, %r13, 8;
	// inline asm
	shl.b32 	%r872, %r432, 2;
	add.s32 	%r873, %r17, %r872;
	xor.b32  	%r874, %r865, %r855;
	xor.b32  	%r875, %r874, %r868;
	xor.b32  	%r876, %r875, %r871;
	ld.shared.u32 	%r877, [%r873];
	xor.b32  	%r511, %r876, %r877;
	// inline asm
	bfe.u32 %r435, %r466, %r113, 8;
	// inline asm
	shl.b32 	%r878, %r435, 2;
	add.s32 	%r879, %r14, %r878;
	ld.shared.u32 	%r880, [%r879];
	// inline asm
	bfe.u32 %r438, %r469, %r12, 8;
	// inline asm
	shl.b32 	%r881, %r438, 2;
	add.s32 	%r882, %r15, %r881;
	ld.shared.u32 	%r883, [%r882];
	// inline asm
	bfe.u32 %r441, %r460, %r119, 8;
	// inline asm
	shl.b32 	%r884, %r441, 2;
	add.s32 	%r885, %r16, %r884;
	ld.shared.u32 	%r886, [%r885];
	// inline asm
	bfe.u32 %r444, %r463, %r13, 8;
	// inline asm
	shl.b32 	%r887, %r444, 2;
	add.s32 	%r888, %r17, %r887;
	xor.b32  	%r889, %r880, %r856;
	xor.b32  	%r890, %r889, %r883;
	xor.b32  	%r891, %r890, %r886;
	ld.shared.u32 	%r892, [%r888];
	xor.b32  	%r514, %r891, %r892;
	// inline asm
	bfe.u32 %r447, %r469, %r113, 8;
	// inline asm
	shl.b32 	%r893, %r447, 2;
	add.s32 	%r894, %r14, %r893;
	ld.shared.u32 	%r895, [%r894];
	// inline asm
	bfe.u32 %r450, %r460, %r12, 8;
	// inline asm
	shl.b32 	%r896, %r450, 2;
	add.s32 	%r897, %r15, %r896;
	ld.shared.u32 	%r898, [%r897];
	// inline asm
	bfe.u32 %r453, %r463, %r119, 8;
	// inline asm
	shl.b32 	%r899, %r453, 2;
	add.s32 	%r900, %r16, %r899;
	ld.shared.u32 	%r901, [%r900];
	// inline asm
	bfe.u32 %r456, %r466, %r13, 8;
	// inline asm
	shl.b32 	%r902, %r456, 2;
	add.s32 	%r903, %r17, %r902;
	xor.b32  	%r904, %r895, %r857;
	xor.b32  	%r905, %r904, %r898;
	xor.b32  	%r906, %r905, %r901;
	ld.shared.u32 	%r907, [%r903];
	xor.b32  	%r517, %r906, %r907;
	// inline asm
	bfe.u32 %r459, %r460, %r113, 8;
	// inline asm
	shl.b32 	%r908, %r459, 2;
	add.s32 	%r909, %r14, %r908;
	ld.shared.u32 	%r910, [%r909];
	// inline asm
	bfe.u32 %r462, %r463, %r12, 8;
	// inline asm
	shl.b32 	%r911, %r462, 2;
	add.s32 	%r912, %r15, %r911;
	ld.shared.u32 	%r913, [%r912];
	// inline asm
	bfe.u32 %r465, %r466, %r119, 8;
	// inline asm
	shl.b32 	%r914, %r465, 2;
	add.s32 	%r915, %r16, %r914;
	ld.shared.u32 	%r916, [%r915];
	// inline asm
	bfe.u32 %r468, %r469, %r13, 8;
	// inline asm
	shl.b32 	%r917, %r468, 2;
	add.s32 	%r918, %r17, %r917;
	xor.b32  	%r919, %r910, %r858;
	xor.b32  	%r920, %r919, %r913;
	xor.b32  	%r921, %r920, %r916;
	ld.shared.u32 	%r922, [%r918];
	xor.b32  	%r508, %r921, %r922;
	add.s64 	%rd36, %rd39, %rd6;
	ld.global.v4.u32 	{%r923, %r924, %r925, %r926}, [%rd36];
	// inline asm
	bfe.u32 %r471, %r511, %r113, 8;
	// inline asm
	shl.b32 	%r931, %r471, 2;
	add.s32 	%r932, %r14, %r931;
	ld.shared.u32 	%r933, [%r932];
	// inline asm
	bfe.u32 %r474, %r514, %r12, 8;
	// inline asm
	shl.b32 	%r934, %r474, 2;
	add.s32 	%r935, %r15, %r934;
	ld.shared.u32 	%r936, [%r935];
	// inline asm
	bfe.u32 %r477, %r517, %r119, 8;
	// inline asm
	shl.b32 	%r937, %r477, 2;
	add.s32 	%r938, %r16, %r937;
	ld.shared.u32 	%r939, [%r938];
	// inline asm
	bfe.u32 %r480, %r508, %r13, 8;
	// inline asm
	shl.b32 	%r940, %r480, 2;
	add.s32 	%r941, %r17, %r940;
	xor.b32  	%r942, %r933, %r923;
	xor.b32  	%r943, %r942, %r936;
	xor.b32  	%r944, %r943, %r939;
	ld.shared.u32 	%r945, [%r941];
	xor.b32  	%r559, %r944, %r945;
	// inline asm
	bfe.u32 %r483, %r514, %r113, 8;
	// inline asm
	shl.b32 	%r946, %r483, 2;
	add.s32 	%r947, %r14, %r946;
	ld.shared.u32 	%r948, [%r947];
	// inline asm
	bfe.u32 %r486, %r517, %r12, 8;
	// inline asm
	shl.b32 	%r949, %r486, 2;
	add.s32 	%r950, %r15, %r949;
	ld.shared.u32 	%r951, [%r950];
	// inline asm
	bfe.u32 %r489, %r508, %r119, 8;
	// inline asm
	shl.b32 	%r952, %r489, 2;
	add.s32 	%r953, %r16, %r952;
	ld.shared.u32 	%r954, [%r953];
	// inline asm
	bfe.u32 %r492, %r511, %r13, 8;
	// inline asm
	shl.b32 	%r955, %r492, 2;
	add.s32 	%r956, %r17, %r955;
	xor.b32  	%r957, %r948, %r924;
	xor.b32  	%r958, %r957, %r951;
	xor.b32  	%r959, %r958, %r954;
	ld.shared.u32 	%r960, [%r956];
	xor.b32  	%r562, %r959, %r960;
	// inline asm
	bfe.u32 %r495, %r517, %r113, 8;
	// inline asm
	shl.b32 	%r961, %r495, 2;
	add.s32 	%r962, %r14, %r961;
	ld.shared.u32 	%r963, [%r962];
	// inline asm
	bfe.u32 %r498, %r508, %r12, 8;
	// inline asm
	shl.b32 	%r964, %r498, 2;
	add.s32 	%r965, %r15, %r964;
	ld.shared.u32 	%r966, [%r965];
	// inline asm
	bfe.u32 %r501, %r511, %r119, 8;
	// inline asm
	shl.b32 	%r967, %r501, 2;
	add.s32 	%r968, %r16, %r967;
	ld.shared.u32 	%r969, [%r968];
	// inline asm
	bfe.u32 %r504, %r514, %r13, 8;
	// inline asm
	shl.b32 	%r970, %r504, 2;
	add.s32 	%r971, %r17, %r970;
	xor.b32  	%r972, %r963, %r925;
	xor.b32  	%r973, %r972, %r966;
	xor.b32  	%r974, %r973, %r969;
	ld.shared.u32 	%r975, [%r971];
	xor.b32  	%r565, %r974, %r975;
	// inline asm
	bfe.u32 %r507, %r508, %r113, 8;
	// inline asm
	shl.b32 	%r976, %r507, 2;
	add.s32 	%r977, %r14, %r976;
	ld.shared.u32 	%r978, [%r977];
	// inline asm
	bfe.u32 %r510, %r511, %r12, 8;
	// inline asm
	shl.b32 	%r979, %r510, 2;
	add.s32 	%r980, %r15, %r979;
	ld.shared.u32 	%r981, [%r980];
	// inline asm
	bfe.u32 %r513, %r514, %r119, 8;
	// inline asm
	shl.b32 	%r982, %r513, 2;
	add.s32 	%r983, %r16, %r982;
	ld.shared.u32 	%r984, [%r983];
	// inline asm
	bfe.u32 %r516, %r517, %r13, 8;
	// inline asm
	shl.b32 	%r985, %r516, 2;
	add.s32 	%r986, %r17, %r985;
	xor.b32  	%r987, %r978, %r926;
	xor.b32  	%r988, %r987, %r981;
	xor.b32  	%r989, %r988, %r984;
	ld.shared.u32 	%r990, [%r986];
	xor.b32  	%r556, %r989, %r990;
	add.s64 	%rd37, %rd39, %rd5;
	ld.global.v4.u32 	{%r991, %r992, %r993, %r994}, [%rd37];
	// inline asm
	bfe.u32 %r519, %r559, %r113, 8;
	// inline asm
	shl.b32 	%r999, %r519, 2;
	add.s32 	%r1000, %r14, %r999;
	ld.shared.u32 	%r1001, [%r1000];
	// inline asm
	bfe.u32 %r522, %r562, %r12, 8;
	// inline asm
	shl.b32 	%r1002, %r522, 2;
	add.s32 	%r1003, %r15, %r1002;
	ld.shared.u32 	%r1004, [%r1003];
	// inline asm
	bfe.u32 %r525, %r565, %r119, 8;
	// inline asm
	shl.b32 	%r1005, %r525, 2;
	add.s32 	%r1006, %r16, %r1005;
	ld.shared.u32 	%r1007, [%r1006];
	// inline asm
	bfe.u32 %r528, %r556, %r13, 8;
	// inline asm
	shl.b32 	%r1008, %r528, 2;
	add.s32 	%r1009, %r17, %r1008;
	xor.b32  	%r1010, %r1001, %r991;
	xor.b32  	%r1011, %r1010, %r1004;
	xor.b32  	%r1012, %r1011, %r1007;
	ld.shared.u32 	%r1013, [%r1009];
	xor.b32  	%r607, %r1012, %r1013;
	// inline asm
	bfe.u32 %r531, %r562, %r113, 8;
	// inline asm
	shl.b32 	%r1014, %r531, 2;
	add.s32 	%r1015, %r14, %r1014;
	ld.shared.u32 	%r1016, [%r1015];
	// inline asm
	bfe.u32 %r534, %r565, %r12, 8;
	// inline asm
	shl.b32 	%r1017, %r534, 2;
	add.s32 	%r1018, %r15, %r1017;
	ld.shared.u32 	%r1019, [%r1018];
	// inline asm
	bfe.u32 %r537, %r556, %r119, 8;
	// inline asm
	shl.b32 	%r1020, %r537, 2;
	add.s32 	%r1021, %r16, %r1020;
	ld.shared.u32 	%r1022, [%r1021];
	// inline asm
	bfe.u32 %r540, %r559, %r13, 8;
	// inline asm
	shl.b32 	%r1023, %r540, 2;
	add.s32 	%r1024, %r17, %r1023;
	xor.b32  	%r1025, %r1016, %r992;
	xor.b32  	%r1026, %r1025, %r1019;
	xor.b32  	%r1027, %r1026, %r1022;
	ld.shared.u32 	%r1028, [%r1024];
	xor.b32  	%r610, %r1027, %r1028;
	// inline asm
	bfe.u32 %r543, %r565, %r113, 8;
	// inline asm
	shl.b32 	%r1029, %r543, 2;
	add.s32 	%r1030, %r14, %r1029;
	ld.shared.u32 	%r1031, [%r1030];
	// inline asm
	bfe.u32 %r546, %r556, %r12, 8;
	// inline asm
	shl.b32 	%r1032, %r546, 2;
	add.s32 	%r1033, %r15, %r1032;
	ld.shared.u32 	%r1034, [%r1033];
	// inline asm
	bfe.u32 %r549, %r559, %r119, 8;
	// inline asm
	shl.b32 	%r1035, %r549, 2;
	add.s32 	%r1036, %r16, %r1035;
	ld.shared.u32 	%r1037, [%r1036];
	// inline asm
	bfe.u32 %r552, %r562, %r13, 8;
	// inline asm
	shl.b32 	%r1038, %r552, 2;
	add.s32 	%r1039, %r17, %r1038;
	xor.b32  	%r1040, %r1031, %r993;
	xor.b32  	%r1041, %r1040, %r1034;
	xor.b32  	%r1042, %r1041, %r1037;
	ld.shared.u32 	%r1043, [%r1039];
	xor.b32  	%r613, %r1042, %r1043;
	// inline asm
	bfe.u32 %r555, %r556, %r113, 8;
	// inline asm
	shl.b32 	%r1044, %r555, 2;
	add.s32 	%r1045, %r14, %r1044;
	ld.shared.u32 	%r1046, [%r1045];
	// inline asm
	bfe.u32 %r558, %r559, %r12, 8;
	// inline asm
	shl.b32 	%r1047, %r558, 2;
	add.s32 	%r1048, %r15, %r1047;
	ld.shared.u32 	%r1049, [%r1048];
	// inline asm
	bfe.u32 %r561, %r562, %r119, 8;
	// inline asm
	shl.b32 	%r1050, %r561, 2;
	add.s32 	%r1051, %r16, %r1050;
	ld.shared.u32 	%r1052, [%r1051];
	// inline asm
	bfe.u32 %r564, %r565, %r13, 8;
	// inline asm
	shl.b32 	%r1053, %r564, 2;
	add.s32 	%r1054, %r17, %r1053;
	xor.b32  	%r1055, %r1046, %r994;
	xor.b32  	%r1056, %r1055, %r1049;
	xor.b32  	%r1057, %r1056, %r1052;
	ld.shared.u32 	%r1058, [%r1054];
	xor.b32  	%r604, %r1057, %r1058;
	add.s64 	%rd38, %rd39, %rd4;
	ld.global.v4.u32 	{%r1059, %r1060, %r1061, %r1062}, [%rd38];
	// inline asm
	bfe.u32 %r567, %r607, %r113, 8;
	// inline asm
	shl.b32 	%r1067, %r567, 2;
	add.s32 	%r1068, %r14, %r1067;
	ld.shared.u32 	%r1069, [%r1068];
	// inline asm
	bfe.u32 %r570, %r610, %r12, 8;
	// inline asm
	shl.b32 	%r1070, %r570, 2;
	add.s32 	%r1071, %r15, %r1070;
	ld.shared.u32 	%r1072, [%r1071];
	// inline asm
	bfe.u32 %r573, %r613, %r119, 8;
	// inline asm
	shl.b32 	%r1073, %r573, 2;
	add.s32 	%r1074, %r16, %r1073;
	ld.shared.u32 	%r1075, [%r1074];
	// inline asm
	bfe.u32 %r576, %r604, %r13, 8;
	// inline asm
	shl.b32 	%r1076, %r576, 2;
	add.s32 	%r1077, %r17, %r1076;
	xor.b32  	%r1078, %r1069, %r1059;
	xor.b32  	%r1079, %r1078, %r1072;
	xor.b32  	%r1080, %r1079, %r1075;
	ld.shared.u32 	%r1081, [%r1077];
	xor.b32  	%r1129, %r1080, %r1081;
	// inline asm
	bfe.u32 %r579, %r610, %r113, 8;
	// inline asm
	shl.b32 	%r1082, %r579, 2;
	add.s32 	%r1083, %r14, %r1082;
	ld.shared.u32 	%r1084, [%r1083];
	// inline asm
	bfe.u32 %r582, %r613, %r12, 8;
	// inline asm
	shl.b32 	%r1085, %r582, 2;
	add.s32 	%r1086, %r15, %r1085;
	ld.shared.u32 	%r1087, [%r1086];
	// inline asm
	bfe.u32 %r585, %r604, %r119, 8;
	// inline asm
	shl.b32 	%r1088, %r585, 2;
	add.s32 	%r1089, %r16, %r1088;
	ld.shared.u32 	%r1090, [%r1089];
	// inline asm
	bfe.u32 %r588, %r607, %r13, 8;
	// inline asm
	shl.b32 	%r1091, %r588, 2;
	add.s32 	%r1092, %r17, %r1091;
	xor.b32  	%r1093, %r1084, %r1060;
	xor.b32  	%r1094, %r1093, %r1087;
	xor.b32  	%r1095, %r1094, %r1090;
	ld.shared.u32 	%r1096, [%r1092];
	xor.b32  	%r1130, %r1095, %r1096;
	// inline asm
	bfe.u32 %r591, %r613, %r113, 8;
	// inline asm
	shl.b32 	%r1097, %r591, 2;
	add.s32 	%r1098, %r14, %r1097;
	ld.shared.u32 	%r1099, [%r1098];
	// inline asm
	bfe.u32 %r594, %r604, %r12, 8;
	// inline asm
	shl.b32 	%r1100, %r594, 2;
	add.s32 	%r1101, %r15, %r1100;
	ld.shared.u32 	%r1102, [%r1101];
	// inline asm
	bfe.u32 %r597, %r607, %r119, 8;
	// inline asm
	shl.b32 	%r1103, %r597, 2;
	add.s32 	%r1104, %r16, %r1103;
	ld.shared.u32 	%r1105, [%r1104];
	// inline asm
	bfe.u32 %r600, %r610, %r13, 8;
	// inline asm
	shl.b32 	%r1106, %r600, 2;
	add.s32 	%r1107, %r17, %r1106;
	xor.b32  	%r1108, %r1099, %r1061;
	xor.b32  	%r1109, %r1108, %r1102;
	xor.b32  	%r1110, %r1109, %r1105;
	ld.shared.u32 	%r1111, [%r1107];
	xor.b32  	%r1131, %r1110, %r1111;
	// inline asm
	bfe.u32 %r603, %r604, %r113, 8;
	// inline asm
	shl.b32 	%r1112, %r603, 2;
	add.s32 	%r1113, %r14, %r1112;
	ld.shared.u32 	%r1114, [%r1113];
	// inline asm
	bfe.u32 %r606, %r607, %r12, 8;
	// inline asm
	shl.b32 	%r1115, %r606, 2;
	add.s32 	%r1116, %r15, %r1115;
	ld.shared.u32 	%r1117, [%r1116];
	// inline asm
	bfe.u32 %r609, %r610, %r119, 8;
	// inline asm
	shl.b32 	%r1118, %r609, 2;
	add.s32 	%r1119, %r16, %r1118;
	ld.shared.u32 	%r1120, [%r1119];
	// inline asm
	bfe.u32 %r612, %r613, %r13, 8;
	// inline asm
	shl.b32 	%r1121, %r612, 2;
	add.s32 	%r1122, %r17, %r1121;
	xor.b32  	%r1123, %r1114, %r1062;
	xor.b32  	%r1124, %r1123, %r1117;
	xor.b32  	%r1125, %r1124, %r1120;
	ld.shared.u32 	%r1126, [%r1122];
	xor.b32  	%r1132, %r1125, %r1126;
	mov.u64 	%rd39, %rd12;

BB11_4:
	mov.u32 	%r113, 0;
	// inline asm
	bfe.u32 %r75, %r1129, %r113, 8;
	// inline asm
	shl.b32 	%r123, %r75, 2;
	add.s32 	%r24, %r14, %r123;
	// inline asm
	bfe.u32 %r78, %r1130, %r12, 8;
	// inline asm
	shl.b32 	%r124, %r78, 2;
	add.s32 	%r25, %r15, %r124;
	mov.u32 	%r119, 16;
	// inline asm
	bfe.u32 %r81, %r1131, %r119, 8;
	// inline asm
	shl.b32 	%r125, %r81, 2;
	add.s32 	%r26, %r16, %r125;
	// inline asm
	bfe.u32 %r84, %r1132, %r13, 8;
	// inline asm
	shl.b32 	%r126, %r84, 2;
	add.s32 	%r27, %r17, %r126;
	// inline asm
	bfe.u32 %r87, %r1130, %r113, 8;
	// inline asm
	shl.b32 	%r127, %r87, 2;
	add.s32 	%r28, %r14, %r127;
	// inline asm
	bfe.u32 %r90, %r1131, %r12, 8;
	// inline asm
	shl.b32 	%r128, %r90, 2;
	add.s32 	%r29, %r15, %r128;
	// inline asm
	bfe.u32 %r93, %r1132, %r119, 8;
	// inline asm
	shl.b32 	%r129, %r93, 2;
	add.s32 	%r30, %r16, %r129;
	// inline asm
	bfe.u32 %r96, %r1129, %r13, 8;
	// inline asm
	shl.b32 	%r130, %r96, 2;
	add.s32 	%r31, %r17, %r130;
	// inline asm
	bfe.u32 %r99, %r1131, %r113, 8;
	// inline asm
	shl.b32 	%r131, %r99, 2;
	add.s32 	%r32, %r14, %r131;
	// inline asm
	bfe.u32 %r102, %r1132, %r12, 8;
	// inline asm
	shl.b32 	%r132, %r102, 2;
	add.s32 	%r33, %r15, %r132;
	// inline asm
	bfe.u32 %r105, %r1129, %r119, 8;
	// inline asm
	shl.b32 	%r133, %r105, 2;
	add.s32 	%r34, %r16, %r133;
	// inline asm
	bfe.u32 %r108, %r1130, %r13, 8;
	// inline asm
	shl.b32 	%r134, %r108, 2;
	add.s32 	%r35, %r17, %r134;
	// inline asm
	bfe.u32 %r111, %r1132, %r113, 8;
	// inline asm
	shl.b32 	%r135, %r111, 2;
	add.s32 	%r36, %r14, %r135;
	// inline asm
	bfe.u32 %r114, %r1129, %r12, 8;
	// inline asm
	shl.b32 	%r136, %r114, 2;
	add.s32 	%r37, %r15, %r136;
	// inline asm
	bfe.u32 %r117, %r1130, %r119, 8;
	// inline asm
	shl.b32 	%r137, %r117, 2;
	add.s32 	%r38, %r16, %r137;
	// inline asm
	bfe.u32 %r120, %r1131, %r13, 8;
	// inline asm
	shl.b32 	%r138, %r120, 2;
	add.s32 	%r39, %r17, %r138;
	add.s32 	%r1128, %r1128, 32;
	setp.lt.u32	%p5, %r1128, 131072;
	@%p5 bra 	BB11_7;

	ld.shared.u32 	%r187, [%r24];
	ld.shared.u32 	%r188, [%r25];
	xor.b32  	%r189, %r187, %r188;
	ld.shared.u32 	%r190, [%r26];
	xor.b32  	%r191, %r189, %r190;
	ld.shared.u32 	%r192, [%r27];
	xor.b32  	%r193, %r191, %r192;
	xor.b32  	%r140, %r193, 298578503;
	ld.shared.u32 	%r194, [%r28];
	ld.shared.u32 	%r195, [%r29];
	xor.b32  	%r196, %r194, %r195;
	ld.shared.u32 	%r197, [%r30];
	xor.b32  	%r198, %r196, %r197;
	ld.shared.u32 	%r199, [%r31];
	xor.b32  	%r200, %r198, %r199;
	xor.b32  	%r143, %r200, 710578844;
	ld.shared.u32 	%r201, [%r32];
	ld.shared.u32 	%r202, [%r33];
	xor.b32  	%r203, %r201, %r202;
	ld.shared.u32 	%r204, [%r34];
	xor.b32  	%r205, %r203, %r204;
	ld.shared.u32 	%r206, [%r35];
	xor.b32  	%r207, %r205, %r206;
	xor.b32  	%r146, %r207, -456828611;
	ld.shared.u32 	%r208, [%r36];
	ld.shared.u32 	%r209, [%r37];
	xor.b32  	%r210, %r208, %r209;
	ld.shared.u32 	%r211, [%r38];
	xor.b32  	%r212, %r210, %r211;
	ld.shared.u32 	%r213, [%r39];
	xor.b32  	%r214, %r212, %r213;
	xor.b32  	%r149, %r214, -2087382397;
	// inline asm
	bfe.u32 %r139, %r140, %r113, 8;
	// inline asm
	shl.b32 	%r215, %r139, 2;
	add.s32 	%r216, %r14, %r215;
	ld.shared.u32 	%r217, [%r216];
	// inline asm
	bfe.u32 %r142, %r143, %r12, 8;
	// inline asm
	shl.b32 	%r218, %r142, 2;
	add.s32 	%r219, %r15, %r218;
	ld.shared.u32 	%r220, [%r219];
	// inline asm
	bfe.u32 %r145, %r146, %r119, 8;
	// inline asm
	shl.b32 	%r221, %r145, 2;
	add.s32 	%r222, %r16, %r221;
	ld.shared.u32 	%r223, [%r222];
	// inline asm
	bfe.u32 %r148, %r149, %r13, 8;
	// inline asm
	shl.b32 	%r224, %r148, 2;
	add.s32 	%r225, %r17, %r224;
	xor.b32  	%r226, %r217, %r220;
	xor.b32  	%r227, %r226, %r223;
	ld.shared.u32 	%r228, [%r225];
	xor.b32  	%r229, %r227, %r228;
	// inline asm
	bfe.u32 %r151, %r143, %r113, 8;
	// inline asm
	shl.b32 	%r230, %r151, 2;
	add.s32 	%r231, %r14, %r230;
	ld.shared.u32 	%r232, [%r231];
	// inline asm
	bfe.u32 %r154, %r146, %r12, 8;
	// inline asm
	shl.b32 	%r233, %r154, 2;
	add.s32 	%r234, %r15, %r233;
	ld.shared.u32 	%r235, [%r234];
	// inline asm
	bfe.u32 %r157, %r149, %r119, 8;
	// inline asm
	shl.b32 	%r236, %r157, 2;
	add.s32 	%r237, %r16, %r236;
	ld.shared.u32 	%r238, [%r237];
	// inline asm
	bfe.u32 %r160, %r140, %r13, 8;
	// inline asm
	shl.b32 	%r239, %r160, 2;
	add.s32 	%r240, %r17, %r239;
	xor.b32  	%r241, %r232, %r235;
	xor.b32  	%r242, %r241, %r238;
	ld.shared.u32 	%r243, [%r240];
	xor.b32  	%r244, %r242, %r243;
	// inline asm
	bfe.u32 %r163, %r146, %r113, 8;
	// inline asm
	shl.b32 	%r245, %r163, 2;
	add.s32 	%r246, %r14, %r245;
	ld.shared.u32 	%r247, [%r246];
	// inline asm
	bfe.u32 %r166, %r149, %r12, 8;
	// inline asm
	shl.b32 	%r248, %r166, 2;
	add.s32 	%r249, %r15, %r248;
	ld.shared.u32 	%r250, [%r249];
	// inline asm
	bfe.u32 %r169, %r140, %r119, 8;
	// inline asm
	shl.b32 	%r251, %r169, 2;
	add.s32 	%r252, %r16, %r251;
	ld.shared.u32 	%r253, [%r252];
	// inline asm
	bfe.u32 %r172, %r143, %r13, 8;
	// inline asm
	shl.b32 	%r254, %r172, 2;
	add.s32 	%r255, %r17, %r254;
	xor.b32  	%r256, %r247, %r250;
	xor.b32  	%r257, %r256, %r253;
	ld.shared.u32 	%r258, [%r255];
	xor.b32  	%r259, %r257, %r258;
	// inline asm
	bfe.u32 %r175, %r149, %r113, 8;
	// inline asm
	shl.b32 	%r260, %r175, 2;
	add.s32 	%r261, %r14, %r260;
	ld.shared.u32 	%r262, [%r261];
	// inline asm
	bfe.u32 %r178, %r140, %r12, 8;
	// inline asm
	shl.b32 	%r263, %r178, 2;
	add.s32 	%r264, %r15, %r263;
	ld.shared.u32 	%r265, [%r264];
	// inline asm
	bfe.u32 %r181, %r143, %r119, 8;
	// inline asm
	shl.b32 	%r266, %r181, 2;
	add.s32 	%r267, %r16, %r266;
	ld.shared.u32 	%r268, [%r267];
	// inline asm
	bfe.u32 %r184, %r146, %r13, 8;
	// inline asm
	shl.b32 	%r269, %r184, 2;
	add.s32 	%r270, %r17, %r269;
	xor.b32  	%r271, %r262, %r265;
	xor.b32  	%r272, %r271, %r268;
	ld.shared.u32 	%r273, [%r270];
	xor.b32  	%r274, %r272, %r273;
	mul.wide.u32 	%rd27, %r18, 4;
	add.s64 	%rd28, %rd27, %rd1;
	cvta.to.global.u64 	%rd29, %rd14;
	shl.b64 	%rd30, %rd28, 4;
	add.s64 	%rd31, %rd29, %rd30;
	xor.b32  	%r275, %r274, -14591054;
	xor.b32  	%r276, %r259, -1413733085;
	xor.b32  	%r277, %r244, 1199304459;
	xor.b32  	%r278, %r229, -830378859;
	st.global.v4.u32 	[%rd31], {%r278, %r277, %r276, %r275};

BB11_6:
	ret;
}

	// .globl	_Z30blake2b_512_single_block_benchILj76EEvPyPKvy
.visible .entry _Z30blake2b_512_single_block_benchILj76EEvPyPKvy(
	.param .u64 _Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_0,
	.param .u64 _Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_1,
	.param .u64 _Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<1739>;
	.reg .b64 	%rd<1287>;


	ld.param.u64 	%rd3, [_Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_1];
	ld.param.u64 	%rd4, [_Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_2];
	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r769, %ntid.x;
	mov.u32 	%r770, %ctaid.x;
	mul.lo.s32 	%r771, %r769, %r770;
	cvt.u64.u32	%rd6, %r771;
	add.s64 	%rd7, %rd6, %rd4;
	mov.u32 	%r772, %tid.x;
	cvt.u64.u32	%rd8, %r772;
	add.s64 	%rd1, %rd7, %rd8;
	ld.global.u64 	%rd9, [%rd5+16];
	ld.global.u64 	%rd10, [%rd5+24];
	ld.global.u64 	%rd11, [%rd5+32];
	ld.global.u64 	%rd12, [%rd5+40];
	ld.global.u64 	%rd13, [%rd5+48];
	ld.global.u64 	%rd14, [%rd5+56];
	ld.global.u64 	%rd15, [%rd5+64];
	ld.global.u32 	%rd16, [%rd5+72];
	add.s64 	%rd17, %rd1, -4965156021692249063;
	xor.b64  	%rd18, %rd17, 5840696475078001309;
	mov.b64	{%r773, %r774}, %rd18;
	mov.b64	%rd19, {%r774, %r773};
	add.s64 	%rd20, %rd19, 7640891576956012808;
	xor.b64  	%rd21, %rd20, 5840696475078001361;
	mov.b64	{%r775, %r776}, %rd21;
	mov.u32 	%r768, 1;
	mov.u32 	%r777, 25923;
	mov.u32 	%r778, 8455;
	prmt.b32 	%r779, %r775, %r776, %r778;
	prmt.b32 	%r780, %r775, %r776, %r777;
	mov.b64	%rd22, {%r780, %r779};
	ld.global.u64 	%rd23, [%rd5+8];
	add.s64 	%rd24, %rd17, %rd23;
	add.s64 	%rd25, %rd24, %rd22;
	xor.b64  	%rd26, %rd25, %rd19;
	mov.b64	{%r781, %r782}, %rd26;
	mov.u32 	%r783, 21554;
	mov.u32 	%r784, 4214;
	prmt.b32 	%r785, %r781, %r782, %r784;
	prmt.b32 	%r786, %r781, %r782, %r783;
	mov.b64	%rd27, {%r786, %r785};
	add.s64 	%rd28, %rd27, %rd20;
	xor.b64  	%rd29, %rd28, %rd22;
	mov.b64	{%r6, %r7}, %rd29;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r768;
	// inline asm
	mov.b64	%rd30, {%r1, %r5};
	add.s64 	%rd31, %rd9, 6227659224458531674;
	xor.b64  	%rd32, %rd31, -7276294671716946913;
	mov.b64	{%r787, %r788}, %rd32;
	mov.b64	%rd33, {%r788, %r787};
	add.s64 	%rd34, %rd33, -4942790177534073029;
	xor.b64  	%rd35, %rd34, -7276294671716946913;
	mov.b64	{%r789, %r790}, %rd35;
	prmt.b32 	%r791, %r789, %r790, %r778;
	prmt.b32 	%r792, %r789, %r790, %r777;
	mov.b64	%rd36, {%r792, %r791};
	add.s64 	%rd37, %rd10, %rd31;
	add.s64 	%rd38, %rd37, %rd36;
	xor.b64  	%rd39, %rd38, %rd33;
	mov.b64	{%r793, %r794}, %rd39;
	prmt.b32 	%r795, %r793, %r794, %r784;
	prmt.b32 	%r796, %r793, %r794, %r783;
	mov.b64	%rd40, {%r796, %r795};
	add.s64 	%rd41, %rd40, %rd34;
	xor.b64  	%rd42, %rd41, %rd36;
	mov.b64	{%r14, %r15}, %rd42;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r768;
	// inline asm
	mov.b64	%rd43, {%r9, %r13};
	add.s64 	%rd44, %rd11, 6625583534739731862;
	xor.b64  	%rd45, %rd44, -2270897969802886508;
	mov.b64	{%r797, %r798}, %rd45;
	mov.b64	%rd46, {%r798, %r797};
	add.s64 	%rd47, %rd46, 4354685564936845355;
	xor.b64  	%rd48, %rd47, 2270897969802886507;
	mov.b64	{%r799, %r800}, %rd48;
	prmt.b32 	%r801, %r799, %r800, %r778;
	prmt.b32 	%r802, %r799, %r800, %r777;
	mov.b64	%rd49, {%r802, %r801};
	add.s64 	%rd50, %rd12, %rd44;
	add.s64 	%rd51, %rd50, %rd49;
	xor.b64  	%rd52, %rd51, %rd46;
	mov.b64	{%r803, %r804}, %rd52;
	prmt.b32 	%r805, %r803, %r804, %r784;
	prmt.b32 	%r806, %r803, %r804, %r783;
	mov.b64	%rd53, {%r806, %r805};
	add.s64 	%rd54, %rd53, %rd47;
	xor.b64  	%rd55, %rd54, %rd49;
	mov.b64	{%r22, %r23}, %rd55;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r768;
	// inline asm
	mov.b64	%rd56, {%r17, %r21};
	add.s64 	%rd57, %rd13, 85782056580896874;
	xor.b64  	%rd58, %rd57, 6620516959819538809;
	mov.b64	{%r807, %r808}, %rd58;
	mov.b64	%rd59, {%r808, %r807};
	add.s64 	%rd60, %rd59, -6534734903238641935;
	xor.b64  	%rd61, %rd60, 6620516959819538809;
	mov.b64	{%r809, %r810}, %rd61;
	prmt.b32 	%r811, %r809, %r810, %r778;
	prmt.b32 	%r812, %r809, %r810, %r777;
	mov.b64	%rd62, {%r812, %r811};
	add.s64 	%rd63, %rd14, %rd57;
	add.s64 	%rd64, %rd63, %rd62;
	xor.b64  	%rd65, %rd64, %rd59;
	mov.b64	{%r813, %r814}, %rd65;
	prmt.b32 	%r815, %r813, %r814, %r784;
	prmt.b32 	%r816, %r813, %r814, %r783;
	mov.b64	%rd66, {%r816, %r815};
	add.s64 	%rd67, %rd66, %rd60;
	xor.b64  	%rd68, %rd67, %rd62;
	mov.b64	{%r30, %r31}, %rd68;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r768;
	// inline asm
	mov.b64	%rd69, {%r25, %r29};
	add.s64 	%rd70, %rd25, %rd15;
	add.s64 	%rd71, %rd70, %rd43;
	xor.b64  	%rd72, %rd66, %rd71;
	mov.b64	{%r817, %r818}, %rd72;
	mov.b64	%rd73, {%r818, %r817};
	add.s64 	%rd74, %rd73, %rd54;
	xor.b64  	%rd75, %rd74, %rd43;
	mov.b64	{%r819, %r820}, %rd75;
	prmt.b32 	%r821, %r819, %r820, %r778;
	prmt.b32 	%r822, %r819, %r820, %r777;
	mov.b64	%rd76, {%r822, %r821};
	add.s64 	%rd77, %rd71, %rd16;
	add.s64 	%rd78, %rd77, %rd76;
	xor.b64  	%rd79, %rd73, %rd78;
	mov.b64	{%r823, %r824}, %rd79;
	prmt.b32 	%r825, %r823, %r824, %r784;
	prmt.b32 	%r826, %r823, %r824, %r783;
	mov.b64	%rd80, {%r826, %r825};
	add.s64 	%rd81, %rd74, %rd80;
	xor.b64  	%rd82, %rd81, %rd76;
	mov.b64	{%r38, %r39}, %rd82;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r768;
	// inline asm
	mov.b64	%rd83, {%r33, %r37};
	add.s64 	%rd84, %rd56, %rd38;
	xor.b64  	%rd85, %rd84, %rd27;
	mov.b64	{%r827, %r828}, %rd85;
	mov.b64	%rd86, {%r828, %r827};
	add.s64 	%rd87, %rd86, %rd67;
	xor.b64  	%rd88, %rd87, %rd56;
	mov.b64	{%r829, %r830}, %rd88;
	prmt.b32 	%r831, %r829, %r830, %r778;
	prmt.b32 	%r832, %r829, %r830, %r777;
	mov.b64	%rd89, {%r832, %r831};
	add.s64 	%rd90, %rd89, %rd84;
	xor.b64  	%rd91, %rd90, %rd86;
	mov.b64	{%r833, %r834}, %rd91;
	prmt.b32 	%r835, %r833, %r834, %r784;
	prmt.b32 	%r836, %r833, %r834, %r783;
	mov.b64	%rd92, {%r836, %r835};
	add.s64 	%rd93, %rd92, %rd87;
	xor.b64  	%rd94, %rd93, %rd89;
	mov.b64	{%r46, %r47}, %rd94;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r768;
	// inline asm
	mov.b64	%rd95, {%r41, %r45};
	add.s64 	%rd96, %rd69, %rd51;
	xor.b64  	%rd97, %rd96, %rd40;
	mov.b64	{%r837, %r838}, %rd97;
	mov.b64	%rd98, {%r838, %r837};
	add.s64 	%rd99, %rd98, %rd28;
	xor.b64  	%rd100, %rd99, %rd69;
	mov.b64	{%r839, %r840}, %rd100;
	prmt.b32 	%r841, %r839, %r840, %r778;
	prmt.b32 	%r842, %r839, %r840, %r777;
	mov.b64	%rd101, {%r842, %r841};
	add.s64 	%rd102, %rd101, %rd96;
	xor.b64  	%rd103, %rd102, %rd98;
	mov.b64	{%r843, %r844}, %rd103;
	prmt.b32 	%r845, %r843, %r844, %r784;
	prmt.b32 	%r846, %r843, %r844, %r783;
	mov.b64	%rd104, {%r846, %r845};
	add.s64 	%rd105, %rd104, %rd99;
	xor.b64  	%rd106, %rd105, %rd101;
	mov.b64	{%r54, %r55}, %rd106;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r768;
	// inline asm
	mov.b64	%rd107, {%r49, %r53};
	add.s64 	%rd108, %rd64, %rd30;
	xor.b64  	%rd109, %rd108, %rd53;
	mov.b64	{%r847, %r848}, %rd109;
	mov.b64	%rd110, {%r848, %r847};
	add.s64 	%rd111, %rd110, %rd41;
	xor.b64  	%rd112, %rd111, %rd30;
	mov.b64	{%r849, %r850}, %rd112;
	prmt.b32 	%r851, %r849, %r850, %r778;
	prmt.b32 	%r852, %r849, %r850, %r777;
	mov.b64	%rd113, {%r852, %r851};
	add.s64 	%rd114, %rd113, %rd108;
	xor.b64  	%rd115, %rd114, %rd110;
	mov.b64	{%r853, %r854}, %rd115;
	prmt.b32 	%r855, %r853, %r854, %r784;
	prmt.b32 	%r856, %r853, %r854, %r783;
	mov.b64	%rd116, {%r856, %r855};
	add.s64 	%rd117, %rd116, %rd111;
	xor.b64  	%rd118, %rd117, %rd113;
	mov.b64	{%r62, %r63}, %rd118;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r768;
	// inline asm
	mov.b64	%rd119, {%r57, %r61};
	add.s64 	%rd120, %rd119, %rd78;
	xor.b64  	%rd121, %rd120, %rd92;
	mov.b64	{%r857, %r858}, %rd121;
	mov.b64	%rd122, {%r858, %r857};
	add.s64 	%rd123, %rd122, %rd105;
	xor.b64  	%rd124, %rd123, %rd119;
	mov.b64	{%r859, %r860}, %rd124;
	prmt.b32 	%r861, %r859, %r860, %r778;
	prmt.b32 	%r862, %r859, %r860, %r777;
	mov.b64	%rd125, {%r862, %r861};
	add.s64 	%rd126, %rd125, %rd120;
	xor.b64  	%rd127, %rd122, %rd126;
	mov.b64	{%r863, %r864}, %rd127;
	prmt.b32 	%r865, %r863, %r864, %r784;
	prmt.b32 	%r866, %r863, %r864, %r783;
	mov.b64	%rd128, {%r866, %r865};
	add.s64 	%rd129, %rd123, %rd128;
	xor.b64  	%rd130, %rd129, %rd125;
	mov.b64	{%r70, %r71}, %rd130;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r768;
	// inline asm
	mov.b64	%rd131, {%r65, %r69};
	add.s64 	%rd132, %rd83, %rd11;
	add.s64 	%rd133, %rd132, %rd90;
	xor.b64  	%rd134, %rd104, %rd133;
	mov.b64	{%r867, %r868}, %rd134;
	mov.b64	%rd135, {%r868, %r867};
	add.s64 	%rd136, %rd117, %rd135;
	xor.b64  	%rd137, %rd136, %rd83;
	mov.b64	{%r869, %r870}, %rd137;
	prmt.b32 	%r871, %r869, %r870, %r778;
	prmt.b32 	%r872, %r869, %r870, %r777;
	mov.b64	%rd138, {%r872, %r871};
	add.s64 	%rd139, %rd133, %rd15;
	add.s64 	%rd140, %rd139, %rd138;
	xor.b64  	%rd141, %rd140, %rd135;
	mov.b64	{%r873, %r874}, %rd141;
	prmt.b32 	%r875, %r873, %r874, %r784;
	prmt.b32 	%r876, %r873, %r874, %r783;
	mov.b64	%rd142, {%r876, %r875};
	add.s64 	%rd143, %rd142, %rd136;
	xor.b64  	%rd144, %rd143, %rd138;
	mov.b64	{%r78, %r79}, %rd144;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r768;
	// inline asm
	mov.b64	%rd145, {%r73, %r77};
	add.s64 	%rd146, %rd95, %rd16;
	add.s64 	%rd147, %rd146, %rd102;
	xor.b64  	%rd148, %rd116, %rd147;
	mov.b64	{%r877, %r878}, %rd148;
	mov.b64	%rd149, {%r878, %r877};
	add.s64 	%rd150, %rd149, %rd81;
	xor.b64  	%rd151, %rd150, %rd95;
	mov.b64	{%r879, %r880}, %rd151;
	prmt.b32 	%r881, %r879, %r880, %r778;
	prmt.b32 	%r882, %r879, %r880, %r777;
	mov.b64	%rd152, {%r882, %r881};
	add.s64 	%rd153, %rd152, %rd147;
	xor.b64  	%rd154, %rd153, %rd149;
	mov.b64	{%r883, %r884}, %rd154;
	prmt.b32 	%r885, %r883, %r884, %r784;
	prmt.b32 	%r886, %r883, %r884, %r783;
	mov.b64	%rd155, {%r886, %r885};
	add.s64 	%rd156, %rd155, %rd150;
	xor.b64  	%rd157, %rd156, %rd152;
	mov.b64	{%r86, %r87}, %rd157;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r768;
	// inline asm
	mov.b64	%rd158, {%r81, %r85};
	add.s64 	%rd159, %rd114, %rd107;
	xor.b64  	%rd160, %rd159, %rd80;
	mov.b64	{%r887, %r888}, %rd160;
	mov.b64	%rd161, {%r888, %r887};
	add.s64 	%rd162, %rd161, %rd93;
	xor.b64  	%rd163, %rd162, %rd107;
	mov.b64	{%r889, %r890}, %rd163;
	prmt.b32 	%r891, %r889, %r890, %r778;
	prmt.b32 	%r892, %r889, %r890, %r777;
	mov.b64	%rd164, {%r892, %r891};
	add.s64 	%rd165, %rd159, %rd13;
	add.s64 	%rd166, %rd165, %rd164;
	xor.b64  	%rd167, %rd166, %rd161;
	mov.b64	{%r893, %r894}, %rd167;
	prmt.b32 	%r895, %r893, %r894, %r784;
	prmt.b32 	%r896, %r893, %r894, %r783;
	mov.b64	%rd168, {%r896, %r895};
	add.s64 	%rd169, %rd168, %rd162;
	xor.b64  	%rd170, %rd169, %rd164;
	mov.b64	{%r94, %r95}, %rd170;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r768;
	// inline asm
	mov.b64	%rd171, {%r89, %r93};
	add.s64 	%rd172, %rd126, %rd23;
	add.s64 	%rd173, %rd172, %rd145;
	xor.b64  	%rd174, %rd168, %rd173;
	mov.b64	{%r897, %r898}, %rd174;
	mov.b64	%rd175, {%r898, %r897};
	add.s64 	%rd176, %rd175, %rd156;
	xor.b64  	%rd177, %rd176, %rd145;
	mov.b64	{%r899, %r900}, %rd177;
	prmt.b32 	%r901, %r899, %r900, %r778;
	prmt.b32 	%r902, %r899, %r900, %r777;
	mov.b64	%rd178, {%r902, %r901};
	add.s64 	%rd179, %rd178, %rd173;
	xor.b64  	%rd180, %rd175, %rd179;
	mov.b64	{%r903, %r904}, %rd180;
	prmt.b32 	%r905, %r903, %r904, %r784;
	prmt.b32 	%r906, %r903, %r904, %r783;
	mov.b64	%rd181, {%r906, %r905};
	add.s64 	%rd182, %rd176, %rd181;
	xor.b64  	%rd183, %rd182, %rd178;
	mov.b64	{%r102, %r103}, %rd183;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r768;
	// inline asm
	mov.b64	%rd184, {%r97, %r101};
	add.s64 	%rd185, %rd140, %rd1;
	add.s64 	%rd186, %rd185, %rd158;
	xor.b64  	%rd187, %rd186, %rd128;
	mov.b64	{%r907, %r908}, %rd187;
	mov.b64	%rd188, {%r908, %r907};
	add.s64 	%rd189, %rd188, %rd169;
	xor.b64  	%rd190, %rd189, %rd158;
	mov.b64	{%r909, %r910}, %rd190;
	prmt.b32 	%r911, %r909, %r910, %r778;
	prmt.b32 	%r912, %r909, %r910, %r777;
	mov.b64	%rd191, {%r912, %r911};
	add.s64 	%rd192, %rd186, %rd9;
	add.s64 	%rd193, %rd192, %rd191;
	xor.b64  	%rd194, %rd193, %rd188;
	mov.b64	{%r913, %r914}, %rd194;
	prmt.b32 	%r915, %r913, %r914, %r784;
	prmt.b32 	%r916, %r913, %r914, %r783;
	mov.b64	%rd195, {%r916, %r915};
	add.s64 	%rd196, %rd195, %rd189;
	xor.b64  	%rd197, %rd196, %rd191;
	mov.b64	{%r110, %r111}, %rd197;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r768;
	// inline asm
	mov.b64	%rd198, {%r105, %r109};
	add.s64 	%rd199, %rd171, %rd153;
	xor.b64  	%rd200, %rd199, %rd142;
	mov.b64	{%r917, %r918}, %rd200;
	mov.b64	%rd201, {%r918, %r917};
	add.s64 	%rd202, %rd201, %rd129;
	xor.b64  	%rd203, %rd202, %rd171;
	mov.b64	{%r919, %r920}, %rd203;
	prmt.b32 	%r921, %r919, %r920, %r778;
	prmt.b32 	%r922, %r919, %r920, %r777;
	mov.b64	%rd204, {%r922, %r921};
	add.s64 	%rd205, %rd199, %rd14;
	add.s64 	%rd206, %rd205, %rd204;
	xor.b64  	%rd207, %rd206, %rd201;
	mov.b64	{%r923, %r924}, %rd207;
	prmt.b32 	%r925, %r923, %r924, %r784;
	prmt.b32 	%r926, %r923, %r924, %r783;
	mov.b64	%rd208, {%r926, %r925};
	add.s64 	%rd209, %rd208, %rd202;
	xor.b64  	%rd210, %rd209, %rd204;
	mov.b64	{%r118, %r119}, %rd210;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r768;
	// inline asm
	mov.b64	%rd211, {%r113, %r117};
	add.s64 	%rd212, %rd131, %rd12;
	add.s64 	%rd213, %rd212, %rd166;
	xor.b64  	%rd214, %rd213, %rd155;
	mov.b64	{%r927, %r928}, %rd214;
	mov.b64	%rd215, {%r928, %r927};
	add.s64 	%rd216, %rd215, %rd143;
	xor.b64  	%rd217, %rd216, %rd131;
	mov.b64	{%r929, %r930}, %rd217;
	prmt.b32 	%r931, %r929, %r930, %r778;
	prmt.b32 	%r932, %r929, %r930, %r777;
	mov.b64	%rd218, {%r932, %r931};
	add.s64 	%rd219, %rd213, %rd10;
	add.s64 	%rd220, %rd219, %rd218;
	xor.b64  	%rd221, %rd220, %rd215;
	mov.b64	{%r933, %r934}, %rd221;
	prmt.b32 	%r935, %r933, %r934, %r784;
	prmt.b32 	%r936, %r933, %r934, %r783;
	mov.b64	%rd222, {%r936, %r935};
	add.s64 	%rd223, %rd222, %rd216;
	xor.b64  	%rd224, %rd223, %rd218;
	mov.b64	{%r126, %r127}, %rd224;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r768;
	// inline asm
	mov.b64	%rd225, {%r121, %r125};
	add.s64 	%rd226, %rd225, %rd179;
	xor.b64  	%rd227, %rd226, %rd195;
	mov.b64	{%r937, %r938}, %rd227;
	mov.b64	%rd228, {%r938, %r937};
	add.s64 	%rd229, %rd228, %rd209;
	xor.b64  	%rd230, %rd229, %rd225;
	mov.b64	{%r939, %r940}, %rd230;
	prmt.b32 	%r941, %r939, %r940, %r778;
	prmt.b32 	%r942, %r939, %r940, %r777;
	mov.b64	%rd231, {%r942, %r941};
	add.s64 	%rd232, %rd226, %rd15;
	add.s64 	%rd233, %rd232, %rd231;
	xor.b64  	%rd234, %rd228, %rd233;
	mov.b64	{%r943, %r944}, %rd234;
	prmt.b32 	%r945, %r943, %r944, %r784;
	prmt.b32 	%r946, %r943, %r944, %r783;
	mov.b64	%rd235, {%r946, %r945};
	add.s64 	%rd236, %rd229, %rd235;
	xor.b64  	%rd237, %rd236, %rd231;
	mov.b64	{%r134, %r135}, %rd237;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r768;
	// inline asm
	mov.b64	%rd238, {%r129, %r133};
	add.s64 	%rd239, %rd193, %rd184;
	xor.b64  	%rd240, %rd208, %rd239;
	mov.b64	{%r947, %r948}, %rd240;
	mov.b64	%rd241, {%r948, %r947};
	add.s64 	%rd242, %rd223, %rd241;
	xor.b64  	%rd243, %rd242, %rd184;
	mov.b64	{%r949, %r950}, %rd243;
	prmt.b32 	%r951, %r949, %r950, %r778;
	prmt.b32 	%r952, %r949, %r950, %r777;
	mov.b64	%rd244, {%r952, %r951};
	add.s64 	%rd245, %rd239, %rd1;
	add.s64 	%rd246, %rd245, %rd244;
	xor.b64  	%rd247, %rd246, %rd241;
	mov.b64	{%r953, %r954}, %rd247;
	prmt.b32 	%r955, %r953, %r954, %r784;
	prmt.b32 	%r956, %r953, %r954, %r783;
	mov.b64	%rd248, {%r956, %r955};
	add.s64 	%rd249, %rd248, %rd242;
	xor.b64  	%rd250, %rd249, %rd244;
	mov.b64	{%r142, %r143}, %rd250;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r768;
	// inline asm
	mov.b64	%rd251, {%r137, %r141};
	add.s64 	%rd252, %rd198, %rd12;
	add.s64 	%rd253, %rd252, %rd206;
	xor.b64  	%rd254, %rd222, %rd253;
	mov.b64	{%r957, %r958}, %rd254;
	mov.b64	%rd255, {%r958, %r957};
	add.s64 	%rd256, %rd255, %rd182;
	xor.b64  	%rd257, %rd256, %rd198;
	mov.b64	{%r959, %r960}, %rd257;
	prmt.b32 	%r961, %r959, %r960, %r778;
	prmt.b32 	%r962, %r959, %r960, %r777;
	mov.b64	%rd258, {%r962, %r961};
	add.s64 	%rd259, %rd253, %rd9;
	add.s64 	%rd260, %rd259, %rd258;
	xor.b64  	%rd261, %rd260, %rd255;
	mov.b64	{%r963, %r964}, %rd261;
	prmt.b32 	%r965, %r963, %r964, %r784;
	prmt.b32 	%r966, %r963, %r964, %r783;
	mov.b64	%rd262, {%r966, %r965};
	add.s64 	%rd263, %rd262, %rd256;
	xor.b64  	%rd264, %rd263, %rd258;
	mov.b64	{%r150, %r151}, %rd264;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r768;
	// inline asm
	mov.b64	%rd265, {%r145, %r149};
	add.s64 	%rd266, %rd220, %rd211;
	xor.b64  	%rd267, %rd266, %rd181;
	mov.b64	{%r967, %r968}, %rd267;
	mov.b64	%rd268, {%r968, %r967};
	add.s64 	%rd269, %rd268, %rd196;
	xor.b64  	%rd270, %rd269, %rd211;
	mov.b64	{%r969, %r970}, %rd270;
	prmt.b32 	%r971, %r969, %r970, %r778;
	prmt.b32 	%r972, %r969, %r970, %r777;
	mov.b64	%rd271, {%r972, %r971};
	add.s64 	%rd272, %rd271, %rd266;
	xor.b64  	%rd273, %rd272, %rd268;
	mov.b64	{%r973, %r974}, %rd273;
	prmt.b32 	%r975, %r973, %r974, %r784;
	prmt.b32 	%r976, %r973, %r974, %r783;
	mov.b64	%rd274, {%r976, %r975};
	add.s64 	%rd275, %rd274, %rd269;
	xor.b64  	%rd276, %rd275, %rd271;
	mov.b64	{%r158, %r159}, %rd276;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r768;
	// inline asm
	mov.b64	%rd277, {%r153, %r157};
	add.s64 	%rd278, %rd251, %rd233;
	xor.b64  	%rd279, %rd274, %rd278;
	mov.b64	{%r977, %r978}, %rd279;
	mov.b64	%rd280, {%r978, %r977};
	add.s64 	%rd281, %rd280, %rd263;
	xor.b64  	%rd282, %rd281, %rd251;
	mov.b64	{%r979, %r980}, %rd282;
	prmt.b32 	%r981, %r979, %r980, %r778;
	prmt.b32 	%r982, %r979, %r980, %r777;
	mov.b64	%rd283, {%r982, %r981};
	add.s64 	%rd284, %rd283, %rd278;
	xor.b64  	%rd285, %rd280, %rd284;
	mov.b64	{%r983, %r984}, %rd285;
	prmt.b32 	%r985, %r983, %r984, %r784;
	prmt.b32 	%r986, %r983, %r984, %r783;
	mov.b64	%rd286, {%r986, %r985};
	add.s64 	%rd287, %rd281, %rd286;
	xor.b64  	%rd288, %rd287, %rd283;
	mov.b64	{%r166, %r167}, %rd288;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r768;
	// inline asm
	mov.b64	%rd289, {%r161, %r165};
	add.s64 	%rd290, %rd246, %rd10;
	add.s64 	%rd291, %rd290, %rd265;
	xor.b64  	%rd292, %rd291, %rd235;
	mov.b64	{%r987, %r988}, %rd292;
	mov.b64	%rd293, {%r988, %r987};
	add.s64 	%rd294, %rd293, %rd275;
	xor.b64  	%rd295, %rd294, %rd265;
	mov.b64	{%r989, %r990}, %rd295;
	prmt.b32 	%r991, %r989, %r990, %r778;
	prmt.b32 	%r992, %r989, %r990, %r777;
	mov.b64	%rd296, {%r992, %r991};
	add.s64 	%rd297, %rd291, %rd13;
	add.s64 	%rd298, %rd297, %rd296;
	xor.b64  	%rd299, %rd298, %rd293;
	mov.b64	{%r993, %r994}, %rd299;
	prmt.b32 	%r995, %r993, %r994, %r784;
	prmt.b32 	%r996, %r993, %r994, %r783;
	mov.b64	%rd300, {%r996, %r995};
	add.s64 	%rd301, %rd300, %rd294;
	xor.b64  	%rd302, %rd301, %rd296;
	mov.b64	{%r174, %r175}, %rd302;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r768;
	// inline asm
	mov.b64	%rd303, {%r169, %r173};
	add.s64 	%rd304, %rd260, %rd14;
	add.s64 	%rd305, %rd304, %rd277;
	xor.b64  	%rd306, %rd305, %rd248;
	mov.b64	{%r997, %r998}, %rd306;
	mov.b64	%rd307, {%r998, %r997};
	add.s64 	%rd308, %rd307, %rd236;
	xor.b64  	%rd309, %rd308, %rd277;
	mov.b64	{%r999, %r1000}, %rd309;
	prmt.b32 	%r1001, %r999, %r1000, %r778;
	prmt.b32 	%r1002, %r999, %r1000, %r777;
	mov.b64	%rd310, {%r1002, %r1001};
	add.s64 	%rd311, %rd305, %rd23;
	add.s64 	%rd312, %rd311, %rd310;
	xor.b64  	%rd313, %rd312, %rd307;
	mov.b64	{%r1003, %r1004}, %rd313;
	prmt.b32 	%r1005, %r1003, %r1004, %r784;
	prmt.b32 	%r1006, %r1003, %r1004, %r783;
	mov.b64	%rd314, {%r1006, %r1005};
	add.s64 	%rd315, %rd314, %rd308;
	xor.b64  	%rd316, %rd315, %rd310;
	mov.b64	{%r182, %r183}, %rd316;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r768;
	// inline asm
	mov.b64	%rd317, {%r177, %r181};
	add.s64 	%rd318, %rd238, %rd16;
	add.s64 	%rd319, %rd318, %rd272;
	xor.b64  	%rd320, %rd319, %rd262;
	mov.b64	{%r1007, %r1008}, %rd320;
	mov.b64	%rd321, {%r1008, %r1007};
	add.s64 	%rd322, %rd321, %rd249;
	xor.b64  	%rd323, %rd322, %rd238;
	mov.b64	{%r1009, %r1010}, %rd323;
	prmt.b32 	%r1011, %r1009, %r1010, %r778;
	prmt.b32 	%r1012, %r1009, %r1010, %r777;
	mov.b64	%rd324, {%r1012, %r1011};
	add.s64 	%rd325, %rd319, %rd11;
	add.s64 	%rd326, %rd325, %rd324;
	xor.b64  	%rd327, %rd326, %rd321;
	mov.b64	{%r1013, %r1014}, %rd327;
	prmt.b32 	%r1015, %r1013, %r1014, %r784;
	prmt.b32 	%r1016, %r1013, %r1014, %r783;
	mov.b64	%rd328, {%r1016, %r1015};
	add.s64 	%rd329, %rd328, %rd322;
	xor.b64  	%rd330, %rd329, %rd324;
	mov.b64	{%r190, %r191}, %rd330;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r768;
	// inline asm
	mov.b64	%rd331, {%r185, %r189};
	add.s64 	%rd332, %rd284, %rd14;
	add.s64 	%rd333, %rd332, %rd331;
	xor.b64  	%rd334, %rd333, %rd300;
	mov.b64	{%r1017, %r1018}, %rd334;
	mov.b64	%rd335, {%r1018, %r1017};
	add.s64 	%rd336, %rd335, %rd315;
	xor.b64  	%rd337, %rd336, %rd331;
	mov.b64	{%r1019, %r1020}, %rd337;
	prmt.b32 	%r1021, %r1019, %r1020, %r778;
	prmt.b32 	%r1022, %r1019, %r1020, %r777;
	mov.b64	%rd338, {%r1022, %r1021};
	add.s64 	%rd339, %rd333, %rd16;
	add.s64 	%rd340, %rd339, %rd338;
	xor.b64  	%rd341, %rd335, %rd340;
	mov.b64	{%r1023, %r1024}, %rd341;
	prmt.b32 	%r1025, %r1023, %r1024, %r784;
	prmt.b32 	%r1026, %r1023, %r1024, %r783;
	mov.b64	%rd342, {%r1026, %r1025};
	add.s64 	%rd343, %rd336, %rd342;
	xor.b64  	%rd344, %rd343, %rd338;
	mov.b64	{%r198, %r199}, %rd344;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r768;
	// inline asm
	mov.b64	%rd345, {%r193, %r197};
	add.s64 	%rd346, %rd289, %rd10;
	add.s64 	%rd347, %rd346, %rd298;
	xor.b64  	%rd348, %rd314, %rd347;
	mov.b64	{%r1027, %r1028}, %rd348;
	mov.b64	%rd349, {%r1028, %r1027};
	add.s64 	%rd350, %rd329, %rd349;
	xor.b64  	%rd351, %rd350, %rd289;
	mov.b64	{%r1029, %r1030}, %rd351;
	prmt.b32 	%r1031, %r1029, %r1030, %r778;
	prmt.b32 	%r1032, %r1029, %r1030, %r777;
	mov.b64	%rd352, {%r1032, %r1031};
	add.s64 	%rd353, %rd347, %rd23;
	add.s64 	%rd354, %rd353, %rd352;
	xor.b64  	%rd355, %rd354, %rd349;
	mov.b64	{%r1033, %r1034}, %rd355;
	prmt.b32 	%r1035, %r1033, %r1034, %r784;
	prmt.b32 	%r1036, %r1033, %r1034, %r783;
	mov.b64	%rd356, {%r1036, %r1035};
	add.s64 	%rd357, %rd356, %rd350;
	xor.b64  	%rd358, %rd357, %rd352;
	mov.b64	{%r206, %r207}, %rd358;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r768;
	// inline asm
	mov.b64	%rd359, {%r201, %r205};
	add.s64 	%rd360, %rd312, %rd303;
	xor.b64  	%rd361, %rd328, %rd360;
	mov.b64	{%r1037, %r1038}, %rd361;
	mov.b64	%rd362, {%r1038, %r1037};
	add.s64 	%rd363, %rd362, %rd287;
	xor.b64  	%rd364, %rd363, %rd303;
	mov.b64	{%r1039, %r1040}, %rd364;
	prmt.b32 	%r1041, %r1039, %r1040, %r778;
	prmt.b32 	%r1042, %r1039, %r1040, %r777;
	mov.b64	%rd365, {%r1042, %r1041};
	add.s64 	%rd366, %rd365, %rd360;
	xor.b64  	%rd367, %rd366, %rd362;
	mov.b64	{%r1043, %r1044}, %rd367;
	prmt.b32 	%r1045, %r1043, %r1044, %r784;
	prmt.b32 	%r1046, %r1043, %r1044, %r783;
	mov.b64	%rd368, {%r1046, %r1045};
	add.s64 	%rd369, %rd368, %rd363;
	xor.b64  	%rd370, %rd369, %rd365;
	mov.b64	{%r214, %r215}, %rd370;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r768;
	// inline asm
	mov.b64	%rd371, {%r209, %r213};
	add.s64 	%rd372, %rd326, %rd317;
	xor.b64  	%rd373, %rd372, %rd286;
	mov.b64	{%r1047, %r1048}, %rd373;
	mov.b64	%rd374, {%r1048, %r1047};
	add.s64 	%rd375, %rd374, %rd301;
	xor.b64  	%rd376, %rd375, %rd317;
	mov.b64	{%r1049, %r1050}, %rd376;
	prmt.b32 	%r1051, %r1049, %r1050, %r778;
	prmt.b32 	%r1052, %r1049, %r1050, %r777;
	mov.b64	%rd377, {%r1052, %r1051};
	add.s64 	%rd378, %rd377, %rd372;
	xor.b64  	%rd379, %rd378, %rd374;
	mov.b64	{%r1053, %r1054}, %rd379;
	prmt.b32 	%r1055, %r1053, %r1054, %r784;
	prmt.b32 	%r1056, %r1053, %r1054, %r783;
	mov.b64	%rd380, {%r1056, %r1055};
	add.s64 	%rd381, %rd380, %rd375;
	xor.b64  	%rd382, %rd381, %rd377;
	mov.b64	{%r222, %r223}, %rd382;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r768;
	// inline asm
	mov.b64	%rd383, {%r217, %r221};
	add.s64 	%rd384, %rd340, %rd9;
	add.s64 	%rd385, %rd384, %rd359;
	xor.b64  	%rd386, %rd380, %rd385;
	mov.b64	{%r1057, %r1058}, %rd386;
	mov.b64	%rd387, {%r1058, %r1057};
	add.s64 	%rd388, %rd387, %rd369;
	xor.b64  	%rd389, %rd388, %rd359;
	mov.b64	{%r1059, %r1060}, %rd389;
	prmt.b32 	%r1061, %r1059, %r1060, %r778;
	prmt.b32 	%r1062, %r1059, %r1060, %r777;
	mov.b64	%rd390, {%r1062, %r1061};
	add.s64 	%rd391, %rd385, %rd13;
	add.s64 	%rd392, %rd391, %rd390;
	xor.b64  	%rd393, %rd387, %rd392;
	mov.b64	{%r1063, %r1064}, %rd393;
	prmt.b32 	%r1065, %r1063, %r1064, %r784;
	prmt.b32 	%r1066, %r1063, %r1064, %r783;
	mov.b64	%rd394, {%r1066, %r1065};
	add.s64 	%rd395, %rd388, %rd394;
	xor.b64  	%rd396, %rd395, %rd390;
	mov.b64	{%r230, %r231}, %rd396;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r768;
	// inline asm
	mov.b64	%rd397, {%r225, %r229};
	add.s64 	%rd398, %rd354, %rd12;
	add.s64 	%rd399, %rd398, %rd371;
	xor.b64  	%rd400, %rd399, %rd342;
	mov.b64	{%r1067, %r1068}, %rd400;
	mov.b64	%rd401, {%r1068, %r1067};
	add.s64 	%rd402, %rd401, %rd381;
	xor.b64  	%rd403, %rd402, %rd371;
	mov.b64	{%r1069, %r1070}, %rd403;
	prmt.b32 	%r1071, %r1069, %r1070, %r778;
	prmt.b32 	%r1072, %r1069, %r1070, %r777;
	mov.b64	%rd404, {%r1072, %r1071};
	add.s64 	%rd405, %rd404, %rd399;
	xor.b64  	%rd406, %rd405, %rd401;
	mov.b64	{%r1073, %r1074}, %rd406;
	prmt.b32 	%r1075, %r1073, %r1074, %r784;
	prmt.b32 	%r1076, %r1073, %r1074, %r783;
	mov.b64	%rd407, {%r1076, %r1075};
	add.s64 	%rd408, %rd407, %rd402;
	xor.b64  	%rd409, %rd408, %rd404;
	mov.b64	{%r238, %r239}, %rd409;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r768;
	// inline asm
	mov.b64	%rd410, {%r233, %r237};
	add.s64 	%rd411, %rd366, %rd11;
	add.s64 	%rd412, %rd411, %rd383;
	xor.b64  	%rd413, %rd412, %rd356;
	mov.b64	{%r1077, %r1078}, %rd413;
	mov.b64	%rd414, {%r1078, %r1077};
	add.s64 	%rd415, %rd414, %rd343;
	xor.b64  	%rd416, %rd415, %rd383;
	mov.b64	{%r1079, %r1080}, %rd416;
	prmt.b32 	%r1081, %r1079, %r1080, %r778;
	prmt.b32 	%r1082, %r1079, %r1080, %r777;
	mov.b64	%rd417, {%r1082, %r1081};
	add.s64 	%rd418, %rd412, %rd1;
	add.s64 	%rd419, %rd418, %rd417;
	xor.b64  	%rd420, %rd419, %rd414;
	mov.b64	{%r1083, %r1084}, %rd420;
	prmt.b32 	%r1085, %r1083, %r1084, %r784;
	prmt.b32 	%r1086, %r1083, %r1084, %r783;
	mov.b64	%rd421, {%r1086, %r1085};
	add.s64 	%rd422, %rd421, %rd415;
	xor.b64  	%rd423, %rd422, %rd417;
	mov.b64	{%r246, %r247}, %rd423;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r768;
	// inline asm
	mov.b64	%rd424, {%r241, %r245};
	add.s64 	%rd425, %rd378, %rd345;
	xor.b64  	%rd426, %rd425, %rd368;
	mov.b64	{%r1087, %r1088}, %rd426;
	mov.b64	%rd427, {%r1088, %r1087};
	add.s64 	%rd428, %rd427, %rd357;
	xor.b64  	%rd429, %rd428, %rd345;
	mov.b64	{%r1089, %r1090}, %rd429;
	prmt.b32 	%r1091, %r1089, %r1090, %r778;
	prmt.b32 	%r1092, %r1089, %r1090, %r777;
	mov.b64	%rd430, {%r1092, %r1091};
	add.s64 	%rd431, %rd425, %rd15;
	add.s64 	%rd432, %rd431, %rd430;
	xor.b64  	%rd433, %rd432, %rd427;
	mov.b64	{%r1093, %r1094}, %rd433;
	prmt.b32 	%r1095, %r1093, %r1094, %r784;
	prmt.b32 	%r1096, %r1093, %r1094, %r783;
	mov.b64	%rd434, {%r1096, %r1095};
	add.s64 	%rd435, %rd434, %rd428;
	xor.b64  	%rd436, %rd435, %rd430;
	mov.b64	{%r254, %r255}, %rd436;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r768;
	// inline asm
	mov.b64	%rd437, {%r249, %r253};
	add.s64 	%rd438, %rd392, %rd16;
	add.s64 	%rd439, %rd438, %rd437;
	xor.b64  	%rd440, %rd439, %rd407;
	mov.b64	{%r1097, %r1098}, %rd440;
	mov.b64	%rd441, {%r1098, %r1097};
	add.s64 	%rd442, %rd441, %rd422;
	xor.b64  	%rd443, %rd442, %rd437;
	mov.b64	{%r1099, %r1100}, %rd443;
	prmt.b32 	%r1101, %r1099, %r1100, %r778;
	prmt.b32 	%r1102, %r1099, %r1100, %r777;
	mov.b64	%rd444, {%r1102, %r1101};
	add.s64 	%rd445, %rd439, %rd1;
	add.s64 	%rd446, %rd445, %rd444;
	xor.b64  	%rd447, %rd441, %rd446;
	mov.b64	{%r1103, %r1104}, %rd447;
	prmt.b32 	%r1105, %r1103, %r1104, %r784;
	prmt.b32 	%r1106, %r1103, %r1104, %r783;
	mov.b64	%rd448, {%r1106, %r1105};
	add.s64 	%rd449, %rd442, %rd448;
	xor.b64  	%rd450, %rd449, %rd444;
	mov.b64	{%r262, %r263}, %rd450;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r768;
	// inline asm
	mov.b64	%rd451, {%r257, %r261};
	add.s64 	%rd452, %rd397, %rd12;
	add.s64 	%rd453, %rd452, %rd405;
	xor.b64  	%rd454, %rd421, %rd453;
	mov.b64	{%r1107, %r1108}, %rd454;
	mov.b64	%rd455, {%r1108, %r1107};
	add.s64 	%rd456, %rd435, %rd455;
	xor.b64  	%rd457, %rd456, %rd397;
	mov.b64	{%r1109, %r1110}, %rd457;
	prmt.b32 	%r1111, %r1109, %r1110, %r778;
	prmt.b32 	%r1112, %r1109, %r1110, %r777;
	mov.b64	%rd458, {%r1112, %r1111};
	add.s64 	%rd459, %rd453, %rd14;
	add.s64 	%rd460, %rd459, %rd458;
	xor.b64  	%rd461, %rd460, %rd455;
	mov.b64	{%r1113, %r1114}, %rd461;
	prmt.b32 	%r1115, %r1113, %r1114, %r784;
	prmt.b32 	%r1116, %r1113, %r1114, %r783;
	mov.b64	%rd462, {%r1116, %r1115};
	add.s64 	%rd463, %rd462, %rd456;
	xor.b64  	%rd464, %rd463, %rd458;
	mov.b64	{%r270, %r271}, %rd464;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r768;
	// inline asm
	mov.b64	%rd465, {%r265, %r269};
	add.s64 	%rd466, %rd410, %rd9;
	add.s64 	%rd467, %rd466, %rd419;
	xor.b64  	%rd468, %rd434, %rd467;
	mov.b64	{%r1117, %r1118}, %rd468;
	mov.b64	%rd469, {%r1118, %r1117};
	add.s64 	%rd470, %rd469, %rd395;
	xor.b64  	%rd471, %rd470, %rd410;
	mov.b64	{%r1119, %r1120}, %rd471;
	prmt.b32 	%r1121, %r1119, %r1120, %r778;
	prmt.b32 	%r1122, %r1119, %r1120, %r777;
	mov.b64	%rd472, {%r1122, %r1121};
	add.s64 	%rd473, %rd467, %rd11;
	add.s64 	%rd474, %rd473, %rd472;
	xor.b64  	%rd475, %rd474, %rd469;
	mov.b64	{%r1123, %r1124}, %rd475;
	prmt.b32 	%r1125, %r1123, %r1124, %r784;
	prmt.b32 	%r1126, %r1123, %r1124, %r783;
	mov.b64	%rd476, {%r1126, %r1125};
	add.s64 	%rd477, %rd476, %rd470;
	xor.b64  	%rd478, %rd477, %rd472;
	mov.b64	{%r278, %r279}, %rd478;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r768;
	// inline asm
	mov.b64	%rd479, {%r273, %r277};
	add.s64 	%rd480, %rd432, %rd424;
	xor.b64  	%rd481, %rd480, %rd394;
	mov.b64	{%r1127, %r1128}, %rd481;
	mov.b64	%rd482, {%r1128, %r1127};
	add.s64 	%rd483, %rd482, %rd408;
	xor.b64  	%rd484, %rd483, %rd424;
	mov.b64	{%r1129, %r1130}, %rd484;
	prmt.b32 	%r1131, %r1129, %r1130, %r778;
	prmt.b32 	%r1132, %r1129, %r1130, %r777;
	mov.b64	%rd485, {%r1132, %r1131};
	add.s64 	%rd486, %rd485, %rd480;
	xor.b64  	%rd487, %rd486, %rd482;
	mov.b64	{%r1133, %r1134}, %rd487;
	prmt.b32 	%r1135, %r1133, %r1134, %r784;
	prmt.b32 	%r1136, %r1133, %r1134, %r783;
	mov.b64	%rd488, {%r1136, %r1135};
	add.s64 	%rd489, %rd488, %rd483;
	xor.b64  	%rd490, %rd489, %rd485;
	mov.b64	{%r286, %r287}, %rd490;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r768;
	// inline asm
	mov.b64	%rd491, {%r281, %r285};
	add.s64 	%rd492, %rd465, %rd446;
	xor.b64  	%rd493, %rd488, %rd492;
	mov.b64	{%r1137, %r1138}, %rd493;
	mov.b64	%rd494, {%r1138, %r1137};
	add.s64 	%rd495, %rd494, %rd477;
	xor.b64  	%rd496, %rd495, %rd465;
	mov.b64	{%r1139, %r1140}, %rd496;
	prmt.b32 	%r1141, %r1139, %r1140, %r778;
	prmt.b32 	%r1142, %r1139, %r1140, %r777;
	mov.b64	%rd497, {%r1142, %r1141};
	add.s64 	%rd498, %rd492, %rd23;
	add.s64 	%rd499, %rd498, %rd497;
	xor.b64  	%rd500, %rd494, %rd499;
	mov.b64	{%r1143, %r1144}, %rd500;
	prmt.b32 	%r1145, %r1143, %r1144, %r784;
	prmt.b32 	%r1146, %r1143, %r1144, %r783;
	mov.b64	%rd501, {%r1146, %r1145};
	add.s64 	%rd502, %rd495, %rd501;
	xor.b64  	%rd503, %rd502, %rd497;
	mov.b64	{%r294, %r295}, %rd503;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r768;
	// inline asm
	mov.b64	%rd504, {%r289, %r293};
	add.s64 	%rd505, %rd479, %rd460;
	xor.b64  	%rd506, %rd505, %rd448;
	mov.b64	{%r1147, %r1148}, %rd506;
	mov.b64	%rd507, {%r1148, %r1147};
	add.s64 	%rd508, %rd507, %rd489;
	xor.b64  	%rd509, %rd508, %rd479;
	mov.b64	{%r1149, %r1150}, %rd509;
	prmt.b32 	%r1151, %r1149, %r1150, %r778;
	prmt.b32 	%r1152, %r1149, %r1150, %r777;
	mov.b64	%rd510, {%r1152, %r1151};
	add.s64 	%rd511, %rd510, %rd505;
	xor.b64  	%rd512, %rd511, %rd507;
	mov.b64	{%r1153, %r1154}, %rd512;
	prmt.b32 	%r1155, %r1153, %r1154, %r784;
	prmt.b32 	%r1156, %r1153, %r1154, %r783;
	mov.b64	%rd513, {%r1156, %r1155};
	add.s64 	%rd514, %rd513, %rd508;
	xor.b64  	%rd515, %rd514, %rd510;
	mov.b64	{%r302, %r303}, %rd515;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r768;
	// inline asm
	mov.b64	%rd516, {%r297, %r301};
	add.s64 	%rd517, %rd474, %rd13;
	add.s64 	%rd518, %rd517, %rd491;
	xor.b64  	%rd519, %rd518, %rd462;
	mov.b64	{%r1157, %r1158}, %rd519;
	mov.b64	%rd520, {%r1158, %r1157};
	add.s64 	%rd521, %rd520, %rd449;
	xor.b64  	%rd522, %rd521, %rd491;
	mov.b64	{%r1159, %r1160}, %rd522;
	prmt.b32 	%r1161, %r1159, %r1160, %r778;
	prmt.b32 	%r1162, %r1159, %r1160, %r777;
	mov.b64	%rd523, {%r1162, %r1161};
	add.s64 	%rd524, %rd518, %rd15;
	add.s64 	%rd525, %rd524, %rd523;
	xor.b64  	%rd526, %rd525, %rd520;
	mov.b64	{%r1163, %r1164}, %rd526;
	prmt.b32 	%r1165, %r1163, %r1164, %r784;
	prmt.b32 	%r1166, %r1163, %r1164, %r783;
	mov.b64	%rd527, {%r1166, %r1165};
	add.s64 	%rd528, %rd527, %rd521;
	xor.b64  	%rd529, %rd528, %rd523;
	mov.b64	{%r310, %r311}, %rd529;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r768;
	// inline asm
	mov.b64	%rd530, {%r305, %r309};
	add.s64 	%rd531, %rd451, %rd10;
	add.s64 	%rd532, %rd531, %rd486;
	xor.b64  	%rd533, %rd532, %rd476;
	mov.b64	{%r1167, %r1168}, %rd533;
	mov.b64	%rd534, {%r1168, %r1167};
	add.s64 	%rd535, %rd534, %rd463;
	xor.b64  	%rd536, %rd535, %rd451;
	mov.b64	{%r1169, %r1170}, %rd536;
	prmt.b32 	%r1171, %r1169, %r1170, %r778;
	prmt.b32 	%r1172, %r1169, %r1170, %r777;
	mov.b64	%rd537, {%r1172, %r1171};
	add.s64 	%rd538, %rd537, %rd532;
	xor.b64  	%rd539, %rd538, %rd534;
	mov.b64	{%r1173, %r1174}, %rd539;
	prmt.b32 	%r1175, %r1173, %r1174, %r784;
	prmt.b32 	%r1176, %r1173, %r1174, %r783;
	mov.b64	%rd540, {%r1176, %r1175};
	add.s64 	%rd541, %rd540, %rd535;
	xor.b64  	%rd542, %rd541, %rd537;
	mov.b64	{%r318, %r319}, %rd542;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r768;
	// inline asm
	mov.b64	%rd543, {%r313, %r317};
	add.s64 	%rd544, %rd499, %rd9;
	add.s64 	%rd545, %rd544, %rd543;
	xor.b64  	%rd546, %rd545, %rd513;
	mov.b64	{%r1177, %r1178}, %rd546;
	mov.b64	%rd547, {%r1178, %r1177};
	add.s64 	%rd548, %rd547, %rd528;
	xor.b64  	%rd549, %rd548, %rd543;
	mov.b64	{%r1179, %r1180}, %rd549;
	prmt.b32 	%r1181, %r1179, %r1180, %r778;
	prmt.b32 	%r1182, %r1179, %r1180, %r777;
	mov.b64	%rd550, {%r1182, %r1181};
	add.s64 	%rd551, %rd550, %rd545;
	xor.b64  	%rd552, %rd547, %rd551;
	mov.b64	{%r1183, %r1184}, %rd552;
	prmt.b32 	%r1185, %r1183, %r1184, %r784;
	prmt.b32 	%r1186, %r1183, %r1184, %r783;
	mov.b64	%rd553, {%r1186, %r1185};
	add.s64 	%rd554, %rd548, %rd553;
	xor.b64  	%rd555, %rd554, %rd550;
	mov.b64	{%r326, %r327}, %rd555;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r768;
	// inline asm
	mov.b64	%rd556, {%r321, %r325};
	add.s64 	%rd557, %rd504, %rd13;
	add.s64 	%rd558, %rd557, %rd511;
	xor.b64  	%rd559, %rd527, %rd558;
	mov.b64	{%r1187, %r1188}, %rd559;
	mov.b64	%rd560, {%r1188, %r1187};
	add.s64 	%rd561, %rd541, %rd560;
	xor.b64  	%rd562, %rd561, %rd504;
	mov.b64	{%r1189, %r1190}, %rd562;
	prmt.b32 	%r1191, %r1189, %r1190, %r778;
	prmt.b32 	%r1192, %r1189, %r1190, %r777;
	mov.b64	%rd563, {%r1192, %r1191};
	add.s64 	%rd564, %rd563, %rd558;
	xor.b64  	%rd565, %rd564, %rd560;
	mov.b64	{%r1193, %r1194}, %rd565;
	prmt.b32 	%r1195, %r1193, %r1194, %r784;
	prmt.b32 	%r1196, %r1193, %r1194, %r783;
	mov.b64	%rd566, {%r1196, %r1195};
	add.s64 	%rd567, %rd566, %rd561;
	xor.b64  	%rd568, %rd567, %rd563;
	mov.b64	{%r334, %r335}, %rd568;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r768;
	// inline asm
	mov.b64	%rd569, {%r329, %r333};
	add.s64 	%rd570, %rd516, %rd1;
	add.s64 	%rd571, %rd570, %rd525;
	xor.b64  	%rd572, %rd540, %rd571;
	mov.b64	{%r1197, %r1198}, %rd572;
	mov.b64	%rd573, {%r1198, %r1197};
	add.s64 	%rd574, %rd573, %rd502;
	xor.b64  	%rd575, %rd574, %rd516;
	mov.b64	{%r1199, %r1200}, %rd575;
	prmt.b32 	%r1201, %r1199, %r1200, %r778;
	prmt.b32 	%r1202, %r1199, %r1200, %r777;
	mov.b64	%rd576, {%r1202, %r1201};
	add.s64 	%rd577, %rd576, %rd571;
	xor.b64  	%rd578, %rd577, %rd573;
	mov.b64	{%r1203, %r1204}, %rd578;
	prmt.b32 	%r1205, %r1203, %r1204, %r784;
	prmt.b32 	%r1206, %r1203, %r1204, %r783;
	mov.b64	%rd579, {%r1206, %r1205};
	add.s64 	%rd580, %rd579, %rd574;
	xor.b64  	%rd581, %rd580, %rd576;
	mov.b64	{%r342, %r343}, %rd581;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r768;
	// inline asm
	mov.b64	%rd582, {%r337, %r341};
	add.s64 	%rd583, %rd530, %rd15;
	add.s64 	%rd584, %rd583, %rd538;
	xor.b64  	%rd585, %rd584, %rd501;
	mov.b64	{%r1207, %r1208}, %rd585;
	mov.b64	%rd586, {%r1208, %r1207};
	add.s64 	%rd587, %rd586, %rd514;
	xor.b64  	%rd588, %rd587, %rd530;
	mov.b64	{%r1209, %r1210}, %rd588;
	prmt.b32 	%r1211, %r1209, %r1210, %r778;
	prmt.b32 	%r1212, %r1209, %r1210, %r777;
	mov.b64	%rd589, {%r1212, %r1211};
	add.s64 	%rd590, %rd584, %rd10;
	add.s64 	%rd591, %rd590, %rd589;
	xor.b64  	%rd592, %rd591, %rd586;
	mov.b64	{%r1213, %r1214}, %rd592;
	prmt.b32 	%r1215, %r1213, %r1214, %r784;
	prmt.b32 	%r1216, %r1213, %r1214, %r783;
	mov.b64	%rd593, {%r1216, %r1215};
	add.s64 	%rd594, %rd593, %rd587;
	xor.b64  	%rd595, %rd594, %rd589;
	mov.b64	{%r350, %r351}, %rd595;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r768;
	// inline asm
	mov.b64	%rd596, {%r345, %r349};
	add.s64 	%rd597, %rd551, %rd11;
	add.s64 	%rd598, %rd597, %rd569;
	xor.b64  	%rd599, %rd593, %rd598;
	mov.b64	{%r1217, %r1218}, %rd599;
	mov.b64	%rd600, {%r1218, %r1217};
	add.s64 	%rd601, %rd600, %rd580;
	xor.b64  	%rd602, %rd601, %rd569;
	mov.b64	{%r1219, %r1220}, %rd602;
	prmt.b32 	%r1221, %r1219, %r1220, %r778;
	prmt.b32 	%r1222, %r1219, %r1220, %r777;
	mov.b64	%rd603, {%r1222, %r1221};
	add.s64 	%rd604, %rd603, %rd598;
	xor.b64  	%rd605, %rd600, %rd604;
	mov.b64	{%r1223, %r1224}, %rd605;
	prmt.b32 	%r1225, %r1223, %r1224, %r784;
	prmt.b32 	%r1226, %r1223, %r1224, %r783;
	mov.b64	%rd606, {%r1226, %r1225};
	add.s64 	%rd607, %rd601, %rd606;
	xor.b64  	%rd608, %rd607, %rd603;
	mov.b64	{%r358, %r359}, %rd608;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r768;
	// inline asm
	mov.b64	%rd609, {%r353, %r357};
	add.s64 	%rd610, %rd564, %rd14;
	add.s64 	%rd611, %rd610, %rd582;
	xor.b64  	%rd612, %rd611, %rd553;
	mov.b64	{%r1227, %r1228}, %rd612;
	mov.b64	%rd613, {%r1228, %r1227};
	add.s64 	%rd614, %rd613, %rd594;
	xor.b64  	%rd615, %rd614, %rd582;
	mov.b64	{%r1229, %r1230}, %rd615;
	prmt.b32 	%r1231, %r1229, %r1230, %r778;
	prmt.b32 	%r1232, %r1229, %r1230, %r777;
	mov.b64	%rd616, {%r1232, %r1231};
	add.s64 	%rd617, %rd611, %rd12;
	add.s64 	%rd618, %rd617, %rd616;
	xor.b64  	%rd619, %rd618, %rd613;
	mov.b64	{%r1233, %r1234}, %rd619;
	prmt.b32 	%r1235, %r1233, %r1234, %r784;
	prmt.b32 	%r1236, %r1233, %r1234, %r783;
	mov.b64	%rd620, {%r1236, %r1235};
	add.s64 	%rd621, %rd620, %rd614;
	xor.b64  	%rd622, %rd621, %rd616;
	mov.b64	{%r366, %r367}, %rd622;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r768;
	// inline asm
	mov.b64	%rd623, {%r361, %r365};
	add.s64 	%rd624, %rd596, %rd577;
	xor.b64  	%rd625, %rd624, %rd566;
	mov.b64	{%r1237, %r1238}, %rd625;
	mov.b64	%rd626, {%r1238, %r1237};
	add.s64 	%rd627, %rd626, %rd554;
	xor.b64  	%rd628, %rd627, %rd596;
	mov.b64	{%r1239, %r1240}, %rd628;
	prmt.b32 	%r1241, %r1239, %r1240, %r778;
	prmt.b32 	%r1242, %r1239, %r1240, %r777;
	mov.b64	%rd629, {%r1242, %r1241};
	add.s64 	%rd630, %rd629, %rd624;
	xor.b64  	%rd631, %rd630, %rd626;
	mov.b64	{%r1243, %r1244}, %rd631;
	prmt.b32 	%r1245, %r1243, %r1244, %r784;
	prmt.b32 	%r1246, %r1243, %r1244, %r783;
	mov.b64	%rd632, {%r1246, %r1245};
	add.s64 	%rd633, %rd632, %rd627;
	xor.b64  	%rd634, %rd633, %rd629;
	mov.b64	{%r374, %r375}, %rd634;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r768;
	// inline asm
	mov.b64	%rd635, {%r369, %r373};
	add.s64 	%rd636, %rd556, %rd23;
	add.s64 	%rd637, %rd636, %rd591;
	xor.b64  	%rd638, %rd637, %rd579;
	mov.b64	{%r1247, %r1248}, %rd638;
	mov.b64	%rd639, {%r1248, %r1247};
	add.s64 	%rd640, %rd639, %rd567;
	xor.b64  	%rd641, %rd640, %rd556;
	mov.b64	{%r1249, %r1250}, %rd641;
	prmt.b32 	%r1251, %r1249, %r1250, %r778;
	prmt.b32 	%r1252, %r1249, %r1250, %r777;
	mov.b64	%rd642, {%r1252, %r1251};
	add.s64 	%rd643, %rd637, %rd16;
	add.s64 	%rd644, %rd643, %rd642;
	xor.b64  	%rd645, %rd644, %rd639;
	mov.b64	{%r1253, %r1254}, %rd645;
	prmt.b32 	%r1255, %r1253, %r1254, %r784;
	prmt.b32 	%r1256, %r1253, %r1254, %r783;
	mov.b64	%rd646, {%r1256, %r1255};
	add.s64 	%rd647, %rd646, %rd640;
	xor.b64  	%rd648, %rd647, %rd642;
	mov.b64	{%r382, %r383}, %rd648;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r768;
	// inline asm
	mov.b64	%rd649, {%r377, %r381};
	add.s64 	%rd650, %rd649, %rd604;
	xor.b64  	%rd651, %rd650, %rd620;
	mov.b64	{%r1257, %r1258}, %rd651;
	mov.b64	%rd652, {%r1258, %r1257};
	add.s64 	%rd653, %rd652, %rd633;
	xor.b64  	%rd654, %rd653, %rd649;
	mov.b64	{%r1259, %r1260}, %rd654;
	prmt.b32 	%r1261, %r1259, %r1260, %r778;
	prmt.b32 	%r1262, %r1259, %r1260, %r777;
	mov.b64	%rd655, {%r1262, %r1261};
	add.s64 	%rd656, %rd650, %rd12;
	add.s64 	%rd657, %rd656, %rd655;
	xor.b64  	%rd658, %rd652, %rd657;
	mov.b64	{%r1263, %r1264}, %rd658;
	prmt.b32 	%r1265, %r1263, %r1264, %r784;
	prmt.b32 	%r1266, %r1263, %r1264, %r783;
	mov.b64	%rd659, {%r1266, %r1265};
	add.s64 	%rd660, %rd653, %rd659;
	xor.b64  	%rd661, %rd660, %rd655;
	mov.b64	{%r390, %r391}, %rd661;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r768;
	// inline asm
	mov.b64	%rd662, {%r385, %r389};
	add.s64 	%rd663, %rd609, %rd23;
	add.s64 	%rd664, %rd663, %rd618;
	xor.b64  	%rd665, %rd632, %rd664;
	mov.b64	{%r1267, %r1268}, %rd665;
	mov.b64	%rd666, {%r1268, %r1267};
	add.s64 	%rd667, %rd647, %rd666;
	xor.b64  	%rd668, %rd667, %rd609;
	mov.b64	{%r1269, %r1270}, %rd668;
	prmt.b32 	%r1271, %r1269, %r1270, %r778;
	prmt.b32 	%r1272, %r1269, %r1270, %r777;
	mov.b64	%rd669, {%r1272, %r1271};
	add.s64 	%rd670, %rd669, %rd664;
	xor.b64  	%rd671, %rd670, %rd666;
	mov.b64	{%r1273, %r1274}, %rd671;
	prmt.b32 	%r1275, %r1273, %r1274, %r784;
	prmt.b32 	%r1276, %r1273, %r1274, %r783;
	mov.b64	%rd672, {%r1276, %r1275};
	add.s64 	%rd673, %rd672, %rd667;
	xor.b64  	%rd674, %rd673, %rd669;
	mov.b64	{%r398, %r399}, %rd674;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r768;
	// inline asm
	mov.b64	%rd675, {%r393, %r397};
	add.s64 	%rd676, %rd630, %rd623;
	xor.b64  	%rd677, %rd646, %rd676;
	mov.b64	{%r1277, %r1278}, %rd677;
	mov.b64	%rd678, {%r1278, %r1277};
	add.s64 	%rd679, %rd678, %rd607;
	xor.b64  	%rd680, %rd679, %rd623;
	mov.b64	{%r1279, %r1280}, %rd680;
	prmt.b32 	%r1281, %r1279, %r1280, %r778;
	prmt.b32 	%r1282, %r1279, %r1280, %r777;
	mov.b64	%rd681, {%r1282, %r1281};
	add.s64 	%rd682, %rd681, %rd676;
	xor.b64  	%rd683, %rd682, %rd678;
	mov.b64	{%r1283, %r1284}, %rd683;
	prmt.b32 	%r1285, %r1283, %r1284, %r784;
	prmt.b32 	%r1286, %r1283, %r1284, %r783;
	mov.b64	%rd684, {%r1286, %r1285};
	add.s64 	%rd685, %rd684, %rd679;
	xor.b64  	%rd686, %rd685, %rd681;
	mov.b64	{%r406, %r407}, %rd686;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r768;
	// inline asm
	mov.b64	%rd687, {%r401, %r405};
	add.s64 	%rd688, %rd635, %rd11;
	add.s64 	%rd689, %rd688, %rd644;
	xor.b64  	%rd690, %rd689, %rd606;
	mov.b64	{%r1287, %r1288}, %rd690;
	mov.b64	%rd691, {%r1288, %r1287};
	add.s64 	%rd692, %rd691, %rd621;
	xor.b64  	%rd693, %rd692, %rd635;
	mov.b64	{%r1289, %r1290}, %rd693;
	prmt.b32 	%r1291, %r1289, %r1290, %r778;
	prmt.b32 	%r1292, %r1289, %r1290, %r777;
	mov.b64	%rd694, {%r1292, %r1291};
	add.s64 	%rd695, %rd694, %rd689;
	xor.b64  	%rd696, %rd695, %rd691;
	mov.b64	{%r1293, %r1294}, %rd696;
	prmt.b32 	%r1295, %r1293, %r1294, %r784;
	prmt.b32 	%r1296, %r1293, %r1294, %r783;
	mov.b64	%rd697, {%r1296, %r1295};
	add.s64 	%rd698, %rd697, %rd692;
	xor.b64  	%rd699, %rd698, %rd694;
	mov.b64	{%r414, %r415}, %rd699;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r768;
	// inline asm
	mov.b64	%rd700, {%r409, %r413};
	add.s64 	%rd701, %rd657, %rd1;
	add.s64 	%rd702, %rd701, %rd675;
	xor.b64  	%rd703, %rd697, %rd702;
	mov.b64	{%r1297, %r1298}, %rd703;
	mov.b64	%rd704, {%r1298, %r1297};
	add.s64 	%rd705, %rd704, %rd685;
	xor.b64  	%rd706, %rd705, %rd675;
	mov.b64	{%r1299, %r1300}, %rd706;
	prmt.b32 	%r1301, %r1299, %r1300, %r778;
	prmt.b32 	%r1302, %r1299, %r1300, %r777;
	mov.b64	%rd707, {%r1302, %r1301};
	add.s64 	%rd708, %rd702, %rd14;
	add.s64 	%rd709, %rd708, %rd707;
	xor.b64  	%rd710, %rd704, %rd709;
	mov.b64	{%r1303, %r1304}, %rd710;
	prmt.b32 	%r1305, %r1303, %r1304, %r784;
	prmt.b32 	%r1306, %r1303, %r1304, %r783;
	mov.b64	%rd711, {%r1306, %r1305};
	add.s64 	%rd712, %rd705, %rd711;
	xor.b64  	%rd713, %rd712, %rd707;
	mov.b64	{%r422, %r423}, %rd713;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r768;
	// inline asm
	mov.b64	%rd714, {%r417, %r421};
	add.s64 	%rd715, %rd670, %rd13;
	add.s64 	%rd716, %rd715, %rd687;
	xor.b64  	%rd717, %rd716, %rd659;
	mov.b64	{%r1307, %r1308}, %rd717;
	mov.b64	%rd718, {%r1308, %r1307};
	add.s64 	%rd719, %rd718, %rd698;
	xor.b64  	%rd720, %rd719, %rd687;
	mov.b64	{%r1309, %r1310}, %rd720;
	prmt.b32 	%r1311, %r1309, %r1310, %r778;
	prmt.b32 	%r1312, %r1309, %r1310, %r777;
	mov.b64	%rd721, {%r1312, %r1311};
	add.s64 	%rd722, %rd716, %rd10;
	add.s64 	%rd723, %rd722, %rd721;
	xor.b64  	%rd724, %rd723, %rd718;
	mov.b64	{%r1313, %r1314}, %rd724;
	prmt.b32 	%r1315, %r1313, %r1314, %r784;
	prmt.b32 	%r1316, %r1313, %r1314, %r783;
	mov.b64	%rd725, {%r1316, %r1315};
	add.s64 	%rd726, %rd725, %rd719;
	xor.b64  	%rd727, %rd726, %rd721;
	mov.b64	{%r430, %r431}, %rd727;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r768;
	// inline asm
	mov.b64	%rd728, {%r425, %r429};
	add.s64 	%rd729, %rd682, %rd16;
	add.s64 	%rd730, %rd729, %rd700;
	xor.b64  	%rd731, %rd730, %rd672;
	mov.b64	{%r1317, %r1318}, %rd731;
	mov.b64	%rd732, {%r1318, %r1317};
	add.s64 	%rd733, %rd732, %rd660;
	xor.b64  	%rd734, %rd733, %rd700;
	mov.b64	{%r1319, %r1320}, %rd734;
	prmt.b32 	%r1321, %r1319, %r1320, %r778;
	prmt.b32 	%r1322, %r1319, %r1320, %r777;
	mov.b64	%rd735, {%r1322, %r1321};
	add.s64 	%rd736, %rd730, %rd9;
	add.s64 	%rd737, %rd736, %rd735;
	xor.b64  	%rd738, %rd737, %rd732;
	mov.b64	{%r1323, %r1324}, %rd738;
	prmt.b32 	%r1325, %r1323, %r1324, %r784;
	prmt.b32 	%r1326, %r1323, %r1324, %r783;
	mov.b64	%rd739, {%r1326, %r1325};
	add.s64 	%rd740, %rd739, %rd733;
	xor.b64  	%rd741, %rd740, %rd735;
	mov.b64	{%r438, %r439}, %rd741;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r768;
	// inline asm
	mov.b64	%rd742, {%r433, %r437};
	add.s64 	%rd743, %rd662, %rd15;
	add.s64 	%rd744, %rd743, %rd695;
	xor.b64  	%rd745, %rd744, %rd684;
	mov.b64	{%r1327, %r1328}, %rd745;
	mov.b64	%rd746, {%r1328, %r1327};
	add.s64 	%rd747, %rd746, %rd673;
	xor.b64  	%rd748, %rd747, %rd662;
	mov.b64	{%r1329, %r1330}, %rd748;
	prmt.b32 	%r1331, %r1329, %r1330, %r778;
	prmt.b32 	%r1332, %r1329, %r1330, %r777;
	mov.b64	%rd749, {%r1332, %r1331};
	add.s64 	%rd750, %rd749, %rd744;
	xor.b64  	%rd751, %rd750, %rd746;
	mov.b64	{%r1333, %r1334}, %rd751;
	prmt.b32 	%r1335, %r1333, %r1334, %r784;
	prmt.b32 	%r1336, %r1333, %r1334, %r783;
	mov.b64	%rd752, {%r1336, %r1335};
	add.s64 	%rd753, %rd752, %rd747;
	xor.b64  	%rd754, %rd753, %rd749;
	mov.b64	{%r446, %r447}, %rd754;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r768;
	// inline asm
	mov.b64	%rd755, {%r441, %r445};
	add.s64 	%rd756, %rd755, %rd709;
	xor.b64  	%rd757, %rd756, %rd725;
	mov.b64	{%r1337, %r1338}, %rd757;
	mov.b64	%rd758, {%r1338, %r1337};
	add.s64 	%rd759, %rd758, %rd740;
	xor.b64  	%rd760, %rd759, %rd755;
	mov.b64	{%r1339, %r1340}, %rd760;
	prmt.b32 	%r1341, %r1339, %r1340, %r778;
	prmt.b32 	%r1342, %r1339, %r1340, %r777;
	mov.b64	%rd761, {%r1342, %r1341};
	add.s64 	%rd762, %rd761, %rd756;
	xor.b64  	%rd763, %rd758, %rd762;
	mov.b64	{%r1343, %r1344}, %rd763;
	prmt.b32 	%r1345, %r1343, %r1344, %r784;
	prmt.b32 	%r1346, %r1343, %r1344, %r783;
	mov.b64	%rd764, {%r1346, %r1345};
	add.s64 	%rd765, %rd759, %rd764;
	xor.b64  	%rd766, %rd765, %rd761;
	mov.b64	{%r454, %r455}, %rd766;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r768;
	// inline asm
	mov.b64	%rd767, {%r449, %r453};
	add.s64 	%rd768, %rd714, %rd14;
	add.s64 	%rd769, %rd768, %rd723;
	xor.b64  	%rd770, %rd739, %rd769;
	mov.b64	{%r1347, %r1348}, %rd770;
	mov.b64	%rd771, {%r1348, %r1347};
	add.s64 	%rd772, %rd753, %rd771;
	xor.b64  	%rd773, %rd772, %rd714;
	mov.b64	{%r1349, %r1350}, %rd773;
	prmt.b32 	%r1351, %r1349, %r1350, %r778;
	prmt.b32 	%r1352, %r1349, %r1350, %r777;
	mov.b64	%rd774, {%r1352, %r1351};
	add.s64 	%rd775, %rd774, %rd769;
	xor.b64  	%rd776, %rd775, %rd771;
	mov.b64	{%r1353, %r1354}, %rd776;
	prmt.b32 	%r1355, %r1353, %r1354, %r784;
	prmt.b32 	%r1356, %r1353, %r1354, %r783;
	mov.b64	%rd777, {%r1356, %r1355};
	add.s64 	%rd778, %rd777, %rd772;
	xor.b64  	%rd779, %rd778, %rd774;
	mov.b64	{%r462, %r463}, %rd779;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r768;
	// inline asm
	mov.b64	%rd780, {%r457, %r461};
	add.s64 	%rd781, %rd737, %rd728;
	xor.b64  	%rd782, %rd752, %rd781;
	mov.b64	{%r1357, %r1358}, %rd782;
	mov.b64	%rd783, {%r1358, %r1357};
	add.s64 	%rd784, %rd783, %rd712;
	xor.b64  	%rd785, %rd784, %rd728;
	mov.b64	{%r1359, %r1360}, %rd785;
	prmt.b32 	%r1361, %r1359, %r1360, %r778;
	prmt.b32 	%r1362, %r1359, %r1360, %r777;
	mov.b64	%rd786, {%r1362, %r1361};
	add.s64 	%rd787, %rd781, %rd23;
	add.s64 	%rd788, %rd787, %rd786;
	xor.b64  	%rd789, %rd788, %rd783;
	mov.b64	{%r1363, %r1364}, %rd789;
	prmt.b32 	%r1365, %r1363, %r1364, %r784;
	prmt.b32 	%r1366, %r1363, %r1364, %r783;
	mov.b64	%rd790, {%r1366, %r1365};
	add.s64 	%rd791, %rd790, %rd784;
	xor.b64  	%rd792, %rd791, %rd786;
	mov.b64	{%r470, %r471}, %rd792;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r768;
	// inline asm
	mov.b64	%rd793, {%r465, %r469};
	add.s64 	%rd794, %rd742, %rd10;
	add.s64 	%rd795, %rd794, %rd750;
	xor.b64  	%rd796, %rd795, %rd711;
	mov.b64	{%r1367, %r1368}, %rd796;
	mov.b64	%rd797, {%r1368, %r1367};
	add.s64 	%rd798, %rd797, %rd726;
	xor.b64  	%rd799, %rd798, %rd742;
	mov.b64	{%r1369, %r1370}, %rd799;
	prmt.b32 	%r1371, %r1369, %r1370, %r778;
	prmt.b32 	%r1372, %r1369, %r1370, %r777;
	mov.b64	%rd800, {%r1372, %r1371};
	add.s64 	%rd801, %rd795, %rd16;
	add.s64 	%rd802, %rd801, %rd800;
	xor.b64  	%rd803, %rd802, %rd797;
	mov.b64	{%r1373, %r1374}, %rd803;
	prmt.b32 	%r1375, %r1373, %r1374, %r784;
	prmt.b32 	%r1376, %r1373, %r1374, %r783;
	mov.b64	%rd804, {%r1376, %r1375};
	add.s64 	%rd805, %rd804, %rd798;
	xor.b64  	%rd806, %rd805, %rd800;
	mov.b64	{%r478, %r479}, %rd806;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r768;
	// inline asm
	mov.b64	%rd807, {%r473, %r477};
	add.s64 	%rd808, %rd762, %rd12;
	add.s64 	%rd809, %rd808, %rd780;
	xor.b64  	%rd810, %rd804, %rd809;
	mov.b64	{%r1377, %r1378}, %rd810;
	mov.b64	%rd811, {%r1378, %r1377};
	add.s64 	%rd812, %rd811, %rd791;
	xor.b64  	%rd813, %rd812, %rd780;
	mov.b64	{%r1379, %r1380}, %rd813;
	prmt.b32 	%r1381, %r1379, %r1380, %r778;
	prmt.b32 	%r1382, %r1379, %r1380, %r777;
	mov.b64	%rd814, {%r1382, %r1381};
	add.s64 	%rd815, %rd809, %rd1;
	add.s64 	%rd816, %rd815, %rd814;
	xor.b64  	%rd817, %rd811, %rd816;
	mov.b64	{%r1383, %r1384}, %rd817;
	prmt.b32 	%r1385, %r1383, %r1384, %r784;
	prmt.b32 	%r1386, %r1383, %r1384, %r783;
	mov.b64	%rd818, {%r1386, %r1385};
	add.s64 	%rd819, %rd812, %rd818;
	xor.b64  	%rd820, %rd819, %rd814;
	mov.b64	{%r486, %r487}, %rd820;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r768;
	// inline asm
	mov.b64	%rd821, {%r481, %r485};
	add.s64 	%rd822, %rd793, %rd775;
	xor.b64  	%rd823, %rd822, %rd764;
	mov.b64	{%r1387, %r1388}, %rd823;
	mov.b64	%rd824, {%r1388, %r1387};
	add.s64 	%rd825, %rd824, %rd805;
	xor.b64  	%rd826, %rd825, %rd793;
	mov.b64	{%r1389, %r1390}, %rd826;
	prmt.b32 	%r1391, %r1389, %r1390, %r778;
	prmt.b32 	%r1392, %r1389, %r1390, %r777;
	mov.b64	%rd827, {%r1392, %r1391};
	add.s64 	%rd828, %rd822, %rd11;
	add.s64 	%rd829, %rd828, %rd827;
	xor.b64  	%rd830, %rd829, %rd824;
	mov.b64	{%r1393, %r1394}, %rd830;
	prmt.b32 	%r1395, %r1393, %r1394, %r784;
	prmt.b32 	%r1396, %r1393, %r1394, %r783;
	mov.b64	%rd831, {%r1396, %r1395};
	add.s64 	%rd832, %rd831, %rd825;
	xor.b64  	%rd833, %rd832, %rd827;
	mov.b64	{%r494, %r495}, %rd833;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r768;
	// inline asm
	mov.b64	%rd834, {%r489, %r493};
	add.s64 	%rd835, %rd788, %rd15;
	add.s64 	%rd836, %rd835, %rd807;
	xor.b64  	%rd837, %rd836, %rd777;
	mov.b64	{%r1397, %r1398}, %rd837;
	mov.b64	%rd838, {%r1398, %r1397};
	add.s64 	%rd839, %rd838, %rd765;
	xor.b64  	%rd840, %rd839, %rd807;
	mov.b64	{%r1399, %r1400}, %rd840;
	prmt.b32 	%r1401, %r1399, %r1400, %r778;
	prmt.b32 	%r1402, %r1399, %r1400, %r777;
	mov.b64	%rd841, {%r1402, %r1401};
	add.s64 	%rd842, %rd836, %rd13;
	add.s64 	%rd843, %rd842, %rd841;
	xor.b64  	%rd844, %rd843, %rd838;
	mov.b64	{%r1403, %r1404}, %rd844;
	prmt.b32 	%r1405, %r1403, %r1404, %r784;
	prmt.b32 	%r1406, %r1403, %r1404, %r783;
	mov.b64	%rd845, {%r1406, %r1405};
	add.s64 	%rd846, %rd845, %rd839;
	xor.b64  	%rd847, %rd846, %rd841;
	mov.b64	{%r502, %r503}, %rd847;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r768;
	// inline asm
	mov.b64	%rd848, {%r497, %r501};
	add.s64 	%rd849, %rd767, %rd9;
	add.s64 	%rd850, %rd849, %rd802;
	xor.b64  	%rd851, %rd850, %rd790;
	mov.b64	{%r1407, %r1408}, %rd851;
	mov.b64	%rd852, {%r1408, %r1407};
	add.s64 	%rd853, %rd852, %rd778;
	xor.b64  	%rd854, %rd853, %rd767;
	mov.b64	{%r1409, %r1410}, %rd854;
	prmt.b32 	%r1411, %r1409, %r1410, %r778;
	prmt.b32 	%r1412, %r1409, %r1410, %r777;
	mov.b64	%rd855, {%r1412, %r1411};
	add.s64 	%rd856, %rd855, %rd850;
	xor.b64  	%rd857, %rd856, %rd852;
	mov.b64	{%r1413, %r1414}, %rd857;
	prmt.b32 	%r1415, %r1413, %r1414, %r784;
	prmt.b32 	%r1416, %r1413, %r1414, %r783;
	mov.b64	%rd858, {%r1416, %r1415};
	add.s64 	%rd859, %rd858, %rd853;
	xor.b64  	%rd860, %rd859, %rd855;
	mov.b64	{%r510, %r511}, %rd860;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r768;
	// inline asm
	mov.b64	%rd861, {%r505, %r509};
	add.s64 	%rd862, %rd816, %rd13;
	add.s64 	%rd863, %rd862, %rd861;
	xor.b64  	%rd864, %rd863, %rd831;
	mov.b64	{%r1417, %r1418}, %rd864;
	mov.b64	%rd865, {%r1418, %r1417};
	add.s64 	%rd866, %rd865, %rd846;
	xor.b64  	%rd867, %rd866, %rd861;
	mov.b64	{%r1419, %r1420}, %rd867;
	prmt.b32 	%r1421, %r1419, %r1420, %r778;
	prmt.b32 	%r1422, %r1419, %r1420, %r777;
	mov.b64	%rd868, {%r1422, %r1421};
	add.s64 	%rd869, %rd868, %rd863;
	xor.b64  	%rd870, %rd865, %rd869;
	mov.b64	{%r1423, %r1424}, %rd870;
	prmt.b32 	%r1425, %r1423, %r1424, %r784;
	prmt.b32 	%r1426, %r1423, %r1424, %r783;
	mov.b64	%rd871, {%r1426, %r1425};
	add.s64 	%rd872, %rd866, %rd871;
	xor.b64  	%rd873, %rd872, %rd868;
	mov.b64	{%r518, %r519}, %rd873;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r768;
	// inline asm
	mov.b64	%rd874, {%r513, %r517};
	add.s64 	%rd875, %rd829, %rd821;
	xor.b64  	%rd876, %rd845, %rd875;
	mov.b64	{%r1427, %r1428}, %rd876;
	mov.b64	%rd877, {%r1428, %r1427};
	add.s64 	%rd878, %rd859, %rd877;
	xor.b64  	%rd879, %rd878, %rd821;
	mov.b64	{%r1429, %r1430}, %rd879;
	prmt.b32 	%r1431, %r1429, %r1430, %r778;
	prmt.b32 	%r1432, %r1429, %r1430, %r777;
	mov.b64	%rd880, {%r1432, %r1431};
	add.s64 	%rd881, %rd875, %rd16;
	add.s64 	%rd882, %rd881, %rd880;
	xor.b64  	%rd883, %rd882, %rd877;
	mov.b64	{%r1433, %r1434}, %rd883;
	prmt.b32 	%r1435, %r1433, %r1434, %r784;
	prmt.b32 	%r1436, %r1433, %r1434, %r783;
	mov.b64	%rd884, {%r1436, %r1435};
	add.s64 	%rd885, %rd884, %rd878;
	xor.b64  	%rd886, %rd885, %rd880;
	mov.b64	{%r526, %r527}, %rd886;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r768;
	// inline asm
	mov.b64	%rd887, {%r521, %r525};
	add.s64 	%rd888, %rd843, %rd834;
	xor.b64  	%rd889, %rd858, %rd888;
	mov.b64	{%r1437, %r1438}, %rd889;
	mov.b64	%rd890, {%r1438, %r1437};
	add.s64 	%rd891, %rd890, %rd819;
	xor.b64  	%rd892, %rd891, %rd834;
	mov.b64	{%r1439, %r1440}, %rd892;
	prmt.b32 	%r1441, %r1439, %r1440, %r778;
	prmt.b32 	%r1442, %r1439, %r1440, %r777;
	mov.b64	%rd893, {%r1442, %r1441};
	add.s64 	%rd894, %rd888, %rd10;
	add.s64 	%rd895, %rd894, %rd893;
	xor.b64  	%rd896, %rd895, %rd890;
	mov.b64	{%r1443, %r1444}, %rd896;
	prmt.b32 	%r1445, %r1443, %r1444, %r784;
	prmt.b32 	%r1446, %r1443, %r1444, %r783;
	mov.b64	%rd897, {%r1446, %r1445};
	add.s64 	%rd898, %rd897, %rd891;
	xor.b64  	%rd899, %rd898, %rd893;
	mov.b64	{%r534, %r535}, %rd899;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r768;
	// inline asm
	mov.b64	%rd900, {%r529, %r533};
	add.s64 	%rd901, %rd848, %rd1;
	add.s64 	%rd902, %rd901, %rd856;
	xor.b64  	%rd903, %rd902, %rd818;
	mov.b64	{%r1447, %r1448}, %rd903;
	mov.b64	%rd904, {%r1448, %r1447};
	add.s64 	%rd905, %rd904, %rd832;
	xor.b64  	%rd906, %rd905, %rd848;
	mov.b64	{%r1449, %r1450}, %rd906;
	prmt.b32 	%r1451, %r1449, %r1450, %r778;
	prmt.b32 	%r1452, %r1449, %r1450, %r777;
	mov.b64	%rd907, {%r1452, %r1451};
	add.s64 	%rd908, %rd902, %rd15;
	add.s64 	%rd909, %rd908, %rd907;
	xor.b64  	%rd910, %rd909, %rd904;
	mov.b64	{%r1453, %r1454}, %rd910;
	prmt.b32 	%r1455, %r1453, %r1454, %r784;
	prmt.b32 	%r1456, %r1453, %r1454, %r783;
	mov.b64	%rd911, {%r1456, %r1455};
	add.s64 	%rd912, %rd911, %rd905;
	xor.b64  	%rd913, %rd912, %rd907;
	mov.b64	{%r542, %r543}, %rd913;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r768;
	// inline asm
	mov.b64	%rd914, {%r537, %r541};
	add.s64 	%rd915, %rd887, %rd869;
	xor.b64  	%rd916, %rd911, %rd915;
	mov.b64	{%r1457, %r1458}, %rd916;
	mov.b64	%rd917, {%r1458, %r1457};
	add.s64 	%rd918, %rd917, %rd898;
	xor.b64  	%rd919, %rd918, %rd887;
	mov.b64	{%r1459, %r1460}, %rd919;
	prmt.b32 	%r1461, %r1459, %r1460, %r778;
	prmt.b32 	%r1462, %r1459, %r1460, %r777;
	mov.b64	%rd920, {%r1462, %r1461};
	add.s64 	%rd921, %rd915, %rd9;
	add.s64 	%rd922, %rd921, %rd920;
	xor.b64  	%rd923, %rd917, %rd922;
	mov.b64	{%r1463, %r1464}, %rd923;
	prmt.b32 	%r1465, %r1463, %r1464, %r784;
	prmt.b32 	%r1466, %r1463, %r1464, %r783;
	mov.b64	%rd924, {%r1466, %r1465};
	add.s64 	%rd925, %rd918, %rd924;
	xor.b64  	%rd926, %rd925, %rd920;
	mov.b64	{%r550, %r551}, %rd926;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r768;
	// inline asm
	mov.b64	%rd927, {%r545, %r549};
	add.s64 	%rd928, %rd900, %rd882;
	xor.b64  	%rd929, %rd928, %rd871;
	mov.b64	{%r1467, %r1468}, %rd929;
	mov.b64	%rd930, {%r1468, %r1467};
	add.s64 	%rd931, %rd930, %rd912;
	xor.b64  	%rd932, %rd931, %rd900;
	mov.b64	{%r1469, %r1470}, %rd932;
	prmt.b32 	%r1471, %r1469, %r1470, %r778;
	prmt.b32 	%r1472, %r1469, %r1470, %r777;
	mov.b64	%rd933, {%r1472, %r1471};
	add.s64 	%rd934, %rd928, %rd14;
	add.s64 	%rd935, %rd934, %rd933;
	xor.b64  	%rd936, %rd935, %rd930;
	mov.b64	{%r1473, %r1474}, %rd936;
	prmt.b32 	%r1475, %r1473, %r1474, %r784;
	prmt.b32 	%r1476, %r1473, %r1474, %r783;
	mov.b64	%rd937, {%r1476, %r1475};
	add.s64 	%rd938, %rd937, %rd931;
	xor.b64  	%rd939, %rd938, %rd933;
	mov.b64	{%r558, %r559}, %rd939;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r768;
	// inline asm
	mov.b64	%rd940, {%r553, %r557};
	add.s64 	%rd941, %rd895, %rd23;
	add.s64 	%rd942, %rd941, %rd914;
	xor.b64  	%rd943, %rd942, %rd884;
	mov.b64	{%r1477, %r1478}, %rd943;
	mov.b64	%rd944, {%r1478, %r1477};
	add.s64 	%rd945, %rd944, %rd872;
	xor.b64  	%rd946, %rd945, %rd914;
	mov.b64	{%r1479, %r1480}, %rd946;
	prmt.b32 	%r1481, %r1479, %r1480, %r778;
	prmt.b32 	%r1482, %r1479, %r1480, %r777;
	mov.b64	%rd947, {%r1482, %r1481};
	add.s64 	%rd948, %rd942, %rd11;
	add.s64 	%rd949, %rd948, %rd947;
	xor.b64  	%rd950, %rd949, %rd944;
	mov.b64	{%r1483, %r1484}, %rd950;
	prmt.b32 	%r1485, %r1483, %r1484, %r784;
	prmt.b32 	%r1486, %r1483, %r1484, %r783;
	mov.b64	%rd951, {%r1486, %r1485};
	add.s64 	%rd952, %rd951, %rd945;
	xor.b64  	%rd953, %rd952, %rd947;
	mov.b64	{%r566, %r567}, %rd953;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r768;
	// inline asm
	mov.b64	%rd954, {%r561, %r565};
	add.s64 	%rd955, %rd909, %rd874;
	xor.b64  	%rd956, %rd955, %rd897;
	mov.b64	{%r1487, %r1488}, %rd956;
	mov.b64	%rd957, {%r1488, %r1487};
	add.s64 	%rd958, %rd957, %rd885;
	xor.b64  	%rd959, %rd958, %rd874;
	mov.b64	{%r1489, %r1490}, %rd959;
	prmt.b32 	%r1491, %r1489, %r1490, %r778;
	prmt.b32 	%r1492, %r1489, %r1490, %r777;
	mov.b64	%rd960, {%r1492, %r1491};
	add.s64 	%rd961, %rd955, %rd12;
	add.s64 	%rd962, %rd961, %rd960;
	xor.b64  	%rd963, %rd962, %rd957;
	mov.b64	{%r1493, %r1494}, %rd963;
	prmt.b32 	%r1495, %r1493, %r1494, %r784;
	prmt.b32 	%r1496, %r1493, %r1494, %r783;
	mov.b64	%rd964, {%r1496, %r1495};
	add.s64 	%rd965, %rd964, %rd958;
	xor.b64  	%rd966, %rd965, %rd960;
	mov.b64	{%r574, %r575}, %rd966;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r768;
	// inline asm
	mov.b64	%rd967, {%r569, %r573};
	add.s64 	%rd968, %rd967, %rd922;
	xor.b64  	%rd969, %rd968, %rd937;
	mov.b64	{%r1497, %r1498}, %rd969;
	mov.b64	%rd970, {%r1498, %r1497};
	add.s64 	%rd971, %rd970, %rd952;
	xor.b64  	%rd972, %rd971, %rd967;
	mov.b64	{%r1499, %r1500}, %rd972;
	prmt.b32 	%r1501, %r1499, %r1500, %r778;
	prmt.b32 	%r1502, %r1499, %r1500, %r777;
	mov.b64	%rd973, {%r1502, %r1501};
	add.s64 	%rd974, %rd968, %rd9;
	add.s64 	%rd975, %rd974, %rd973;
	xor.b64  	%rd976, %rd970, %rd975;
	mov.b64	{%r1503, %r1504}, %rd976;
	prmt.b32 	%r1505, %r1503, %r1504, %r784;
	prmt.b32 	%r1506, %r1503, %r1504, %r783;
	mov.b64	%rd977, {%r1506, %r1505};
	add.s64 	%rd978, %rd971, %rd977;
	xor.b64  	%rd979, %rd978, %rd973;
	mov.b64	{%r582, %r583}, %rd979;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r768;
	// inline asm
	mov.b64	%rd980, {%r577, %r581};
	add.s64 	%rd981, %rd927, %rd15;
	add.s64 	%rd982, %rd981, %rd935;
	xor.b64  	%rd983, %rd951, %rd982;
	mov.b64	{%r1507, %r1508}, %rd983;
	mov.b64	%rd984, {%r1508, %r1507};
	add.s64 	%rd985, %rd965, %rd984;
	xor.b64  	%rd986, %rd985, %rd927;
	mov.b64	{%r1509, %r1510}, %rd986;
	prmt.b32 	%r1511, %r1509, %r1510, %r778;
	prmt.b32 	%r1512, %r1509, %r1510, %r777;
	mov.b64	%rd987, {%r1512, %r1511};
	add.s64 	%rd988, %rd982, %rd11;
	add.s64 	%rd989, %rd988, %rd987;
	xor.b64  	%rd990, %rd989, %rd984;
	mov.b64	{%r1513, %r1514}, %rd990;
	prmt.b32 	%r1515, %r1513, %r1514, %r784;
	prmt.b32 	%r1516, %r1513, %r1514, %r783;
	mov.b64	%rd991, {%r1516, %r1515};
	add.s64 	%rd992, %rd991, %rd985;
	xor.b64  	%rd993, %rd992, %rd987;
	mov.b64	{%r590, %r591}, %rd993;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r768;
	// inline asm
	mov.b64	%rd994, {%r585, %r589};
	add.s64 	%rd995, %rd940, %rd14;
	add.s64 	%rd996, %rd995, %rd949;
	xor.b64  	%rd997, %rd964, %rd996;
	mov.b64	{%r1517, %r1518}, %rd997;
	mov.b64	%rd998, {%r1518, %r1517};
	add.s64 	%rd999, %rd998, %rd925;
	xor.b64  	%rd1000, %rd999, %rd940;
	mov.b64	{%r1519, %r1520}, %rd1000;
	prmt.b32 	%r1521, %r1519, %r1520, %r778;
	prmt.b32 	%r1522, %r1519, %r1520, %r777;
	mov.b64	%rd1001, {%r1522, %r1521};
	add.s64 	%rd1002, %rd996, %rd13;
	add.s64 	%rd1003, %rd1002, %rd1001;
	xor.b64  	%rd1004, %rd1003, %rd998;
	mov.b64	{%r1523, %r1524}, %rd1004;
	prmt.b32 	%r1525, %r1523, %r1524, %r784;
	prmt.b32 	%r1526, %r1523, %r1524, %r783;
	mov.b64	%rd1005, {%r1526, %r1525};
	add.s64 	%rd1006, %rd1005, %rd999;
	xor.b64  	%rd1007, %rd1006, %rd1001;
	mov.b64	{%r598, %r599}, %rd1007;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r768;
	// inline asm
	mov.b64	%rd1008, {%r593, %r597};
	add.s64 	%rd1009, %rd954, %rd23;
	add.s64 	%rd1010, %rd1009, %rd962;
	xor.b64  	%rd1011, %rd1010, %rd924;
	mov.b64	{%r1527, %r1528}, %rd1011;
	mov.b64	%rd1012, {%r1528, %r1527};
	add.s64 	%rd1013, %rd1012, %rd938;
	xor.b64  	%rd1014, %rd1013, %rd954;
	mov.b64	{%r1529, %r1530}, %rd1014;
	prmt.b32 	%r1531, %r1529, %r1530, %r778;
	prmt.b32 	%r1532, %r1529, %r1530, %r777;
	mov.b64	%rd1015, {%r1532, %r1531};
	add.s64 	%rd1016, %rd1010, %rd12;
	add.s64 	%rd1017, %rd1016, %rd1015;
	xor.b64  	%rd1018, %rd1017, %rd1012;
	mov.b64	{%r1533, %r1534}, %rd1018;
	prmt.b32 	%r1535, %r1533, %r1534, %r784;
	prmt.b32 	%r1536, %r1533, %r1534, %r783;
	mov.b64	%rd1019, {%r1536, %r1535};
	add.s64 	%rd1020, %rd1019, %rd1013;
	xor.b64  	%rd1021, %rd1020, %rd1015;
	mov.b64	{%r606, %r607}, %rd1021;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r768;
	// inline asm
	mov.b64	%rd1022, {%r601, %r605};
	add.s64 	%rd1023, %rd994, %rd975;
	xor.b64  	%rd1024, %rd1019, %rd1023;
	mov.b64	{%r1537, %r1538}, %rd1024;
	mov.b64	%rd1025, {%r1538, %r1537};
	add.s64 	%rd1026, %rd1025, %rd1006;
	xor.b64  	%rd1027, %rd1026, %rd994;
	mov.b64	{%r1539, %r1540}, %rd1027;
	prmt.b32 	%r1541, %r1539, %r1540, %r778;
	prmt.b32 	%r1542, %r1539, %r1540, %r777;
	mov.b64	%rd1028, {%r1542, %r1541};
	add.s64 	%rd1029, %rd1028, %rd1023;
	xor.b64  	%rd1030, %rd1025, %rd1029;
	mov.b64	{%r1543, %r1544}, %rd1030;
	prmt.b32 	%r1545, %r1543, %r1544, %r784;
	prmt.b32 	%r1546, %r1543, %r1544, %r783;
	mov.b64	%rd1031, {%r1546, %r1545};
	add.s64 	%rd1032, %rd1026, %rd1031;
	xor.b64  	%rd1033, %rd1032, %rd1028;
	mov.b64	{%r614, %r615}, %rd1033;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r768;
	// inline asm
	mov.b64	%rd1034, {%r609, %r613};
	add.s64 	%rd1035, %rd989, %rd16;
	add.s64 	%rd1036, %rd1035, %rd1008;
	xor.b64  	%rd1037, %rd1036, %rd977;
	mov.b64	{%r1547, %r1548}, %rd1037;
	mov.b64	%rd1038, {%r1548, %r1547};
	add.s64 	%rd1039, %rd1038, %rd1020;
	xor.b64  	%rd1040, %rd1039, %rd1008;
	mov.b64	{%r1549, %r1550}, %rd1040;
	prmt.b32 	%r1551, %r1549, %r1550, %r778;
	prmt.b32 	%r1552, %r1549, %r1550, %r777;
	mov.b64	%rd1041, {%r1552, %r1551};
	add.s64 	%rd1042, %rd1041, %rd1036;
	xor.b64  	%rd1043, %rd1042, %rd1038;
	mov.b64	{%r1553, %r1554}, %rd1043;
	prmt.b32 	%r1555, %r1553, %r1554, %r784;
	prmt.b32 	%r1556, %r1553, %r1554, %r783;
	mov.b64	%rd1044, {%r1556, %r1555};
	add.s64 	%rd1045, %rd1044, %rd1039;
	xor.b64  	%rd1046, %rd1045, %rd1041;
	mov.b64	{%r622, %r623}, %rd1046;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r768;
	// inline asm
	mov.b64	%rd1047, {%r617, %r621};
	add.s64 	%rd1048, %rd1003, %rd10;
	add.s64 	%rd1049, %rd1048, %rd1022;
	xor.b64  	%rd1050, %rd1049, %rd991;
	mov.b64	{%r1557, %r1558}, %rd1050;
	mov.b64	%rd1051, {%r1558, %r1557};
	add.s64 	%rd1052, %rd1051, %rd978;
	xor.b64  	%rd1053, %rd1052, %rd1022;
	mov.b64	{%r1559, %r1560}, %rd1053;
	prmt.b32 	%r1561, %r1559, %r1560, %r778;
	prmt.b32 	%r1562, %r1559, %r1560, %r777;
	mov.b64	%rd1054, {%r1562, %r1561};
	add.s64 	%rd1055, %rd1054, %rd1049;
	xor.b64  	%rd1056, %rd1055, %rd1051;
	mov.b64	{%r1563, %r1564}, %rd1056;
	prmt.b32 	%r1565, %r1563, %r1564, %r784;
	prmt.b32 	%r1566, %r1563, %r1564, %r783;
	mov.b64	%rd1057, {%r1566, %r1565};
	add.s64 	%rd1058, %rd1057, %rd1052;
	xor.b64  	%rd1059, %rd1058, %rd1054;
	mov.b64	{%r630, %r631}, %rd1059;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r768;
	// inline asm
	mov.b64	%rd1060, {%r625, %r629};
	add.s64 	%rd1061, %rd1017, %rd980;
	xor.b64  	%rd1062, %rd1061, %rd1005;
	mov.b64	{%r1567, %r1568}, %rd1062;
	mov.b64	%rd1063, {%r1568, %r1567};
	add.s64 	%rd1064, %rd1063, %rd992;
	xor.b64  	%rd1065, %rd1064, %rd980;
	mov.b64	{%r1569, %r1570}, %rd1065;
	prmt.b32 	%r1571, %r1569, %r1570, %r778;
	prmt.b32 	%r1572, %r1569, %r1570, %r777;
	mov.b64	%rd1066, {%r1572, %r1571};
	add.s64 	%rd1067, %rd1061, %rd1;
	add.s64 	%rd1068, %rd1067, %rd1066;
	xor.b64  	%rd1069, %rd1068, %rd1063;
	mov.b64	{%r1573, %r1574}, %rd1069;
	prmt.b32 	%r1575, %r1573, %r1574, %r784;
	prmt.b32 	%r1576, %r1573, %r1574, %r783;
	mov.b64	%rd1070, {%r1576, %r1575};
	add.s64 	%rd1071, %rd1070, %rd1064;
	xor.b64  	%rd1072, %rd1071, %rd1066;
	mov.b64	{%r638, %r639}, %rd1072;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r768;
	// inline asm
	mov.b64	%rd1073, {%r633, %r637};
	add.s64 	%rd1074, %rd1029, %rd1;
	add.s64 	%rd1075, %rd1074, %rd1073;
	xor.b64  	%rd1076, %rd1075, %rd1044;
	mov.b64	{%r1577, %r1578}, %rd1076;
	mov.b64	%rd1077, {%r1578, %r1577};
	add.s64 	%rd1078, %rd1077, %rd1058;
	xor.b64  	%rd1079, %rd1078, %rd1073;
	mov.b64	{%r1579, %r1580}, %rd1079;
	prmt.b32 	%r1581, %r1579, %r1580, %r778;
	prmt.b32 	%r1582, %r1579, %r1580, %r777;
	mov.b64	%rd1080, {%r1582, %r1581};
	add.s64 	%rd1081, %rd1075, %rd23;
	add.s64 	%rd1082, %rd1081, %rd1080;
	xor.b64  	%rd1083, %rd1077, %rd1082;
	mov.b64	{%r1583, %r1584}, %rd1083;
	prmt.b32 	%r1585, %r1583, %r1584, %r784;
	prmt.b32 	%r1586, %r1583, %r1584, %r783;
	mov.b64	%rd1084, {%r1586, %r1585};
	add.s64 	%rd1085, %rd1078, %rd1084;
	xor.b64  	%rd1086, %rd1085, %rd1080;
	mov.b64	{%r646, %r647}, %rd1086;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r768;
	// inline asm
	mov.b64	%rd1087, {%r641, %r645};
	add.s64 	%rd1088, %rd1034, %rd9;
	add.s64 	%rd1089, %rd1088, %rd1042;
	xor.b64  	%rd1090, %rd1057, %rd1089;
	mov.b64	{%r1587, %r1588}, %rd1090;
	mov.b64	%rd1091, {%r1588, %r1587};
	add.s64 	%rd1092, %rd1071, %rd1091;
	xor.b64  	%rd1093, %rd1092, %rd1034;
	mov.b64	{%r1589, %r1590}, %rd1093;
	prmt.b32 	%r1591, %r1589, %r1590, %r778;
	prmt.b32 	%r1592, %r1589, %r1590, %r777;
	mov.b64	%rd1094, {%r1592, %r1591};
	add.s64 	%rd1095, %rd1089, %rd10;
	add.s64 	%rd1096, %rd1095, %rd1094;
	xor.b64  	%rd1097, %rd1096, %rd1091;
	mov.b64	{%r1593, %r1594}, %rd1097;
	prmt.b32 	%r1595, %r1593, %r1594, %r784;
	prmt.b32 	%r1596, %r1593, %r1594, %r783;
	mov.b64	%rd1098, {%r1596, %r1595};
	add.s64 	%rd1099, %rd1098, %rd1092;
	xor.b64  	%rd1100, %rd1099, %rd1094;
	mov.b64	{%r654, %r655}, %rd1100;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r768;
	// inline asm
	mov.b64	%rd1101, {%r649, %r653};
	add.s64 	%rd1102, %rd1047, %rd11;
	add.s64 	%rd1103, %rd1102, %rd1055;
	xor.b64  	%rd1104, %rd1070, %rd1103;
	mov.b64	{%r1597, %r1598}, %rd1104;
	mov.b64	%rd1105, {%r1598, %r1597};
	add.s64 	%rd1106, %rd1105, %rd1032;
	xor.b64  	%rd1107, %rd1106, %rd1047;
	mov.b64	{%r1599, %r1600}, %rd1107;
	prmt.b32 	%r1601, %r1599, %r1600, %r778;
	prmt.b32 	%r1602, %r1599, %r1600, %r777;
	mov.b64	%rd1108, {%r1602, %r1601};
	add.s64 	%rd1109, %rd1103, %rd12;
	add.s64 	%rd1110, %rd1109, %rd1108;
	xor.b64  	%rd1111, %rd1110, %rd1105;
	mov.b64	{%r1603, %r1604}, %rd1111;
	prmt.b32 	%r1605, %r1603, %r1604, %r784;
	prmt.b32 	%r1606, %r1603, %r1604, %r783;
	mov.b64	%rd1112, {%r1606, %r1605};
	add.s64 	%rd1113, %rd1112, %rd1106;
	xor.b64  	%rd1114, %rd1113, %rd1108;
	mov.b64	{%r662, %r663}, %rd1114;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r768;
	// inline asm
	mov.b64	%rd1115, {%r657, %r661};
	add.s64 	%rd1116, %rd1060, %rd13;
	add.s64 	%rd1117, %rd1116, %rd1068;
	xor.b64  	%rd1118, %rd1117, %rd1031;
	mov.b64	{%r1607, %r1608}, %rd1118;
	mov.b64	%rd1119, {%r1608, %r1607};
	add.s64 	%rd1120, %rd1119, %rd1045;
	xor.b64  	%rd1121, %rd1120, %rd1060;
	mov.b64	{%r1609, %r1610}, %rd1121;
	prmt.b32 	%r1611, %r1609, %r1610, %r778;
	prmt.b32 	%r1612, %r1609, %r1610, %r777;
	mov.b64	%rd1122, {%r1612, %r1611};
	add.s64 	%rd1123, %rd1117, %rd14;
	add.s64 	%rd1124, %rd1123, %rd1122;
	xor.b64  	%rd1125, %rd1124, %rd1119;
	mov.b64	{%r1613, %r1614}, %rd1125;
	prmt.b32 	%r1615, %r1613, %r1614, %r784;
	prmt.b32 	%r1616, %r1613, %r1614, %r783;
	mov.b64	%rd1126, {%r1616, %r1615};
	add.s64 	%rd1127, %rd1126, %rd1120;
	xor.b64  	%rd1128, %rd1127, %rd1122;
	mov.b64	{%r670, %r671}, %rd1128;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r768;
	// inline asm
	mov.b64	%rd1129, {%r665, %r669};
	add.s64 	%rd1130, %rd1082, %rd15;
	add.s64 	%rd1131, %rd1130, %rd1101;
	xor.b64  	%rd1132, %rd1126, %rd1131;
	mov.b64	{%r1617, %r1618}, %rd1132;
	mov.b64	%rd1133, {%r1618, %r1617};
	add.s64 	%rd1134, %rd1133, %rd1113;
	xor.b64  	%rd1135, %rd1134, %rd1101;
	mov.b64	{%r1619, %r1620}, %rd1135;
	prmt.b32 	%r1621, %r1619, %r1620, %r778;
	prmt.b32 	%r1622, %r1619, %r1620, %r777;
	mov.b64	%rd1136, {%r1622, %r1621};
	add.s64 	%rd1137, %rd1131, %rd16;
	add.s64 	%rd1138, %rd1137, %rd1136;
	xor.b64  	%rd1139, %rd1133, %rd1138;
	mov.b64	{%r1623, %r1624}, %rd1139;
	prmt.b32 	%r1625, %r1623, %r1624, %r784;
	prmt.b32 	%r1626, %r1623, %r1624, %r783;
	mov.b64	%rd1140, {%r1626, %r1625};
	add.s64 	%rd1141, %rd1134, %rd1140;
	xor.b64  	%rd1142, %rd1141, %rd1136;
	mov.b64	{%r678, %r679}, %rd1142;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r768;
	// inline asm
	mov.b64	%rd1143, {%r673, %r677};
	add.s64 	%rd1144, %rd1115, %rd1096;
	xor.b64  	%rd1145, %rd1144, %rd1084;
	mov.b64	{%r1627, %r1628}, %rd1145;
	mov.b64	%rd1146, {%r1628, %r1627};
	add.s64 	%rd1147, %rd1146, %rd1127;
	xor.b64  	%rd1148, %rd1147, %rd1115;
	mov.b64	{%r1629, %r1630}, %rd1148;
	prmt.b32 	%r1631, %r1629, %r1630, %r778;
	prmt.b32 	%r1632, %r1629, %r1630, %r777;
	mov.b64	%rd1149, {%r1632, %r1631};
	add.s64 	%rd1150, %rd1149, %rd1144;
	xor.b64  	%rd1151, %rd1150, %rd1146;
	mov.b64	{%r1633, %r1634}, %rd1151;
	prmt.b32 	%r1635, %r1633, %r1634, %r784;
	prmt.b32 	%r1636, %r1633, %r1634, %r783;
	mov.b64	%rd1152, {%r1636, %r1635};
	add.s64 	%rd1153, %rd1152, %rd1147;
	xor.b64  	%rd1154, %rd1153, %rd1149;
	mov.b64	{%r686, %r687}, %rd1154;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r768;
	// inline asm
	mov.b64	%rd1155, {%r681, %r685};
	add.s64 	%rd1156, %rd1129, %rd1110;
	xor.b64  	%rd1157, %rd1156, %rd1098;
	mov.b64	{%r1637, %r1638}, %rd1157;
	mov.b64	%rd1158, {%r1638, %r1637};
	add.s64 	%rd1159, %rd1158, %rd1085;
	xor.b64  	%rd1160, %rd1159, %rd1129;
	mov.b64	{%r1639, %r1640}, %rd1160;
	prmt.b32 	%r1641, %r1639, %r1640, %r778;
	prmt.b32 	%r1642, %r1639, %r1640, %r777;
	mov.b64	%rd1161, {%r1642, %r1641};
	add.s64 	%rd1162, %rd1161, %rd1156;
	xor.b64  	%rd1163, %rd1162, %rd1158;
	mov.b64	{%r1643, %r1644}, %rd1163;
	prmt.b32 	%r1645, %r1643, %r1644, %r784;
	prmt.b32 	%r1646, %r1643, %r1644, %r783;
	mov.b64	%rd1164, {%r1646, %r1645};
	add.s64 	%rd1165, %rd1164, %rd1159;
	xor.b64  	%rd1166, %rd1165, %rd1161;
	mov.b64	{%r694, %r695}, %rd1166;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r768;
	// inline asm
	mov.b64	%rd1167, {%r689, %r693};
	add.s64 	%rd1168, %rd1124, %rd1087;
	xor.b64  	%rd1169, %rd1168, %rd1112;
	mov.b64	{%r1647, %r1648}, %rd1169;
	mov.b64	%rd1170, {%r1648, %r1647};
	add.s64 	%rd1171, %rd1170, %rd1099;
	xor.b64  	%rd1172, %rd1171, %rd1087;
	mov.b64	{%r1649, %r1650}, %rd1172;
	prmt.b32 	%r1651, %r1649, %r1650, %r778;
	prmt.b32 	%r1652, %r1649, %r1650, %r777;
	mov.b64	%rd1173, {%r1652, %r1651};
	add.s64 	%rd1174, %rd1173, %rd1168;
	xor.b64  	%rd1175, %rd1174, %rd1170;
	mov.b64	{%r1653, %r1654}, %rd1175;
	prmt.b32 	%r1655, %r1653, %r1654, %r784;
	prmt.b32 	%r1656, %r1653, %r1654, %r783;
	mov.b64	%rd1176, {%r1656, %r1655};
	add.s64 	%rd1177, %rd1176, %rd1171;
	xor.b64  	%rd1178, %rd1177, %rd1173;
	mov.b64	{%r702, %r703}, %rd1178;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r768;
	// inline asm
	mov.b64	%rd1179, {%r697, %r701};
	add.s64 	%rd1180, %rd1179, %rd1138;
	xor.b64  	%rd1181, %rd1180, %rd1152;
	mov.b64	{%r1657, %r1658}, %rd1181;
	mov.b64	%rd1182, {%r1658, %r1657};
	add.s64 	%rd1183, %rd1182, %rd1165;
	xor.b64  	%rd1184, %rd1183, %rd1179;
	mov.b64	{%r1659, %r1660}, %rd1184;
	prmt.b32 	%r1661, %r1659, %r1660, %r778;
	prmt.b32 	%r1662, %r1659, %r1660, %r777;
	mov.b64	%rd1185, {%r1662, %r1661};
	add.s64 	%rd1186, %rd1185, %rd1180;
	xor.b64  	%rd1187, %rd1182, %rd1186;
	mov.b64	{%r1663, %r1664}, %rd1187;
	prmt.b32 	%r1665, %r1663, %r1664, %r784;
	prmt.b32 	%r1666, %r1663, %r1664, %r783;
	mov.b64	%rd1188, {%r1666, %r1665};
	add.s64 	%rd1189, %rd1183, %rd1188;
	xor.b64  	%rd1190, %rd1189, %rd1185;
	mov.b64	{%r710, %r711}, %rd1190;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r768;
	// inline asm
	mov.b64	%rd1191, {%r705, %r709};
	add.s64 	%rd1192, %rd1143, %rd11;
	add.s64 	%rd1193, %rd1192, %rd1150;
	xor.b64  	%rd1194, %rd1164, %rd1193;
	mov.b64	{%r1667, %r1668}, %rd1194;
	mov.b64	%rd1195, {%r1668, %r1667};
	add.s64 	%rd1196, %rd1177, %rd1195;
	xor.b64  	%rd1197, %rd1196, %rd1143;
	mov.b64	{%r1669, %r1670}, %rd1197;
	prmt.b32 	%r1671, %r1669, %r1670, %r778;
	prmt.b32 	%r1672, %r1669, %r1670, %r777;
	mov.b64	%rd1198, {%r1672, %r1671};
	add.s64 	%rd1199, %rd1193, %rd15;
	add.s64 	%rd1200, %rd1199, %rd1198;
	xor.b64  	%rd1201, %rd1200, %rd1195;
	mov.b64	{%r1673, %r1674}, %rd1201;
	prmt.b32 	%r1675, %r1673, %r1674, %r784;
	prmt.b32 	%r1676, %r1673, %r1674, %r783;
	mov.b64	%rd1202, {%r1676, %r1675};
	add.s64 	%rd1203, %rd1202, %rd1196;
	xor.b64  	%rd1204, %rd1203, %rd1198;
	mov.b64	{%r718, %r719}, %rd1204;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r768;
	// inline asm
	mov.b64	%rd1205, {%r713, %r717};
	add.s64 	%rd1206, %rd1155, %rd16;
	add.s64 	%rd1207, %rd1206, %rd1162;
	xor.b64  	%rd1208, %rd1176, %rd1207;
	mov.b64	{%r1677, %r1678}, %rd1208;
	mov.b64	%rd1209, {%r1678, %r1677};
	add.s64 	%rd1210, %rd1209, %rd1141;
	xor.b64  	%rd1211, %rd1210, %rd1155;
	mov.b64	{%r1679, %r1680}, %rd1211;
	prmt.b32 	%r1681, %r1679, %r1680, %r778;
	prmt.b32 	%r1682, %r1679, %r1680, %r777;
	mov.b64	%rd1212, {%r1682, %r1681};
	add.s64 	%rd1213, %rd1212, %rd1207;
	xor.b64  	%rd1214, %rd1213, %rd1209;
	mov.b64	{%r1683, %r1684}, %rd1214;
	prmt.b32 	%r1685, %r1683, %r1684, %r784;
	prmt.b32 	%r1686, %r1683, %r1684, %r783;
	mov.b64	%rd1215, {%r1686, %r1685};
	add.s64 	%rd1216, %rd1215, %rd1210;
	xor.b64  	%rd1217, %rd1216, %rd1212;
	mov.b64	{%r726, %r727}, %rd1217;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r768;
	// inline asm
	mov.b64	%rd1218, {%r721, %r725};
	add.s64 	%rd1219, %rd1174, %rd1167;
	xor.b64  	%rd1220, %rd1219, %rd1140;
	mov.b64	{%r1687, %r1688}, %rd1220;
	mov.b64	%rd1221, {%r1688, %r1687};
	add.s64 	%rd1222, %rd1221, %rd1153;
	xor.b64  	%rd1223, %rd1222, %rd1167;
	mov.b64	{%r1689, %r1690}, %rd1223;
	prmt.b32 	%r1691, %r1689, %r1690, %r778;
	prmt.b32 	%r1692, %r1689, %r1690, %r777;
	mov.b64	%rd1224, {%r1692, %r1691};
	add.s64 	%rd1225, %rd1219, %rd13;
	add.s64 	%rd1226, %rd1225, %rd1224;
	xor.b64  	%rd1227, %rd1226, %rd1221;
	mov.b64	{%r1693, %r1694}, %rd1227;
	prmt.b32 	%r1695, %r1693, %r1694, %r784;
	prmt.b32 	%r1696, %r1693, %r1694, %r783;
	mov.b64	%rd1228, {%r1696, %r1695};
	add.s64 	%rd1229, %rd1228, %rd1222;
	xor.b64  	%rd1230, %rd1229, %rd1224;
	mov.b64	{%r734, %r735}, %rd1230;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r768;
	// inline asm
	mov.b64	%rd1231, {%r729, %r733};
	add.s64 	%rd1232, %rd1186, %rd23;
	add.s64 	%rd1233, %rd1232, %rd1205;
	xor.b64  	%rd1234, %rd1228, %rd1233;
	mov.b64	{%r1697, %r1698}, %rd1234;
	mov.b64	%rd1235, {%r1698, %r1697};
	add.s64 	%rd1236, %rd1235, %rd1216;
	xor.b64  	%rd1237, %rd1236, %rd1205;
	mov.b64	{%r1699, %r1700}, %rd1237;
	prmt.b32 	%r1701, %r1699, %r1700, %r778;
	prmt.b32 	%r1702, %r1699, %r1700, %r777;
	mov.b64	%rd1238, {%r1702, %r1701};
	add.s64 	%rd1239, %rd1238, %rd1233;
	xor.b64  	%rd1240, %rd1235, %rd1239;
	mov.b64	{%r1703, %r1704}, %rd1240;
	prmt.b32 	%r1705, %r1703, %r1704, %r784;
	prmt.b32 	%r1706, %r1703, %r1704, %r783;
	mov.b64	%rd1241, {%r1706, %r1705};
	add.s64 	%rd1242, %rd1236, %rd1241;
	xor.b64  	%rd1243, %rd1242, %rd1238;
	mov.b64	{%r742, %r743}, %rd1243;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r768;
	// inline asm
	add.s64 	%rd1244, %rd1200, %rd1;
	add.s64 	%rd1245, %rd1244, %rd1218;
	xor.b64  	%rd1246, %rd1245, %rd1188;
	mov.b64	{%r1707, %r1708}, %rd1246;
	mov.b64	%rd1247, {%r1708, %r1707};
	add.s64 	%rd1248, %rd1247, %rd1229;
	xor.b64  	%rd1249, %rd1248, %rd1218;
	mov.b64	{%r1709, %r1710}, %rd1249;
	prmt.b32 	%r1711, %r1709, %r1710, %r778;
	prmt.b32 	%r1712, %r1709, %r1710, %r777;
	mov.b64	%rd1250, {%r1712, %r1711};
	add.s64 	%rd1251, %rd1245, %rd9;
	add.s64 	%rd1252, %rd1251, %rd1250;
	xor.b64  	%rd1253, %rd1252, %rd1247;
	mov.b64	{%r1713, %r1714}, %rd1253;
	prmt.b32 	%r1715, %r1713, %r1714, %r784;
	prmt.b32 	%r1716, %r1713, %r1714, %r783;
	mov.b64	%rd1254, {%r1716, %r1715};
	add.s64 	%rd1255, %rd1254, %rd1248;
	xor.b64  	%rd1256, %rd1255, %rd1250;
	mov.b64	{%r750, %r751}, %rd1256;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r768;
	// inline asm
	add.s64 	%rd1257, %rd1231, %rd1213;
	xor.b64  	%rd1258, %rd1257, %rd1202;
	mov.b64	{%r1717, %r1718}, %rd1258;
	mov.b64	%rd1259, {%r1718, %r1717};
	add.s64 	%rd1260, %rd1259, %rd1189;
	xor.b64  	%rd1261, %rd1260, %rd1231;
	mov.b64	{%r1719, %r1720}, %rd1261;
	prmt.b32 	%r1721, %r1719, %r1720, %r778;
	prmt.b32 	%r1722, %r1719, %r1720, %r777;
	mov.b64	%rd1262, {%r1722, %r1721};
	add.s64 	%rd1263, %rd1257, %rd14;
	add.s64 	%rd1264, %rd1263, %rd1262;
	xor.b64  	%rd1265, %rd1264, %rd1259;
	mov.b64	{%r1723, %r1724}, %rd1265;
	prmt.b32 	%r1725, %r1723, %r1724, %r784;
	prmt.b32 	%r1726, %r1723, %r1724, %r783;
	mov.b64	%rd1266, {%r1726, %r1725};
	add.s64 	%rd1267, %rd1266, %rd1260;
	xor.b64  	%rd1268, %rd1267, %rd1262;
	mov.b64	{%r758, %r759}, %rd1268;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r768;
	// inline asm
	mov.b64	%rd1269, {%r753, %r757};
	add.s64 	%rd1270, %rd1191, %rd12;
	add.s64 	%rd1271, %rd1270, %rd1226;
	xor.b64  	%rd1272, %rd1271, %rd1215;
	mov.b64	{%r1727, %r1728}, %rd1272;
	mov.b64	%rd1273, {%r1728, %r1727};
	add.s64 	%rd1274, %rd1273, %rd1203;
	xor.b64  	%rd1275, %rd1274, %rd1191;
	mov.b64	{%r1729, %r1730}, %rd1275;
	prmt.b32 	%r1731, %r1729, %r1730, %r778;
	prmt.b32 	%r1732, %r1729, %r1730, %r777;
	mov.b64	%rd1276, {%r1732, %r1731};
	add.s64 	%rd1277, %rd1271, %rd10;
	add.s64 	%rd1278, %rd1277, %rd1276;
	xor.b64  	%rd1279, %rd1278, %rd1273;
	mov.b64	{%r1733, %r1734}, %rd1279;
	prmt.b32 	%r1735, %r1733, %r1734, %r784;
	prmt.b32 	%r1736, %r1733, %r1734, %r783;
	mov.b64	%rd1280, {%r1736, %r1735};
	add.s64 	%rd1281, %rd1280, %rd1274;
	xor.b64  	%rd1282, %rd1281, %rd1276;
	mov.b64	{%r766, %r767}, %rd1282;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r768;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r768;
	// inline asm
	xor.b64  	%rd1283, %rd1241, %rd1269;
	xor.b64  	%rd1284, %rd1283, 6620516959819538809;
	mov.b64	{%r1737, %r1738}, %rd1284;
	setp.ne.s32	%p1, %r1738, 0;
	@%p1 bra 	BB12_2;

	ld.param.u64 	%rd1286, [_Z30blake2b_512_single_block_benchILj76EEvPyPKvy_param_0];
	cvta.to.global.u64 	%rd1285, %rd1286;
	st.global.u64 	[%rd1285], %rd1;

BB12_2:
	ret;
}

	// .globl	_Z30blake2b_512_double_block_benchILj256EEvPyPKvy
.visible .entry _Z30blake2b_512_double_block_benchILj256EEvPyPKvy(
	.param .u64 _Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_0,
	.param .u64 _Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_1,
	.param .u64 _Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<3467>;
	.reg .b64 	%rd<2741>;


	ld.param.u64 	%rd3, [_Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_1];
	ld.param.u64 	%rd4, [_Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_2];
	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r1537, %ntid.x;
	mov.u32 	%r1538, %ctaid.x;
	mul.lo.s32 	%r1539, %r1537, %r1538;
	cvt.u64.u32	%rd6, %r1539;
	add.s64 	%rd7, %rd6, %rd4;
	mov.u32 	%r1540, %tid.x;
	cvt.u64.u32	%rd8, %r1540;
	add.s64 	%rd1, %rd7, %rd8;
	ld.global.u64 	%rd9, [%rd5+16];
	ld.global.u64 	%rd10, [%rd5+24];
	ld.global.u64 	%rd11, [%rd5+32];
	ld.global.u64 	%rd12, [%rd5+40];
	ld.global.u64 	%rd13, [%rd5+48];
	ld.global.u64 	%rd14, [%rd5+56];
	ld.global.u64 	%rd15, [%rd5+64];
	ld.global.u64 	%rd16, [%rd5+72];
	ld.global.u64 	%rd17, [%rd5+80];
	ld.global.u64 	%rd18, [%rd5+88];
	ld.global.u64 	%rd19, [%rd5+96];
	ld.global.u64 	%rd20, [%rd5+104];
	ld.global.u64 	%rd21, [%rd5+112];
	ld.global.u64 	%rd22, [%rd5+120];
	add.s64 	%rd23, %rd1, -4965156021692249063;
	xor.b64  	%rd24, %rd23, 5840696475078001233;
	mov.b64	{%r1541, %r1542}, %rd24;
	mov.b64	%rd25, {%r1542, %r1541};
	add.s64 	%rd26, %rd25, 7640891576956012808;
	xor.b64  	%rd27, %rd26, 5840696475078001361;
	mov.b64	{%r1543, %r1544}, %rd27;
	mov.u32 	%r1536, 1;
	mov.u32 	%r1545, 25923;
	mov.u32 	%r1546, 8455;
	prmt.b32 	%r1547, %r1543, %r1544, %r1546;
	prmt.b32 	%r1548, %r1543, %r1544, %r1545;
	mov.b64	%rd28, {%r1548, %r1547};
	ld.global.u64 	%rd29, [%rd5+8];
	add.s64 	%rd30, %rd23, %rd29;
	add.s64 	%rd31, %rd30, %rd28;
	xor.b64  	%rd32, %rd31, %rd25;
	mov.b64	{%r1549, %r1550}, %rd32;
	mov.u32 	%r1551, 21554;
	mov.u32 	%r1552, 4214;
	prmt.b32 	%r1553, %r1549, %r1550, %r1552;
	prmt.b32 	%r1554, %r1549, %r1550, %r1551;
	mov.b64	%rd33, {%r1554, %r1553};
	add.s64 	%rd34, %rd33, %rd26;
	xor.b64  	%rd35, %rd34, %rd28;
	mov.b64	{%r6, %r7}, %rd35;
	// inline asm
	shf.l.wrap.b32 %r1, %r7, %r6, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r5, %r6, %r7, %r1536;
	// inline asm
	mov.b64	%rd36, {%r1, %r5};
	add.s64 	%rd37, %rd9, 6227659224458531674;
	xor.b64  	%rd38, %rd37, -7276294671716946913;
	mov.b64	{%r1555, %r1556}, %rd38;
	mov.b64	%rd39, {%r1556, %r1555};
	add.s64 	%rd40, %rd39, -4942790177534073029;
	xor.b64  	%rd41, %rd40, -7276294671716946913;
	mov.b64	{%r1557, %r1558}, %rd41;
	prmt.b32 	%r1559, %r1557, %r1558, %r1546;
	prmt.b32 	%r1560, %r1557, %r1558, %r1545;
	mov.b64	%rd42, {%r1560, %r1559};
	add.s64 	%rd43, %rd10, %rd37;
	add.s64 	%rd44, %rd43, %rd42;
	xor.b64  	%rd45, %rd44, %rd39;
	mov.b64	{%r1561, %r1562}, %rd45;
	prmt.b32 	%r1563, %r1561, %r1562, %r1552;
	prmt.b32 	%r1564, %r1561, %r1562, %r1551;
	mov.b64	%rd46, {%r1564, %r1563};
	add.s64 	%rd47, %rd46, %rd40;
	xor.b64  	%rd48, %rd47, %rd42;
	mov.b64	{%r14, %r15}, %rd48;
	// inline asm
	shf.l.wrap.b32 %r9, %r15, %r14, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r13, %r14, %r15, %r1536;
	// inline asm
	mov.b64	%rd49, {%r9, %r13};
	add.s64 	%rd50, %rd11, 6625583534739731862;
	xor.b64  	%rd51, %rd50, 2270897969802886507;
	mov.b64	{%r1565, %r1566}, %rd51;
	mov.b64	%rd52, {%r1566, %r1565};
	add.s64 	%rd53, %rd52, 4354685564936845355;
	xor.b64  	%rd54, %rd53, 2270897969802886507;
	mov.b64	{%r1567, %r1568}, %rd54;
	prmt.b32 	%r1569, %r1567, %r1568, %r1546;
	prmt.b32 	%r1570, %r1567, %r1568, %r1545;
	mov.b64	%rd55, {%r1570, %r1569};
	add.s64 	%rd56, %rd12, %rd50;
	add.s64 	%rd57, %rd56, %rd55;
	xor.b64  	%rd58, %rd57, %rd52;
	mov.b64	{%r1571, %r1572}, %rd58;
	prmt.b32 	%r1573, %r1571, %r1572, %r1552;
	prmt.b32 	%r1574, %r1571, %r1572, %r1551;
	mov.b64	%rd59, {%r1574, %r1573};
	add.s64 	%rd60, %rd59, %rd53;
	xor.b64  	%rd61, %rd60, %rd55;
	mov.b64	{%r22, %r23}, %rd61;
	// inline asm
	shf.l.wrap.b32 %r17, %r23, %r22, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r21, %r22, %r23, %r1536;
	// inline asm
	mov.b64	%rd62, {%r17, %r21};
	add.s64 	%rd63, %rd13, 85782056580896874;
	xor.b64  	%rd64, %rd63, 6620516959819538809;
	mov.b64	{%r1575, %r1576}, %rd64;
	mov.b64	%rd65, {%r1576, %r1575};
	add.s64 	%rd66, %rd65, -6534734903238641935;
	xor.b64  	%rd67, %rd66, 6620516959819538809;
	mov.b64	{%r1577, %r1578}, %rd67;
	prmt.b32 	%r1579, %r1577, %r1578, %r1546;
	prmt.b32 	%r1580, %r1577, %r1578, %r1545;
	mov.b64	%rd68, {%r1580, %r1579};
	add.s64 	%rd69, %rd14, %rd63;
	add.s64 	%rd70, %rd69, %rd68;
	xor.b64  	%rd71, %rd70, %rd65;
	mov.b64	{%r1581, %r1582}, %rd71;
	prmt.b32 	%r1583, %r1581, %r1582, %r1552;
	prmt.b32 	%r1584, %r1581, %r1582, %r1551;
	mov.b64	%rd72, {%r1584, %r1583};
	add.s64 	%rd73, %rd72, %rd66;
	xor.b64  	%rd74, %rd73, %rd68;
	mov.b64	{%r30, %r31}, %rd74;
	// inline asm
	shf.l.wrap.b32 %r25, %r31, %r30, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r29, %r30, %r31, %r1536;
	// inline asm
	mov.b64	%rd75, {%r25, %r29};
	add.s64 	%rd76, %rd31, %rd15;
	add.s64 	%rd77, %rd76, %rd49;
	xor.b64  	%rd78, %rd72, %rd77;
	mov.b64	{%r1585, %r1586}, %rd78;
	mov.b64	%rd79, {%r1586, %r1585};
	add.s64 	%rd80, %rd79, %rd60;
	xor.b64  	%rd81, %rd80, %rd49;
	mov.b64	{%r1587, %r1588}, %rd81;
	prmt.b32 	%r1589, %r1587, %r1588, %r1546;
	prmt.b32 	%r1590, %r1587, %r1588, %r1545;
	mov.b64	%rd82, {%r1590, %r1589};
	add.s64 	%rd83, %rd77, %rd16;
	add.s64 	%rd84, %rd83, %rd82;
	xor.b64  	%rd85, %rd79, %rd84;
	mov.b64	{%r1591, %r1592}, %rd85;
	prmt.b32 	%r1593, %r1591, %r1592, %r1552;
	prmt.b32 	%r1594, %r1591, %r1592, %r1551;
	mov.b64	%rd86, {%r1594, %r1593};
	add.s64 	%rd87, %rd86, %rd80;
	xor.b64  	%rd88, %rd87, %rd82;
	mov.b64	{%r38, %r39}, %rd88;
	// inline asm
	shf.l.wrap.b32 %r33, %r39, %r38, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r37, %r38, %r39, %r1536;
	// inline asm
	mov.b64	%rd89, {%r33, %r37};
	add.s64 	%rd90, %rd44, %rd17;
	add.s64 	%rd91, %rd90, %rd62;
	xor.b64  	%rd92, %rd91, %rd33;
	mov.b64	{%r1595, %r1596}, %rd92;
	mov.b64	%rd93, {%r1596, %r1595};
	add.s64 	%rd94, %rd93, %rd73;
	xor.b64  	%rd95, %rd94, %rd62;
	mov.b64	{%r1597, %r1598}, %rd95;
	prmt.b32 	%r1599, %r1597, %r1598, %r1546;
	prmt.b32 	%r1600, %r1597, %r1598, %r1545;
	mov.b64	%rd96, {%r1600, %r1599};
	add.s64 	%rd97, %rd91, %rd18;
	add.s64 	%rd98, %rd97, %rd96;
	xor.b64  	%rd99, %rd98, %rd93;
	mov.b64	{%r1601, %r1602}, %rd99;
	prmt.b32 	%r1603, %r1601, %r1602, %r1552;
	prmt.b32 	%r1604, %r1601, %r1602, %r1551;
	mov.b64	%rd100, {%r1604, %r1603};
	add.s64 	%rd101, %rd100, %rd94;
	xor.b64  	%rd102, %rd101, %rd96;
	mov.b64	{%r46, %r47}, %rd102;
	// inline asm
	shf.l.wrap.b32 %r41, %r47, %r46, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r45, %r46, %r47, %r1536;
	// inline asm
	mov.b64	%rd103, {%r41, %r45};
	add.s64 	%rd104, %rd57, %rd19;
	add.s64 	%rd105, %rd104, %rd75;
	xor.b64  	%rd106, %rd105, %rd46;
	mov.b64	{%r1605, %r1606}, %rd106;
	mov.b64	%rd107, {%r1606, %r1605};
	add.s64 	%rd108, %rd107, %rd34;
	xor.b64  	%rd109, %rd108, %rd75;
	mov.b64	{%r1607, %r1608}, %rd109;
	prmt.b32 	%r1609, %r1607, %r1608, %r1546;
	prmt.b32 	%r1610, %r1607, %r1608, %r1545;
	mov.b64	%rd110, {%r1610, %r1609};
	add.s64 	%rd111, %rd105, %rd20;
	add.s64 	%rd112, %rd111, %rd110;
	xor.b64  	%rd113, %rd112, %rd107;
	mov.b64	{%r1611, %r1612}, %rd113;
	prmt.b32 	%r1613, %r1611, %r1612, %r1552;
	prmt.b32 	%r1614, %r1611, %r1612, %r1551;
	mov.b64	%rd114, {%r1614, %r1613};
	add.s64 	%rd115, %rd114, %rd108;
	xor.b64  	%rd116, %rd115, %rd110;
	mov.b64	{%r54, %r55}, %rd116;
	// inline asm
	shf.l.wrap.b32 %r49, %r55, %r54, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r53, %r54, %r55, %r1536;
	// inline asm
	mov.b64	%rd117, {%r49, %r53};
	add.s64 	%rd118, %rd36, %rd21;
	add.s64 	%rd119, %rd118, %rd70;
	xor.b64  	%rd120, %rd119, %rd59;
	mov.b64	{%r1615, %r1616}, %rd120;
	mov.b64	%rd121, {%r1616, %r1615};
	add.s64 	%rd122, %rd121, %rd47;
	xor.b64  	%rd123, %rd122, %rd36;
	mov.b64	{%r1617, %r1618}, %rd123;
	prmt.b32 	%r1619, %r1617, %r1618, %r1546;
	prmt.b32 	%r1620, %r1617, %r1618, %r1545;
	mov.b64	%rd124, {%r1620, %r1619};
	add.s64 	%rd125, %rd119, %rd22;
	add.s64 	%rd126, %rd125, %rd124;
	xor.b64  	%rd127, %rd126, %rd121;
	mov.b64	{%r1621, %r1622}, %rd127;
	prmt.b32 	%r1623, %r1621, %r1622, %r1552;
	prmt.b32 	%r1624, %r1621, %r1622, %r1551;
	mov.b64	%rd128, {%r1624, %r1623};
	add.s64 	%rd129, %rd128, %rd122;
	xor.b64  	%rd130, %rd129, %rd124;
	mov.b64	{%r62, %r63}, %rd130;
	// inline asm
	shf.l.wrap.b32 %r57, %r63, %r62, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r61, %r62, %r63, %r1536;
	// inline asm
	mov.b64	%rd131, {%r57, %r61};
	add.s64 	%rd132, %rd84, %rd21;
	add.s64 	%rd133, %rd132, %rd131;
	xor.b64  	%rd134, %rd133, %rd100;
	mov.b64	{%r1625, %r1626}, %rd134;
	mov.b64	%rd135, {%r1626, %r1625};
	add.s64 	%rd136, %rd135, %rd115;
	xor.b64  	%rd137, %rd136, %rd131;
	mov.b64	{%r1627, %r1628}, %rd137;
	prmt.b32 	%r1629, %r1627, %r1628, %r1546;
	prmt.b32 	%r1630, %r1627, %r1628, %r1545;
	mov.b64	%rd138, {%r1630, %r1629};
	add.s64 	%rd139, %rd133, %rd17;
	add.s64 	%rd140, %rd139, %rd138;
	xor.b64  	%rd141, %rd135, %rd140;
	mov.b64	{%r1631, %r1632}, %rd141;
	prmt.b32 	%r1633, %r1631, %r1632, %r1552;
	prmt.b32 	%r1634, %r1631, %r1632, %r1551;
	mov.b64	%rd142, {%r1634, %r1633};
	add.s64 	%rd143, %rd136, %rd142;
	xor.b64  	%rd144, %rd143, %rd138;
	mov.b64	{%r70, %r71}, %rd144;
	// inline asm
	shf.l.wrap.b32 %r65, %r71, %r70, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r69, %r70, %r71, %r1536;
	// inline asm
	mov.b64	%rd145, {%r65, %r69};
	add.s64 	%rd146, %rd89, %rd11;
	add.s64 	%rd147, %rd146, %rd98;
	xor.b64  	%rd148, %rd114, %rd147;
	mov.b64	{%r1635, %r1636}, %rd148;
	mov.b64	%rd149, {%r1636, %r1635};
	add.s64 	%rd150, %rd129, %rd149;
	xor.b64  	%rd151, %rd150, %rd89;
	mov.b64	{%r1637, %r1638}, %rd151;
	prmt.b32 	%r1639, %r1637, %r1638, %r1546;
	prmt.b32 	%r1640, %r1637, %r1638, %r1545;
	mov.b64	%rd152, {%r1640, %r1639};
	add.s64 	%rd153, %rd147, %rd15;
	add.s64 	%rd154, %rd153, %rd152;
	xor.b64  	%rd155, %rd154, %rd149;
	mov.b64	{%r1641, %r1642}, %rd155;
	prmt.b32 	%r1643, %r1641, %r1642, %r1552;
	prmt.b32 	%r1644, %r1641, %r1642, %r1551;
	mov.b64	%rd156, {%r1644, %r1643};
	add.s64 	%rd157, %rd156, %rd150;
	xor.b64  	%rd158, %rd157, %rd152;
	mov.b64	{%r78, %r79}, %rd158;
	// inline asm
	shf.l.wrap.b32 %r73, %r79, %r78, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r77, %r78, %r79, %r1536;
	// inline asm
	mov.b64	%rd159, {%r73, %r77};
	add.s64 	%rd160, %rd103, %rd16;
	add.s64 	%rd161, %rd160, %rd112;
	xor.b64  	%rd162, %rd128, %rd161;
	mov.b64	{%r1645, %r1646}, %rd162;
	mov.b64	%rd163, {%r1646, %r1645};
	add.s64 	%rd164, %rd163, %rd87;
	xor.b64  	%rd165, %rd164, %rd103;
	mov.b64	{%r1647, %r1648}, %rd165;
	prmt.b32 	%r1649, %r1647, %r1648, %r1546;
	prmt.b32 	%r1650, %r1647, %r1648, %r1545;
	mov.b64	%rd166, {%r1650, %r1649};
	add.s64 	%rd167, %rd161, %rd22;
	add.s64 	%rd168, %rd167, %rd166;
	xor.b64  	%rd169, %rd168, %rd163;
	mov.b64	{%r1651, %r1652}, %rd169;
	prmt.b32 	%r1653, %r1651, %r1652, %r1552;
	prmt.b32 	%r1654, %r1651, %r1652, %r1551;
	mov.b64	%rd170, {%r1654, %r1653};
	add.s64 	%rd171, %rd170, %rd164;
	xor.b64  	%rd172, %rd171, %rd166;
	mov.b64	{%r86, %r87}, %rd172;
	// inline asm
	shf.l.wrap.b32 %r81, %r87, %r86, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r85, %r86, %r87, %r1536;
	// inline asm
	mov.b64	%rd173, {%r81, %r85};
	add.s64 	%rd174, %rd117, %rd20;
	add.s64 	%rd175, %rd174, %rd126;
	xor.b64  	%rd176, %rd175, %rd86;
	mov.b64	{%r1655, %r1656}, %rd176;
	mov.b64	%rd177, {%r1656, %r1655};
	add.s64 	%rd178, %rd177, %rd101;
	xor.b64  	%rd179, %rd178, %rd117;
	mov.b64	{%r1657, %r1658}, %rd179;
	prmt.b32 	%r1659, %r1657, %r1658, %r1546;
	prmt.b32 	%r1660, %r1657, %r1658, %r1545;
	mov.b64	%rd180, {%r1660, %r1659};
	add.s64 	%rd181, %rd175, %rd13;
	add.s64 	%rd182, %rd181, %rd180;
	xor.b64  	%rd183, %rd182, %rd177;
	mov.b64	{%r1661, %r1662}, %rd183;
	prmt.b32 	%r1663, %r1661, %r1662, %r1552;
	prmt.b32 	%r1664, %r1661, %r1662, %r1551;
	mov.b64	%rd184, {%r1664, %r1663};
	add.s64 	%rd185, %rd184, %rd178;
	xor.b64  	%rd186, %rd185, %rd180;
	mov.b64	{%r94, %r95}, %rd186;
	// inline asm
	shf.l.wrap.b32 %r89, %r95, %r94, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r93, %r94, %r95, %r1536;
	// inline asm
	mov.b64	%rd187, {%r89, %r93};
	add.s64 	%rd188, %rd140, %rd29;
	add.s64 	%rd189, %rd188, %rd159;
	xor.b64  	%rd190, %rd184, %rd189;
	mov.b64	{%r1665, %r1666}, %rd190;
	mov.b64	%rd191, {%r1666, %r1665};
	add.s64 	%rd192, %rd191, %rd171;
	xor.b64  	%rd193, %rd192, %rd159;
	mov.b64	{%r1667, %r1668}, %rd193;
	prmt.b32 	%r1669, %r1667, %r1668, %r1546;
	prmt.b32 	%r1670, %r1667, %r1668, %r1545;
	mov.b64	%rd194, {%r1670, %r1669};
	add.s64 	%rd195, %rd189, %rd19;
	add.s64 	%rd196, %rd195, %rd194;
	xor.b64  	%rd197, %rd191, %rd196;
	mov.b64	{%r1671, %r1672}, %rd197;
	prmt.b32 	%r1673, %r1671, %r1672, %r1552;
	prmt.b32 	%r1674, %r1671, %r1672, %r1551;
	mov.b64	%rd198, {%r1674, %r1673};
	add.s64 	%rd199, %rd198, %rd192;
	xor.b64  	%rd200, %rd199, %rd194;
	mov.b64	{%r102, %r103}, %rd200;
	// inline asm
	shf.l.wrap.b32 %r97, %r103, %r102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r101, %r102, %r103, %r1536;
	// inline asm
	mov.b64	%rd201, {%r97, %r101};
	add.s64 	%rd202, %rd154, %rd1;
	add.s64 	%rd203, %rd202, %rd173;
	xor.b64  	%rd204, %rd203, %rd142;
	mov.b64	{%r1675, %r1676}, %rd204;
	mov.b64	%rd205, {%r1676, %r1675};
	add.s64 	%rd206, %rd205, %rd185;
	xor.b64  	%rd207, %rd206, %rd173;
	mov.b64	{%r1677, %r1678}, %rd207;
	prmt.b32 	%r1679, %r1677, %r1678, %r1546;
	prmt.b32 	%r1680, %r1677, %r1678, %r1545;
	mov.b64	%rd208, {%r1680, %r1679};
	add.s64 	%rd209, %rd203, %rd9;
	add.s64 	%rd210, %rd209, %rd208;
	xor.b64  	%rd211, %rd210, %rd205;
	mov.b64	{%r1681, %r1682}, %rd211;
	prmt.b32 	%r1683, %r1681, %r1682, %r1552;
	prmt.b32 	%r1684, %r1681, %r1682, %r1551;
	mov.b64	%rd212, {%r1684, %r1683};
	add.s64 	%rd213, %rd212, %rd206;
	xor.b64  	%rd214, %rd213, %rd208;
	mov.b64	{%r110, %r111}, %rd214;
	// inline asm
	shf.l.wrap.b32 %r105, %r111, %r110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r109, %r110, %r111, %r1536;
	// inline asm
	mov.b64	%rd215, {%r105, %r109};
	add.s64 	%rd216, %rd168, %rd18;
	add.s64 	%rd217, %rd216, %rd187;
	xor.b64  	%rd218, %rd217, %rd156;
	mov.b64	{%r1685, %r1686}, %rd218;
	mov.b64	%rd219, {%r1686, %r1685};
	add.s64 	%rd220, %rd219, %rd143;
	xor.b64  	%rd221, %rd220, %rd187;
	mov.b64	{%r1687, %r1688}, %rd221;
	prmt.b32 	%r1689, %r1687, %r1688, %r1546;
	prmt.b32 	%r1690, %r1687, %r1688, %r1545;
	mov.b64	%rd222, {%r1690, %r1689};
	add.s64 	%rd223, %rd217, %rd14;
	add.s64 	%rd224, %rd223, %rd222;
	xor.b64  	%rd225, %rd224, %rd219;
	mov.b64	{%r1691, %r1692}, %rd225;
	prmt.b32 	%r1693, %r1691, %r1692, %r1552;
	prmt.b32 	%r1694, %r1691, %r1692, %r1551;
	mov.b64	%rd226, {%r1694, %r1693};
	add.s64 	%rd227, %rd226, %rd220;
	xor.b64  	%rd228, %rd227, %rd222;
	mov.b64	{%r118, %r119}, %rd228;
	// inline asm
	shf.l.wrap.b32 %r113, %r119, %r118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r117, %r118, %r119, %r1536;
	// inline asm
	mov.b64	%rd229, {%r113, %r117};
	add.s64 	%rd230, %rd145, %rd12;
	add.s64 	%rd231, %rd230, %rd182;
	xor.b64  	%rd232, %rd231, %rd170;
	mov.b64	{%r1695, %r1696}, %rd232;
	mov.b64	%rd233, {%r1696, %r1695};
	add.s64 	%rd234, %rd233, %rd157;
	xor.b64  	%rd235, %rd234, %rd145;
	mov.b64	{%r1697, %r1698}, %rd235;
	prmt.b32 	%r1699, %r1697, %r1698, %r1546;
	prmt.b32 	%r1700, %r1697, %r1698, %r1545;
	mov.b64	%rd236, {%r1700, %r1699};
	add.s64 	%rd237, %rd231, %rd10;
	add.s64 	%rd238, %rd237, %rd236;
	xor.b64  	%rd239, %rd238, %rd233;
	mov.b64	{%r1701, %r1702}, %rd239;
	prmt.b32 	%r1703, %r1701, %r1702, %r1552;
	prmt.b32 	%r1704, %r1701, %r1702, %r1551;
	mov.b64	%rd240, {%r1704, %r1703};
	add.s64 	%rd241, %rd240, %rd234;
	xor.b64  	%rd242, %rd241, %rd236;
	mov.b64	{%r126, %r127}, %rd242;
	// inline asm
	shf.l.wrap.b32 %r121, %r127, %r126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r125, %r126, %r127, %r1536;
	// inline asm
	mov.b64	%rd243, {%r121, %r125};
	add.s64 	%rd244, %rd196, %rd18;
	add.s64 	%rd245, %rd244, %rd243;
	xor.b64  	%rd246, %rd245, %rd212;
	mov.b64	{%r1705, %r1706}, %rd246;
	mov.b64	%rd247, {%r1706, %r1705};
	add.s64 	%rd248, %rd247, %rd227;
	xor.b64  	%rd249, %rd248, %rd243;
	mov.b64	{%r1707, %r1708}, %rd249;
	prmt.b32 	%r1709, %r1707, %r1708, %r1546;
	prmt.b32 	%r1710, %r1707, %r1708, %r1545;
	mov.b64	%rd250, {%r1710, %r1709};
	add.s64 	%rd251, %rd245, %rd15;
	add.s64 	%rd252, %rd251, %rd250;
	xor.b64  	%rd253, %rd247, %rd252;
	mov.b64	{%r1711, %r1712}, %rd253;
	prmt.b32 	%r1713, %r1711, %r1712, %r1552;
	prmt.b32 	%r1714, %r1711, %r1712, %r1551;
	mov.b64	%rd254, {%r1714, %r1713};
	add.s64 	%rd255, %rd248, %rd254;
	xor.b64  	%rd256, %rd255, %rd250;
	mov.b64	{%r134, %r135}, %rd256;
	// inline asm
	shf.l.wrap.b32 %r129, %r135, %r134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r133, %r134, %r135, %r1536;
	// inline asm
	mov.b64	%rd257, {%r129, %r133};
	add.s64 	%rd258, %rd201, %rd19;
	add.s64 	%rd259, %rd258, %rd210;
	xor.b64  	%rd260, %rd226, %rd259;
	mov.b64	{%r1715, %r1716}, %rd260;
	mov.b64	%rd261, {%r1716, %r1715};
	add.s64 	%rd262, %rd241, %rd261;
	xor.b64  	%rd263, %rd262, %rd201;
	mov.b64	{%r1717, %r1718}, %rd263;
	prmt.b32 	%r1719, %r1717, %r1718, %r1546;
	prmt.b32 	%r1720, %r1717, %r1718, %r1545;
	mov.b64	%rd264, {%r1720, %r1719};
	add.s64 	%rd265, %rd259, %rd1;
	add.s64 	%rd266, %rd265, %rd264;
	xor.b64  	%rd267, %rd266, %rd261;
	mov.b64	{%r1721, %r1722}, %rd267;
	prmt.b32 	%r1723, %r1721, %r1722, %r1552;
	prmt.b32 	%r1724, %r1721, %r1722, %r1551;
	mov.b64	%rd268, {%r1724, %r1723};
	add.s64 	%rd269, %rd268, %rd262;
	xor.b64  	%rd270, %rd269, %rd264;
	mov.b64	{%r142, %r143}, %rd270;
	// inline asm
	shf.l.wrap.b32 %r137, %r143, %r142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r141, %r142, %r143, %r1536;
	// inline asm
	mov.b64	%rd271, {%r137, %r141};
	add.s64 	%rd272, %rd215, %rd12;
	add.s64 	%rd273, %rd272, %rd224;
	xor.b64  	%rd274, %rd240, %rd273;
	mov.b64	{%r1725, %r1726}, %rd274;
	mov.b64	%rd275, {%r1726, %r1725};
	add.s64 	%rd276, %rd275, %rd199;
	xor.b64  	%rd277, %rd276, %rd215;
	mov.b64	{%r1727, %r1728}, %rd277;
	prmt.b32 	%r1729, %r1727, %r1728, %r1546;
	prmt.b32 	%r1730, %r1727, %r1728, %r1545;
	mov.b64	%rd278, {%r1730, %r1729};
	add.s64 	%rd279, %rd273, %rd9;
	add.s64 	%rd280, %rd279, %rd278;
	xor.b64  	%rd281, %rd280, %rd275;
	mov.b64	{%r1731, %r1732}, %rd281;
	prmt.b32 	%r1733, %r1731, %r1732, %r1552;
	prmt.b32 	%r1734, %r1731, %r1732, %r1551;
	mov.b64	%rd282, {%r1734, %r1733};
	add.s64 	%rd283, %rd282, %rd276;
	xor.b64  	%rd284, %rd283, %rd278;
	mov.b64	{%r150, %r151}, %rd284;
	// inline asm
	shf.l.wrap.b32 %r145, %r151, %r150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r149, %r150, %r151, %r1536;
	// inline asm
	mov.b64	%rd285, {%r145, %r149};
	add.s64 	%rd286, %rd229, %rd22;
	add.s64 	%rd287, %rd286, %rd238;
	xor.b64  	%rd288, %rd287, %rd198;
	mov.b64	{%r1735, %r1736}, %rd288;
	mov.b64	%rd289, {%r1736, %r1735};
	add.s64 	%rd290, %rd289, %rd213;
	xor.b64  	%rd291, %rd290, %rd229;
	mov.b64	{%r1737, %r1738}, %rd291;
	prmt.b32 	%r1739, %r1737, %r1738, %r1546;
	prmt.b32 	%r1740, %r1737, %r1738, %r1545;
	mov.b64	%rd292, {%r1740, %r1739};
	add.s64 	%rd293, %rd287, %rd20;
	add.s64 	%rd294, %rd293, %rd292;
	xor.b64  	%rd295, %rd294, %rd289;
	mov.b64	{%r1741, %r1742}, %rd295;
	prmt.b32 	%r1743, %r1741, %r1742, %r1552;
	prmt.b32 	%r1744, %r1741, %r1742, %r1551;
	mov.b64	%rd296, {%r1744, %r1743};
	add.s64 	%rd297, %rd296, %rd290;
	xor.b64  	%rd298, %rd297, %rd292;
	mov.b64	{%r158, %r159}, %rd298;
	// inline asm
	shf.l.wrap.b32 %r153, %r159, %r158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r157, %r158, %r159, %r1536;
	// inline asm
	mov.b64	%rd299, {%r153, %r157};
	add.s64 	%rd300, %rd252, %rd17;
	add.s64 	%rd301, %rd300, %rd271;
	xor.b64  	%rd302, %rd296, %rd301;
	mov.b64	{%r1745, %r1746}, %rd302;
	mov.b64	%rd303, {%r1746, %r1745};
	add.s64 	%rd304, %rd303, %rd283;
	xor.b64  	%rd305, %rd304, %rd271;
	mov.b64	{%r1747, %r1748}, %rd305;
	prmt.b32 	%r1749, %r1747, %r1748, %r1546;
	prmt.b32 	%r1750, %r1747, %r1748, %r1545;
	mov.b64	%rd306, {%r1750, %r1749};
	add.s64 	%rd307, %rd301, %rd21;
	add.s64 	%rd308, %rd307, %rd306;
	xor.b64  	%rd309, %rd303, %rd308;
	mov.b64	{%r1751, %r1752}, %rd309;
	prmt.b32 	%r1753, %r1751, %r1752, %r1552;
	prmt.b32 	%r1754, %r1751, %r1752, %r1551;
	mov.b64	%rd310, {%r1754, %r1753};
	add.s64 	%rd311, %rd310, %rd304;
	xor.b64  	%rd312, %rd311, %rd306;
	mov.b64	{%r166, %r167}, %rd312;
	// inline asm
	shf.l.wrap.b32 %r161, %r167, %r166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r165, %r166, %r167, %r1536;
	// inline asm
	mov.b64	%rd313, {%r161, %r165};
	add.s64 	%rd314, %rd266, %rd10;
	add.s64 	%rd315, %rd314, %rd285;
	xor.b64  	%rd316, %rd315, %rd254;
	mov.b64	{%r1755, %r1756}, %rd316;
	mov.b64	%rd317, {%r1756, %r1755};
	add.s64 	%rd318, %rd317, %rd297;
	xor.b64  	%rd319, %rd318, %rd285;
	mov.b64	{%r1757, %r1758}, %rd319;
	prmt.b32 	%r1759, %r1757, %r1758, %r1546;
	prmt.b32 	%r1760, %r1757, %r1758, %r1545;
	mov.b64	%rd320, {%r1760, %r1759};
	add.s64 	%rd321, %rd315, %rd13;
	add.s64 	%rd322, %rd321, %rd320;
	xor.b64  	%rd323, %rd322, %rd317;
	mov.b64	{%r1761, %r1762}, %rd323;
	prmt.b32 	%r1763, %r1761, %r1762, %r1552;
	prmt.b32 	%r1764, %r1761, %r1762, %r1551;
	mov.b64	%rd324, {%r1764, %r1763};
	add.s64 	%rd325, %rd324, %rd318;
	xor.b64  	%rd326, %rd325, %rd320;
	mov.b64	{%r174, %r175}, %rd326;
	// inline asm
	shf.l.wrap.b32 %r169, %r175, %r174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r173, %r174, %r175, %r1536;
	// inline asm
	mov.b64	%rd327, {%r169, %r173};
	add.s64 	%rd328, %rd280, %rd14;
	add.s64 	%rd329, %rd328, %rd299;
	xor.b64  	%rd330, %rd329, %rd268;
	mov.b64	{%r1765, %r1766}, %rd330;
	mov.b64	%rd331, {%r1766, %r1765};
	add.s64 	%rd332, %rd331, %rd255;
	xor.b64  	%rd333, %rd332, %rd299;
	mov.b64	{%r1767, %r1768}, %rd333;
	prmt.b32 	%r1769, %r1767, %r1768, %r1546;
	prmt.b32 	%r1770, %r1767, %r1768, %r1545;
	mov.b64	%rd334, {%r1770, %r1769};
	add.s64 	%rd335, %rd329, %rd29;
	add.s64 	%rd336, %rd335, %rd334;
	xor.b64  	%rd337, %rd336, %rd331;
	mov.b64	{%r1771, %r1772}, %rd337;
	prmt.b32 	%r1773, %r1771, %r1772, %r1552;
	prmt.b32 	%r1774, %r1771, %r1772, %r1551;
	mov.b64	%rd338, {%r1774, %r1773};
	add.s64 	%rd339, %rd338, %rd332;
	xor.b64  	%rd340, %rd339, %rd334;
	mov.b64	{%r182, %r183}, %rd340;
	// inline asm
	shf.l.wrap.b32 %r177, %r183, %r182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r181, %r182, %r183, %r1536;
	// inline asm
	mov.b64	%rd341, {%r177, %r181};
	add.s64 	%rd342, %rd257, %rd16;
	add.s64 	%rd343, %rd342, %rd294;
	xor.b64  	%rd344, %rd343, %rd282;
	mov.b64	{%r1775, %r1776}, %rd344;
	mov.b64	%rd345, {%r1776, %r1775};
	add.s64 	%rd346, %rd345, %rd269;
	xor.b64  	%rd347, %rd346, %rd257;
	mov.b64	{%r1777, %r1778}, %rd347;
	prmt.b32 	%r1779, %r1777, %r1778, %r1546;
	prmt.b32 	%r1780, %r1777, %r1778, %r1545;
	mov.b64	%rd348, {%r1780, %r1779};
	add.s64 	%rd349, %rd343, %rd11;
	add.s64 	%rd350, %rd349, %rd348;
	xor.b64  	%rd351, %rd350, %rd345;
	mov.b64	{%r1781, %r1782}, %rd351;
	prmt.b32 	%r1783, %r1781, %r1782, %r1552;
	prmt.b32 	%r1784, %r1781, %r1782, %r1551;
	mov.b64	%rd352, {%r1784, %r1783};
	add.s64 	%rd353, %rd352, %rd346;
	xor.b64  	%rd354, %rd353, %rd348;
	mov.b64	{%r190, %r191}, %rd354;
	// inline asm
	shf.l.wrap.b32 %r185, %r191, %r190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r189, %r190, %r191, %r1536;
	// inline asm
	mov.b64	%rd355, {%r185, %r189};
	add.s64 	%rd356, %rd308, %rd14;
	add.s64 	%rd357, %rd356, %rd355;
	xor.b64  	%rd358, %rd357, %rd324;
	mov.b64	{%r1785, %r1786}, %rd358;
	mov.b64	%rd359, {%r1786, %r1785};
	add.s64 	%rd360, %rd359, %rd339;
	xor.b64  	%rd361, %rd360, %rd355;
	mov.b64	{%r1787, %r1788}, %rd361;
	prmt.b32 	%r1789, %r1787, %r1788, %r1546;
	prmt.b32 	%r1790, %r1787, %r1788, %r1545;
	mov.b64	%rd362, {%r1790, %r1789};
	add.s64 	%rd363, %rd357, %rd16;
	add.s64 	%rd364, %rd363, %rd362;
	xor.b64  	%rd365, %rd359, %rd364;
	mov.b64	{%r1791, %r1792}, %rd365;
	prmt.b32 	%r1793, %r1791, %r1792, %r1552;
	prmt.b32 	%r1794, %r1791, %r1792, %r1551;
	mov.b64	%rd366, {%r1794, %r1793};
	add.s64 	%rd367, %rd360, %rd366;
	xor.b64  	%rd368, %rd367, %rd362;
	mov.b64	{%r198, %r199}, %rd368;
	// inline asm
	shf.l.wrap.b32 %r193, %r199, %r198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r197, %r198, %r199, %r1536;
	// inline asm
	mov.b64	%rd369, {%r193, %r197};
	add.s64 	%rd370, %rd313, %rd10;
	add.s64 	%rd371, %rd370, %rd322;
	xor.b64  	%rd372, %rd338, %rd371;
	mov.b64	{%r1795, %r1796}, %rd372;
	mov.b64	%rd373, {%r1796, %r1795};
	add.s64 	%rd374, %rd353, %rd373;
	xor.b64  	%rd375, %rd374, %rd313;
	mov.b64	{%r1797, %r1798}, %rd375;
	prmt.b32 	%r1799, %r1797, %r1798, %r1546;
	prmt.b32 	%r1800, %r1797, %r1798, %r1545;
	mov.b64	%rd376, {%r1800, %r1799};
	add.s64 	%rd377, %rd371, %rd29;
	add.s64 	%rd378, %rd377, %rd376;
	xor.b64  	%rd379, %rd378, %rd373;
	mov.b64	{%r1801, %r1802}, %rd379;
	prmt.b32 	%r1803, %r1801, %r1802, %r1552;
	prmt.b32 	%r1804, %r1801, %r1802, %r1551;
	mov.b64	%rd380, {%r1804, %r1803};
	add.s64 	%rd381, %rd380, %rd374;
	xor.b64  	%rd382, %rd381, %rd376;
	mov.b64	{%r206, %r207}, %rd382;
	// inline asm
	shf.l.wrap.b32 %r201, %r207, %r206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r205, %r206, %r207, %r1536;
	// inline asm
	mov.b64	%rd383, {%r201, %r205};
	add.s64 	%rd384, %rd327, %rd20;
	add.s64 	%rd385, %rd384, %rd336;
	xor.b64  	%rd386, %rd352, %rd385;
	mov.b64	{%r1805, %r1806}, %rd386;
	mov.b64	%rd387, {%r1806, %r1805};
	add.s64 	%rd388, %rd387, %rd311;
	xor.b64  	%rd389, %rd388, %rd327;
	mov.b64	{%r1807, %r1808}, %rd389;
	prmt.b32 	%r1809, %r1807, %r1808, %r1546;
	prmt.b32 	%r1810, %r1807, %r1808, %r1545;
	mov.b64	%rd390, {%r1810, %r1809};
	add.s64 	%rd391, %rd385, %rd19;
	add.s64 	%rd392, %rd391, %rd390;
	xor.b64  	%rd393, %rd392, %rd387;
	mov.b64	{%r1811, %r1812}, %rd393;
	prmt.b32 	%r1813, %r1811, %r1812, %r1552;
	prmt.b32 	%r1814, %r1811, %r1812, %r1551;
	mov.b64	%rd394, {%r1814, %r1813};
	add.s64 	%rd395, %rd394, %rd388;
	xor.b64  	%rd396, %rd395, %rd390;
	mov.b64	{%r214, %r215}, %rd396;
	// inline asm
	shf.l.wrap.b32 %r209, %r215, %r214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r213, %r214, %r215, %r1536;
	// inline asm
	mov.b64	%rd397, {%r209, %r213};
	add.s64 	%rd398, %rd341, %rd18;
	add.s64 	%rd399, %rd398, %rd350;
	xor.b64  	%rd400, %rd399, %rd310;
	mov.b64	{%r1815, %r1816}, %rd400;
	mov.b64	%rd401, {%r1816, %r1815};
	add.s64 	%rd402, %rd401, %rd325;
	xor.b64  	%rd403, %rd402, %rd341;
	mov.b64	{%r1817, %r1818}, %rd403;
	prmt.b32 	%r1819, %r1817, %r1818, %r1546;
	prmt.b32 	%r1820, %r1817, %r1818, %r1545;
	mov.b64	%rd404, {%r1820, %r1819};
	add.s64 	%rd405, %rd399, %rd21;
	add.s64 	%rd406, %rd405, %rd404;
	xor.b64  	%rd407, %rd406, %rd401;
	mov.b64	{%r1821, %r1822}, %rd407;
	prmt.b32 	%r1823, %r1821, %r1822, %r1552;
	prmt.b32 	%r1824, %r1821, %r1822, %r1551;
	mov.b64	%rd408, {%r1824, %r1823};
	add.s64 	%rd409, %rd408, %rd402;
	xor.b64  	%rd410, %rd409, %rd404;
	mov.b64	{%r222, %r223}, %rd410;
	// inline asm
	shf.l.wrap.b32 %r217, %r223, %r222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r221, %r222, %r223, %r1536;
	// inline asm
	mov.b64	%rd411, {%r217, %r221};
	add.s64 	%rd412, %rd364, %rd9;
	add.s64 	%rd413, %rd412, %rd383;
	xor.b64  	%rd414, %rd408, %rd413;
	mov.b64	{%r1825, %r1826}, %rd414;
	mov.b64	%rd415, {%r1826, %r1825};
	add.s64 	%rd416, %rd415, %rd395;
	xor.b64  	%rd417, %rd416, %rd383;
	mov.b64	{%r1827, %r1828}, %rd417;
	prmt.b32 	%r1829, %r1827, %r1828, %r1546;
	prmt.b32 	%r1830, %r1827, %r1828, %r1545;
	mov.b64	%rd418, {%r1830, %r1829};
	add.s64 	%rd419, %rd413, %rd13;
	add.s64 	%rd420, %rd419, %rd418;
	xor.b64  	%rd421, %rd415, %rd420;
	mov.b64	{%r1831, %r1832}, %rd421;
	prmt.b32 	%r1833, %r1831, %r1832, %r1552;
	prmt.b32 	%r1834, %r1831, %r1832, %r1551;
	mov.b64	%rd422, {%r1834, %r1833};
	add.s64 	%rd423, %rd422, %rd416;
	xor.b64  	%rd424, %rd423, %rd418;
	mov.b64	{%r230, %r231}, %rd424;
	// inline asm
	shf.l.wrap.b32 %r225, %r231, %r230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r229, %r230, %r231, %r1536;
	// inline asm
	mov.b64	%rd425, {%r225, %r229};
	add.s64 	%rd426, %rd378, %rd12;
	add.s64 	%rd427, %rd426, %rd397;
	xor.b64  	%rd428, %rd427, %rd366;
	mov.b64	{%r1835, %r1836}, %rd428;
	mov.b64	%rd429, {%r1836, %r1835};
	add.s64 	%rd430, %rd429, %rd409;
	xor.b64  	%rd431, %rd430, %rd397;
	mov.b64	{%r1837, %r1838}, %rd431;
	prmt.b32 	%r1839, %r1837, %r1838, %r1546;
	prmt.b32 	%r1840, %r1837, %r1838, %r1545;
	mov.b64	%rd432, {%r1840, %r1839};
	add.s64 	%rd433, %rd427, %rd17;
	add.s64 	%rd434, %rd433, %rd432;
	xor.b64  	%rd435, %rd434, %rd429;
	mov.b64	{%r1841, %r1842}, %rd435;
	prmt.b32 	%r1843, %r1841, %r1842, %r1552;
	prmt.b32 	%r1844, %r1841, %r1842, %r1551;
	mov.b64	%rd436, {%r1844, %r1843};
	add.s64 	%rd437, %rd436, %rd430;
	xor.b64  	%rd438, %rd437, %rd432;
	mov.b64	{%r238, %r239}, %rd438;
	// inline asm
	shf.l.wrap.b32 %r233, %r239, %r238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r237, %r238, %r239, %r1536;
	// inline asm
	mov.b64	%rd439, {%r233, %r237};
	add.s64 	%rd440, %rd392, %rd11;
	add.s64 	%rd441, %rd440, %rd411;
	xor.b64  	%rd442, %rd441, %rd380;
	mov.b64	{%r1845, %r1846}, %rd442;
	mov.b64	%rd443, {%r1846, %r1845};
	add.s64 	%rd444, %rd443, %rd367;
	xor.b64  	%rd445, %rd444, %rd411;
	mov.b64	{%r1847, %r1848}, %rd445;
	prmt.b32 	%r1849, %r1847, %r1848, %r1546;
	prmt.b32 	%r1850, %r1847, %r1848, %r1545;
	mov.b64	%rd446, {%r1850, %r1849};
	add.s64 	%rd447, %rd441, %rd1;
	add.s64 	%rd448, %rd447, %rd446;
	xor.b64  	%rd449, %rd448, %rd443;
	mov.b64	{%r1851, %r1852}, %rd449;
	prmt.b32 	%r1853, %r1851, %r1852, %r1552;
	prmt.b32 	%r1854, %r1851, %r1852, %r1551;
	mov.b64	%rd450, {%r1854, %r1853};
	add.s64 	%rd451, %rd450, %rd444;
	xor.b64  	%rd452, %rd451, %rd446;
	mov.b64	{%r246, %r247}, %rd452;
	// inline asm
	shf.l.wrap.b32 %r241, %r247, %r246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r245, %r246, %r247, %r1536;
	// inline asm
	mov.b64	%rd453, {%r241, %r245};
	add.s64 	%rd454, %rd369, %rd22;
	add.s64 	%rd455, %rd454, %rd406;
	xor.b64  	%rd456, %rd455, %rd394;
	mov.b64	{%r1855, %r1856}, %rd456;
	mov.b64	%rd457, {%r1856, %r1855};
	add.s64 	%rd458, %rd457, %rd381;
	xor.b64  	%rd459, %rd458, %rd369;
	mov.b64	{%r1857, %r1858}, %rd459;
	prmt.b32 	%r1859, %r1857, %r1858, %r1546;
	prmt.b32 	%r1860, %r1857, %r1858, %r1545;
	mov.b64	%rd460, {%r1860, %r1859};
	add.s64 	%rd461, %rd455, %rd15;
	add.s64 	%rd462, %rd461, %rd460;
	xor.b64  	%rd463, %rd462, %rd457;
	mov.b64	{%r1861, %r1862}, %rd463;
	prmt.b32 	%r1863, %r1861, %r1862, %r1552;
	prmt.b32 	%r1864, %r1861, %r1862, %r1551;
	mov.b64	%rd464, {%r1864, %r1863};
	add.s64 	%rd465, %rd464, %rd458;
	xor.b64  	%rd466, %rd465, %rd460;
	mov.b64	{%r254, %r255}, %rd466;
	// inline asm
	shf.l.wrap.b32 %r249, %r255, %r254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r253, %r254, %r255, %r1536;
	// inline asm
	mov.b64	%rd467, {%r249, %r253};
	add.s64 	%rd468, %rd420, %rd16;
	add.s64 	%rd469, %rd468, %rd467;
	xor.b64  	%rd470, %rd469, %rd436;
	mov.b64	{%r1865, %r1866}, %rd470;
	mov.b64	%rd471, {%r1866, %r1865};
	add.s64 	%rd472, %rd471, %rd451;
	xor.b64  	%rd473, %rd472, %rd467;
	mov.b64	{%r1867, %r1868}, %rd473;
	prmt.b32 	%r1869, %r1867, %r1868, %r1546;
	prmt.b32 	%r1870, %r1867, %r1868, %r1545;
	mov.b64	%rd474, {%r1870, %r1869};
	add.s64 	%rd475, %rd469, %rd1;
	add.s64 	%rd476, %rd475, %rd474;
	xor.b64  	%rd477, %rd471, %rd476;
	mov.b64	{%r1871, %r1872}, %rd477;
	prmt.b32 	%r1873, %r1871, %r1872, %r1552;
	prmt.b32 	%r1874, %r1871, %r1872, %r1551;
	mov.b64	%rd478, {%r1874, %r1873};
	add.s64 	%rd479, %rd472, %rd478;
	xor.b64  	%rd480, %rd479, %rd474;
	mov.b64	{%r262, %r263}, %rd480;
	// inline asm
	shf.l.wrap.b32 %r257, %r263, %r262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r261, %r262, %r263, %r1536;
	// inline asm
	mov.b64	%rd481, {%r257, %r261};
	add.s64 	%rd482, %rd425, %rd12;
	add.s64 	%rd483, %rd482, %rd434;
	xor.b64  	%rd484, %rd450, %rd483;
	mov.b64	{%r1875, %r1876}, %rd484;
	mov.b64	%rd485, {%r1876, %r1875};
	add.s64 	%rd486, %rd465, %rd485;
	xor.b64  	%rd487, %rd486, %rd425;
	mov.b64	{%r1877, %r1878}, %rd487;
	prmt.b32 	%r1879, %r1877, %r1878, %r1546;
	prmt.b32 	%r1880, %r1877, %r1878, %r1545;
	mov.b64	%rd488, {%r1880, %r1879};
	add.s64 	%rd489, %rd483, %rd14;
	add.s64 	%rd490, %rd489, %rd488;
	xor.b64  	%rd491, %rd490, %rd485;
	mov.b64	{%r1881, %r1882}, %rd491;
	prmt.b32 	%r1883, %r1881, %r1882, %r1552;
	prmt.b32 	%r1884, %r1881, %r1882, %r1551;
	mov.b64	%rd492, {%r1884, %r1883};
	add.s64 	%rd493, %rd492, %rd486;
	xor.b64  	%rd494, %rd493, %rd488;
	mov.b64	{%r270, %r271}, %rd494;
	// inline asm
	shf.l.wrap.b32 %r265, %r271, %r270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r269, %r270, %r271, %r1536;
	// inline asm
	mov.b64	%rd495, {%r265, %r269};
	add.s64 	%rd496, %rd439, %rd9;
	add.s64 	%rd497, %rd496, %rd448;
	xor.b64  	%rd498, %rd464, %rd497;
	mov.b64	{%r1885, %r1886}, %rd498;
	mov.b64	%rd499, {%r1886, %r1885};
	add.s64 	%rd500, %rd499, %rd423;
	xor.b64  	%rd501, %rd500, %rd439;
	mov.b64	{%r1887, %r1888}, %rd501;
	prmt.b32 	%r1889, %r1887, %r1888, %r1546;
	prmt.b32 	%r1890, %r1887, %r1888, %r1545;
	mov.b64	%rd502, {%r1890, %r1889};
	add.s64 	%rd503, %rd497, %rd11;
	add.s64 	%rd504, %rd503, %rd502;
	xor.b64  	%rd505, %rd504, %rd499;
	mov.b64	{%r1891, %r1892}, %rd505;
	prmt.b32 	%r1893, %r1891, %r1892, %r1552;
	prmt.b32 	%r1894, %r1891, %r1892, %r1551;
	mov.b64	%rd506, {%r1894, %r1893};
	add.s64 	%rd507, %rd506, %rd500;
	xor.b64  	%rd508, %rd507, %rd502;
	mov.b64	{%r278, %r279}, %rd508;
	// inline asm
	shf.l.wrap.b32 %r273, %r279, %r278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r277, %r278, %r279, %r1536;
	// inline asm
	mov.b64	%rd509, {%r273, %r277};
	add.s64 	%rd510, %rd453, %rd17;
	add.s64 	%rd511, %rd510, %rd462;
	xor.b64  	%rd512, %rd511, %rd422;
	mov.b64	{%r1895, %r1896}, %rd512;
	mov.b64	%rd513, {%r1896, %r1895};
	add.s64 	%rd514, %rd513, %rd437;
	xor.b64  	%rd515, %rd514, %rd453;
	mov.b64	{%r1897, %r1898}, %rd515;
	prmt.b32 	%r1899, %r1897, %r1898, %r1546;
	prmt.b32 	%r1900, %r1897, %r1898, %r1545;
	mov.b64	%rd516, {%r1900, %r1899};
	add.s64 	%rd517, %rd511, %rd22;
	add.s64 	%rd518, %rd517, %rd516;
	xor.b64  	%rd519, %rd518, %rd513;
	mov.b64	{%r1901, %r1902}, %rd519;
	prmt.b32 	%r1903, %r1901, %r1902, %r1552;
	prmt.b32 	%r1904, %r1901, %r1902, %r1551;
	mov.b64	%rd520, {%r1904, %r1903};
	add.s64 	%rd521, %rd520, %rd514;
	xor.b64  	%rd522, %rd521, %rd516;
	mov.b64	{%r286, %r287}, %rd522;
	// inline asm
	shf.l.wrap.b32 %r281, %r287, %r286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r285, %r286, %r287, %r1536;
	// inline asm
	mov.b64	%rd523, {%r281, %r285};
	add.s64 	%rd524, %rd476, %rd21;
	add.s64 	%rd525, %rd524, %rd495;
	xor.b64  	%rd526, %rd520, %rd525;
	mov.b64	{%r1905, %r1906}, %rd526;
	mov.b64	%rd527, {%r1906, %r1905};
	add.s64 	%rd528, %rd527, %rd507;
	xor.b64  	%rd529, %rd528, %rd495;
	mov.b64	{%r1907, %r1908}, %rd529;
	prmt.b32 	%r1909, %r1907, %r1908, %r1546;
	prmt.b32 	%r1910, %r1907, %r1908, %r1545;
	mov.b64	%rd530, {%r1910, %r1909};
	add.s64 	%rd531, %rd525, %rd29;
	add.s64 	%rd532, %rd531, %rd530;
	xor.b64  	%rd533, %rd527, %rd532;
	mov.b64	{%r1911, %r1912}, %rd533;
	prmt.b32 	%r1913, %r1911, %r1912, %r1552;
	prmt.b32 	%r1914, %r1911, %r1912, %r1551;
	mov.b64	%rd534, {%r1914, %r1913};
	add.s64 	%rd535, %rd534, %rd528;
	xor.b64  	%rd536, %rd535, %rd530;
	mov.b64	{%r294, %r295}, %rd536;
	// inline asm
	shf.l.wrap.b32 %r289, %r295, %r294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r293, %r294, %r295, %r1536;
	// inline asm
	mov.b64	%rd537, {%r289, %r293};
	add.s64 	%rd538, %rd490, %rd18;
	add.s64 	%rd539, %rd538, %rd509;
	xor.b64  	%rd540, %rd539, %rd478;
	mov.b64	{%r1915, %r1916}, %rd540;
	mov.b64	%rd541, {%r1916, %r1915};
	add.s64 	%rd542, %rd541, %rd521;
	xor.b64  	%rd543, %rd542, %rd509;
	mov.b64	{%r1917, %r1918}, %rd543;
	prmt.b32 	%r1919, %r1917, %r1918, %r1546;
	prmt.b32 	%r1920, %r1917, %r1918, %r1545;
	mov.b64	%rd544, {%r1920, %r1919};
	add.s64 	%rd545, %rd539, %rd19;
	add.s64 	%rd546, %rd545, %rd544;
	xor.b64  	%rd547, %rd546, %rd541;
	mov.b64	{%r1921, %r1922}, %rd547;
	prmt.b32 	%r1923, %r1921, %r1922, %r1552;
	prmt.b32 	%r1924, %r1921, %r1922, %r1551;
	mov.b64	%rd548, {%r1924, %r1923};
	add.s64 	%rd549, %rd548, %rd542;
	xor.b64  	%rd550, %rd549, %rd544;
	mov.b64	{%r302, %r303}, %rd550;
	// inline asm
	shf.l.wrap.b32 %r297, %r303, %r302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r301, %r302, %r303, %r1536;
	// inline asm
	mov.b64	%rd551, {%r297, %r301};
	add.s64 	%rd552, %rd504, %rd13;
	add.s64 	%rd553, %rd552, %rd523;
	xor.b64  	%rd554, %rd553, %rd492;
	mov.b64	{%r1925, %r1926}, %rd554;
	mov.b64	%rd555, {%r1926, %r1925};
	add.s64 	%rd556, %rd555, %rd479;
	xor.b64  	%rd557, %rd556, %rd523;
	mov.b64	{%r1927, %r1928}, %rd557;
	prmt.b32 	%r1929, %r1927, %r1928, %r1546;
	prmt.b32 	%r1930, %r1927, %r1928, %r1545;
	mov.b64	%rd558, {%r1930, %r1929};
	add.s64 	%rd559, %rd553, %rd15;
	add.s64 	%rd560, %rd559, %rd558;
	xor.b64  	%rd561, %rd560, %rd555;
	mov.b64	{%r1931, %r1932}, %rd561;
	prmt.b32 	%r1933, %r1931, %r1932, %r1552;
	prmt.b32 	%r1934, %r1931, %r1932, %r1551;
	mov.b64	%rd562, {%r1934, %r1933};
	add.s64 	%rd563, %rd562, %rd556;
	xor.b64  	%rd564, %rd563, %rd558;
	mov.b64	{%r310, %r311}, %rd564;
	// inline asm
	shf.l.wrap.b32 %r305, %r311, %r310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r309, %r310, %r311, %r1536;
	// inline asm
	mov.b64	%rd565, {%r305, %r309};
	add.s64 	%rd566, %rd481, %rd10;
	add.s64 	%rd567, %rd566, %rd518;
	xor.b64  	%rd568, %rd567, %rd506;
	mov.b64	{%r1935, %r1936}, %rd568;
	mov.b64	%rd569, {%r1936, %r1935};
	add.s64 	%rd570, %rd569, %rd493;
	xor.b64  	%rd571, %rd570, %rd481;
	mov.b64	{%r1937, %r1938}, %rd571;
	prmt.b32 	%r1939, %r1937, %r1938, %r1546;
	prmt.b32 	%r1940, %r1937, %r1938, %r1545;
	mov.b64	%rd572, {%r1940, %r1939};
	add.s64 	%rd573, %rd567, %rd20;
	add.s64 	%rd574, %rd573, %rd572;
	xor.b64  	%rd575, %rd574, %rd569;
	mov.b64	{%r1941, %r1942}, %rd575;
	prmt.b32 	%r1943, %r1941, %r1942, %r1552;
	prmt.b32 	%r1944, %r1941, %r1942, %r1551;
	mov.b64	%rd576, {%r1944, %r1943};
	add.s64 	%rd577, %rd576, %rd570;
	xor.b64  	%rd578, %rd577, %rd572;
	mov.b64	{%r318, %r319}, %rd578;
	// inline asm
	shf.l.wrap.b32 %r313, %r319, %r318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r317, %r318, %r319, %r1536;
	// inline asm
	mov.b64	%rd579, {%r313, %r317};
	add.s64 	%rd580, %rd532, %rd9;
	add.s64 	%rd581, %rd580, %rd579;
	xor.b64  	%rd582, %rd581, %rd548;
	mov.b64	{%r1945, %r1946}, %rd582;
	mov.b64	%rd583, {%r1946, %r1945};
	add.s64 	%rd584, %rd583, %rd563;
	xor.b64  	%rd585, %rd584, %rd579;
	mov.b64	{%r1947, %r1948}, %rd585;
	prmt.b32 	%r1949, %r1947, %r1948, %r1546;
	prmt.b32 	%r1950, %r1947, %r1948, %r1545;
	mov.b64	%rd586, {%r1950, %r1949};
	add.s64 	%rd587, %rd581, %rd19;
	add.s64 	%rd588, %rd587, %rd586;
	xor.b64  	%rd589, %rd583, %rd588;
	mov.b64	{%r1951, %r1952}, %rd589;
	prmt.b32 	%r1953, %r1951, %r1952, %r1552;
	prmt.b32 	%r1954, %r1951, %r1952, %r1551;
	mov.b64	%rd590, {%r1954, %r1953};
	add.s64 	%rd591, %rd584, %rd590;
	xor.b64  	%rd592, %rd591, %rd586;
	mov.b64	{%r326, %r327}, %rd592;
	// inline asm
	shf.l.wrap.b32 %r321, %r327, %r326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r325, %r326, %r327, %r1536;
	// inline asm
	mov.b64	%rd593, {%r321, %r325};
	add.s64 	%rd594, %rd537, %rd13;
	add.s64 	%rd595, %rd594, %rd546;
	xor.b64  	%rd596, %rd562, %rd595;
	mov.b64	{%r1955, %r1956}, %rd596;
	mov.b64	%rd597, {%r1956, %r1955};
	add.s64 	%rd598, %rd577, %rd597;
	xor.b64  	%rd599, %rd598, %rd537;
	mov.b64	{%r1957, %r1958}, %rd599;
	prmt.b32 	%r1959, %r1957, %r1958, %r1546;
	prmt.b32 	%r1960, %r1957, %r1958, %r1545;
	mov.b64	%rd600, {%r1960, %r1959};
	add.s64 	%rd601, %rd595, %rd17;
	add.s64 	%rd602, %rd601, %rd600;
	xor.b64  	%rd603, %rd602, %rd597;
	mov.b64	{%r1961, %r1962}, %rd603;
	prmt.b32 	%r1963, %r1961, %r1962, %r1552;
	prmt.b32 	%r1964, %r1961, %r1962, %r1551;
	mov.b64	%rd604, {%r1964, %r1963};
	add.s64 	%rd605, %rd604, %rd598;
	xor.b64  	%rd606, %rd605, %rd600;
	mov.b64	{%r334, %r335}, %rd606;
	// inline asm
	shf.l.wrap.b32 %r329, %r335, %r334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r333, %r334, %r335, %r1536;
	// inline asm
	mov.b64	%rd607, {%r329, %r333};
	add.s64 	%rd608, %rd551, %rd1;
	add.s64 	%rd609, %rd608, %rd560;
	xor.b64  	%rd610, %rd576, %rd609;
	mov.b64	{%r1965, %r1966}, %rd610;
	mov.b64	%rd611, {%r1966, %r1965};
	add.s64 	%rd612, %rd611, %rd535;
	xor.b64  	%rd613, %rd612, %rd551;
	mov.b64	{%r1967, %r1968}, %rd613;
	prmt.b32 	%r1969, %r1967, %r1968, %r1546;
	prmt.b32 	%r1970, %r1967, %r1968, %r1545;
	mov.b64	%rd614, {%r1970, %r1969};
	add.s64 	%rd615, %rd609, %rd18;
	add.s64 	%rd616, %rd615, %rd614;
	xor.b64  	%rd617, %rd616, %rd611;
	mov.b64	{%r1971, %r1972}, %rd617;
	prmt.b32 	%r1973, %r1971, %r1972, %r1552;
	prmt.b32 	%r1974, %r1971, %r1972, %r1551;
	mov.b64	%rd618, {%r1974, %r1973};
	add.s64 	%rd619, %rd618, %rd612;
	xor.b64  	%rd620, %rd619, %rd614;
	mov.b64	{%r342, %r343}, %rd620;
	// inline asm
	shf.l.wrap.b32 %r337, %r343, %r342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r341, %r342, %r343, %r1536;
	// inline asm
	mov.b64	%rd621, {%r337, %r341};
	add.s64 	%rd622, %rd565, %rd15;
	add.s64 	%rd623, %rd622, %rd574;
	xor.b64  	%rd624, %rd623, %rd534;
	mov.b64	{%r1975, %r1976}, %rd624;
	mov.b64	%rd625, {%r1976, %r1975};
	add.s64 	%rd626, %rd625, %rd549;
	xor.b64  	%rd627, %rd626, %rd565;
	mov.b64	{%r1977, %r1978}, %rd627;
	prmt.b32 	%r1979, %r1977, %r1978, %r1546;
	prmt.b32 	%r1980, %r1977, %r1978, %r1545;
	mov.b64	%rd628, {%r1980, %r1979};
	add.s64 	%rd629, %rd623, %rd10;
	add.s64 	%rd630, %rd629, %rd628;
	xor.b64  	%rd631, %rd630, %rd625;
	mov.b64	{%r1981, %r1982}, %rd631;
	prmt.b32 	%r1983, %r1981, %r1982, %r1552;
	prmt.b32 	%r1984, %r1981, %r1982, %r1551;
	mov.b64	%rd632, {%r1984, %r1983};
	add.s64 	%rd633, %rd632, %rd626;
	xor.b64  	%rd634, %rd633, %rd628;
	mov.b64	{%r350, %r351}, %rd634;
	// inline asm
	shf.l.wrap.b32 %r345, %r351, %r350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r349, %r350, %r351, %r1536;
	// inline asm
	mov.b64	%rd635, {%r345, %r349};
	add.s64 	%rd636, %rd588, %rd11;
	add.s64 	%rd637, %rd636, %rd607;
	xor.b64  	%rd638, %rd632, %rd637;
	mov.b64	{%r1985, %r1986}, %rd638;
	mov.b64	%rd639, {%r1986, %r1985};
	add.s64 	%rd640, %rd639, %rd619;
	xor.b64  	%rd641, %rd640, %rd607;
	mov.b64	{%r1987, %r1988}, %rd641;
	prmt.b32 	%r1989, %r1987, %r1988, %r1546;
	prmt.b32 	%r1990, %r1987, %r1988, %r1545;
	mov.b64	%rd642, {%r1990, %r1989};
	add.s64 	%rd643, %rd637, %rd20;
	add.s64 	%rd644, %rd643, %rd642;
	xor.b64  	%rd645, %rd639, %rd644;
	mov.b64	{%r1991, %r1992}, %rd645;
	prmt.b32 	%r1993, %r1991, %r1992, %r1552;
	prmt.b32 	%r1994, %r1991, %r1992, %r1551;
	mov.b64	%rd646, {%r1994, %r1993};
	add.s64 	%rd647, %rd646, %rd640;
	xor.b64  	%rd648, %rd647, %rd642;
	mov.b64	{%r358, %r359}, %rd648;
	// inline asm
	shf.l.wrap.b32 %r353, %r359, %r358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r357, %r358, %r359, %r1536;
	// inline asm
	mov.b64	%rd649, {%r353, %r357};
	add.s64 	%rd650, %rd602, %rd14;
	add.s64 	%rd651, %rd650, %rd621;
	xor.b64  	%rd652, %rd651, %rd590;
	mov.b64	{%r1995, %r1996}, %rd652;
	mov.b64	%rd653, {%r1996, %r1995};
	add.s64 	%rd654, %rd653, %rd633;
	xor.b64  	%rd655, %rd654, %rd621;
	mov.b64	{%r1997, %r1998}, %rd655;
	prmt.b32 	%r1999, %r1997, %r1998, %r1546;
	prmt.b32 	%r2000, %r1997, %r1998, %r1545;
	mov.b64	%rd656, {%r2000, %r1999};
	add.s64 	%rd657, %rd651, %rd12;
	add.s64 	%rd658, %rd657, %rd656;
	xor.b64  	%rd659, %rd658, %rd653;
	mov.b64	{%r2001, %r2002}, %rd659;
	prmt.b32 	%r2003, %r2001, %r2002, %r1552;
	prmt.b32 	%r2004, %r2001, %r2002, %r1551;
	mov.b64	%rd660, {%r2004, %r2003};
	add.s64 	%rd661, %rd660, %rd654;
	xor.b64  	%rd662, %rd661, %rd656;
	mov.b64	{%r366, %r367}, %rd662;
	// inline asm
	shf.l.wrap.b32 %r361, %r367, %r366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r365, %r366, %r367, %r1536;
	// inline asm
	mov.b64	%rd663, {%r361, %r365};
	add.s64 	%rd664, %rd616, %rd22;
	add.s64 	%rd665, %rd664, %rd635;
	xor.b64  	%rd666, %rd665, %rd604;
	mov.b64	{%r2005, %r2006}, %rd666;
	mov.b64	%rd667, {%r2006, %r2005};
	add.s64 	%rd668, %rd667, %rd591;
	xor.b64  	%rd669, %rd668, %rd635;
	mov.b64	{%r2007, %r2008}, %rd669;
	prmt.b32 	%r2009, %r2007, %r2008, %r1546;
	prmt.b32 	%r2010, %r2007, %r2008, %r1545;
	mov.b64	%rd670, {%r2010, %r2009};
	add.s64 	%rd671, %rd665, %rd21;
	add.s64 	%rd672, %rd671, %rd670;
	xor.b64  	%rd673, %rd672, %rd667;
	mov.b64	{%r2011, %r2012}, %rd673;
	prmt.b32 	%r2013, %r2011, %r2012, %r1552;
	prmt.b32 	%r2014, %r2011, %r2012, %r1551;
	mov.b64	%rd674, {%r2014, %r2013};
	add.s64 	%rd675, %rd674, %rd668;
	xor.b64  	%rd676, %rd675, %rd670;
	mov.b64	{%r374, %r375}, %rd676;
	// inline asm
	shf.l.wrap.b32 %r369, %r375, %r374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r373, %r374, %r375, %r1536;
	// inline asm
	mov.b64	%rd677, {%r369, %r373};
	add.s64 	%rd678, %rd593, %rd29;
	add.s64 	%rd679, %rd678, %rd630;
	xor.b64  	%rd680, %rd679, %rd618;
	mov.b64	{%r2015, %r2016}, %rd680;
	mov.b64	%rd681, {%r2016, %r2015};
	add.s64 	%rd682, %rd681, %rd605;
	xor.b64  	%rd683, %rd682, %rd593;
	mov.b64	{%r2017, %r2018}, %rd683;
	prmt.b32 	%r2019, %r2017, %r2018, %r1546;
	prmt.b32 	%r2020, %r2017, %r2018, %r1545;
	mov.b64	%rd684, {%r2020, %r2019};
	add.s64 	%rd685, %rd679, %rd16;
	add.s64 	%rd686, %rd685, %rd684;
	xor.b64  	%rd687, %rd686, %rd681;
	mov.b64	{%r2021, %r2022}, %rd687;
	prmt.b32 	%r2023, %r2021, %r2022, %r1552;
	prmt.b32 	%r2024, %r2021, %r2022, %r1551;
	mov.b64	%rd688, {%r2024, %r2023};
	add.s64 	%rd689, %rd688, %rd682;
	xor.b64  	%rd690, %rd689, %rd684;
	mov.b64	{%r382, %r383}, %rd690;
	// inline asm
	shf.l.wrap.b32 %r377, %r383, %r382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r381, %r382, %r383, %r1536;
	// inline asm
	mov.b64	%rd691, {%r377, %r381};
	add.s64 	%rd692, %rd644, %rd19;
	add.s64 	%rd693, %rd692, %rd691;
	xor.b64  	%rd694, %rd693, %rd660;
	mov.b64	{%r2025, %r2026}, %rd694;
	mov.b64	%rd695, {%r2026, %r2025};
	add.s64 	%rd696, %rd695, %rd675;
	xor.b64  	%rd697, %rd696, %rd691;
	mov.b64	{%r2027, %r2028}, %rd697;
	prmt.b32 	%r2029, %r2027, %r2028, %r1546;
	prmt.b32 	%r2030, %r2027, %r2028, %r1545;
	mov.b64	%rd698, {%r2030, %r2029};
	add.s64 	%rd699, %rd693, %rd12;
	add.s64 	%rd700, %rd699, %rd698;
	xor.b64  	%rd701, %rd695, %rd700;
	mov.b64	{%r2031, %r2032}, %rd701;
	prmt.b32 	%r2033, %r2031, %r2032, %r1552;
	prmt.b32 	%r2034, %r2031, %r2032, %r1551;
	mov.b64	%rd702, {%r2034, %r2033};
	add.s64 	%rd703, %rd696, %rd702;
	xor.b64  	%rd704, %rd703, %rd698;
	mov.b64	{%r390, %r391}, %rd704;
	// inline asm
	shf.l.wrap.b32 %r385, %r391, %r390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r389, %r390, %r391, %r1536;
	// inline asm
	mov.b64	%rd705, {%r385, %r389};
	add.s64 	%rd706, %rd649, %rd29;
	add.s64 	%rd707, %rd706, %rd658;
	xor.b64  	%rd708, %rd674, %rd707;
	mov.b64	{%r2035, %r2036}, %rd708;
	mov.b64	%rd709, {%r2036, %r2035};
	add.s64 	%rd710, %rd689, %rd709;
	xor.b64  	%rd711, %rd710, %rd649;
	mov.b64	{%r2037, %r2038}, %rd711;
	prmt.b32 	%r2039, %r2037, %r2038, %r1546;
	prmt.b32 	%r2040, %r2037, %r2038, %r1545;
	mov.b64	%rd712, {%r2040, %r2039};
	add.s64 	%rd713, %rd707, %rd22;
	add.s64 	%rd714, %rd713, %rd712;
	xor.b64  	%rd715, %rd714, %rd709;
	mov.b64	{%r2041, %r2042}, %rd715;
	prmt.b32 	%r2043, %r2041, %r2042, %r1552;
	prmt.b32 	%r2044, %r2041, %r2042, %r1551;
	mov.b64	%rd716, {%r2044, %r2043};
	add.s64 	%rd717, %rd716, %rd710;
	xor.b64  	%rd718, %rd717, %rd712;
	mov.b64	{%r398, %r399}, %rd718;
	// inline asm
	shf.l.wrap.b32 %r393, %r399, %r398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r397, %r398, %r399, %r1536;
	// inline asm
	mov.b64	%rd719, {%r393, %r397};
	add.s64 	%rd720, %rd663, %rd21;
	add.s64 	%rd721, %rd720, %rd672;
	xor.b64  	%rd722, %rd688, %rd721;
	mov.b64	{%r2045, %r2046}, %rd722;
	mov.b64	%rd723, {%r2046, %r2045};
	add.s64 	%rd724, %rd723, %rd647;
	xor.b64  	%rd725, %rd724, %rd663;
	mov.b64	{%r2047, %r2048}, %rd725;
	prmt.b32 	%r2049, %r2047, %r2048, %r1546;
	prmt.b32 	%r2050, %r2047, %r2048, %r1545;
	mov.b64	%rd726, {%r2050, %r2049};
	add.s64 	%rd727, %rd721, %rd20;
	add.s64 	%rd728, %rd727, %rd726;
	xor.b64  	%rd729, %rd728, %rd723;
	mov.b64	{%r2051, %r2052}, %rd729;
	prmt.b32 	%r2053, %r2051, %r2052, %r1552;
	prmt.b32 	%r2054, %r2051, %r2052, %r1551;
	mov.b64	%rd730, {%r2054, %r2053};
	add.s64 	%rd731, %rd730, %rd724;
	xor.b64  	%rd732, %rd731, %rd726;
	mov.b64	{%r406, %r407}, %rd732;
	// inline asm
	shf.l.wrap.b32 %r401, %r407, %r406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r405, %r406, %r407, %r1536;
	// inline asm
	mov.b64	%rd733, {%r401, %r405};
	add.s64 	%rd734, %rd677, %rd11;
	add.s64 	%rd735, %rd734, %rd686;
	xor.b64  	%rd736, %rd735, %rd646;
	mov.b64	{%r2055, %r2056}, %rd736;
	mov.b64	%rd737, {%r2056, %r2055};
	add.s64 	%rd738, %rd737, %rd661;
	xor.b64  	%rd739, %rd738, %rd677;
	mov.b64	{%r2057, %r2058}, %rd739;
	prmt.b32 	%r2059, %r2057, %r2058, %r1546;
	prmt.b32 	%r2060, %r2057, %r2058, %r1545;
	mov.b64	%rd740, {%r2060, %r2059};
	add.s64 	%rd741, %rd735, %rd17;
	add.s64 	%rd742, %rd741, %rd740;
	xor.b64  	%rd743, %rd742, %rd737;
	mov.b64	{%r2061, %r2062}, %rd743;
	prmt.b32 	%r2063, %r2061, %r2062, %r1552;
	prmt.b32 	%r2064, %r2061, %r2062, %r1551;
	mov.b64	%rd744, {%r2064, %r2063};
	add.s64 	%rd745, %rd744, %rd738;
	xor.b64  	%rd746, %rd745, %rd740;
	mov.b64	{%r414, %r415}, %rd746;
	// inline asm
	shf.l.wrap.b32 %r409, %r415, %r414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r413, %r414, %r415, %r1536;
	// inline asm
	mov.b64	%rd747, {%r409, %r413};
	add.s64 	%rd748, %rd700, %rd1;
	add.s64 	%rd749, %rd748, %rd719;
	xor.b64  	%rd750, %rd744, %rd749;
	mov.b64	{%r2065, %r2066}, %rd750;
	mov.b64	%rd751, {%r2066, %r2065};
	add.s64 	%rd752, %rd751, %rd731;
	xor.b64  	%rd753, %rd752, %rd719;
	mov.b64	{%r2067, %r2068}, %rd753;
	prmt.b32 	%r2069, %r2067, %r2068, %r1546;
	prmt.b32 	%r2070, %r2067, %r2068, %r1545;
	mov.b64	%rd754, {%r2070, %r2069};
	add.s64 	%rd755, %rd749, %rd14;
	add.s64 	%rd756, %rd755, %rd754;
	xor.b64  	%rd757, %rd751, %rd756;
	mov.b64	{%r2071, %r2072}, %rd757;
	prmt.b32 	%r2073, %r2071, %r2072, %r1552;
	prmt.b32 	%r2074, %r2071, %r2072, %r1551;
	mov.b64	%rd758, {%r2074, %r2073};
	add.s64 	%rd759, %rd758, %rd752;
	xor.b64  	%rd760, %rd759, %rd754;
	mov.b64	{%r422, %r423}, %rd760;
	// inline asm
	shf.l.wrap.b32 %r417, %r423, %r422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r421, %r422, %r423, %r1536;
	// inline asm
	mov.b64	%rd761, {%r417, %r421};
	add.s64 	%rd762, %rd714, %rd13;
	add.s64 	%rd763, %rd762, %rd733;
	xor.b64  	%rd764, %rd763, %rd702;
	mov.b64	{%r2075, %r2076}, %rd764;
	mov.b64	%rd765, {%r2076, %r2075};
	add.s64 	%rd766, %rd765, %rd745;
	xor.b64  	%rd767, %rd766, %rd733;
	mov.b64	{%r2077, %r2078}, %rd767;
	prmt.b32 	%r2079, %r2077, %r2078, %r1546;
	prmt.b32 	%r2080, %r2077, %r2078, %r1545;
	mov.b64	%rd768, {%r2080, %r2079};
	add.s64 	%rd769, %rd763, %rd10;
	add.s64 	%rd770, %rd769, %rd768;
	xor.b64  	%rd771, %rd770, %rd765;
	mov.b64	{%r2081, %r2082}, %rd771;
	prmt.b32 	%r2083, %r2081, %r2082, %r1552;
	prmt.b32 	%r2084, %r2081, %r2082, %r1551;
	mov.b64	%rd772, {%r2084, %r2083};
	add.s64 	%rd773, %rd772, %rd766;
	xor.b64  	%rd774, %rd773, %rd768;
	mov.b64	{%r430, %r431}, %rd774;
	// inline asm
	shf.l.wrap.b32 %r425, %r431, %r430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r429, %r430, %r431, %r1536;
	// inline asm
	mov.b64	%rd775, {%r425, %r429};
	add.s64 	%rd776, %rd728, %rd16;
	add.s64 	%rd777, %rd776, %rd747;
	xor.b64  	%rd778, %rd777, %rd716;
	mov.b64	{%r2085, %r2086}, %rd778;
	mov.b64	%rd779, {%r2086, %r2085};
	add.s64 	%rd780, %rd779, %rd703;
	xor.b64  	%rd781, %rd780, %rd747;
	mov.b64	{%r2087, %r2088}, %rd781;
	prmt.b32 	%r2089, %r2087, %r2088, %r1546;
	prmt.b32 	%r2090, %r2087, %r2088, %r1545;
	mov.b64	%rd782, {%r2090, %r2089};
	add.s64 	%rd783, %rd777, %rd9;
	add.s64 	%rd784, %rd783, %rd782;
	xor.b64  	%rd785, %rd784, %rd779;
	mov.b64	{%r2091, %r2092}, %rd785;
	prmt.b32 	%r2093, %r2091, %r2092, %r1552;
	prmt.b32 	%r2094, %r2091, %r2092, %r1551;
	mov.b64	%rd786, {%r2094, %r2093};
	add.s64 	%rd787, %rd786, %rd780;
	xor.b64  	%rd788, %rd787, %rd782;
	mov.b64	{%r438, %r439}, %rd788;
	// inline asm
	shf.l.wrap.b32 %r433, %r439, %r438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r437, %r438, %r439, %r1536;
	// inline asm
	mov.b64	%rd789, {%r433, %r437};
	add.s64 	%rd790, %rd705, %rd15;
	add.s64 	%rd791, %rd790, %rd742;
	xor.b64  	%rd792, %rd791, %rd730;
	mov.b64	{%r2095, %r2096}, %rd792;
	mov.b64	%rd793, {%r2096, %r2095};
	add.s64 	%rd794, %rd793, %rd717;
	xor.b64  	%rd795, %rd794, %rd705;
	mov.b64	{%r2097, %r2098}, %rd795;
	prmt.b32 	%r2099, %r2097, %r2098, %r1546;
	prmt.b32 	%r2100, %r2097, %r2098, %r1545;
	mov.b64	%rd796, {%r2100, %r2099};
	add.s64 	%rd797, %rd791, %rd18;
	add.s64 	%rd798, %rd797, %rd796;
	xor.b64  	%rd799, %rd798, %rd793;
	mov.b64	{%r2101, %r2102}, %rd799;
	prmt.b32 	%r2103, %r2101, %r2102, %r1552;
	prmt.b32 	%r2104, %r2101, %r2102, %r1551;
	mov.b64	%rd800, {%r2104, %r2103};
	add.s64 	%rd801, %rd800, %rd794;
	xor.b64  	%rd802, %rd801, %rd796;
	mov.b64	{%r446, %r447}, %rd802;
	// inline asm
	shf.l.wrap.b32 %r441, %r447, %r446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r445, %r446, %r447, %r1536;
	// inline asm
	mov.b64	%rd803, {%r441, %r445};
	add.s64 	%rd804, %rd756, %rd20;
	add.s64 	%rd805, %rd804, %rd803;
	xor.b64  	%rd806, %rd805, %rd772;
	mov.b64	{%r2105, %r2106}, %rd806;
	mov.b64	%rd807, {%r2106, %r2105};
	add.s64 	%rd808, %rd807, %rd787;
	xor.b64  	%rd809, %rd808, %rd803;
	mov.b64	{%r2107, %r2108}, %rd809;
	prmt.b32 	%r2109, %r2107, %r2108, %r1546;
	prmt.b32 	%r2110, %r2107, %r2108, %r1545;
	mov.b64	%rd810, {%r2110, %r2109};
	add.s64 	%rd811, %rd805, %rd18;
	add.s64 	%rd812, %rd811, %rd810;
	xor.b64  	%rd813, %rd807, %rd812;
	mov.b64	{%r2111, %r2112}, %rd813;
	prmt.b32 	%r2113, %r2111, %r2112, %r1552;
	prmt.b32 	%r2114, %r2111, %r2112, %r1551;
	mov.b64	%rd814, {%r2114, %r2113};
	add.s64 	%rd815, %rd808, %rd814;
	xor.b64  	%rd816, %rd815, %rd810;
	mov.b64	{%r454, %r455}, %rd816;
	// inline asm
	shf.l.wrap.b32 %r449, %r455, %r454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r453, %r454, %r455, %r1536;
	// inline asm
	mov.b64	%rd817, {%r449, %r453};
	add.s64 	%rd818, %rd761, %rd14;
	add.s64 	%rd819, %rd818, %rd770;
	xor.b64  	%rd820, %rd786, %rd819;
	mov.b64	{%r2115, %r2116}, %rd820;
	mov.b64	%rd821, {%r2116, %r2115};
	add.s64 	%rd822, %rd801, %rd821;
	xor.b64  	%rd823, %rd822, %rd761;
	mov.b64	{%r2117, %r2118}, %rd823;
	prmt.b32 	%r2119, %r2117, %r2118, %r1546;
	prmt.b32 	%r2120, %r2117, %r2118, %r1545;
	mov.b64	%rd824, {%r2120, %r2119};
	add.s64 	%rd825, %rd819, %rd21;
	add.s64 	%rd826, %rd825, %rd824;
	xor.b64  	%rd827, %rd826, %rd821;
	mov.b64	{%r2121, %r2122}, %rd827;
	prmt.b32 	%r2123, %r2121, %r2122, %r1552;
	prmt.b32 	%r2124, %r2121, %r2122, %r1551;
	mov.b64	%rd828, {%r2124, %r2123};
	add.s64 	%rd829, %rd828, %rd822;
	xor.b64  	%rd830, %rd829, %rd824;
	mov.b64	{%r462, %r463}, %rd830;
	// inline asm
	shf.l.wrap.b32 %r457, %r463, %r462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r461, %r462, %r463, %r1536;
	// inline asm
	mov.b64	%rd831, {%r457, %r461};
	add.s64 	%rd832, %rd775, %rd19;
	add.s64 	%rd833, %rd832, %rd784;
	xor.b64  	%rd834, %rd800, %rd833;
	mov.b64	{%r2125, %r2126}, %rd834;
	mov.b64	%rd835, {%r2126, %r2125};
	add.s64 	%rd836, %rd835, %rd759;
	xor.b64  	%rd837, %rd836, %rd775;
	mov.b64	{%r2127, %r2128}, %rd837;
	prmt.b32 	%r2129, %r2127, %r2128, %r1546;
	prmt.b32 	%r2130, %r2127, %r2128, %r1545;
	mov.b64	%rd838, {%r2130, %r2129};
	add.s64 	%rd839, %rd833, %rd29;
	add.s64 	%rd840, %rd839, %rd838;
	xor.b64  	%rd841, %rd840, %rd835;
	mov.b64	{%r2131, %r2132}, %rd841;
	prmt.b32 	%r2133, %r2131, %r2132, %r1552;
	prmt.b32 	%r2134, %r2131, %r2132, %r1551;
	mov.b64	%rd842, {%r2134, %r2133};
	add.s64 	%rd843, %rd842, %rd836;
	xor.b64  	%rd844, %rd843, %rd838;
	mov.b64	{%r470, %r471}, %rd844;
	// inline asm
	shf.l.wrap.b32 %r465, %r471, %r470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r469, %r470, %r471, %r1536;
	// inline asm
	mov.b64	%rd845, {%r465, %r469};
	add.s64 	%rd846, %rd789, %rd10;
	add.s64 	%rd847, %rd846, %rd798;
	xor.b64  	%rd848, %rd847, %rd758;
	mov.b64	{%r2135, %r2136}, %rd848;
	mov.b64	%rd849, {%r2136, %r2135};
	add.s64 	%rd850, %rd849, %rd773;
	xor.b64  	%rd851, %rd850, %rd789;
	mov.b64	{%r2137, %r2138}, %rd851;
	prmt.b32 	%r2139, %r2137, %r2138, %r1546;
	prmt.b32 	%r2140, %r2137, %r2138, %r1545;
	mov.b64	%rd852, {%r2140, %r2139};
	add.s64 	%rd853, %rd847, %rd16;
	add.s64 	%rd854, %rd853, %rd852;
	xor.b64  	%rd855, %rd854, %rd849;
	mov.b64	{%r2141, %r2142}, %rd855;
	prmt.b32 	%r2143, %r2141, %r2142, %r1552;
	prmt.b32 	%r2144, %r2141, %r2142, %r1551;
	mov.b64	%rd856, {%r2144, %r2143};
	add.s64 	%rd857, %rd856, %rd850;
	xor.b64  	%rd858, %rd857, %rd852;
	mov.b64	{%r478, %r479}, %rd858;
	// inline asm
	shf.l.wrap.b32 %r473, %r479, %r478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r477, %r478, %r479, %r1536;
	// inline asm
	mov.b64	%rd859, {%r473, %r477};
	add.s64 	%rd860, %rd812, %rd12;
	add.s64 	%rd861, %rd860, %rd831;
	xor.b64  	%rd862, %rd856, %rd861;
	mov.b64	{%r2145, %r2146}, %rd862;
	mov.b64	%rd863, {%r2146, %r2145};
	add.s64 	%rd864, %rd863, %rd843;
	xor.b64  	%rd865, %rd864, %rd831;
	mov.b64	{%r2147, %r2148}, %rd865;
	prmt.b32 	%r2149, %r2147, %r2148, %r1546;
	prmt.b32 	%r2150, %r2147, %r2148, %r1545;
	mov.b64	%rd866, {%r2150, %r2149};
	add.s64 	%rd867, %rd861, %rd1;
	add.s64 	%rd868, %rd867, %rd866;
	xor.b64  	%rd869, %rd863, %rd868;
	mov.b64	{%r2151, %r2152}, %rd869;
	prmt.b32 	%r2153, %r2151, %r2152, %r1552;
	prmt.b32 	%r2154, %r2151, %r2152, %r1551;
	mov.b64	%rd870, {%r2154, %r2153};
	add.s64 	%rd871, %rd870, %rd864;
	xor.b64  	%rd872, %rd871, %rd866;
	mov.b64	{%r486, %r487}, %rd872;
	// inline asm
	shf.l.wrap.b32 %r481, %r487, %r486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r485, %r486, %r487, %r1536;
	// inline asm
	mov.b64	%rd873, {%r481, %r485};
	add.s64 	%rd874, %rd826, %rd22;
	add.s64 	%rd875, %rd874, %rd845;
	xor.b64  	%rd876, %rd875, %rd814;
	mov.b64	{%r2155, %r2156}, %rd876;
	mov.b64	%rd877, {%r2156, %r2155};
	add.s64 	%rd878, %rd877, %rd857;
	xor.b64  	%rd879, %rd878, %rd845;
	mov.b64	{%r2157, %r2158}, %rd879;
	prmt.b32 	%r2159, %r2157, %r2158, %r1546;
	prmt.b32 	%r2160, %r2157, %r2158, %r1545;
	mov.b64	%rd880, {%r2160, %r2159};
	add.s64 	%rd881, %rd875, %rd11;
	add.s64 	%rd882, %rd881, %rd880;
	xor.b64  	%rd883, %rd882, %rd877;
	mov.b64	{%r2161, %r2162}, %rd883;
	prmt.b32 	%r2163, %r2161, %r2162, %r1552;
	prmt.b32 	%r2164, %r2161, %r2162, %r1551;
	mov.b64	%rd884, {%r2164, %r2163};
	add.s64 	%rd885, %rd884, %rd878;
	xor.b64  	%rd886, %rd885, %rd880;
	mov.b64	{%r494, %r495}, %rd886;
	// inline asm
	shf.l.wrap.b32 %r489, %r495, %r494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r493, %r494, %r495, %r1536;
	// inline asm
	mov.b64	%rd887, {%r489, %r493};
	add.s64 	%rd888, %rd840, %rd15;
	add.s64 	%rd889, %rd888, %rd859;
	xor.b64  	%rd890, %rd889, %rd828;
	mov.b64	{%r2165, %r2166}, %rd890;
	mov.b64	%rd891, {%r2166, %r2165};
	add.s64 	%rd892, %rd891, %rd815;
	xor.b64  	%rd893, %rd892, %rd859;
	mov.b64	{%r2167, %r2168}, %rd893;
	prmt.b32 	%r2169, %r2167, %r2168, %r1546;
	prmt.b32 	%r2170, %r2167, %r2168, %r1545;
	mov.b64	%rd894, {%r2170, %r2169};
	add.s64 	%rd895, %rd889, %rd13;
	add.s64 	%rd896, %rd895, %rd894;
	xor.b64  	%rd897, %rd896, %rd891;
	mov.b64	{%r2171, %r2172}, %rd897;
	prmt.b32 	%r2173, %r2171, %r2172, %r1552;
	prmt.b32 	%r2174, %r2171, %r2172, %r1551;
	mov.b64	%rd898, {%r2174, %r2173};
	add.s64 	%rd899, %rd898, %rd892;
	xor.b64  	%rd900, %rd899, %rd894;
	mov.b64	{%r502, %r503}, %rd900;
	// inline asm
	shf.l.wrap.b32 %r497, %r503, %r502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r501, %r502, %r503, %r1536;
	// inline asm
	mov.b64	%rd901, {%r497, %r501};
	add.s64 	%rd902, %rd817, %rd9;
	add.s64 	%rd903, %rd902, %rd854;
	xor.b64  	%rd904, %rd903, %rd842;
	mov.b64	{%r2175, %r2176}, %rd904;
	mov.b64	%rd905, {%r2176, %r2175};
	add.s64 	%rd906, %rd905, %rd829;
	xor.b64  	%rd907, %rd906, %rd817;
	mov.b64	{%r2177, %r2178}, %rd907;
	prmt.b32 	%r2179, %r2177, %r2178, %r1546;
	prmt.b32 	%r2180, %r2177, %r2178, %r1545;
	mov.b64	%rd908, {%r2180, %r2179};
	add.s64 	%rd909, %rd903, %rd17;
	add.s64 	%rd910, %rd909, %rd908;
	xor.b64  	%rd911, %rd910, %rd905;
	mov.b64	{%r2181, %r2182}, %rd911;
	prmt.b32 	%r2183, %r2181, %r2182, %r1552;
	prmt.b32 	%r2184, %r2181, %r2182, %r1551;
	mov.b64	%rd912, {%r2184, %r2183};
	add.s64 	%rd913, %rd912, %rd906;
	xor.b64  	%rd914, %rd913, %rd908;
	mov.b64	{%r510, %r511}, %rd914;
	// inline asm
	shf.l.wrap.b32 %r505, %r511, %r510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r509, %r510, %r511, %r1536;
	// inline asm
	mov.b64	%rd915, {%r505, %r509};
	add.s64 	%rd916, %rd868, %rd13;
	add.s64 	%rd917, %rd916, %rd915;
	xor.b64  	%rd918, %rd917, %rd884;
	mov.b64	{%r2185, %r2186}, %rd918;
	mov.b64	%rd919, {%r2186, %r2185};
	add.s64 	%rd920, %rd919, %rd899;
	xor.b64  	%rd921, %rd920, %rd915;
	mov.b64	{%r2187, %r2188}, %rd921;
	prmt.b32 	%r2189, %r2187, %r2188, %r1546;
	prmt.b32 	%r2190, %r2187, %r2188, %r1545;
	mov.b64	%rd922, {%r2190, %r2189};
	add.s64 	%rd923, %rd917, %rd22;
	add.s64 	%rd924, %rd923, %rd922;
	xor.b64  	%rd925, %rd919, %rd924;
	mov.b64	{%r2191, %r2192}, %rd925;
	prmt.b32 	%r2193, %r2191, %r2192, %r1552;
	prmt.b32 	%r2194, %r2191, %r2192, %r1551;
	mov.b64	%rd926, {%r2194, %r2193};
	add.s64 	%rd927, %rd920, %rd926;
	xor.b64  	%rd928, %rd927, %rd922;
	mov.b64	{%r518, %r519}, %rd928;
	// inline asm
	shf.l.wrap.b32 %r513, %r519, %r518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r517, %r518, %r519, %r1536;
	// inline asm
	mov.b64	%rd929, {%r513, %r517};
	add.s64 	%rd930, %rd873, %rd21;
	add.s64 	%rd931, %rd930, %rd882;
	xor.b64  	%rd932, %rd898, %rd931;
	mov.b64	{%r2195, %r2196}, %rd932;
	mov.b64	%rd933, {%r2196, %r2195};
	add.s64 	%rd934, %rd913, %rd933;
	xor.b64  	%rd935, %rd934, %rd873;
	mov.b64	{%r2197, %r2198}, %rd935;
	prmt.b32 	%r2199, %r2197, %r2198, %r1546;
	prmt.b32 	%r2200, %r2197, %r2198, %r1545;
	mov.b64	%rd936, {%r2200, %r2199};
	add.s64 	%rd937, %rd931, %rd16;
	add.s64 	%rd938, %rd937, %rd936;
	xor.b64  	%rd939, %rd938, %rd933;
	mov.b64	{%r2201, %r2202}, %rd939;
	prmt.b32 	%r2203, %r2201, %r2202, %r1552;
	prmt.b32 	%r2204, %r2201, %r2202, %r1551;
	mov.b64	%rd940, {%r2204, %r2203};
	add.s64 	%rd941, %rd940, %rd934;
	xor.b64  	%rd942, %rd941, %rd936;
	mov.b64	{%r526, %r527}, %rd942;
	// inline asm
	shf.l.wrap.b32 %r521, %r527, %r526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r525, %r526, %r527, %r1536;
	// inline asm
	mov.b64	%rd943, {%r521, %r525};
	add.s64 	%rd944, %rd887, %rd18;
	add.s64 	%rd945, %rd944, %rd896;
	xor.b64  	%rd946, %rd912, %rd945;
	mov.b64	{%r2205, %r2206}, %rd946;
	mov.b64	%rd947, {%r2206, %r2205};
	add.s64 	%rd948, %rd947, %rd871;
	xor.b64  	%rd949, %rd948, %rd887;
	mov.b64	{%r2207, %r2208}, %rd949;
	prmt.b32 	%r2209, %r2207, %r2208, %r1546;
	prmt.b32 	%r2210, %r2207, %r2208, %r1545;
	mov.b64	%rd950, {%r2210, %r2209};
	add.s64 	%rd951, %rd945, %rd10;
	add.s64 	%rd952, %rd951, %rd950;
	xor.b64  	%rd953, %rd952, %rd947;
	mov.b64	{%r2211, %r2212}, %rd953;
	prmt.b32 	%r2213, %r2211, %r2212, %r1552;
	prmt.b32 	%r2214, %r2211, %r2212, %r1551;
	mov.b64	%rd954, {%r2214, %r2213};
	add.s64 	%rd955, %rd954, %rd948;
	xor.b64  	%rd956, %rd955, %rd950;
	mov.b64	{%r534, %r535}, %rd956;
	// inline asm
	shf.l.wrap.b32 %r529, %r535, %r534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r533, %r534, %r535, %r1536;
	// inline asm
	mov.b64	%rd957, {%r529, %r533};
	add.s64 	%rd958, %rd901, %rd1;
	add.s64 	%rd959, %rd958, %rd910;
	xor.b64  	%rd960, %rd959, %rd870;
	mov.b64	{%r2215, %r2216}, %rd960;
	mov.b64	%rd961, {%r2216, %r2215};
	add.s64 	%rd962, %rd961, %rd885;
	xor.b64  	%rd963, %rd962, %rd901;
	mov.b64	{%r2217, %r2218}, %rd963;
	prmt.b32 	%r2219, %r2217, %r2218, %r1546;
	prmt.b32 	%r2220, %r2217, %r2218, %r1545;
	mov.b64	%rd964, {%r2220, %r2219};
	add.s64 	%rd965, %rd959, %rd15;
	add.s64 	%rd966, %rd965, %rd964;
	xor.b64  	%rd967, %rd966, %rd961;
	mov.b64	{%r2221, %r2222}, %rd967;
	prmt.b32 	%r2223, %r2221, %r2222, %r1552;
	prmt.b32 	%r2224, %r2221, %r2222, %r1551;
	mov.b64	%rd968, {%r2224, %r2223};
	add.s64 	%rd969, %rd968, %rd962;
	xor.b64  	%rd970, %rd969, %rd964;
	mov.b64	{%r542, %r543}, %rd970;
	// inline asm
	shf.l.wrap.b32 %r537, %r543, %r542, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r541, %r542, %r543, %r1536;
	// inline asm
	mov.b64	%rd971, {%r537, %r541};
	add.s64 	%rd972, %rd924, %rd19;
	add.s64 	%rd973, %rd972, %rd943;
	xor.b64  	%rd974, %rd968, %rd973;
	mov.b64	{%r2225, %r2226}, %rd974;
	mov.b64	%rd975, {%r2226, %r2225};
	add.s64 	%rd976, %rd975, %rd955;
	xor.b64  	%rd977, %rd976, %rd943;
	mov.b64	{%r2227, %r2228}, %rd977;
	prmt.b32 	%r2229, %r2227, %r2228, %r1546;
	prmt.b32 	%r2230, %r2227, %r2228, %r1545;
	mov.b64	%rd978, {%r2230, %r2229};
	add.s64 	%rd979, %rd973, %rd9;
	add.s64 	%rd980, %rd979, %rd978;
	xor.b64  	%rd981, %rd975, %rd980;
	mov.b64	{%r2231, %r2232}, %rd981;
	prmt.b32 	%r2233, %r2231, %r2232, %r1552;
	prmt.b32 	%r2234, %r2231, %r2232, %r1551;
	mov.b64	%rd982, {%r2234, %r2233};
	add.s64 	%rd983, %rd982, %rd976;
	xor.b64  	%rd984, %rd983, %rd978;
	mov.b64	{%r550, %r551}, %rd984;
	// inline asm
	shf.l.wrap.b32 %r545, %r551, %r550, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r549, %r550, %r551, %r1536;
	// inline asm
	mov.b64	%rd985, {%r545, %r549};
	add.s64 	%rd986, %rd938, %rd20;
	add.s64 	%rd987, %rd986, %rd957;
	xor.b64  	%rd988, %rd987, %rd926;
	mov.b64	{%r2235, %r2236}, %rd988;
	mov.b64	%rd989, {%r2236, %r2235};
	add.s64 	%rd990, %rd989, %rd969;
	xor.b64  	%rd991, %rd990, %rd957;
	mov.b64	{%r2237, %r2238}, %rd991;
	prmt.b32 	%r2239, %r2237, %r2238, %r1546;
	prmt.b32 	%r2240, %r2237, %r2238, %r1545;
	mov.b64	%rd992, {%r2240, %r2239};
	add.s64 	%rd993, %rd987, %rd14;
	add.s64 	%rd994, %rd993, %rd992;
	xor.b64  	%rd995, %rd994, %rd989;
	mov.b64	{%r2241, %r2242}, %rd995;
	prmt.b32 	%r2243, %r2241, %r2242, %r1552;
	prmt.b32 	%r2244, %r2241, %r2242, %r1551;
	mov.b64	%rd996, {%r2244, %r2243};
	add.s64 	%rd997, %rd996, %rd990;
	xor.b64  	%rd998, %rd997, %rd992;
	mov.b64	{%r558, %r559}, %rd998;
	// inline asm
	shf.l.wrap.b32 %r553, %r559, %r558, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r557, %r558, %r559, %r1536;
	// inline asm
	mov.b64	%rd999, {%r553, %r557};
	add.s64 	%rd1000, %rd952, %rd29;
	add.s64 	%rd1001, %rd1000, %rd971;
	xor.b64  	%rd1002, %rd1001, %rd940;
	mov.b64	{%r2245, %r2246}, %rd1002;
	mov.b64	%rd1003, {%r2246, %r2245};
	add.s64 	%rd1004, %rd1003, %rd927;
	xor.b64  	%rd1005, %rd1004, %rd971;
	mov.b64	{%r2247, %r2248}, %rd1005;
	prmt.b32 	%r2249, %r2247, %r2248, %r1546;
	prmt.b32 	%r2250, %r2247, %r2248, %r1545;
	mov.b64	%rd1006, {%r2250, %r2249};
	add.s64 	%rd1007, %rd1001, %rd11;
	add.s64 	%rd1008, %rd1007, %rd1006;
	xor.b64  	%rd1009, %rd1008, %rd1003;
	mov.b64	{%r2251, %r2252}, %rd1009;
	prmt.b32 	%r2253, %r2251, %r2252, %r1552;
	prmt.b32 	%r2254, %r2251, %r2252, %r1551;
	mov.b64	%rd1010, {%r2254, %r2253};
	add.s64 	%rd1011, %rd1010, %rd1004;
	xor.b64  	%rd1012, %rd1011, %rd1006;
	mov.b64	{%r566, %r567}, %rd1012;
	// inline asm
	shf.l.wrap.b32 %r561, %r567, %r566, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r565, %r566, %r567, %r1536;
	// inline asm
	mov.b64	%rd1013, {%r561, %r565};
	add.s64 	%rd1014, %rd929, %rd17;
	add.s64 	%rd1015, %rd1014, %rd966;
	xor.b64  	%rd1016, %rd1015, %rd954;
	mov.b64	{%r2255, %r2256}, %rd1016;
	mov.b64	%rd1017, {%r2256, %r2255};
	add.s64 	%rd1018, %rd1017, %rd941;
	xor.b64  	%rd1019, %rd1018, %rd929;
	mov.b64	{%r2257, %r2258}, %rd1019;
	prmt.b32 	%r2259, %r2257, %r2258, %r1546;
	prmt.b32 	%r2260, %r2257, %r2258, %r1545;
	mov.b64	%rd1020, {%r2260, %r2259};
	add.s64 	%rd1021, %rd1015, %rd12;
	add.s64 	%rd1022, %rd1021, %rd1020;
	xor.b64  	%rd1023, %rd1022, %rd1017;
	mov.b64	{%r2261, %r2262}, %rd1023;
	prmt.b32 	%r2263, %r2261, %r2262, %r1552;
	prmt.b32 	%r2264, %r2261, %r2262, %r1551;
	mov.b64	%rd1024, {%r2264, %r2263};
	add.s64 	%rd1025, %rd1024, %rd1018;
	xor.b64  	%rd1026, %rd1025, %rd1020;
	mov.b64	{%r574, %r575}, %rd1026;
	// inline asm
	shf.l.wrap.b32 %r569, %r575, %r574, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r573, %r574, %r575, %r1536;
	// inline asm
	mov.b64	%rd1027, {%r569, %r573};
	add.s64 	%rd1028, %rd980, %rd17;
	add.s64 	%rd1029, %rd1028, %rd1027;
	xor.b64  	%rd1030, %rd1029, %rd996;
	mov.b64	{%r2265, %r2266}, %rd1030;
	mov.b64	%rd1031, {%r2266, %r2265};
	add.s64 	%rd1032, %rd1031, %rd1011;
	xor.b64  	%rd1033, %rd1032, %rd1027;
	mov.b64	{%r2267, %r2268}, %rd1033;
	prmt.b32 	%r2269, %r2267, %r2268, %r1546;
	prmt.b32 	%r2270, %r2267, %r2268, %r1545;
	mov.b64	%rd1034, {%r2270, %r2269};
	add.s64 	%rd1035, %rd1029, %rd9;
	add.s64 	%rd1036, %rd1035, %rd1034;
	xor.b64  	%rd1037, %rd1031, %rd1036;
	mov.b64	{%r2271, %r2272}, %rd1037;
	prmt.b32 	%r2273, %r2271, %r2272, %r1552;
	prmt.b32 	%r2274, %r2271, %r2272, %r1551;
	mov.b64	%rd1038, {%r2274, %r2273};
	add.s64 	%rd1039, %rd1032, %rd1038;
	xor.b64  	%rd1040, %rd1039, %rd1034;
	mov.b64	{%r582, %r583}, %rd1040;
	// inline asm
	shf.l.wrap.b32 %r577, %r583, %r582, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r581, %r582, %r583, %r1536;
	// inline asm
	mov.b64	%rd1041, {%r577, %r581};
	add.s64 	%rd1042, %rd985, %rd15;
	add.s64 	%rd1043, %rd1042, %rd994;
	xor.b64  	%rd1044, %rd1010, %rd1043;
	mov.b64	{%r2275, %r2276}, %rd1044;
	mov.b64	%rd1045, {%r2276, %r2275};
	add.s64 	%rd1046, %rd1025, %rd1045;
	xor.b64  	%rd1047, %rd1046, %rd985;
	mov.b64	{%r2277, %r2278}, %rd1047;
	prmt.b32 	%r2279, %r2277, %r2278, %r1546;
	prmt.b32 	%r2280, %r2277, %r2278, %r1545;
	mov.b64	%rd1048, {%r2280, %r2279};
	add.s64 	%rd1049, %rd1043, %rd11;
	add.s64 	%rd1050, %rd1049, %rd1048;
	xor.b64  	%rd1051, %rd1050, %rd1045;
	mov.b64	{%r2281, %r2282}, %rd1051;
	prmt.b32 	%r2283, %r2281, %r2282, %r1552;
	prmt.b32 	%r2284, %r2281, %r2282, %r1551;
	mov.b64	%rd1052, {%r2284, %r2283};
	add.s64 	%rd1053, %rd1052, %rd1046;
	xor.b64  	%rd1054, %rd1053, %rd1048;
	mov.b64	{%r590, %r591}, %rd1054;
	// inline asm
	shf.l.wrap.b32 %r585, %r591, %r590, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r589, %r590, %r591, %r1536;
	// inline asm
	mov.b64	%rd1055, {%r585, %r589};
	add.s64 	%rd1056, %rd999, %rd14;
	add.s64 	%rd1057, %rd1056, %rd1008;
	xor.b64  	%rd1058, %rd1024, %rd1057;
	mov.b64	{%r2285, %r2286}, %rd1058;
	mov.b64	%rd1059, {%r2286, %r2285};
	add.s64 	%rd1060, %rd1059, %rd983;
	xor.b64  	%rd1061, %rd1060, %rd999;
	mov.b64	{%r2287, %r2288}, %rd1061;
	prmt.b32 	%r2289, %r2287, %r2288, %r1546;
	prmt.b32 	%r2290, %r2287, %r2288, %r1545;
	mov.b64	%rd1062, {%r2290, %r2289};
	add.s64 	%rd1063, %rd1057, %rd13;
	add.s64 	%rd1064, %rd1063, %rd1062;
	xor.b64  	%rd1065, %rd1064, %rd1059;
	mov.b64	{%r2291, %r2292}, %rd1065;
	prmt.b32 	%r2293, %r2291, %r2292, %r1552;
	prmt.b32 	%r2294, %r2291, %r2292, %r1551;
	mov.b64	%rd1066, {%r2294, %r2293};
	add.s64 	%rd1067, %rd1066, %rd1060;
	xor.b64  	%rd1068, %rd1067, %rd1062;
	mov.b64	{%r598, %r599}, %rd1068;
	// inline asm
	shf.l.wrap.b32 %r593, %r599, %r598, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r597, %r598, %r599, %r1536;
	// inline asm
	mov.b64	%rd1069, {%r593, %r597};
	add.s64 	%rd1070, %rd1013, %rd29;
	add.s64 	%rd1071, %rd1070, %rd1022;
	xor.b64  	%rd1072, %rd1071, %rd982;
	mov.b64	{%r2295, %r2296}, %rd1072;
	mov.b64	%rd1073, {%r2296, %r2295};
	add.s64 	%rd1074, %rd1073, %rd997;
	xor.b64  	%rd1075, %rd1074, %rd1013;
	mov.b64	{%r2297, %r2298}, %rd1075;
	prmt.b32 	%r2299, %r2297, %r2298, %r1546;
	prmt.b32 	%r2300, %r2297, %r2298, %r1545;
	mov.b64	%rd1076, {%r2300, %r2299};
	add.s64 	%rd1077, %rd1071, %rd12;
	add.s64 	%rd1078, %rd1077, %rd1076;
	xor.b64  	%rd1079, %rd1078, %rd1073;
	mov.b64	{%r2301, %r2302}, %rd1079;
	prmt.b32 	%r2303, %r2301, %r2302, %r1552;
	prmt.b32 	%r2304, %r2301, %r2302, %r1551;
	mov.b64	%rd1080, {%r2304, %r2303};
	add.s64 	%rd1081, %rd1080, %rd1074;
	xor.b64  	%rd1082, %rd1081, %rd1076;
	mov.b64	{%r606, %r607}, %rd1082;
	// inline asm
	shf.l.wrap.b32 %r601, %r607, %r606, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r605, %r606, %r607, %r1536;
	// inline asm
	mov.b64	%rd1083, {%r601, %r605};
	add.s64 	%rd1084, %rd1036, %rd22;
	add.s64 	%rd1085, %rd1084, %rd1055;
	xor.b64  	%rd1086, %rd1080, %rd1085;
	mov.b64	{%r2305, %r2306}, %rd1086;
	mov.b64	%rd1087, {%r2306, %r2305};
	add.s64 	%rd1088, %rd1087, %rd1067;
	xor.b64  	%rd1089, %rd1088, %rd1055;
	mov.b64	{%r2307, %r2308}, %rd1089;
	prmt.b32 	%r2309, %r2307, %r2308, %r1546;
	prmt.b32 	%r2310, %r2307, %r2308, %r1545;
	mov.b64	%rd1090, {%r2310, %r2309};
	add.s64 	%rd1091, %rd1085, %rd18;
	add.s64 	%rd1092, %rd1091, %rd1090;
	xor.b64  	%rd1093, %rd1087, %rd1092;
	mov.b64	{%r2311, %r2312}, %rd1093;
	prmt.b32 	%r2313, %r2311, %r2312, %r1552;
	prmt.b32 	%r2314, %r2311, %r2312, %r1551;
	mov.b64	%rd1094, {%r2314, %r2313};
	add.s64 	%rd1095, %rd1094, %rd1088;
	xor.b64  	%rd1096, %rd1095, %rd1090;
	mov.b64	{%r614, %r615}, %rd1096;
	// inline asm
	shf.l.wrap.b32 %r609, %r615, %r614, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r613, %r614, %r615, %r1536;
	// inline asm
	mov.b64	%rd1097, {%r609, %r613};
	add.s64 	%rd1098, %rd1050, %rd16;
	add.s64 	%rd1099, %rd1098, %rd1069;
	xor.b64  	%rd1100, %rd1099, %rd1038;
	mov.b64	{%r2315, %r2316}, %rd1100;
	mov.b64	%rd1101, {%r2316, %r2315};
	add.s64 	%rd1102, %rd1101, %rd1081;
	xor.b64  	%rd1103, %rd1102, %rd1069;
	mov.b64	{%r2317, %r2318}, %rd1103;
	prmt.b32 	%r2319, %r2317, %r2318, %r1546;
	prmt.b32 	%r2320, %r2317, %r2318, %r1545;
	mov.b64	%rd1104, {%r2320, %r2319};
	add.s64 	%rd1105, %rd1099, %rd21;
	add.s64 	%rd1106, %rd1105, %rd1104;
	xor.b64  	%rd1107, %rd1106, %rd1101;
	mov.b64	{%r2321, %r2322}, %rd1107;
	prmt.b32 	%r2323, %r2321, %r2322, %r1552;
	prmt.b32 	%r2324, %r2321, %r2322, %r1551;
	mov.b64	%rd1108, {%r2324, %r2323};
	add.s64 	%rd1109, %rd1108, %rd1102;
	xor.b64  	%rd1110, %rd1109, %rd1104;
	mov.b64	{%r622, %r623}, %rd1110;
	// inline asm
	shf.l.wrap.b32 %r617, %r623, %r622, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r621, %r622, %r623, %r1536;
	// inline asm
	mov.b64	%rd1111, {%r617, %r621};
	add.s64 	%rd1112, %rd1064, %rd10;
	add.s64 	%rd1113, %rd1112, %rd1083;
	xor.b64  	%rd1114, %rd1113, %rd1052;
	mov.b64	{%r2325, %r2326}, %rd1114;
	mov.b64	%rd1115, {%r2326, %r2325};
	add.s64 	%rd1116, %rd1115, %rd1039;
	xor.b64  	%rd1117, %rd1116, %rd1083;
	mov.b64	{%r2327, %r2328}, %rd1117;
	prmt.b32 	%r2329, %r2327, %r2328, %r1546;
	prmt.b32 	%r2330, %r2327, %r2328, %r1545;
	mov.b64	%rd1118, {%r2330, %r2329};
	add.s64 	%rd1119, %rd1113, %rd19;
	add.s64 	%rd1120, %rd1119, %rd1118;
	xor.b64  	%rd1121, %rd1120, %rd1115;
	mov.b64	{%r2331, %r2332}, %rd1121;
	prmt.b32 	%r2333, %r2331, %r2332, %r1552;
	prmt.b32 	%r2334, %r2331, %r2332, %r1551;
	mov.b64	%rd1122, {%r2334, %r2333};
	add.s64 	%rd1123, %rd1122, %rd1116;
	xor.b64  	%rd1124, %rd1123, %rd1118;
	mov.b64	{%r630, %r631}, %rd1124;
	// inline asm
	shf.l.wrap.b32 %r625, %r631, %r630, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r629, %r630, %r631, %r1536;
	// inline asm
	mov.b64	%rd1125, {%r625, %r629};
	add.s64 	%rd1126, %rd1041, %rd20;
	add.s64 	%rd1127, %rd1126, %rd1078;
	xor.b64  	%rd1128, %rd1127, %rd1066;
	mov.b64	{%r2335, %r2336}, %rd1128;
	mov.b64	%rd1129, {%r2336, %r2335};
	add.s64 	%rd1130, %rd1129, %rd1053;
	xor.b64  	%rd1131, %rd1130, %rd1041;
	mov.b64	{%r2337, %r2338}, %rd1131;
	prmt.b32 	%r2339, %r2337, %r2338, %r1546;
	prmt.b32 	%r2340, %r2337, %r2338, %r1545;
	mov.b64	%rd1132, {%r2340, %r2339};
	add.s64 	%rd1133, %rd1127, %rd1;
	add.s64 	%rd1134, %rd1133, %rd1132;
	xor.b64  	%rd1135, %rd1134, %rd1129;
	mov.b64	{%r2341, %r2342}, %rd1135;
	prmt.b32 	%r2343, %r2341, %r2342, %r1552;
	prmt.b32 	%r2344, %r2341, %r2342, %r1551;
	mov.b64	%rd1136, {%r2344, %r2343};
	add.s64 	%rd1137, %rd1136, %rd1130;
	xor.b64  	%rd1138, %rd1137, %rd1132;
	mov.b64	{%r638, %r639}, %rd1138;
	// inline asm
	shf.l.wrap.b32 %r633, %r639, %r638, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r637, %r638, %r639, %r1536;
	// inline asm
	mov.b64	%rd1139, {%r633, %r637};
	add.s64 	%rd1140, %rd1092, %rd1;
	add.s64 	%rd1141, %rd1140, %rd1139;
	xor.b64  	%rd1142, %rd1141, %rd1108;
	mov.b64	{%r2345, %r2346}, %rd1142;
	mov.b64	%rd1143, {%r2346, %r2345};
	add.s64 	%rd1144, %rd1143, %rd1123;
	xor.b64  	%rd1145, %rd1144, %rd1139;
	mov.b64	{%r2347, %r2348}, %rd1145;
	prmt.b32 	%r2349, %r2347, %r2348, %r1546;
	prmt.b32 	%r2350, %r2347, %r2348, %r1545;
	mov.b64	%rd1146, {%r2350, %r2349};
	add.s64 	%rd1147, %rd1141, %rd29;
	add.s64 	%rd1148, %rd1147, %rd1146;
	xor.b64  	%rd1149, %rd1143, %rd1148;
	mov.b64	{%r2351, %r2352}, %rd1149;
	prmt.b32 	%r2353, %r2351, %r2352, %r1552;
	prmt.b32 	%r2354, %r2351, %r2352, %r1551;
	mov.b64	%rd1150, {%r2354, %r2353};
	add.s64 	%rd1151, %rd1144, %rd1150;
	xor.b64  	%rd1152, %rd1151, %rd1146;
	mov.b64	{%r646, %r647}, %rd1152;
	// inline asm
	shf.l.wrap.b32 %r641, %r647, %r646, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r645, %r646, %r647, %r1536;
	// inline asm
	mov.b64	%rd1153, {%r641, %r645};
	add.s64 	%rd1154, %rd1097, %rd9;
	add.s64 	%rd1155, %rd1154, %rd1106;
	xor.b64  	%rd1156, %rd1122, %rd1155;
	mov.b64	{%r2355, %r2356}, %rd1156;
	mov.b64	%rd1157, {%r2356, %r2355};
	add.s64 	%rd1158, %rd1137, %rd1157;
	xor.b64  	%rd1159, %rd1158, %rd1097;
	mov.b64	{%r2357, %r2358}, %rd1159;
	prmt.b32 	%r2359, %r2357, %r2358, %r1546;
	prmt.b32 	%r2360, %r2357, %r2358, %r1545;
	mov.b64	%rd1160, {%r2360, %r2359};
	add.s64 	%rd1161, %rd1155, %rd10;
	add.s64 	%rd1162, %rd1161, %rd1160;
	xor.b64  	%rd1163, %rd1162, %rd1157;
	mov.b64	{%r2361, %r2362}, %rd1163;
	prmt.b32 	%r2363, %r2361, %r2362, %r1552;
	prmt.b32 	%r2364, %r2361, %r2362, %r1551;
	mov.b64	%rd1164, {%r2364, %r2363};
	add.s64 	%rd1165, %rd1164, %rd1158;
	xor.b64  	%rd1166, %rd1165, %rd1160;
	mov.b64	{%r654, %r655}, %rd1166;
	// inline asm
	shf.l.wrap.b32 %r649, %r655, %r654, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r653, %r654, %r655, %r1536;
	// inline asm
	mov.b64	%rd1167, {%r649, %r653};
	add.s64 	%rd1168, %rd1111, %rd11;
	add.s64 	%rd1169, %rd1168, %rd1120;
	xor.b64  	%rd1170, %rd1136, %rd1169;
	mov.b64	{%r2365, %r2366}, %rd1170;
	mov.b64	%rd1171, {%r2366, %r2365};
	add.s64 	%rd1172, %rd1171, %rd1095;
	xor.b64  	%rd1173, %rd1172, %rd1111;
	mov.b64	{%r2367, %r2368}, %rd1173;
	prmt.b32 	%r2369, %r2367, %r2368, %r1546;
	prmt.b32 	%r2370, %r2367, %r2368, %r1545;
	mov.b64	%rd1174, {%r2370, %r2369};
	add.s64 	%rd1175, %rd1169, %rd12;
	add.s64 	%rd1176, %rd1175, %rd1174;
	xor.b64  	%rd1177, %rd1176, %rd1171;
	mov.b64	{%r2371, %r2372}, %rd1177;
	prmt.b32 	%r2373, %r2371, %r2372, %r1552;
	prmt.b32 	%r2374, %r2371, %r2372, %r1551;
	mov.b64	%rd1178, {%r2374, %r2373};
	add.s64 	%rd1179, %rd1178, %rd1172;
	xor.b64  	%rd1180, %rd1179, %rd1174;
	mov.b64	{%r662, %r663}, %rd1180;
	// inline asm
	shf.l.wrap.b32 %r657, %r663, %r662, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r661, %r662, %r663, %r1536;
	// inline asm
	mov.b64	%rd1181, {%r657, %r661};
	add.s64 	%rd1182, %rd1125, %rd13;
	add.s64 	%rd1183, %rd1182, %rd1134;
	xor.b64  	%rd1184, %rd1183, %rd1094;
	mov.b64	{%r2375, %r2376}, %rd1184;
	mov.b64	%rd1185, {%r2376, %r2375};
	add.s64 	%rd1186, %rd1185, %rd1109;
	xor.b64  	%rd1187, %rd1186, %rd1125;
	mov.b64	{%r2377, %r2378}, %rd1187;
	prmt.b32 	%r2379, %r2377, %r2378, %r1546;
	prmt.b32 	%r2380, %r2377, %r2378, %r1545;
	mov.b64	%rd1188, {%r2380, %r2379};
	add.s64 	%rd1189, %rd1183, %rd14;
	add.s64 	%rd1190, %rd1189, %rd1188;
	xor.b64  	%rd1191, %rd1190, %rd1185;
	mov.b64	{%r2381, %r2382}, %rd1191;
	prmt.b32 	%r2383, %r2381, %r2382, %r1552;
	prmt.b32 	%r2384, %r2381, %r2382, %r1551;
	mov.b64	%rd1192, {%r2384, %r2383};
	add.s64 	%rd1193, %rd1192, %rd1186;
	xor.b64  	%rd1194, %rd1193, %rd1188;
	mov.b64	{%r670, %r671}, %rd1194;
	// inline asm
	shf.l.wrap.b32 %r665, %r671, %r670, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r669, %r670, %r671, %r1536;
	// inline asm
	mov.b64	%rd1195, {%r665, %r669};
	add.s64 	%rd1196, %rd1148, %rd15;
	add.s64 	%rd1197, %rd1196, %rd1167;
	xor.b64  	%rd1198, %rd1192, %rd1197;
	mov.b64	{%r2385, %r2386}, %rd1198;
	mov.b64	%rd1199, {%r2386, %r2385};
	add.s64 	%rd1200, %rd1199, %rd1179;
	xor.b64  	%rd1201, %rd1200, %rd1167;
	mov.b64	{%r2387, %r2388}, %rd1201;
	prmt.b32 	%r2389, %r2387, %r2388, %r1546;
	prmt.b32 	%r2390, %r2387, %r2388, %r1545;
	mov.b64	%rd1202, {%r2390, %r2389};
	add.s64 	%rd1203, %rd1197, %rd16;
	add.s64 	%rd1204, %rd1203, %rd1202;
	xor.b64  	%rd1205, %rd1199, %rd1204;
	mov.b64	{%r2391, %r2392}, %rd1205;
	prmt.b32 	%r2393, %r2391, %r2392, %r1552;
	prmt.b32 	%r2394, %r2391, %r2392, %r1551;
	mov.b64	%rd1206, {%r2394, %r2393};
	add.s64 	%rd1207, %rd1206, %rd1200;
	xor.b64  	%rd1208, %rd1207, %rd1202;
	mov.b64	{%r678, %r679}, %rd1208;
	// inline asm
	shf.l.wrap.b32 %r673, %r679, %r678, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r677, %r678, %r679, %r1536;
	// inline asm
	mov.b64	%rd1209, {%r673, %r677};
	add.s64 	%rd1210, %rd1162, %rd17;
	add.s64 	%rd1211, %rd1210, %rd1181;
	xor.b64  	%rd1212, %rd1211, %rd1150;
	mov.b64	{%r2395, %r2396}, %rd1212;
	mov.b64	%rd1213, {%r2396, %r2395};
	add.s64 	%rd1214, %rd1213, %rd1193;
	xor.b64  	%rd1215, %rd1214, %rd1181;
	mov.b64	{%r2397, %r2398}, %rd1215;
	prmt.b32 	%r2399, %r2397, %r2398, %r1546;
	prmt.b32 	%r2400, %r2397, %r2398, %r1545;
	mov.b64	%rd1216, {%r2400, %r2399};
	add.s64 	%rd1217, %rd1211, %rd18;
	add.s64 	%rd1218, %rd1217, %rd1216;
	xor.b64  	%rd1219, %rd1218, %rd1213;
	mov.b64	{%r2401, %r2402}, %rd1219;
	prmt.b32 	%r2403, %r2401, %r2402, %r1552;
	prmt.b32 	%r2404, %r2401, %r2402, %r1551;
	mov.b64	%rd1220, {%r2404, %r2403};
	add.s64 	%rd1221, %rd1220, %rd1214;
	xor.b64  	%rd1222, %rd1221, %rd1216;
	mov.b64	{%r686, %r687}, %rd1222;
	// inline asm
	shf.l.wrap.b32 %r681, %r687, %r686, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r685, %r686, %r687, %r1536;
	// inline asm
	mov.b64	%rd1223, {%r681, %r685};
	add.s64 	%rd1224, %rd1176, %rd19;
	add.s64 	%rd1225, %rd1224, %rd1195;
	xor.b64  	%rd1226, %rd1225, %rd1164;
	mov.b64	{%r2405, %r2406}, %rd1226;
	mov.b64	%rd1227, {%r2406, %r2405};
	add.s64 	%rd1228, %rd1227, %rd1151;
	xor.b64  	%rd1229, %rd1228, %rd1195;
	mov.b64	{%r2407, %r2408}, %rd1229;
	prmt.b32 	%r2409, %r2407, %r2408, %r1546;
	prmt.b32 	%r2410, %r2407, %r2408, %r1545;
	mov.b64	%rd1230, {%r2410, %r2409};
	add.s64 	%rd1231, %rd1225, %rd20;
	add.s64 	%rd1232, %rd1231, %rd1230;
	xor.b64  	%rd1233, %rd1232, %rd1227;
	mov.b64	{%r2411, %r2412}, %rd1233;
	prmt.b32 	%r2413, %r2411, %r2412, %r1552;
	prmt.b32 	%r2414, %r2411, %r2412, %r1551;
	mov.b64	%rd1234, {%r2414, %r2413};
	add.s64 	%rd1235, %rd1234, %rd1228;
	xor.b64  	%rd1236, %rd1235, %rd1230;
	mov.b64	{%r694, %r695}, %rd1236;
	// inline asm
	shf.l.wrap.b32 %r689, %r695, %r694, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r693, %r694, %r695, %r1536;
	// inline asm
	mov.b64	%rd1237, {%r689, %r693};
	add.s64 	%rd1238, %rd1153, %rd21;
	add.s64 	%rd1239, %rd1238, %rd1190;
	xor.b64  	%rd1240, %rd1239, %rd1178;
	mov.b64	{%r2415, %r2416}, %rd1240;
	mov.b64	%rd1241, {%r2416, %r2415};
	add.s64 	%rd1242, %rd1241, %rd1165;
	xor.b64  	%rd1243, %rd1242, %rd1153;
	mov.b64	{%r2417, %r2418}, %rd1243;
	prmt.b32 	%r2419, %r2417, %r2418, %r1546;
	prmt.b32 	%r2420, %r2417, %r2418, %r1545;
	mov.b64	%rd1244, {%r2420, %r2419};
	add.s64 	%rd1245, %rd1239, %rd22;
	add.s64 	%rd1246, %rd1245, %rd1244;
	xor.b64  	%rd1247, %rd1246, %rd1241;
	mov.b64	{%r2421, %r2422}, %rd1247;
	prmt.b32 	%r2423, %r2421, %r2422, %r1552;
	prmt.b32 	%r2424, %r2421, %r2422, %r1551;
	mov.b64	%rd1248, {%r2424, %r2423};
	add.s64 	%rd1249, %rd1248, %rd1242;
	xor.b64  	%rd1250, %rd1249, %rd1244;
	mov.b64	{%r702, %r703}, %rd1250;
	// inline asm
	shf.l.wrap.b32 %r697, %r703, %r702, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r701, %r702, %r703, %r1536;
	// inline asm
	mov.b64	%rd1251, {%r697, %r701};
	add.s64 	%rd1252, %rd1204, %rd21;
	add.s64 	%rd1253, %rd1252, %rd1251;
	xor.b64  	%rd1254, %rd1253, %rd1220;
	mov.b64	{%r2425, %r2426}, %rd1254;
	mov.b64	%rd1255, {%r2426, %r2425};
	add.s64 	%rd1256, %rd1255, %rd1235;
	xor.b64  	%rd1257, %rd1256, %rd1251;
	mov.b64	{%r2427, %r2428}, %rd1257;
	prmt.b32 	%r2429, %r2427, %r2428, %r1546;
	prmt.b32 	%r2430, %r2427, %r2428, %r1545;
	mov.b64	%rd1258, {%r2430, %r2429};
	add.s64 	%rd1259, %rd1253, %rd17;
	add.s64 	%rd1260, %rd1259, %rd1258;
	xor.b64  	%rd1261, %rd1255, %rd1260;
	mov.b64	{%r2431, %r2432}, %rd1261;
	prmt.b32 	%r2433, %r2431, %r2432, %r1552;
	prmt.b32 	%r2434, %r2431, %r2432, %r1551;
	mov.b64	%rd1262, {%r2434, %r2433};
	add.s64 	%rd1263, %rd1256, %rd1262;
	xor.b64  	%rd1264, %rd1263, %rd1258;
	mov.b64	{%r710, %r711}, %rd1264;
	// inline asm
	shf.l.wrap.b32 %r705, %r711, %r710, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r709, %r710, %r711, %r1536;
	// inline asm
	mov.b64	%rd1265, {%r705, %r709};
	add.s64 	%rd1266, %rd1209, %rd11;
	add.s64 	%rd1267, %rd1266, %rd1218;
	xor.b64  	%rd1268, %rd1234, %rd1267;
	mov.b64	{%r2435, %r2436}, %rd1268;
	mov.b64	%rd1269, {%r2436, %r2435};
	add.s64 	%rd1270, %rd1249, %rd1269;
	xor.b64  	%rd1271, %rd1270, %rd1209;
	mov.b64	{%r2437, %r2438}, %rd1271;
	prmt.b32 	%r2439, %r2437, %r2438, %r1546;
	prmt.b32 	%r2440, %r2437, %r2438, %r1545;
	mov.b64	%rd1272, {%r2440, %r2439};
	add.s64 	%rd1273, %rd1267, %rd15;
	add.s64 	%rd1274, %rd1273, %rd1272;
	xor.b64  	%rd1275, %rd1274, %rd1269;
	mov.b64	{%r2441, %r2442}, %rd1275;
	prmt.b32 	%r2443, %r2441, %r2442, %r1552;
	prmt.b32 	%r2444, %r2441, %r2442, %r1551;
	mov.b64	%rd1276, {%r2444, %r2443};
	add.s64 	%rd1277, %rd1276, %rd1270;
	xor.b64  	%rd1278, %rd1277, %rd1272;
	mov.b64	{%r718, %r719}, %rd1278;
	// inline asm
	shf.l.wrap.b32 %r713, %r719, %r718, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r717, %r718, %r719, %r1536;
	// inline asm
	mov.b64	%rd1279, {%r713, %r717};
	add.s64 	%rd1280, %rd1223, %rd16;
	add.s64 	%rd1281, %rd1280, %rd1232;
	xor.b64  	%rd1282, %rd1248, %rd1281;
	mov.b64	{%r2445, %r2446}, %rd1282;
	mov.b64	%rd1283, {%r2446, %r2445};
	add.s64 	%rd1284, %rd1283, %rd1207;
	xor.b64  	%rd1285, %rd1284, %rd1223;
	mov.b64	{%r2447, %r2448}, %rd1285;
	prmt.b32 	%r2449, %r2447, %r2448, %r1546;
	prmt.b32 	%r2450, %r2447, %r2448, %r1545;
	mov.b64	%rd1286, {%r2450, %r2449};
	add.s64 	%rd1287, %rd1281, %rd22;
	add.s64 	%rd1288, %rd1287, %rd1286;
	xor.b64  	%rd1289, %rd1288, %rd1283;
	mov.b64	{%r2451, %r2452}, %rd1289;
	prmt.b32 	%r2453, %r2451, %r2452, %r1552;
	prmt.b32 	%r2454, %r2451, %r2452, %r1551;
	mov.b64	%rd1290, {%r2454, %r2453};
	add.s64 	%rd1291, %rd1290, %rd1284;
	xor.b64  	%rd1292, %rd1291, %rd1286;
	mov.b64	{%r726, %r727}, %rd1292;
	// inline asm
	shf.l.wrap.b32 %r721, %r727, %r726, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r725, %r726, %r727, %r1536;
	// inline asm
	mov.b64	%rd1293, {%r721, %r725};
	add.s64 	%rd1294, %rd1237, %rd20;
	add.s64 	%rd1295, %rd1294, %rd1246;
	xor.b64  	%rd1296, %rd1295, %rd1206;
	mov.b64	{%r2455, %r2456}, %rd1296;
	mov.b64	%rd1297, {%r2456, %r2455};
	add.s64 	%rd1298, %rd1297, %rd1221;
	xor.b64  	%rd1299, %rd1298, %rd1237;
	mov.b64	{%r2457, %r2458}, %rd1299;
	prmt.b32 	%r2459, %r2457, %r2458, %r1546;
	prmt.b32 	%r2460, %r2457, %r2458, %r1545;
	mov.b64	%rd1300, {%r2460, %r2459};
	add.s64 	%rd1301, %rd1295, %rd13;
	add.s64 	%rd1302, %rd1301, %rd1300;
	xor.b64  	%rd1303, %rd1302, %rd1297;
	mov.b64	{%r2461, %r2462}, %rd1303;
	prmt.b32 	%r2463, %r2461, %r2462, %r1552;
	prmt.b32 	%r2464, %r2461, %r2462, %r1551;
	mov.b64	%rd1304, {%r2464, %r2463};
	add.s64 	%rd1305, %rd1304, %rd1298;
	xor.b64  	%rd1306, %rd1305, %rd1300;
	mov.b64	{%r734, %r735}, %rd1306;
	// inline asm
	shf.l.wrap.b32 %r729, %r735, %r734, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r733, %r734, %r735, %r1536;
	// inline asm
	mov.b64	%rd1307, {%r729, %r733};
	add.s64 	%rd1308, %rd1260, %rd29;
	add.s64 	%rd1309, %rd1308, %rd1279;
	xor.b64  	%rd1310, %rd1304, %rd1309;
	mov.b64	{%r2465, %r2466}, %rd1310;
	mov.b64	%rd1311, {%r2466, %r2465};
	add.s64 	%rd1312, %rd1311, %rd1291;
	xor.b64  	%rd1313, %rd1312, %rd1279;
	mov.b64	{%r2467, %r2468}, %rd1313;
	prmt.b32 	%r2469, %r2467, %r2468, %r1546;
	prmt.b32 	%r2470, %r2467, %r2468, %r1545;
	mov.b64	%rd1314, {%r2470, %r2469};
	add.s64 	%rd1315, %rd1309, %rd19;
	add.s64 	%rd1316, %rd1315, %rd1314;
	xor.b64  	%rd1317, %rd1311, %rd1316;
	mov.b64	{%r2471, %r2472}, %rd1317;
	prmt.b32 	%r2473, %r2471, %r2472, %r1552;
	prmt.b32 	%r2474, %r2471, %r2472, %r1551;
	mov.b64	%rd1318, {%r2474, %r2473};
	add.s64 	%rd1319, %rd1318, %rd1312;
	xor.b64  	%rd1320, %rd1319, %rd1314;
	mov.b64	{%r742, %r743}, %rd1320;
	// inline asm
	shf.l.wrap.b32 %r737, %r743, %r742, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r741, %r742, %r743, %r1536;
	// inline asm
	mov.b64	%rd1321, {%r737, %r741};
	add.s64 	%rd1322, %rd1274, %rd1;
	add.s64 	%rd1323, %rd1322, %rd1293;
	xor.b64  	%rd1324, %rd1323, %rd1262;
	mov.b64	{%r2475, %r2476}, %rd1324;
	mov.b64	%rd1325, {%r2476, %r2475};
	add.s64 	%rd1326, %rd1325, %rd1305;
	xor.b64  	%rd1327, %rd1326, %rd1293;
	mov.b64	{%r2477, %r2478}, %rd1327;
	prmt.b32 	%r2479, %r2477, %r2478, %r1546;
	prmt.b32 	%r2480, %r2477, %r2478, %r1545;
	mov.b64	%rd1328, {%r2480, %r2479};
	add.s64 	%rd1329, %rd1323, %rd9;
	add.s64 	%rd1330, %rd1329, %rd1328;
	xor.b64  	%rd1331, %rd1330, %rd1325;
	mov.b64	{%r2481, %r2482}, %rd1331;
	prmt.b32 	%r2483, %r2481, %r2482, %r1552;
	prmt.b32 	%r2484, %r2481, %r2482, %r1551;
	mov.b64	%rd1332, {%r2484, %r2483};
	add.s64 	%rd1333, %rd1332, %rd1326;
	xor.b64  	%rd1334, %rd1333, %rd1328;
	mov.b64	{%r750, %r751}, %rd1334;
	// inline asm
	shf.l.wrap.b32 %r745, %r751, %r750, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r749, %r750, %r751, %r1536;
	// inline asm
	mov.b64	%rd1335, {%r745, %r749};
	add.s64 	%rd1336, %rd1288, %rd18;
	add.s64 	%rd1337, %rd1336, %rd1307;
	xor.b64  	%rd1338, %rd1337, %rd1276;
	mov.b64	{%r2485, %r2486}, %rd1338;
	mov.b64	%rd1339, {%r2486, %r2485};
	add.s64 	%rd1340, %rd1339, %rd1263;
	xor.b64  	%rd1341, %rd1340, %rd1307;
	mov.b64	{%r2487, %r2488}, %rd1341;
	prmt.b32 	%r2489, %r2487, %r2488, %r1546;
	prmt.b32 	%r2490, %r2487, %r2488, %r1545;
	mov.b64	%rd1342, {%r2490, %r2489};
	add.s64 	%rd1343, %rd1337, %rd14;
	add.s64 	%rd1344, %rd1343, %rd1342;
	xor.b64  	%rd1345, %rd1344, %rd1339;
	mov.b64	{%r2491, %r2492}, %rd1345;
	prmt.b32 	%r2493, %r2491, %r2492, %r1552;
	prmt.b32 	%r2494, %r2491, %r2492, %r1551;
	mov.b64	%rd1346, {%r2494, %r2493};
	add.s64 	%rd1347, %rd1346, %rd1340;
	xor.b64  	%rd1348, %rd1347, %rd1342;
	mov.b64	{%r758, %r759}, %rd1348;
	// inline asm
	shf.l.wrap.b32 %r753, %r759, %r758, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r757, %r758, %r759, %r1536;
	// inline asm
	mov.b64	%rd1349, {%r753, %r757};
	add.s64 	%rd1350, %rd1265, %rd12;
	add.s64 	%rd1351, %rd1350, %rd1302;
	xor.b64  	%rd1352, %rd1351, %rd1290;
	mov.b64	{%r2495, %r2496}, %rd1352;
	mov.b64	%rd1353, {%r2496, %r2495};
	add.s64 	%rd1354, %rd1353, %rd1277;
	xor.b64  	%rd1355, %rd1354, %rd1265;
	mov.b64	{%r2497, %r2498}, %rd1355;
	prmt.b32 	%r2499, %r2497, %r2498, %r1546;
	prmt.b32 	%r2500, %r2497, %r2498, %r1545;
	mov.b64	%rd1356, {%r2500, %r2499};
	add.s64 	%rd1357, %rd1351, %rd10;
	add.s64 	%rd1358, %rd1357, %rd1356;
	xor.b64  	%rd1359, %rd1358, %rd1353;
	mov.b64	{%r2501, %r2502}, %rd1359;
	prmt.b32 	%r2503, %r2501, %r2502, %r1552;
	prmt.b32 	%r2504, %r2501, %r2502, %r1551;
	mov.b64	%rd1360, {%r2504, %r2503};
	add.s64 	%rd1361, %rd1360, %rd1354;
	xor.b64  	%rd1362, %rd1361, %rd1356;
	mov.b64	{%r766, %r767}, %rd1362;
	// inline asm
	shf.l.wrap.b32 %r761, %r767, %r766, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r765, %r766, %r767, %r1536;
	// inline asm
	mov.b64	%rd1363, {%r761, %r765};
	xor.b64  	%rd1364, %rd1316, %rd1347;
	xor.b64  	%rd1365, %rd1364, 7640891576939301192;
	xor.b64  	%rd1366, %rd1330, %rd1361;
	xor.b64  	%rd1367, %rd1366, -4942790177534073029;
	xor.b64  	%rd1368, %rd1319, %rd1344;
	xor.b64  	%rd1369, %rd1368, 4354685564936845355;
	xor.b64  	%rd1370, %rd1333, %rd1358;
	xor.b64  	%rd1371, %rd1370, -6534734903238641935;
	xor.b64  	%rd1372, %rd1332, %rd1363;
	xor.b64  	%rd1373, %rd1372, 5840696475078001361;
	xor.b64  	%rd1374, %rd1321, %rd1346;
	xor.b64  	%rd1375, %rd1374, -7276294671716946913;
	xor.b64  	%rd1376, %rd1335, %rd1360;
	xor.b64  	%rd1377, %rd1376, 2270897969802886507;
	xor.b64  	%rd1378, %rd1318, %rd1349;
	xor.b64  	%rd1379, %rd1378, 6620516959819538809;
	ld.global.u64 	%rd1380, [%rd5+144];
	ld.global.u64 	%rd1381, [%rd5+152];
	ld.global.u64 	%rd1382, [%rd5+160];
	ld.global.u64 	%rd1383, [%rd5+168];
	ld.global.u64 	%rd1384, [%rd5+176];
	ld.global.u64 	%rd1385, [%rd5+184];
	ld.global.u64 	%rd1386, [%rd5+192];
	ld.global.u64 	%rd1387, [%rd5+200];
	ld.global.u64 	%rd1388, [%rd5+208];
	ld.global.u64 	%rd1389, [%rd5+216];
	ld.global.u64 	%rd1390, [%rd5+224];
	ld.global.u64 	%rd1391, [%rd5+232];
	ld.global.u64 	%rd1392, [%rd5+240];
	ld.global.u64 	%rd1393, [%rd5+248];
	ld.global.u64 	%rd1394, [%rd5+128];
	add.s64 	%rd1395, %rd1394, %rd1365;
	add.s64 	%rd1396, %rd1395, %rd1373;
	xor.b64  	%rd1397, %rd1396, 5840696475078001617;
	mov.b64	{%r2505, %r2506}, %rd1397;
	mov.b64	%rd1398, {%r2506, %r2505};
	add.s64 	%rd1399, %rd1398, 7640891576956012808;
	xor.b64  	%rd1400, %rd1399, %rd1373;
	mov.b64	{%r2507, %r2508}, %rd1400;
	prmt.b32 	%r2509, %r2507, %r2508, %r1546;
	prmt.b32 	%r2510, %r2507, %r2508, %r1545;
	mov.b64	%rd1401, {%r2510, %r2509};
	ld.global.u64 	%rd1402, [%rd5+136];
	add.s64 	%rd1403, %rd1396, %rd1402;
	add.s64 	%rd1404, %rd1403, %rd1401;
	xor.b64  	%rd1405, %rd1404, %rd1398;
	mov.b64	{%r2511, %r2512}, %rd1405;
	prmt.b32 	%r2513, %r2511, %r2512, %r1552;
	prmt.b32 	%r2514, %r2511, %r2512, %r1551;
	mov.b64	%rd1406, {%r2514, %r2513};
	add.s64 	%rd1407, %rd1406, %rd1399;
	xor.b64  	%rd1408, %rd1407, %rd1401;
	mov.b64	{%r774, %r775}, %rd1408;
	// inline asm
	shf.l.wrap.b32 %r769, %r775, %r774, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r773, %r774, %r775, %r1536;
	// inline asm
	mov.b64	%rd1409, {%r769, %r773};
	add.s64 	%rd1410, %rd1367, %rd1375;
	add.s64 	%rd1411, %rd1410, %rd1380;
	xor.b64  	%rd1412, %rd1411, -7276294671716946913;
	mov.b64	{%r2515, %r2516}, %rd1412;
	mov.b64	%rd1413, {%r2516, %r2515};
	add.s64 	%rd1414, %rd1413, -4942790177534073029;
	xor.b64  	%rd1415, %rd1414, %rd1375;
	mov.b64	{%r2517, %r2518}, %rd1415;
	prmt.b32 	%r2519, %r2517, %r2518, %r1546;
	prmt.b32 	%r2520, %r2517, %r2518, %r1545;
	mov.b64	%rd1416, {%r2520, %r2519};
	add.s64 	%rd1417, %rd1411, %rd1381;
	add.s64 	%rd1418, %rd1417, %rd1416;
	xor.b64  	%rd1419, %rd1418, %rd1413;
	mov.b64	{%r2521, %r2522}, %rd1419;
	prmt.b32 	%r2523, %r2521, %r2522, %r1552;
	prmt.b32 	%r2524, %r2521, %r2522, %r1551;
	mov.b64	%rd1420, {%r2524, %r2523};
	add.s64 	%rd1421, %rd1420, %rd1414;
	xor.b64  	%rd1422, %rd1421, %rd1416;
	mov.b64	{%r782, %r783}, %rd1422;
	// inline asm
	shf.l.wrap.b32 %r777, %r783, %r782, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r781, %r782, %r783, %r1536;
	// inline asm
	mov.b64	%rd1423, {%r777, %r781};
	add.s64 	%rd1424, %rd1377, %rd1369;
	add.s64 	%rd1425, %rd1424, %rd1382;
	xor.b64  	%rd1426, %rd1425, -2270897969802886508;
	mov.b64	{%r2525, %r2526}, %rd1426;
	mov.b64	%rd1427, {%r2526, %r2525};
	add.s64 	%rd1428, %rd1427, 4354685564936845355;
	xor.b64  	%rd1429, %rd1428, %rd1377;
	mov.b64	{%r2527, %r2528}, %rd1429;
	prmt.b32 	%r2529, %r2527, %r2528, %r1546;
	prmt.b32 	%r2530, %r2527, %r2528, %r1545;
	mov.b64	%rd1430, {%r2530, %r2529};
	add.s64 	%rd1431, %rd1383, %rd1425;
	add.s64 	%rd1432, %rd1431, %rd1430;
	xor.b64  	%rd1433, %rd1432, %rd1427;
	mov.b64	{%r2531, %r2532}, %rd1433;
	prmt.b32 	%r2533, %r2531, %r2532, %r1552;
	prmt.b32 	%r2534, %r2531, %r2532, %r1551;
	mov.b64	%rd1434, {%r2534, %r2533};
	add.s64 	%rd1435, %rd1434, %rd1428;
	xor.b64  	%rd1436, %rd1435, %rd1430;
	mov.b64	{%r790, %r791}, %rd1436;
	// inline asm
	shf.l.wrap.b32 %r785, %r791, %r790, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r789, %r790, %r791, %r1536;
	// inline asm
	mov.b64	%rd1437, {%r785, %r789};
	add.s64 	%rd1438, %rd1371, %rd1379;
	add.s64 	%rd1439, %rd1438, %rd1384;
	xor.b64  	%rd1440, %rd1439, 6620516959819538809;
	mov.b64	{%r2535, %r2536}, %rd1440;
	mov.b64	%rd1441, {%r2536, %r2535};
	add.s64 	%rd1442, %rd1441, -6534734903238641935;
	xor.b64  	%rd1443, %rd1442, %rd1379;
	mov.b64	{%r2537, %r2538}, %rd1443;
	prmt.b32 	%r2539, %r2537, %r2538, %r1546;
	prmt.b32 	%r2540, %r2537, %r2538, %r1545;
	mov.b64	%rd1444, {%r2540, %r2539};
	add.s64 	%rd1445, %rd1385, %rd1439;
	add.s64 	%rd1446, %rd1445, %rd1444;
	xor.b64  	%rd1447, %rd1446, %rd1441;
	mov.b64	{%r2541, %r2542}, %rd1447;
	prmt.b32 	%r2543, %r2541, %r2542, %r1552;
	prmt.b32 	%r2544, %r2541, %r2542, %r1551;
	mov.b64	%rd1448, {%r2544, %r2543};
	add.s64 	%rd1449, %rd1448, %rd1442;
	xor.b64  	%rd1450, %rd1449, %rd1444;
	mov.b64	{%r798, %r799}, %rd1450;
	// inline asm
	shf.l.wrap.b32 %r793, %r799, %r798, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r797, %r798, %r799, %r1536;
	// inline asm
	mov.b64	%rd1451, {%r793, %r797};
	add.s64 	%rd1452, %rd1404, %rd1386;
	add.s64 	%rd1453, %rd1452, %rd1423;
	xor.b64  	%rd1454, %rd1448, %rd1453;
	mov.b64	{%r2545, %r2546}, %rd1454;
	mov.b64	%rd1455, {%r2546, %r2545};
	add.s64 	%rd1456, %rd1455, %rd1435;
	xor.b64  	%rd1457, %rd1456, %rd1423;
	mov.b64	{%r2547, %r2548}, %rd1457;
	prmt.b32 	%r2549, %r2547, %r2548, %r1546;
	prmt.b32 	%r2550, %r2547, %r2548, %r1545;
	mov.b64	%rd1458, {%r2550, %r2549};
	add.s64 	%rd1459, %rd1453, %rd1387;
	add.s64 	%rd1460, %rd1459, %rd1458;
	xor.b64  	%rd1461, %rd1455, %rd1460;
	mov.b64	{%r2551, %r2552}, %rd1461;
	prmt.b32 	%r2553, %r2551, %r2552, %r1552;
	prmt.b32 	%r2554, %r2551, %r2552, %r1551;
	mov.b64	%rd1462, {%r2554, %r2553};
	add.s64 	%rd1463, %rd1462, %rd1456;
	xor.b64  	%rd1464, %rd1463, %rd1458;
	mov.b64	{%r806, %r807}, %rd1464;
	// inline asm
	shf.l.wrap.b32 %r801, %r807, %r806, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r805, %r806, %r807, %r1536;
	// inline asm
	mov.b64	%rd1465, {%r801, %r805};
	add.s64 	%rd1466, %rd1418, %rd1388;
	add.s64 	%rd1467, %rd1466, %rd1437;
	xor.b64  	%rd1468, %rd1467, %rd1406;
	mov.b64	{%r2555, %r2556}, %rd1468;
	mov.b64	%rd1469, {%r2556, %r2555};
	add.s64 	%rd1470, %rd1469, %rd1449;
	xor.b64  	%rd1471, %rd1470, %rd1437;
	mov.b64	{%r2557, %r2558}, %rd1471;
	prmt.b32 	%r2559, %r2557, %r2558, %r1546;
	prmt.b32 	%r2560, %r2557, %r2558, %r1545;
	mov.b64	%rd1472, {%r2560, %r2559};
	add.s64 	%rd1473, %rd1467, %rd1389;
	add.s64 	%rd1474, %rd1473, %rd1472;
	xor.b64  	%rd1475, %rd1474, %rd1469;
	mov.b64	{%r2561, %r2562}, %rd1475;
	prmt.b32 	%r2563, %r2561, %r2562, %r1552;
	prmt.b32 	%r2564, %r2561, %r2562, %r1551;
	mov.b64	%rd1476, {%r2564, %r2563};
	add.s64 	%rd1477, %rd1476, %rd1470;
	xor.b64  	%rd1478, %rd1477, %rd1472;
	mov.b64	{%r814, %r815}, %rd1478;
	// inline asm
	shf.l.wrap.b32 %r809, %r815, %r814, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r813, %r814, %r815, %r1536;
	// inline asm
	mov.b64	%rd1479, {%r809, %r813};
	add.s64 	%rd1480, %rd1432, %rd1390;
	add.s64 	%rd1481, %rd1480, %rd1451;
	xor.b64  	%rd1482, %rd1481, %rd1420;
	mov.b64	{%r2565, %r2566}, %rd1482;
	mov.b64	%rd1483, {%r2566, %r2565};
	add.s64 	%rd1484, %rd1483, %rd1407;
	xor.b64  	%rd1485, %rd1484, %rd1451;
	mov.b64	{%r2567, %r2568}, %rd1485;
	prmt.b32 	%r2569, %r2567, %r2568, %r1546;
	prmt.b32 	%r2570, %r2567, %r2568, %r1545;
	mov.b64	%rd1486, {%r2570, %r2569};
	add.s64 	%rd1487, %rd1481, %rd1391;
	add.s64 	%rd1488, %rd1487, %rd1486;
	xor.b64  	%rd1489, %rd1488, %rd1483;
	mov.b64	{%r2571, %r2572}, %rd1489;
	prmt.b32 	%r2573, %r2571, %r2572, %r1552;
	prmt.b32 	%r2574, %r2571, %r2572, %r1551;
	mov.b64	%rd1490, {%r2574, %r2573};
	add.s64 	%rd1491, %rd1490, %rd1484;
	xor.b64  	%rd1492, %rd1491, %rd1486;
	mov.b64	{%r822, %r823}, %rd1492;
	// inline asm
	shf.l.wrap.b32 %r817, %r823, %r822, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r821, %r822, %r823, %r1536;
	// inline asm
	mov.b64	%rd1493, {%r817, %r821};
	add.s64 	%rd1494, %rd1409, %rd1392;
	add.s64 	%rd1495, %rd1494, %rd1446;
	xor.b64  	%rd1496, %rd1495, %rd1434;
	mov.b64	{%r2575, %r2576}, %rd1496;
	mov.b64	%rd1497, {%r2576, %r2575};
	add.s64 	%rd1498, %rd1497, %rd1421;
	xor.b64  	%rd1499, %rd1498, %rd1409;
	mov.b64	{%r2577, %r2578}, %rd1499;
	prmt.b32 	%r2579, %r2577, %r2578, %r1546;
	prmt.b32 	%r2580, %r2577, %r2578, %r1545;
	mov.b64	%rd1500, {%r2580, %r2579};
	add.s64 	%rd1501, %rd1495, %rd1393;
	add.s64 	%rd1502, %rd1501, %rd1500;
	xor.b64  	%rd1503, %rd1502, %rd1497;
	mov.b64	{%r2581, %r2582}, %rd1503;
	prmt.b32 	%r2583, %r2581, %r2582, %r1552;
	prmt.b32 	%r2584, %r2581, %r2582, %r1551;
	mov.b64	%rd1504, {%r2584, %r2583};
	add.s64 	%rd1505, %rd1504, %rd1498;
	xor.b64  	%rd1506, %rd1505, %rd1500;
	mov.b64	{%r830, %r831}, %rd1506;
	// inline asm
	shf.l.wrap.b32 %r825, %r831, %r830, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r829, %r830, %r831, %r1536;
	// inline asm
	mov.b64	%rd1507, {%r825, %r829};
	add.s64 	%rd1508, %rd1460, %rd1392;
	add.s64 	%rd1509, %rd1508, %rd1507;
	xor.b64  	%rd1510, %rd1509, %rd1476;
	mov.b64	{%r2585, %r2586}, %rd1510;
	mov.b64	%rd1511, {%r2586, %r2585};
	add.s64 	%rd1512, %rd1511, %rd1491;
	xor.b64  	%rd1513, %rd1512, %rd1507;
	mov.b64	{%r2587, %r2588}, %rd1513;
	prmt.b32 	%r2589, %r2587, %r2588, %r1546;
	prmt.b32 	%r2590, %r2587, %r2588, %r1545;
	mov.b64	%rd1514, {%r2590, %r2589};
	add.s64 	%rd1515, %rd1509, %rd1388;
	add.s64 	%rd1516, %rd1515, %rd1514;
	xor.b64  	%rd1517, %rd1511, %rd1516;
	mov.b64	{%r2591, %r2592}, %rd1517;
	prmt.b32 	%r2593, %r2591, %r2592, %r1552;
	prmt.b32 	%r2594, %r2591, %r2592, %r1551;
	mov.b64	%rd1518, {%r2594, %r2593};
	add.s64 	%rd1519, %rd1512, %rd1518;
	xor.b64  	%rd1520, %rd1519, %rd1514;
	mov.b64	{%r838, %r839}, %rd1520;
	// inline asm
	shf.l.wrap.b32 %r833, %r839, %r838, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r837, %r838, %r839, %r1536;
	// inline asm
	mov.b64	%rd1521, {%r833, %r837};
	add.s64 	%rd1522, %rd1465, %rd1382;
	add.s64 	%rd1523, %rd1522, %rd1474;
	xor.b64  	%rd1524, %rd1490, %rd1523;
	mov.b64	{%r2595, %r2596}, %rd1524;
	mov.b64	%rd1525, {%r2596, %r2595};
	add.s64 	%rd1526, %rd1505, %rd1525;
	xor.b64  	%rd1527, %rd1526, %rd1465;
	mov.b64	{%r2597, %r2598}, %rd1527;
	prmt.b32 	%r2599, %r2597, %r2598, %r1546;
	prmt.b32 	%r2600, %r2597, %r2598, %r1545;
	mov.b64	%rd1528, {%r2600, %r2599};
	add.s64 	%rd1529, %rd1523, %rd1386;
	add.s64 	%rd1530, %rd1529, %rd1528;
	xor.b64  	%rd1531, %rd1530, %rd1525;
	mov.b64	{%r2601, %r2602}, %rd1531;
	prmt.b32 	%r2603, %r2601, %r2602, %r1552;
	prmt.b32 	%r2604, %r2601, %r2602, %r1551;
	mov.b64	%rd1532, {%r2604, %r2603};
	add.s64 	%rd1533, %rd1532, %rd1526;
	xor.b64  	%rd1534, %rd1533, %rd1528;
	mov.b64	{%r846, %r847}, %rd1534;
	// inline asm
	shf.l.wrap.b32 %r841, %r847, %r846, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r845, %r846, %r847, %r1536;
	// inline asm
	mov.b64	%rd1535, {%r841, %r845};
	add.s64 	%rd1536, %rd1479, %rd1387;
	add.s64 	%rd1537, %rd1536, %rd1488;
	xor.b64  	%rd1538, %rd1504, %rd1537;
	mov.b64	{%r2605, %r2606}, %rd1538;
	mov.b64	%rd1539, {%r2606, %r2605};
	add.s64 	%rd1540, %rd1539, %rd1463;
	xor.b64  	%rd1541, %rd1540, %rd1479;
	mov.b64	{%r2607, %r2608}, %rd1541;
	prmt.b32 	%r2609, %r2607, %r2608, %r1546;
	prmt.b32 	%r2610, %r2607, %r2608, %r1545;
	mov.b64	%rd1542, {%r2610, %r2609};
	add.s64 	%rd1543, %rd1537, %rd1393;
	add.s64 	%rd1544, %rd1543, %rd1542;
	xor.b64  	%rd1545, %rd1544, %rd1539;
	mov.b64	{%r2611, %r2612}, %rd1545;
	prmt.b32 	%r2613, %r2611, %r2612, %r1552;
	prmt.b32 	%r2614, %r2611, %r2612, %r1551;
	mov.b64	%rd1546, {%r2614, %r2613};
	add.s64 	%rd1547, %rd1546, %rd1540;
	xor.b64  	%rd1548, %rd1547, %rd1542;
	mov.b64	{%r854, %r855}, %rd1548;
	// inline asm
	shf.l.wrap.b32 %r849, %r855, %r854, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r853, %r854, %r855, %r1536;
	// inline asm
	mov.b64	%rd1549, {%r849, %r853};
	add.s64 	%rd1550, %rd1493, %rd1391;
	add.s64 	%rd1551, %rd1550, %rd1502;
	xor.b64  	%rd1552, %rd1551, %rd1462;
	mov.b64	{%r2615, %r2616}, %rd1552;
	mov.b64	%rd1553, {%r2616, %r2615};
	add.s64 	%rd1554, %rd1553, %rd1477;
	xor.b64  	%rd1555, %rd1554, %rd1493;
	mov.b64	{%r2617, %r2618}, %rd1555;
	prmt.b32 	%r2619, %r2617, %r2618, %r1546;
	prmt.b32 	%r2620, %r2617, %r2618, %r1545;
	mov.b64	%rd1556, {%r2620, %r2619};
	add.s64 	%rd1557, %rd1551, %rd1384;
	add.s64 	%rd1558, %rd1557, %rd1556;
	xor.b64  	%rd1559, %rd1558, %rd1553;
	mov.b64	{%r2621, %r2622}, %rd1559;
	prmt.b32 	%r2623, %r2621, %r2622, %r1552;
	prmt.b32 	%r2624, %r2621, %r2622, %r1551;
	mov.b64	%rd1560, {%r2624, %r2623};
	add.s64 	%rd1561, %rd1560, %rd1554;
	xor.b64  	%rd1562, %rd1561, %rd1556;
	mov.b64	{%r862, %r863}, %rd1562;
	// inline asm
	shf.l.wrap.b32 %r857, %r863, %r862, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r861, %r862, %r863, %r1536;
	// inline asm
	mov.b64	%rd1563, {%r857, %r861};
	add.s64 	%rd1564, %rd1516, %rd1402;
	add.s64 	%rd1565, %rd1564, %rd1535;
	xor.b64  	%rd1566, %rd1560, %rd1565;
	mov.b64	{%r2625, %r2626}, %rd1566;
	mov.b64	%rd1567, {%r2626, %r2625};
	add.s64 	%rd1568, %rd1567, %rd1547;
	xor.b64  	%rd1569, %rd1568, %rd1535;
	mov.b64	{%r2627, %r2628}, %rd1569;
	prmt.b32 	%r2629, %r2627, %r2628, %r1546;
	prmt.b32 	%r2630, %r2627, %r2628, %r1545;
	mov.b64	%rd1570, {%r2630, %r2629};
	add.s64 	%rd1571, %rd1565, %rd1390;
	add.s64 	%rd1572, %rd1571, %rd1570;
	xor.b64  	%rd1573, %rd1567, %rd1572;
	mov.b64	{%r2631, %r2632}, %rd1573;
	prmt.b32 	%r2633, %r2631, %r2632, %r1552;
	prmt.b32 	%r2634, %r2631, %r2632, %r1551;
	mov.b64	%rd1574, {%r2634, %r2633};
	add.s64 	%rd1575, %rd1574, %rd1568;
	xor.b64  	%rd1576, %rd1575, %rd1570;
	mov.b64	{%r870, %r871}, %rd1576;
	// inline asm
	shf.l.wrap.b32 %r865, %r871, %r870, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r869, %r870, %r871, %r1536;
	// inline asm
	mov.b64	%rd1577, {%r865, %r869};
	add.s64 	%rd1578, %rd1530, %rd1394;
	add.s64 	%rd1579, %rd1578, %rd1549;
	xor.b64  	%rd1580, %rd1579, %rd1518;
	mov.b64	{%r2635, %r2636}, %rd1580;
	mov.b64	%rd1581, {%r2636, %r2635};
	add.s64 	%rd1582, %rd1581, %rd1561;
	xor.b64  	%rd1583, %rd1582, %rd1549;
	mov.b64	{%r2637, %r2638}, %rd1583;
	prmt.b32 	%r2639, %r2637, %r2638, %r1546;
	prmt.b32 	%r2640, %r2637, %r2638, %r1545;
	mov.b64	%rd1584, {%r2640, %r2639};
	add.s64 	%rd1585, %rd1579, %rd1380;
	add.s64 	%rd1586, %rd1585, %rd1584;
	xor.b64  	%rd1587, %rd1586, %rd1581;
	mov.b64	{%r2641, %r2642}, %rd1587;
	prmt.b32 	%r2643, %r2641, %r2642, %r1552;
	prmt.b32 	%r2644, %r2641, %r2642, %r1551;
	mov.b64	%rd1588, {%r2644, %r2643};
	add.s64 	%rd1589, %rd1588, %rd1582;
	xor.b64  	%rd1590, %rd1589, %rd1584;
	mov.b64	{%r878, %r879}, %rd1590;
	// inline asm
	shf.l.wrap.b32 %r873, %r879, %r878, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r877, %r878, %r879, %r1536;
	// inline asm
	mov.b64	%rd1591, {%r873, %r877};
	add.s64 	%rd1592, %rd1544, %rd1389;
	add.s64 	%rd1593, %rd1592, %rd1563;
	xor.b64  	%rd1594, %rd1593, %rd1532;
	mov.b64	{%r2645, %r2646}, %rd1594;
	mov.b64	%rd1595, {%r2646, %r2645};
	add.s64 	%rd1596, %rd1595, %rd1519;
	xor.b64  	%rd1597, %rd1596, %rd1563;
	mov.b64	{%r2647, %r2648}, %rd1597;
	prmt.b32 	%r2649, %r2647, %r2648, %r1546;
	prmt.b32 	%r2650, %r2647, %r2648, %r1545;
	mov.b64	%rd1598, {%r2650, %r2649};
	add.s64 	%rd1599, %rd1593, %rd1385;
	add.s64 	%rd1600, %rd1599, %rd1598;
	xor.b64  	%rd1601, %rd1600, %rd1595;
	mov.b64	{%r2651, %r2652}, %rd1601;
	prmt.b32 	%r2653, %r2651, %r2652, %r1552;
	prmt.b32 	%r2654, %r2651, %r2652, %r1551;
	mov.b64	%rd1602, {%r2654, %r2653};
	add.s64 	%rd1603, %rd1602, %rd1596;
	xor.b64  	%rd1604, %rd1603, %rd1598;
	mov.b64	{%r886, %r887}, %rd1604;
	// inline asm
	shf.l.wrap.b32 %r881, %r887, %r886, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r885, %r886, %r887, %r1536;
	// inline asm
	mov.b64	%rd1605, {%r881, %r885};
	add.s64 	%rd1606, %rd1521, %rd1383;
	add.s64 	%rd1607, %rd1606, %rd1558;
	xor.b64  	%rd1608, %rd1607, %rd1546;
	mov.b64	{%r2655, %r2656}, %rd1608;
	mov.b64	%rd1609, {%r2656, %r2655};
	add.s64 	%rd1610, %rd1609, %rd1533;
	xor.b64  	%rd1611, %rd1610, %rd1521;
	mov.b64	{%r2657, %r2658}, %rd1611;
	prmt.b32 	%r2659, %r2657, %r2658, %r1546;
	prmt.b32 	%r2660, %r2657, %r2658, %r1545;
	mov.b64	%rd1612, {%r2660, %r2659};
	add.s64 	%rd1613, %rd1607, %rd1381;
	add.s64 	%rd1614, %rd1613, %rd1612;
	xor.b64  	%rd1615, %rd1614, %rd1609;
	mov.b64	{%r2661, %r2662}, %rd1615;
	prmt.b32 	%r2663, %r2661, %r2662, %r1552;
	prmt.b32 	%r2664, %r2661, %r2662, %r1551;
	mov.b64	%rd1616, {%r2664, %r2663};
	add.s64 	%rd1617, %rd1616, %rd1610;
	xor.b64  	%rd1618, %rd1617, %rd1612;
	mov.b64	{%r894, %r895}, %rd1618;
	// inline asm
	shf.l.wrap.b32 %r889, %r895, %r894, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r893, %r894, %r895, %r1536;
	// inline asm
	mov.b64	%rd1619, {%r889, %r893};
	add.s64 	%rd1620, %rd1572, %rd1389;
	add.s64 	%rd1621, %rd1620, %rd1619;
	xor.b64  	%rd1622, %rd1621, %rd1588;
	mov.b64	{%r2665, %r2666}, %rd1622;
	mov.b64	%rd1623, {%r2666, %r2665};
	add.s64 	%rd1624, %rd1623, %rd1603;
	xor.b64  	%rd1625, %rd1624, %rd1619;
	mov.b64	{%r2667, %r2668}, %rd1625;
	prmt.b32 	%r2669, %r2667, %r2668, %r1546;
	prmt.b32 	%r2670, %r2667, %r2668, %r1545;
	mov.b64	%rd1626, {%r2670, %r2669};
	add.s64 	%rd1627, %rd1621, %rd1386;
	add.s64 	%rd1628, %rd1627, %rd1626;
	xor.b64  	%rd1629, %rd1623, %rd1628;
	mov.b64	{%r2671, %r2672}, %rd1629;
	prmt.b32 	%r2673, %r2671, %r2672, %r1552;
	prmt.b32 	%r2674, %r2671, %r2672, %r1551;
	mov.b64	%rd1630, {%r2674, %r2673};
	add.s64 	%rd1631, %rd1624, %rd1630;
	xor.b64  	%rd1632, %rd1631, %rd1626;
	mov.b64	{%r902, %r903}, %rd1632;
	// inline asm
	shf.l.wrap.b32 %r897, %r903, %r902, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r901, %r902, %r903, %r1536;
	// inline asm
	mov.b64	%rd1633, {%r897, %r901};
	add.s64 	%rd1634, %rd1577, %rd1390;
	add.s64 	%rd1635, %rd1634, %rd1586;
	xor.b64  	%rd1636, %rd1602, %rd1635;
	mov.b64	{%r2675, %r2676}, %rd1636;
	mov.b64	%rd1637, {%r2676, %r2675};
	add.s64 	%rd1638, %rd1617, %rd1637;
	xor.b64  	%rd1639, %rd1638, %rd1577;
	mov.b64	{%r2677, %r2678}, %rd1639;
	prmt.b32 	%r2679, %r2677, %r2678, %r1546;
	prmt.b32 	%r2680, %r2677, %r2678, %r1545;
	mov.b64	%rd1640, {%r2680, %r2679};
	add.s64 	%rd1641, %rd1635, %rd1394;
	add.s64 	%rd1642, %rd1641, %rd1640;
	xor.b64  	%rd1643, %rd1642, %rd1637;
	mov.b64	{%r2681, %r2682}, %rd1643;
	prmt.b32 	%r2683, %r2681, %r2682, %r1552;
	prmt.b32 	%r2684, %r2681, %r2682, %r1551;
	mov.b64	%rd1644, {%r2684, %r2683};
	add.s64 	%rd1645, %rd1644, %rd1638;
	xor.b64  	%rd1646, %rd1645, %rd1640;
	mov.b64	{%r910, %r911}, %rd1646;
	// inline asm
	shf.l.wrap.b32 %r905, %r911, %r910, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r909, %r910, %r911, %r1536;
	// inline asm
	mov.b64	%rd1647, {%r905, %r909};
	add.s64 	%rd1648, %rd1591, %rd1383;
	add.s64 	%rd1649, %rd1648, %rd1600;
	xor.b64  	%rd1650, %rd1616, %rd1649;
	mov.b64	{%r2685, %r2686}, %rd1650;
	mov.b64	%rd1651, {%r2686, %r2685};
	add.s64 	%rd1652, %rd1651, %rd1575;
	xor.b64  	%rd1653, %rd1652, %rd1591;
	mov.b64	{%r2687, %r2688}, %rd1653;
	prmt.b32 	%r2689, %r2687, %r2688, %r1546;
	prmt.b32 	%r2690, %r2687, %r2688, %r1545;
	mov.b64	%rd1654, {%r2690, %r2689};
	add.s64 	%rd1655, %rd1649, %rd1380;
	add.s64 	%rd1656, %rd1655, %rd1654;
	xor.b64  	%rd1657, %rd1656, %rd1651;
	mov.b64	{%r2691, %r2692}, %rd1657;
	prmt.b32 	%r2693, %r2691, %r2692, %r1552;
	prmt.b32 	%r2694, %r2691, %r2692, %r1551;
	mov.b64	%rd1658, {%r2694, %r2693};
	add.s64 	%rd1659, %rd1658, %rd1652;
	xor.b64  	%rd1660, %rd1659, %rd1654;
	mov.b64	{%r918, %r919}, %rd1660;
	// inline asm
	shf.l.wrap.b32 %r913, %r919, %r918, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r917, %r918, %r919, %r1536;
	// inline asm
	mov.b64	%rd1661, {%r913, %r917};
	add.s64 	%rd1662, %rd1605, %rd1393;
	add.s64 	%rd1663, %rd1662, %rd1614;
	xor.b64  	%rd1664, %rd1663, %rd1574;
	mov.b64	{%r2695, %r2696}, %rd1664;
	mov.b64	%rd1665, {%r2696, %r2695};
	add.s64 	%rd1666, %rd1665, %rd1589;
	xor.b64  	%rd1667, %rd1666, %rd1605;
	mov.b64	{%r2697, %r2698}, %rd1667;
	prmt.b32 	%r2699, %r2697, %r2698, %r1546;
	prmt.b32 	%r2700, %r2697, %r2698, %r1545;
	mov.b64	%rd1668, {%r2700, %r2699};
	add.s64 	%rd1669, %rd1663, %rd1391;
	add.s64 	%rd1670, %rd1669, %rd1668;
	xor.b64  	%rd1671, %rd1670, %rd1665;
	mov.b64	{%r2701, %r2702}, %rd1671;
	prmt.b32 	%r2703, %r2701, %r2702, %r1552;
	prmt.b32 	%r2704, %r2701, %r2702, %r1551;
	mov.b64	%rd1672, {%r2704, %r2703};
	add.s64 	%rd1673, %rd1672, %rd1666;
	xor.b64  	%rd1674, %rd1673, %rd1668;
	mov.b64	{%r926, %r927}, %rd1674;
	// inline asm
	shf.l.wrap.b32 %r921, %r927, %r926, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r925, %r926, %r927, %r1536;
	// inline asm
	mov.b64	%rd1675, {%r921, %r925};
	add.s64 	%rd1676, %rd1628, %rd1388;
	add.s64 	%rd1677, %rd1676, %rd1647;
	xor.b64  	%rd1678, %rd1672, %rd1677;
	mov.b64	{%r2705, %r2706}, %rd1678;
	mov.b64	%rd1679, {%r2706, %r2705};
	add.s64 	%rd1680, %rd1679, %rd1659;
	xor.b64  	%rd1681, %rd1680, %rd1647;
	mov.b64	{%r2707, %r2708}, %rd1681;
	prmt.b32 	%r2709, %r2707, %r2708, %r1546;
	prmt.b32 	%r2710, %r2707, %r2708, %r1545;
	mov.b64	%rd1682, {%r2710, %r2709};
	add.s64 	%rd1683, %rd1677, %rd1392;
	add.s64 	%rd1684, %rd1683, %rd1682;
	xor.b64  	%rd1685, %rd1679, %rd1684;
	mov.b64	{%r2711, %r2712}, %rd1685;
	prmt.b32 	%r2713, %r2711, %r2712, %r1552;
	prmt.b32 	%r2714, %r2711, %r2712, %r1551;
	mov.b64	%rd1686, {%r2714, %r2713};
	add.s64 	%rd1687, %rd1686, %rd1680;
	xor.b64  	%rd1688, %rd1687, %rd1682;
	mov.b64	{%r934, %r935}, %rd1688;
	// inline asm
	shf.l.wrap.b32 %r929, %r935, %r934, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r933, %r934, %r935, %r1536;
	// inline asm
	mov.b64	%rd1689, {%r929, %r933};
	add.s64 	%rd1690, %rd1642, %rd1381;
	add.s64 	%rd1691, %rd1690, %rd1661;
	xor.b64  	%rd1692, %rd1691, %rd1630;
	mov.b64	{%r2715, %r2716}, %rd1692;
	mov.b64	%rd1693, {%r2716, %r2715};
	add.s64 	%rd1694, %rd1693, %rd1673;
	xor.b64  	%rd1695, %rd1694, %rd1661;
	mov.b64	{%r2717, %r2718}, %rd1695;
	prmt.b32 	%r2719, %r2717, %r2718, %r1546;
	prmt.b32 	%r2720, %r2717, %r2718, %r1545;
	mov.b64	%rd1696, {%r2720, %r2719};
	add.s64 	%rd1697, %rd1691, %rd1384;
	add.s64 	%rd1698, %rd1697, %rd1696;
	xor.b64  	%rd1699, %rd1698, %rd1693;
	mov.b64	{%r2721, %r2722}, %rd1699;
	prmt.b32 	%r2723, %r2721, %r2722, %r1552;
	prmt.b32 	%r2724, %r2721, %r2722, %r1551;
	mov.b64	%rd1700, {%r2724, %r2723};
	add.s64 	%rd1701, %rd1700, %rd1694;
	xor.b64  	%rd1702, %rd1701, %rd1696;
	mov.b64	{%r942, %r943}, %rd1702;
	// inline asm
	shf.l.wrap.b32 %r937, %r943, %r942, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r941, %r942, %r943, %r1536;
	// inline asm
	mov.b64	%rd1703, {%r937, %r941};
	add.s64 	%rd1704, %rd1656, %rd1385;
	add.s64 	%rd1705, %rd1704, %rd1675;
	xor.b64  	%rd1706, %rd1705, %rd1644;
	mov.b64	{%r2725, %r2726}, %rd1706;
	mov.b64	%rd1707, {%r2726, %r2725};
	add.s64 	%rd1708, %rd1707, %rd1631;
	xor.b64  	%rd1709, %rd1708, %rd1675;
	mov.b64	{%r2727, %r2728}, %rd1709;
	prmt.b32 	%r2729, %r2727, %r2728, %r1546;
	prmt.b32 	%r2730, %r2727, %r2728, %r1545;
	mov.b64	%rd1710, {%r2730, %r2729};
	add.s64 	%rd1711, %rd1705, %rd1402;
	add.s64 	%rd1712, %rd1711, %rd1710;
	xor.b64  	%rd1713, %rd1712, %rd1707;
	mov.b64	{%r2731, %r2732}, %rd1713;
	prmt.b32 	%r2733, %r2731, %r2732, %r1552;
	prmt.b32 	%r2734, %r2731, %r2732, %r1551;
	mov.b64	%rd1714, {%r2734, %r2733};
	add.s64 	%rd1715, %rd1714, %rd1708;
	xor.b64  	%rd1716, %rd1715, %rd1710;
	mov.b64	{%r950, %r951}, %rd1716;
	// inline asm
	shf.l.wrap.b32 %r945, %r951, %r950, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r949, %r950, %r951, %r1536;
	// inline asm
	mov.b64	%rd1717, {%r945, %r949};
	add.s64 	%rd1718, %rd1633, %rd1387;
	add.s64 	%rd1719, %rd1718, %rd1670;
	xor.b64  	%rd1720, %rd1719, %rd1658;
	mov.b64	{%r2735, %r2736}, %rd1720;
	mov.b64	%rd1721, {%r2736, %r2735};
	add.s64 	%rd1722, %rd1721, %rd1645;
	xor.b64  	%rd1723, %rd1722, %rd1633;
	mov.b64	{%r2737, %r2738}, %rd1723;
	prmt.b32 	%r2739, %r2737, %r2738, %r1546;
	prmt.b32 	%r2740, %r2737, %r2738, %r1545;
	mov.b64	%rd1724, {%r2740, %r2739};
	add.s64 	%rd1725, %rd1719, %rd1382;
	add.s64 	%rd1726, %rd1725, %rd1724;
	xor.b64  	%rd1727, %rd1726, %rd1721;
	mov.b64	{%r2741, %r2742}, %rd1727;
	prmt.b32 	%r2743, %r2741, %r2742, %r1552;
	prmt.b32 	%r2744, %r2741, %r2742, %r1551;
	mov.b64	%rd1728, {%r2744, %r2743};
	add.s64 	%rd1729, %rd1728, %rd1722;
	xor.b64  	%rd1730, %rd1729, %rd1724;
	mov.b64	{%r958, %r959}, %rd1730;
	// inline asm
	shf.l.wrap.b32 %r953, %r959, %r958, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r957, %r958, %r959, %r1536;
	// inline asm
	mov.b64	%rd1731, {%r953, %r957};
	add.s64 	%rd1732, %rd1684, %rd1385;
	add.s64 	%rd1733, %rd1732, %rd1731;
	xor.b64  	%rd1734, %rd1733, %rd1700;
	mov.b64	{%r2745, %r2746}, %rd1734;
	mov.b64	%rd1735, {%r2746, %r2745};
	add.s64 	%rd1736, %rd1735, %rd1715;
	xor.b64  	%rd1737, %rd1736, %rd1731;
	mov.b64	{%r2747, %r2748}, %rd1737;
	prmt.b32 	%r2749, %r2747, %r2748, %r1546;
	prmt.b32 	%r2750, %r2747, %r2748, %r1545;
	mov.b64	%rd1738, {%r2750, %r2749};
	add.s64 	%rd1739, %rd1733, %rd1387;
	add.s64 	%rd1740, %rd1739, %rd1738;
	xor.b64  	%rd1741, %rd1735, %rd1740;
	mov.b64	{%r2751, %r2752}, %rd1741;
	prmt.b32 	%r2753, %r2751, %r2752, %r1552;
	prmt.b32 	%r2754, %r2751, %r2752, %r1551;
	mov.b64	%rd1742, {%r2754, %r2753};
	add.s64 	%rd1743, %rd1736, %rd1742;
	xor.b64  	%rd1744, %rd1743, %rd1738;
	mov.b64	{%r966, %r967}, %rd1744;
	// inline asm
	shf.l.wrap.b32 %r961, %r967, %r966, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r965, %r966, %r967, %r1536;
	// inline asm
	mov.b64	%rd1745, {%r961, %r965};
	add.s64 	%rd1746, %rd1689, %rd1381;
	add.s64 	%rd1747, %rd1746, %rd1698;
	xor.b64  	%rd1748, %rd1714, %rd1747;
	mov.b64	{%r2755, %r2756}, %rd1748;
	mov.b64	%rd1749, {%r2756, %r2755};
	add.s64 	%rd1750, %rd1729, %rd1749;
	xor.b64  	%rd1751, %rd1750, %rd1689;
	mov.b64	{%r2757, %r2758}, %rd1751;
	prmt.b32 	%r2759, %r2757, %r2758, %r1546;
	prmt.b32 	%r2760, %r2757, %r2758, %r1545;
	mov.b64	%rd1752, {%r2760, %r2759};
	add.s64 	%rd1753, %rd1747, %rd1402;
	add.s64 	%rd1754, %rd1753, %rd1752;
	xor.b64  	%rd1755, %rd1754, %rd1749;
	mov.b64	{%r2761, %r2762}, %rd1755;
	prmt.b32 	%r2763, %r2761, %r2762, %r1552;
	prmt.b32 	%r2764, %r2761, %r2762, %r1551;
	mov.b64	%rd1756, {%r2764, %r2763};
	add.s64 	%rd1757, %rd1756, %rd1750;
	xor.b64  	%rd1758, %rd1757, %rd1752;
	mov.b64	{%r974, %r975}, %rd1758;
	// inline asm
	shf.l.wrap.b32 %r969, %r975, %r974, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r973, %r974, %r975, %r1536;
	// inline asm
	mov.b64	%rd1759, {%r969, %r973};
	add.s64 	%rd1760, %rd1703, %rd1391;
	add.s64 	%rd1761, %rd1760, %rd1712;
	xor.b64  	%rd1762, %rd1728, %rd1761;
	mov.b64	{%r2765, %r2766}, %rd1762;
	mov.b64	%rd1763, {%r2766, %r2765};
	add.s64 	%rd1764, %rd1763, %rd1687;
	xor.b64  	%rd1765, %rd1764, %rd1703;
	mov.b64	{%r2767, %r2768}, %rd1765;
	prmt.b32 	%r2769, %r2767, %r2768, %r1546;
	prmt.b32 	%r2770, %r2767, %r2768, %r1545;
	mov.b64	%rd1766, {%r2770, %r2769};
	add.s64 	%rd1767, %rd1761, %rd1390;
	add.s64 	%rd1768, %rd1767, %rd1766;
	xor.b64  	%rd1769, %rd1768, %rd1763;
	mov.b64	{%r2771, %r2772}, %rd1769;
	prmt.b32 	%r2773, %r2771, %r2772, %r1552;
	prmt.b32 	%r2774, %r2771, %r2772, %r1551;
	mov.b64	%rd1770, {%r2774, %r2773};
	add.s64 	%rd1771, %rd1770, %rd1764;
	xor.b64  	%rd1772, %rd1771, %rd1766;
	mov.b64	{%r982, %r983}, %rd1772;
	// inline asm
	shf.l.wrap.b32 %r977, %r983, %r982, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r981, %r982, %r983, %r1536;
	// inline asm
	mov.b64	%rd1773, {%r977, %r981};
	add.s64 	%rd1774, %rd1717, %rd1389;
	add.s64 	%rd1775, %rd1774, %rd1726;
	xor.b64  	%rd1776, %rd1775, %rd1686;
	mov.b64	{%r2775, %r2776}, %rd1776;
	mov.b64	%rd1777, {%r2776, %r2775};
	add.s64 	%rd1778, %rd1777, %rd1701;
	xor.b64  	%rd1779, %rd1778, %rd1717;
	mov.b64	{%r2777, %r2778}, %rd1779;
	prmt.b32 	%r2779, %r2777, %r2778, %r1546;
	prmt.b32 	%r2780, %r2777, %r2778, %r1545;
	mov.b64	%rd1780, {%r2780, %r2779};
	add.s64 	%rd1781, %rd1775, %rd1392;
	add.s64 	%rd1782, %rd1781, %rd1780;
	xor.b64  	%rd1783, %rd1782, %rd1777;
	mov.b64	{%r2781, %r2782}, %rd1783;
	prmt.b32 	%r2783, %r2781, %r2782, %r1552;
	prmt.b32 	%r2784, %r2781, %r2782, %r1551;
	mov.b64	%rd1784, {%r2784, %r2783};
	add.s64 	%rd1785, %rd1784, %rd1778;
	xor.b64  	%rd1786, %rd1785, %rd1780;
	mov.b64	{%r990, %r991}, %rd1786;
	// inline asm
	shf.l.wrap.b32 %r985, %r991, %r990, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r989, %r990, %r991, %r1536;
	// inline asm
	mov.b64	%rd1787, {%r985, %r989};
	add.s64 	%rd1788, %rd1740, %rd1380;
	add.s64 	%rd1789, %rd1788, %rd1759;
	xor.b64  	%rd1790, %rd1784, %rd1789;
	mov.b64	{%r2785, %r2786}, %rd1790;
	mov.b64	%rd1791, {%r2786, %r2785};
	add.s64 	%rd1792, %rd1791, %rd1771;
	xor.b64  	%rd1793, %rd1792, %rd1759;
	mov.b64	{%r2787, %r2788}, %rd1793;
	prmt.b32 	%r2789, %r2787, %r2788, %r1546;
	prmt.b32 	%r2790, %r2787, %r2788, %r1545;
	mov.b64	%rd1794, {%r2790, %r2789};
	add.s64 	%rd1795, %rd1789, %rd1384;
	add.s64 	%rd1796, %rd1795, %rd1794;
	xor.b64  	%rd1797, %rd1791, %rd1796;
	mov.b64	{%r2791, %r2792}, %rd1797;
	prmt.b32 	%r2793, %r2791, %r2792, %r1552;
	prmt.b32 	%r2794, %r2791, %r2792, %r1551;
	mov.b64	%rd1798, {%r2794, %r2793};
	add.s64 	%rd1799, %rd1798, %rd1792;
	xor.b64  	%rd1800, %rd1799, %rd1794;
	mov.b64	{%r998, %r999}, %rd1800;
	// inline asm
	shf.l.wrap.b32 %r993, %r999, %r998, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r997, %r998, %r999, %r1536;
	// inline asm
	mov.b64	%rd1801, {%r993, %r997};
	add.s64 	%rd1802, %rd1754, %rd1383;
	add.s64 	%rd1803, %rd1802, %rd1773;
	xor.b64  	%rd1804, %rd1803, %rd1742;
	mov.b64	{%r2795, %r2796}, %rd1804;
	mov.b64	%rd1805, {%r2796, %r2795};
	add.s64 	%rd1806, %rd1805, %rd1785;
	xor.b64  	%rd1807, %rd1806, %rd1773;
	mov.b64	{%r2797, %r2798}, %rd1807;
	prmt.b32 	%r2799, %r2797, %r2798, %r1546;
	prmt.b32 	%r2800, %r2797, %r2798, %r1545;
	mov.b64	%rd1808, {%r2800, %r2799};
	add.s64 	%rd1809, %rd1803, %rd1388;
	add.s64 	%rd1810, %rd1809, %rd1808;
	xor.b64  	%rd1811, %rd1810, %rd1805;
	mov.b64	{%r2801, %r2802}, %rd1811;
	prmt.b32 	%r2803, %r2801, %r2802, %r1552;
	prmt.b32 	%r2804, %r2801, %r2802, %r1551;
	mov.b64	%rd1812, {%r2804, %r2803};
	add.s64 	%rd1813, %rd1812, %rd1806;
	xor.b64  	%rd1814, %rd1813, %rd1808;
	mov.b64	{%r1006, %r1007}, %rd1814;
	// inline asm
	shf.l.wrap.b32 %r1001, %r1007, %r1006, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1005, %r1006, %r1007, %r1536;
	// inline asm
	mov.b64	%rd1815, {%r1001, %r1005};
	add.s64 	%rd1816, %rd1768, %rd1382;
	add.s64 	%rd1817, %rd1816, %rd1787;
	xor.b64  	%rd1818, %rd1817, %rd1756;
	mov.b64	{%r2805, %r2806}, %rd1818;
	mov.b64	%rd1819, {%r2806, %r2805};
	add.s64 	%rd1820, %rd1819, %rd1743;
	xor.b64  	%rd1821, %rd1820, %rd1787;
	mov.b64	{%r2807, %r2808}, %rd1821;
	prmt.b32 	%r2809, %r2807, %r2808, %r1546;
	prmt.b32 	%r2810, %r2807, %r2808, %r1545;
	mov.b64	%rd1822, {%r2810, %r2809};
	add.s64 	%rd1823, %rd1817, %rd1394;
	add.s64 	%rd1824, %rd1823, %rd1822;
	xor.b64  	%rd1825, %rd1824, %rd1819;
	mov.b64	{%r2811, %r2812}, %rd1825;
	prmt.b32 	%r2813, %r2811, %r2812, %r1552;
	prmt.b32 	%r2814, %r2811, %r2812, %r1551;
	mov.b64	%rd1826, {%r2814, %r2813};
	add.s64 	%rd1827, %rd1826, %rd1820;
	xor.b64  	%rd1828, %rd1827, %rd1822;
	mov.b64	{%r1014, %r1015}, %rd1828;
	// inline asm
	shf.l.wrap.b32 %r1009, %r1015, %r1014, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1013, %r1014, %r1015, %r1536;
	// inline asm
	mov.b64	%rd1829, {%r1009, %r1013};
	add.s64 	%rd1830, %rd1745, %rd1393;
	add.s64 	%rd1831, %rd1830, %rd1782;
	xor.b64  	%rd1832, %rd1831, %rd1770;
	mov.b64	{%r2815, %r2816}, %rd1832;
	mov.b64	%rd1833, {%r2816, %r2815};
	add.s64 	%rd1834, %rd1833, %rd1757;
	xor.b64  	%rd1835, %rd1834, %rd1745;
	mov.b64	{%r2817, %r2818}, %rd1835;
	prmt.b32 	%r2819, %r2817, %r2818, %r1546;
	prmt.b32 	%r2820, %r2817, %r2818, %r1545;
	mov.b64	%rd1836, {%r2820, %r2819};
	add.s64 	%rd1837, %rd1831, %rd1386;
	add.s64 	%rd1838, %rd1837, %rd1836;
	xor.b64  	%rd1839, %rd1838, %rd1833;
	mov.b64	{%r2821, %r2822}, %rd1839;
	prmt.b32 	%r2823, %r2821, %r2822, %r1552;
	prmt.b32 	%r2824, %r2821, %r2822, %r1551;
	mov.b64	%rd1840, {%r2824, %r2823};
	add.s64 	%rd1841, %rd1840, %rd1834;
	xor.b64  	%rd1842, %rd1841, %rd1836;
	mov.b64	{%r1022, %r1023}, %rd1842;
	// inline asm
	shf.l.wrap.b32 %r1017, %r1023, %r1022, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1021, %r1022, %r1023, %r1536;
	// inline asm
	mov.b64	%rd1843, {%r1017, %r1021};
	add.s64 	%rd1844, %rd1796, %rd1387;
	add.s64 	%rd1845, %rd1844, %rd1843;
	xor.b64  	%rd1846, %rd1845, %rd1812;
	mov.b64	{%r2825, %r2826}, %rd1846;
	mov.b64	%rd1847, {%r2826, %r2825};
	add.s64 	%rd1848, %rd1847, %rd1827;
	xor.b64  	%rd1849, %rd1848, %rd1843;
	mov.b64	{%r2827, %r2828}, %rd1849;
	prmt.b32 	%r2829, %r2827, %r2828, %r1546;
	prmt.b32 	%r2830, %r2827, %r2828, %r1545;
	mov.b64	%rd1850, {%r2830, %r2829};
	add.s64 	%rd1851, %rd1845, %rd1394;
	add.s64 	%rd1852, %rd1851, %rd1850;
	xor.b64  	%rd1853, %rd1847, %rd1852;
	mov.b64	{%r2831, %r2832}, %rd1853;
	prmt.b32 	%r2833, %r2831, %r2832, %r1552;
	prmt.b32 	%r2834, %r2831, %r2832, %r1551;
	mov.b64	%rd1854, {%r2834, %r2833};
	add.s64 	%rd1855, %rd1848, %rd1854;
	xor.b64  	%rd1856, %rd1855, %rd1850;
	mov.b64	{%r1030, %r1031}, %rd1856;
	// inline asm
	shf.l.wrap.b32 %r1025, %r1031, %r1030, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1029, %r1030, %r1031, %r1536;
	// inline asm
	mov.b64	%rd1857, {%r1025, %r1029};
	add.s64 	%rd1858, %rd1801, %rd1383;
	add.s64 	%rd1859, %rd1858, %rd1810;
	xor.b64  	%rd1860, %rd1826, %rd1859;
	mov.b64	{%r2835, %r2836}, %rd1860;
	mov.b64	%rd1861, {%r2836, %r2835};
	add.s64 	%rd1862, %rd1841, %rd1861;
	xor.b64  	%rd1863, %rd1862, %rd1801;
	mov.b64	{%r2837, %r2838}, %rd1863;
	prmt.b32 	%r2839, %r2837, %r2838, %r1546;
	prmt.b32 	%r2840, %r2837, %r2838, %r1545;
	mov.b64	%rd1864, {%r2840, %r2839};
	add.s64 	%rd1865, %rd1859, %rd1385;
	add.s64 	%rd1866, %rd1865, %rd1864;
	xor.b64  	%rd1867, %rd1866, %rd1861;
	mov.b64	{%r2841, %r2842}, %rd1867;
	prmt.b32 	%r2843, %r2841, %r2842, %r1552;
	prmt.b32 	%r2844, %r2841, %r2842, %r1551;
	mov.b64	%rd1868, {%r2844, %r2843};
	add.s64 	%rd1869, %rd1868, %rd1862;
	xor.b64  	%rd1870, %rd1869, %rd1864;
	mov.b64	{%r1038, %r1039}, %rd1870;
	// inline asm
	shf.l.wrap.b32 %r1033, %r1039, %r1038, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1037, %r1038, %r1039, %r1536;
	// inline asm
	mov.b64	%rd1871, {%r1033, %r1037};
	add.s64 	%rd1872, %rd1815, %rd1380;
	add.s64 	%rd1873, %rd1872, %rd1824;
	xor.b64  	%rd1874, %rd1840, %rd1873;
	mov.b64	{%r2845, %r2846}, %rd1874;
	mov.b64	%rd1875, {%r2846, %r2845};
	add.s64 	%rd1876, %rd1875, %rd1799;
	xor.b64  	%rd1877, %rd1876, %rd1815;
	mov.b64	{%r2847, %r2848}, %rd1877;
	prmt.b32 	%r2849, %r2847, %r2848, %r1546;
	prmt.b32 	%r2850, %r2847, %r2848, %r1545;
	mov.b64	%rd1878, {%r2850, %r2849};
	add.s64 	%rd1879, %rd1873, %rd1382;
	add.s64 	%rd1880, %rd1879, %rd1878;
	xor.b64  	%rd1881, %rd1880, %rd1875;
	mov.b64	{%r2851, %r2852}, %rd1881;
	prmt.b32 	%r2853, %r2851, %r2852, %r1552;
	prmt.b32 	%r2854, %r2851, %r2852, %r1551;
	mov.b64	%rd1882, {%r2854, %r2853};
	add.s64 	%rd1883, %rd1882, %rd1876;
	xor.b64  	%rd1884, %rd1883, %rd1878;
	mov.b64	{%r1046, %r1047}, %rd1884;
	// inline asm
	shf.l.wrap.b32 %r1041, %r1047, %r1046, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1045, %r1046, %r1047, %r1536;
	// inline asm
	mov.b64	%rd1885, {%r1041, %r1045};
	add.s64 	%rd1886, %rd1829, %rd1388;
	add.s64 	%rd1887, %rd1886, %rd1838;
	xor.b64  	%rd1888, %rd1887, %rd1798;
	mov.b64	{%r2855, %r2856}, %rd1888;
	mov.b64	%rd1889, {%r2856, %r2855};
	add.s64 	%rd1890, %rd1889, %rd1813;
	xor.b64  	%rd1891, %rd1890, %rd1829;
	mov.b64	{%r2857, %r2858}, %rd1891;
	prmt.b32 	%r2859, %r2857, %r2858, %r1546;
	prmt.b32 	%r2860, %r2857, %r2858, %r1545;
	mov.b64	%rd1892, {%r2860, %r2859};
	add.s64 	%rd1893, %rd1887, %rd1393;
	add.s64 	%rd1894, %rd1893, %rd1892;
	xor.b64  	%rd1895, %rd1894, %rd1889;
	mov.b64	{%r2861, %r2862}, %rd1895;
	prmt.b32 	%r2863, %r2861, %r2862, %r1552;
	prmt.b32 	%r2864, %r2861, %r2862, %r1551;
	mov.b64	%rd1896, {%r2864, %r2863};
	add.s64 	%rd1897, %rd1896, %rd1890;
	xor.b64  	%rd1898, %rd1897, %rd1892;
	mov.b64	{%r1054, %r1055}, %rd1898;
	// inline asm
	shf.l.wrap.b32 %r1049, %r1055, %r1054, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1053, %r1054, %r1055, %r1536;
	// inline asm
	mov.b64	%rd1899, {%r1049, %r1053};
	add.s64 	%rd1900, %rd1852, %rd1392;
	add.s64 	%rd1901, %rd1900, %rd1871;
	xor.b64  	%rd1902, %rd1896, %rd1901;
	mov.b64	{%r2865, %r2866}, %rd1902;
	mov.b64	%rd1903, {%r2866, %r2865};
	add.s64 	%rd1904, %rd1903, %rd1883;
	xor.b64  	%rd1905, %rd1904, %rd1871;
	mov.b64	{%r2867, %r2868}, %rd1905;
	prmt.b32 	%r2869, %r2867, %r2868, %r1546;
	prmt.b32 	%r2870, %r2867, %r2868, %r1545;
	mov.b64	%rd1906, {%r2870, %r2869};
	add.s64 	%rd1907, %rd1901, %rd1402;
	add.s64 	%rd1908, %rd1907, %rd1906;
	xor.b64  	%rd1909, %rd1903, %rd1908;
	mov.b64	{%r2871, %r2872}, %rd1909;
	prmt.b32 	%r2873, %r2871, %r2872, %r1552;
	prmt.b32 	%r2874, %r2871, %r2872, %r1551;
	mov.b64	%rd1910, {%r2874, %r2873};
	add.s64 	%rd1911, %rd1910, %rd1904;
	xor.b64  	%rd1912, %rd1911, %rd1906;
	mov.b64	{%r1062, %r1063}, %rd1912;
	// inline asm
	shf.l.wrap.b32 %r1057, %r1063, %r1062, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1061, %r1062, %r1063, %r1536;
	// inline asm
	mov.b64	%rd1913, {%r1057, %r1061};
	add.s64 	%rd1914, %rd1866, %rd1389;
	add.s64 	%rd1915, %rd1914, %rd1885;
	xor.b64  	%rd1916, %rd1915, %rd1854;
	mov.b64	{%r2875, %r2876}, %rd1916;
	mov.b64	%rd1917, {%r2876, %r2875};
	add.s64 	%rd1918, %rd1917, %rd1897;
	xor.b64  	%rd1919, %rd1918, %rd1885;
	mov.b64	{%r2877, %r2878}, %rd1919;
	prmt.b32 	%r2879, %r2877, %r2878, %r1546;
	prmt.b32 	%r2880, %r2877, %r2878, %r1545;
	mov.b64	%rd1920, {%r2880, %r2879};
	add.s64 	%rd1921, %rd1915, %rd1390;
	add.s64 	%rd1922, %rd1921, %rd1920;
	xor.b64  	%rd1923, %rd1922, %rd1917;
	mov.b64	{%r2881, %r2882}, %rd1923;
	prmt.b32 	%r2883, %r2881, %r2882, %r1552;
	prmt.b32 	%r2884, %r2881, %r2882, %r1551;
	mov.b64	%rd1924, {%r2884, %r2883};
	add.s64 	%rd1925, %rd1924, %rd1918;
	xor.b64  	%rd1926, %rd1925, %rd1920;
	mov.b64	{%r1070, %r1071}, %rd1926;
	// inline asm
	shf.l.wrap.b32 %r1065, %r1071, %r1070, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1069, %r1070, %r1071, %r1536;
	// inline asm
	mov.b64	%rd1927, {%r1065, %r1069};
	add.s64 	%rd1928, %rd1880, %rd1384;
	add.s64 	%rd1929, %rd1928, %rd1899;
	xor.b64  	%rd1930, %rd1929, %rd1868;
	mov.b64	{%r2885, %r2886}, %rd1930;
	mov.b64	%rd1931, {%r2886, %r2885};
	add.s64 	%rd1932, %rd1931, %rd1855;
	xor.b64  	%rd1933, %rd1932, %rd1899;
	mov.b64	{%r2887, %r2888}, %rd1933;
	prmt.b32 	%r2889, %r2887, %r2888, %r1546;
	prmt.b32 	%r2890, %r2887, %r2888, %r1545;
	mov.b64	%rd1934, {%r2890, %r2889};
	add.s64 	%rd1935, %rd1929, %rd1386;
	add.s64 	%rd1936, %rd1935, %rd1934;
	xor.b64  	%rd1937, %rd1936, %rd1931;
	mov.b64	{%r2891, %r2892}, %rd1937;
	prmt.b32 	%r2893, %r2891, %r2892, %r1552;
	prmt.b32 	%r2894, %r2891, %r2892, %r1551;
	mov.b64	%rd1938, {%r2894, %r2893};
	add.s64 	%rd1939, %rd1938, %rd1932;
	xor.b64  	%rd1940, %rd1939, %rd1934;
	mov.b64	{%r1078, %r1079}, %rd1940;
	// inline asm
	shf.l.wrap.b32 %r1073, %r1079, %r1078, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1077, %r1078, %r1079, %r1536;
	// inline asm
	mov.b64	%rd1941, {%r1073, %r1077};
	add.s64 	%rd1942, %rd1857, %rd1381;
	add.s64 	%rd1943, %rd1942, %rd1894;
	xor.b64  	%rd1944, %rd1943, %rd1882;
	mov.b64	{%r2895, %r2896}, %rd1944;
	mov.b64	%rd1945, {%r2896, %r2895};
	add.s64 	%rd1946, %rd1945, %rd1869;
	xor.b64  	%rd1947, %rd1946, %rd1857;
	mov.b64	{%r2897, %r2898}, %rd1947;
	prmt.b32 	%r2899, %r2897, %r2898, %r1546;
	prmt.b32 	%r2900, %r2897, %r2898, %r1545;
	mov.b64	%rd1948, {%r2900, %r2899};
	add.s64 	%rd1949, %rd1943, %rd1391;
	add.s64 	%rd1950, %rd1949, %rd1948;
	xor.b64  	%rd1951, %rd1950, %rd1945;
	mov.b64	{%r2901, %r2902}, %rd1951;
	prmt.b32 	%r2903, %r2901, %r2902, %r1552;
	prmt.b32 	%r2904, %r2901, %r2902, %r1551;
	mov.b64	%rd1952, {%r2904, %r2903};
	add.s64 	%rd1953, %rd1952, %rd1946;
	xor.b64  	%rd1954, %rd1953, %rd1948;
	mov.b64	{%r1086, %r1087}, %rd1954;
	// inline asm
	shf.l.wrap.b32 %r1081, %r1087, %r1086, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1085, %r1086, %r1087, %r1536;
	// inline asm
	mov.b64	%rd1955, {%r1081, %r1085};
	add.s64 	%rd1956, %rd1908, %rd1380;
	add.s64 	%rd1957, %rd1956, %rd1955;
	xor.b64  	%rd1958, %rd1957, %rd1924;
	mov.b64	{%r2905, %r2906}, %rd1958;
	mov.b64	%rd1959, {%r2906, %r2905};
	add.s64 	%rd1960, %rd1959, %rd1939;
	xor.b64  	%rd1961, %rd1960, %rd1955;
	mov.b64	{%r2907, %r2908}, %rd1961;
	prmt.b32 	%r2909, %r2907, %r2908, %r1546;
	prmt.b32 	%r2910, %r2907, %r2908, %r1545;
	mov.b64	%rd1962, {%r2910, %r2909};
	add.s64 	%rd1963, %rd1957, %rd1390;
	add.s64 	%rd1964, %rd1963, %rd1962;
	xor.b64  	%rd1965, %rd1959, %rd1964;
	mov.b64	{%r2911, %r2912}, %rd1965;
	prmt.b32 	%r2913, %r2911, %r2912, %r1552;
	prmt.b32 	%r2914, %r2911, %r2912, %r1551;
	mov.b64	%rd1966, {%r2914, %r2913};
	add.s64 	%rd1967, %rd1960, %rd1966;
	xor.b64  	%rd1968, %rd1967, %rd1962;
	mov.b64	{%r1094, %r1095}, %rd1968;
	// inline asm
	shf.l.wrap.b32 %r1089, %r1095, %r1094, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1093, %r1094, %r1095, %r1536;
	// inline asm
	mov.b64	%rd1969, {%r1089, %r1093};
	add.s64 	%rd1970, %rd1913, %rd1384;
	add.s64 	%rd1971, %rd1970, %rd1922;
	xor.b64  	%rd1972, %rd1938, %rd1971;
	mov.b64	{%r2915, %r2916}, %rd1972;
	mov.b64	%rd1973, {%r2916, %r2915};
	add.s64 	%rd1974, %rd1953, %rd1973;
	xor.b64  	%rd1975, %rd1974, %rd1913;
	mov.b64	{%r2917, %r2918}, %rd1975;
	prmt.b32 	%r2919, %r2917, %r2918, %r1546;
	prmt.b32 	%r2920, %r2917, %r2918, %r1545;
	mov.b64	%rd1976, {%r2920, %r2919};
	add.s64 	%rd1977, %rd1971, %rd1388;
	add.s64 	%rd1978, %rd1977, %rd1976;
	xor.b64  	%rd1979, %rd1978, %rd1973;
	mov.b64	{%r2921, %r2922}, %rd1979;
	prmt.b32 	%r2923, %r2921, %r2922, %r1552;
	prmt.b32 	%r2924, %r2921, %r2922, %r1551;
	mov.b64	%rd1980, {%r2924, %r2923};
	add.s64 	%rd1981, %rd1980, %rd1974;
	xor.b64  	%rd1982, %rd1981, %rd1976;
	mov.b64	{%r1102, %r1103}, %rd1982;
	// inline asm
	shf.l.wrap.b32 %r1097, %r1103, %r1102, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1101, %r1102, %r1103, %r1536;
	// inline asm
	mov.b64	%rd1983, {%r1097, %r1101};
	add.s64 	%rd1984, %rd1927, %rd1394;
	add.s64 	%rd1985, %rd1984, %rd1936;
	xor.b64  	%rd1986, %rd1952, %rd1985;
	mov.b64	{%r2925, %r2926}, %rd1986;
	mov.b64	%rd1987, {%r2926, %r2925};
	add.s64 	%rd1988, %rd1987, %rd1911;
	xor.b64  	%rd1989, %rd1988, %rd1927;
	mov.b64	{%r2927, %r2928}, %rd1989;
	prmt.b32 	%r2929, %r2927, %r2928, %r1546;
	prmt.b32 	%r2930, %r2927, %r2928, %r1545;
	mov.b64	%rd1990, {%r2930, %r2929};
	add.s64 	%rd1991, %rd1985, %rd1389;
	add.s64 	%rd1992, %rd1991, %rd1990;
	xor.b64  	%rd1993, %rd1992, %rd1987;
	mov.b64	{%r2931, %r2932}, %rd1993;
	prmt.b32 	%r2933, %r2931, %r2932, %r1552;
	prmt.b32 	%r2934, %r2931, %r2932, %r1551;
	mov.b64	%rd1994, {%r2934, %r2933};
	add.s64 	%rd1995, %rd1994, %rd1988;
	xor.b64  	%rd1996, %rd1995, %rd1990;
	mov.b64	{%r1110, %r1111}, %rd1996;
	// inline asm
	shf.l.wrap.b32 %r1105, %r1111, %r1110, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1109, %r1110, %r1111, %r1536;
	// inline asm
	mov.b64	%rd1997, {%r1105, %r1109};
	add.s64 	%rd1998, %rd1941, %rd1386;
	add.s64 	%rd1999, %rd1998, %rd1950;
	xor.b64  	%rd2000, %rd1999, %rd1910;
	mov.b64	{%r2935, %r2936}, %rd2000;
	mov.b64	%rd2001, {%r2936, %r2935};
	add.s64 	%rd2002, %rd2001, %rd1925;
	xor.b64  	%rd2003, %rd2002, %rd1941;
	mov.b64	{%r2937, %r2938}, %rd2003;
	prmt.b32 	%r2939, %r2937, %r2938, %r1546;
	prmt.b32 	%r2940, %r2937, %r2938, %r1545;
	mov.b64	%rd2004, {%r2940, %r2939};
	add.s64 	%rd2005, %rd1999, %rd1381;
	add.s64 	%rd2006, %rd2005, %rd2004;
	xor.b64  	%rd2007, %rd2006, %rd2001;
	mov.b64	{%r2941, %r2942}, %rd2007;
	prmt.b32 	%r2943, %r2941, %r2942, %r1552;
	prmt.b32 	%r2944, %r2941, %r2942, %r1551;
	mov.b64	%rd2008, {%r2944, %r2943};
	add.s64 	%rd2009, %rd2008, %rd2002;
	xor.b64  	%rd2010, %rd2009, %rd2004;
	mov.b64	{%r1118, %r1119}, %rd2010;
	// inline asm
	shf.l.wrap.b32 %r1113, %r1119, %r1118, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1117, %r1118, %r1119, %r1536;
	// inline asm
	mov.b64	%rd2011, {%r1113, %r1117};
	add.s64 	%rd2012, %rd1964, %rd1382;
	add.s64 	%rd2013, %rd2012, %rd1983;
	xor.b64  	%rd2014, %rd2008, %rd2013;
	mov.b64	{%r2945, %r2946}, %rd2014;
	mov.b64	%rd2015, {%r2946, %r2945};
	add.s64 	%rd2016, %rd2015, %rd1995;
	xor.b64  	%rd2017, %rd2016, %rd1983;
	mov.b64	{%r2947, %r2948}, %rd2017;
	prmt.b32 	%r2949, %r2947, %r2948, %r1546;
	prmt.b32 	%r2950, %r2947, %r2948, %r1545;
	mov.b64	%rd2018, {%r2950, %r2949};
	add.s64 	%rd2019, %rd2013, %rd1391;
	add.s64 	%rd2020, %rd2019, %rd2018;
	xor.b64  	%rd2021, %rd2015, %rd2020;
	mov.b64	{%r2951, %r2952}, %rd2021;
	prmt.b32 	%r2953, %r2951, %r2952, %r1552;
	prmt.b32 	%r2954, %r2951, %r2952, %r1551;
	mov.b64	%rd2022, {%r2954, %r2953};
	add.s64 	%rd2023, %rd2022, %rd2016;
	xor.b64  	%rd2024, %rd2023, %rd2018;
	mov.b64	{%r1126, %r1127}, %rd2024;
	// inline asm
	shf.l.wrap.b32 %r1121, %r1127, %r1126, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1125, %r1126, %r1127, %r1536;
	// inline asm
	mov.b64	%rd2025, {%r1121, %r1125};
	add.s64 	%rd2026, %rd1978, %rd1385;
	add.s64 	%rd2027, %rd2026, %rd1997;
	xor.b64  	%rd2028, %rd2027, %rd1966;
	mov.b64	{%r2955, %r2956}, %rd2028;
	mov.b64	%rd2029, {%r2956, %r2955};
	add.s64 	%rd2030, %rd2029, %rd2009;
	xor.b64  	%rd2031, %rd2030, %rd1997;
	mov.b64	{%r2957, %r2958}, %rd2031;
	prmt.b32 	%r2959, %r2957, %r2958, %r1546;
	prmt.b32 	%r2960, %r2957, %r2958, %r1545;
	mov.b64	%rd2032, {%r2960, %r2959};
	add.s64 	%rd2033, %rd2027, %rd1383;
	add.s64 	%rd2034, %rd2033, %rd2032;
	xor.b64  	%rd2035, %rd2034, %rd2029;
	mov.b64	{%r2961, %r2962}, %rd2035;
	prmt.b32 	%r2963, %r2961, %r2962, %r1552;
	prmt.b32 	%r2964, %r2961, %r2962, %r1551;
	mov.b64	%rd2036, {%r2964, %r2963};
	add.s64 	%rd2037, %rd2036, %rd2030;
	xor.b64  	%rd2038, %rd2037, %rd2032;
	mov.b64	{%r1134, %r1135}, %rd2038;
	// inline asm
	shf.l.wrap.b32 %r1129, %r1135, %r1134, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1133, %r1134, %r1135, %r1536;
	// inline asm
	mov.b64	%rd2039, {%r1129, %r1133};
	add.s64 	%rd2040, %rd1992, %rd1393;
	add.s64 	%rd2041, %rd2040, %rd2011;
	xor.b64  	%rd2042, %rd2041, %rd1980;
	mov.b64	{%r2965, %r2966}, %rd2042;
	mov.b64	%rd2043, {%r2966, %r2965};
	add.s64 	%rd2044, %rd2043, %rd1967;
	xor.b64  	%rd2045, %rd2044, %rd2011;
	mov.b64	{%r2967, %r2968}, %rd2045;
	prmt.b32 	%r2969, %r2967, %r2968, %r1546;
	prmt.b32 	%r2970, %r2967, %r2968, %r1545;
	mov.b64	%rd2046, {%r2970, %r2969};
	add.s64 	%rd2047, %rd2041, %rd1392;
	add.s64 	%rd2048, %rd2047, %rd2046;
	xor.b64  	%rd2049, %rd2048, %rd2043;
	mov.b64	{%r2971, %r2972}, %rd2049;
	prmt.b32 	%r2973, %r2971, %r2972, %r1552;
	prmt.b32 	%r2974, %r2971, %r2972, %r1551;
	mov.b64	%rd2050, {%r2974, %r2973};
	add.s64 	%rd2051, %rd2050, %rd2044;
	xor.b64  	%rd2052, %rd2051, %rd2046;
	mov.b64	{%r1142, %r1143}, %rd2052;
	// inline asm
	shf.l.wrap.b32 %r1137, %r1143, %r1142, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1141, %r1142, %r1143, %r1536;
	// inline asm
	mov.b64	%rd2053, {%r1137, %r1141};
	add.s64 	%rd2054, %rd1969, %rd1402;
	add.s64 	%rd2055, %rd2054, %rd2006;
	xor.b64  	%rd2056, %rd2055, %rd1994;
	mov.b64	{%r2975, %r2976}, %rd2056;
	mov.b64	%rd2057, {%r2976, %r2975};
	add.s64 	%rd2058, %rd2057, %rd1981;
	xor.b64  	%rd2059, %rd2058, %rd1969;
	mov.b64	{%r2977, %r2978}, %rd2059;
	prmt.b32 	%r2979, %r2977, %r2978, %r1546;
	prmt.b32 	%r2980, %r2977, %r2978, %r1545;
	mov.b64	%rd2060, {%r2980, %r2979};
	add.s64 	%rd2061, %rd2055, %rd1387;
	add.s64 	%rd2062, %rd2061, %rd2060;
	xor.b64  	%rd2063, %rd2062, %rd2057;
	mov.b64	{%r2981, %r2982}, %rd2063;
	prmt.b32 	%r2983, %r2981, %r2982, %r1552;
	prmt.b32 	%r2984, %r2981, %r2982, %r1551;
	mov.b64	%rd2064, {%r2984, %r2983};
	add.s64 	%rd2065, %rd2064, %rd2058;
	xor.b64  	%rd2066, %rd2065, %rd2060;
	mov.b64	{%r1150, %r1151}, %rd2066;
	// inline asm
	shf.l.wrap.b32 %r1145, %r1151, %r1150, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1149, %r1150, %r1151, %r1536;
	// inline asm
	mov.b64	%rd2067, {%r1145, %r1149};
	add.s64 	%rd2068, %rd2020, %rd1390;
	add.s64 	%rd2069, %rd2068, %rd2067;
	xor.b64  	%rd2070, %rd2069, %rd2036;
	mov.b64	{%r2985, %r2986}, %rd2070;
	mov.b64	%rd2071, {%r2986, %r2985};
	add.s64 	%rd2072, %rd2071, %rd2051;
	xor.b64  	%rd2073, %rd2072, %rd2067;
	mov.b64	{%r2987, %r2988}, %rd2073;
	prmt.b32 	%r2989, %r2987, %r2988, %r1546;
	prmt.b32 	%r2990, %r2987, %r2988, %r1545;
	mov.b64	%rd2074, {%r2990, %r2989};
	add.s64 	%rd2075, %rd2069, %rd1383;
	add.s64 	%rd2076, %rd2075, %rd2074;
	xor.b64  	%rd2077, %rd2071, %rd2076;
	mov.b64	{%r2991, %r2992}, %rd2077;
	prmt.b32 	%r2993, %r2991, %r2992, %r1552;
	prmt.b32 	%r2994, %r2991, %r2992, %r1551;
	mov.b64	%rd2078, {%r2994, %r2993};
	add.s64 	%rd2079, %rd2072, %rd2078;
	xor.b64  	%rd2080, %rd2079, %rd2074;
	mov.b64	{%r1158, %r1159}, %rd2080;
	// inline asm
	shf.l.wrap.b32 %r1153, %r1159, %r1158, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1157, %r1158, %r1159, %r1536;
	// inline asm
	mov.b64	%rd2081, {%r1153, %r1157};
	add.s64 	%rd2082, %rd2025, %rd1402;
	add.s64 	%rd2083, %rd2082, %rd2034;
	xor.b64  	%rd2084, %rd2050, %rd2083;
	mov.b64	{%r2995, %r2996}, %rd2084;
	mov.b64	%rd2085, {%r2996, %r2995};
	add.s64 	%rd2086, %rd2065, %rd2085;
	xor.b64  	%rd2087, %rd2086, %rd2025;
	mov.b64	{%r2997, %r2998}, %rd2087;
	prmt.b32 	%r2999, %r2997, %r2998, %r1546;
	prmt.b32 	%r3000, %r2997, %r2998, %r1545;
	mov.b64	%rd2088, {%r3000, %r2999};
	add.s64 	%rd2089, %rd2083, %rd1393;
	add.s64 	%rd2090, %rd2089, %rd2088;
	xor.b64  	%rd2091, %rd2090, %rd2085;
	mov.b64	{%r3001, %r3002}, %rd2091;
	prmt.b32 	%r3003, %r3001, %r3002, %r1552;
	prmt.b32 	%r3004, %r3001, %r3002, %r1551;
	mov.b64	%rd2092, {%r3004, %r3003};
	add.s64 	%rd2093, %rd2092, %rd2086;
	xor.b64  	%rd2094, %rd2093, %rd2088;
	mov.b64	{%r1166, %r1167}, %rd2094;
	// inline asm
	shf.l.wrap.b32 %r1161, %r1167, %r1166, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1165, %r1166, %r1167, %r1536;
	// inline asm
	mov.b64	%rd2095, {%r1161, %r1165};
	add.s64 	%rd2096, %rd2039, %rd1392;
	add.s64 	%rd2097, %rd2096, %rd2048;
	xor.b64  	%rd2098, %rd2064, %rd2097;
	mov.b64	{%r3005, %r3006}, %rd2098;
	mov.b64	%rd2099, {%r3006, %r3005};
	add.s64 	%rd2100, %rd2099, %rd2023;
	xor.b64  	%rd2101, %rd2100, %rd2039;
	mov.b64	{%r3007, %r3008}, %rd2101;
	prmt.b32 	%r3009, %r3007, %r3008, %r1546;
	prmt.b32 	%r3010, %r3007, %r3008, %r1545;
	mov.b64	%rd2102, {%r3010, %r3009};
	add.s64 	%rd2103, %rd2097, %rd1391;
	add.s64 	%rd2104, %rd2103, %rd2102;
	xor.b64  	%rd2105, %rd2104, %rd2099;
	mov.b64	{%r3011, %r3012}, %rd2105;
	prmt.b32 	%r3013, %r3011, %r3012, %r1552;
	prmt.b32 	%r3014, %r3011, %r3012, %r1551;
	mov.b64	%rd2106, {%r3014, %r3013};
	add.s64 	%rd2107, %rd2106, %rd2100;
	xor.b64  	%rd2108, %rd2107, %rd2102;
	mov.b64	{%r1174, %r1175}, %rd2108;
	// inline asm
	shf.l.wrap.b32 %r1169, %r1175, %r1174, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1173, %r1174, %r1175, %r1536;
	// inline asm
	mov.b64	%rd2109, {%r1169, %r1173};
	add.s64 	%rd2110, %rd2053, %rd1382;
	add.s64 	%rd2111, %rd2110, %rd2062;
	xor.b64  	%rd2112, %rd2111, %rd2022;
	mov.b64	{%r3015, %r3016}, %rd2112;
	mov.b64	%rd2113, {%r3016, %r3015};
	add.s64 	%rd2114, %rd2113, %rd2037;
	xor.b64  	%rd2115, %rd2114, %rd2053;
	mov.b64	{%r3017, %r3018}, %rd2115;
	prmt.b32 	%r3019, %r3017, %r3018, %r1546;
	prmt.b32 	%r3020, %r3017, %r3018, %r1545;
	mov.b64	%rd2116, {%r3020, %r3019};
	add.s64 	%rd2117, %rd2111, %rd1388;
	add.s64 	%rd2118, %rd2117, %rd2116;
	xor.b64  	%rd2119, %rd2118, %rd2113;
	mov.b64	{%r3021, %r3022}, %rd2119;
	prmt.b32 	%r3023, %r3021, %r3022, %r1552;
	prmt.b32 	%r3024, %r3021, %r3022, %r1551;
	mov.b64	%rd2120, {%r3024, %r3023};
	add.s64 	%rd2121, %rd2120, %rd2114;
	xor.b64  	%rd2122, %rd2121, %rd2116;
	mov.b64	{%r1182, %r1183}, %rd2122;
	// inline asm
	shf.l.wrap.b32 %r1177, %r1183, %r1182, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1181, %r1182, %r1183, %r1536;
	// inline asm
	mov.b64	%rd2123, {%r1177, %r1181};
	add.s64 	%rd2124, %rd2076, %rd1394;
	add.s64 	%rd2125, %rd2124, %rd2095;
	xor.b64  	%rd2126, %rd2120, %rd2125;
	mov.b64	{%r3025, %r3026}, %rd2126;
	mov.b64	%rd2127, {%r3026, %r3025};
	add.s64 	%rd2128, %rd2127, %rd2107;
	xor.b64  	%rd2129, %rd2128, %rd2095;
	mov.b64	{%r3027, %r3028}, %rd2129;
	prmt.b32 	%r3029, %r3027, %r3028, %r1546;
	prmt.b32 	%r3030, %r3027, %r3028, %r1545;
	mov.b64	%rd2130, {%r3030, %r3029};
	add.s64 	%rd2131, %rd2125, %rd1385;
	add.s64 	%rd2132, %rd2131, %rd2130;
	xor.b64  	%rd2133, %rd2127, %rd2132;
	mov.b64	{%r3031, %r3032}, %rd2133;
	prmt.b32 	%r3033, %r3031, %r3032, %r1552;
	prmt.b32 	%r3034, %r3031, %r3032, %r1551;
	mov.b64	%rd2134, {%r3034, %r3033};
	add.s64 	%rd2135, %rd2134, %rd2128;
	xor.b64  	%rd2136, %rd2135, %rd2130;
	mov.b64	{%r1190, %r1191}, %rd2136;
	// inline asm
	shf.l.wrap.b32 %r1185, %r1191, %r1190, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1189, %r1190, %r1191, %r1536;
	// inline asm
	mov.b64	%rd2137, {%r1185, %r1189};
	add.s64 	%rd2138, %rd2090, %rd1384;
	add.s64 	%rd2139, %rd2138, %rd2109;
	xor.b64  	%rd2140, %rd2139, %rd2078;
	mov.b64	{%r3035, %r3036}, %rd2140;
	mov.b64	%rd2141, {%r3036, %r3035};
	add.s64 	%rd2142, %rd2141, %rd2121;
	xor.b64  	%rd2143, %rd2142, %rd2109;
	mov.b64	{%r3037, %r3038}, %rd2143;
	prmt.b32 	%r3039, %r3037, %r3038, %r1546;
	prmt.b32 	%r3040, %r3037, %r3038, %r1545;
	mov.b64	%rd2144, {%r3040, %r3039};
	add.s64 	%rd2145, %rd2139, %rd1381;
	add.s64 	%rd2146, %rd2145, %rd2144;
	xor.b64  	%rd2147, %rd2146, %rd2141;
	mov.b64	{%r3041, %r3042}, %rd2147;
	prmt.b32 	%r3043, %r3041, %r3042, %r1552;
	prmt.b32 	%r3044, %r3041, %r3042, %r1551;
	mov.b64	%rd2148, {%r3044, %r3043};
	add.s64 	%rd2149, %rd2148, %rd2142;
	xor.b64  	%rd2150, %rd2149, %rd2144;
	mov.b64	{%r1198, %r1199}, %rd2150;
	// inline asm
	shf.l.wrap.b32 %r1193, %r1199, %r1198, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1197, %r1198, %r1199, %r1536;
	// inline asm
	mov.b64	%rd2151, {%r1193, %r1197};
	add.s64 	%rd2152, %rd2104, %rd1387;
	add.s64 	%rd2153, %rd2152, %rd2123;
	xor.b64  	%rd2154, %rd2153, %rd2092;
	mov.b64	{%r3045, %r3046}, %rd2154;
	mov.b64	%rd2155, {%r3046, %r3045};
	add.s64 	%rd2156, %rd2155, %rd2079;
	xor.b64  	%rd2157, %rd2156, %rd2123;
	mov.b64	{%r3047, %r3048}, %rd2157;
	prmt.b32 	%r3049, %r3047, %r3048, %r1546;
	prmt.b32 	%r3050, %r3047, %r3048, %r1545;
	mov.b64	%rd2158, {%r3050, %r3049};
	add.s64 	%rd2159, %rd2153, %rd1380;
	add.s64 	%rd2160, %rd2159, %rd2158;
	xor.b64  	%rd2161, %rd2160, %rd2155;
	mov.b64	{%r3051, %r3052}, %rd2161;
	prmt.b32 	%r3053, %r3051, %r3052, %r1552;
	prmt.b32 	%r3054, %r3051, %r3052, %r1551;
	mov.b64	%rd2162, {%r3054, %r3053};
	add.s64 	%rd2163, %rd2162, %rd2156;
	xor.b64  	%rd2164, %rd2163, %rd2158;
	mov.b64	{%r1206, %r1207}, %rd2164;
	// inline asm
	shf.l.wrap.b32 %r1201, %r1207, %r1206, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1205, %r1206, %r1207, %r1536;
	// inline asm
	mov.b64	%rd2165, {%r1201, %r1205};
	add.s64 	%rd2166, %rd2081, %rd1386;
	add.s64 	%rd2167, %rd2166, %rd2118;
	xor.b64  	%rd2168, %rd2167, %rd2106;
	mov.b64	{%r3055, %r3056}, %rd2168;
	mov.b64	%rd2169, {%r3056, %r3055};
	add.s64 	%rd2170, %rd2169, %rd2093;
	xor.b64  	%rd2171, %rd2170, %rd2081;
	mov.b64	{%r3057, %r3058}, %rd2171;
	prmt.b32 	%r3059, %r3057, %r3058, %r1546;
	prmt.b32 	%r3060, %r3057, %r3058, %r1545;
	mov.b64	%rd2172, {%r3060, %r3059};
	add.s64 	%rd2173, %rd2167, %rd1389;
	add.s64 	%rd2174, %rd2173, %rd2172;
	xor.b64  	%rd2175, %rd2174, %rd2169;
	mov.b64	{%r3061, %r3062}, %rd2175;
	prmt.b32 	%r3063, %r3061, %r3062, %r1552;
	prmt.b32 	%r3064, %r3061, %r3062, %r1551;
	mov.b64	%rd2176, {%r3064, %r3063};
	add.s64 	%rd2177, %rd2176, %rd2170;
	xor.b64  	%rd2178, %rd2177, %rd2172;
	mov.b64	{%r1214, %r1215}, %rd2178;
	// inline asm
	shf.l.wrap.b32 %r1209, %r1215, %r1214, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1213, %r1214, %r1215, %r1536;
	// inline asm
	mov.b64	%rd2179, {%r1209, %r1213};
	add.s64 	%rd2180, %rd2132, %rd1391;
	add.s64 	%rd2181, %rd2180, %rd2179;
	xor.b64  	%rd2182, %rd2181, %rd2148;
	mov.b64	{%r3065, %r3066}, %rd2182;
	mov.b64	%rd2183, {%r3066, %r3065};
	add.s64 	%rd2184, %rd2183, %rd2163;
	xor.b64  	%rd2185, %rd2184, %rd2179;
	mov.b64	{%r3067, %r3068}, %rd2185;
	prmt.b32 	%r3069, %r3067, %r3068, %r1546;
	prmt.b32 	%r3070, %r3067, %r3068, %r1545;
	mov.b64	%rd2186, {%r3070, %r3069};
	add.s64 	%rd2187, %rd2181, %rd1389;
	add.s64 	%rd2188, %rd2187, %rd2186;
	xor.b64  	%rd2189, %rd2183, %rd2188;
	mov.b64	{%r3071, %r3072}, %rd2189;
	prmt.b32 	%r3073, %r3071, %r3072, %r1552;
	prmt.b32 	%r3074, %r3071, %r3072, %r1551;
	mov.b64	%rd2190, {%r3074, %r3073};
	add.s64 	%rd2191, %rd2184, %rd2190;
	xor.b64  	%rd2192, %rd2191, %rd2186;
	mov.b64	{%r1222, %r1223}, %rd2192;
	// inline asm
	shf.l.wrap.b32 %r1217, %r1223, %r1222, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1221, %r1222, %r1223, %r1536;
	// inline asm
	mov.b64	%rd2193, {%r1217, %r1221};
	add.s64 	%rd2194, %rd2137, %rd1385;
	add.s64 	%rd2195, %rd2194, %rd2146;
	xor.b64  	%rd2196, %rd2162, %rd2195;
	mov.b64	{%r3075, %r3076}, %rd2196;
	mov.b64	%rd2197, {%r3076, %r3075};
	add.s64 	%rd2198, %rd2177, %rd2197;
	xor.b64  	%rd2199, %rd2198, %rd2137;
	mov.b64	{%r3077, %r3078}, %rd2199;
	prmt.b32 	%r3079, %r3077, %r3078, %r1546;
	prmt.b32 	%r3080, %r3077, %r3078, %r1545;
	mov.b64	%rd2200, {%r3080, %r3079};
	add.s64 	%rd2201, %rd2195, %rd1392;
	add.s64 	%rd2202, %rd2201, %rd2200;
	xor.b64  	%rd2203, %rd2202, %rd2197;
	mov.b64	{%r3081, %r3082}, %rd2203;
	prmt.b32 	%r3083, %r3081, %r3082, %r1552;
	prmt.b32 	%r3084, %r3081, %r3082, %r1551;
	mov.b64	%rd2204, {%r3084, %r3083};
	add.s64 	%rd2205, %rd2204, %rd2198;
	xor.b64  	%rd2206, %rd2205, %rd2200;
	mov.b64	{%r1230, %r1231}, %rd2206;
	// inline asm
	shf.l.wrap.b32 %r1225, %r1231, %r1230, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1229, %r1230, %r1231, %r1536;
	// inline asm
	mov.b64	%rd2207, {%r1225, %r1229};
	add.s64 	%rd2208, %rd2151, %rd1390;
	add.s64 	%rd2209, %rd2208, %rd2160;
	xor.b64  	%rd2210, %rd2176, %rd2209;
	mov.b64	{%r3085, %r3086}, %rd2210;
	mov.b64	%rd2211, {%r3086, %r3085};
	add.s64 	%rd2212, %rd2211, %rd2135;
	xor.b64  	%rd2213, %rd2212, %rd2151;
	mov.b64	{%r3087, %r3088}, %rd2213;
	prmt.b32 	%r3089, %r3087, %r3088, %r1546;
	prmt.b32 	%r3090, %r3087, %r3088, %r1545;
	mov.b64	%rd2214, {%r3090, %r3089};
	add.s64 	%rd2215, %rd2209, %rd1402;
	add.s64 	%rd2216, %rd2215, %rd2214;
	xor.b64  	%rd2217, %rd2216, %rd2211;
	mov.b64	{%r3091, %r3092}, %rd2217;
	prmt.b32 	%r3093, %r3091, %r3092, %r1552;
	prmt.b32 	%r3094, %r3091, %r3092, %r1551;
	mov.b64	%rd2218, {%r3094, %r3093};
	add.s64 	%rd2219, %rd2218, %rd2212;
	xor.b64  	%rd2220, %rd2219, %rd2214;
	mov.b64	{%r1238, %r1239}, %rd2220;
	// inline asm
	shf.l.wrap.b32 %r1233, %r1239, %r1238, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1237, %r1238, %r1239, %r1536;
	// inline asm
	mov.b64	%rd2221, {%r1233, %r1237};
	add.s64 	%rd2222, %rd2165, %rd1381;
	add.s64 	%rd2223, %rd2222, %rd2174;
	xor.b64  	%rd2224, %rd2223, %rd2134;
	mov.b64	{%r3095, %r3096}, %rd2224;
	mov.b64	%rd2225, {%r3096, %r3095};
	add.s64 	%rd2226, %rd2225, %rd2149;
	xor.b64  	%rd2227, %rd2226, %rd2165;
	mov.b64	{%r3097, %r3098}, %rd2227;
	prmt.b32 	%r3099, %r3097, %r3098, %r1546;
	prmt.b32 	%r3100, %r3097, %r3098, %r1545;
	mov.b64	%rd2228, {%r3100, %r3099};
	add.s64 	%rd2229, %rd2223, %rd1387;
	add.s64 	%rd2230, %rd2229, %rd2228;
	xor.b64  	%rd2231, %rd2230, %rd2225;
	mov.b64	{%r3101, %r3102}, %rd2231;
	prmt.b32 	%r3103, %r3101, %r3102, %r1552;
	prmt.b32 	%r3104, %r3101, %r3102, %r1551;
	mov.b64	%rd2232, {%r3104, %r3103};
	add.s64 	%rd2233, %rd2232, %rd2226;
	xor.b64  	%rd2234, %rd2233, %rd2228;
	mov.b64	{%r1246, %r1247}, %rd2234;
	// inline asm
	shf.l.wrap.b32 %r1241, %r1247, %r1246, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1245, %r1246, %r1247, %r1536;
	// inline asm
	mov.b64	%rd2235, {%r1241, %r1245};
	add.s64 	%rd2236, %rd2188, %rd1383;
	add.s64 	%rd2237, %rd2236, %rd2207;
	xor.b64  	%rd2238, %rd2232, %rd2237;
	mov.b64	{%r3105, %r3106}, %rd2238;
	mov.b64	%rd2239, {%r3106, %r3105};
	add.s64 	%rd2240, %rd2239, %rd2219;
	xor.b64  	%rd2241, %rd2240, %rd2207;
	mov.b64	{%r3107, %r3108}, %rd2241;
	prmt.b32 	%r3109, %r3107, %r3108, %r1546;
	prmt.b32 	%r3110, %r3107, %r3108, %r1545;
	mov.b64	%rd2242, {%r3110, %r3109};
	add.s64 	%rd2243, %rd2237, %rd1394;
	add.s64 	%rd2244, %rd2243, %rd2242;
	xor.b64  	%rd2245, %rd2239, %rd2244;
	mov.b64	{%r3111, %r3112}, %rd2245;
	prmt.b32 	%r3113, %r3111, %r3112, %r1552;
	prmt.b32 	%r3114, %r3111, %r3112, %r1551;
	mov.b64	%rd2246, {%r3114, %r3113};
	add.s64 	%rd2247, %rd2246, %rd2240;
	xor.b64  	%rd2248, %rd2247, %rd2242;
	mov.b64	{%r1254, %r1255}, %rd2248;
	// inline asm
	shf.l.wrap.b32 %r1249, %r1255, %r1254, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1253, %r1254, %r1255, %r1536;
	// inline asm
	mov.b64	%rd2249, {%r1249, %r1253};
	add.s64 	%rd2250, %rd2202, %rd1393;
	add.s64 	%rd2251, %rd2250, %rd2221;
	xor.b64  	%rd2252, %rd2251, %rd2190;
	mov.b64	{%r3115, %r3116}, %rd2252;
	mov.b64	%rd2253, {%r3116, %r3115};
	add.s64 	%rd2254, %rd2253, %rd2233;
	xor.b64  	%rd2255, %rd2254, %rd2221;
	mov.b64	{%r3117, %r3118}, %rd2255;
	prmt.b32 	%r3119, %r3117, %r3118, %r1546;
	prmt.b32 	%r3120, %r3117, %r3118, %r1545;
	mov.b64	%rd2256, {%r3120, %r3119};
	add.s64 	%rd2257, %rd2251, %rd1382;
	add.s64 	%rd2258, %rd2257, %rd2256;
	xor.b64  	%rd2259, %rd2258, %rd2253;
	mov.b64	{%r3121, %r3122}, %rd2259;
	prmt.b32 	%r3123, %r3121, %r3122, %r1552;
	prmt.b32 	%r3124, %r3121, %r3122, %r1551;
	mov.b64	%rd2260, {%r3124, %r3123};
	add.s64 	%rd2261, %rd2260, %rd2254;
	xor.b64  	%rd2262, %rd2261, %rd2256;
	mov.b64	{%r1262, %r1263}, %rd2262;
	// inline asm
	shf.l.wrap.b32 %r1257, %r1263, %r1262, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1261, %r1262, %r1263, %r1536;
	// inline asm
	mov.b64	%rd2263, {%r1257, %r1261};
	add.s64 	%rd2264, %rd2216, %rd1386;
	add.s64 	%rd2265, %rd2264, %rd2235;
	xor.b64  	%rd2266, %rd2265, %rd2204;
	mov.b64	{%r3125, %r3126}, %rd2266;
	mov.b64	%rd2267, {%r3126, %r3125};
	add.s64 	%rd2268, %rd2267, %rd2191;
	xor.b64  	%rd2269, %rd2268, %rd2235;
	mov.b64	{%r3127, %r3128}, %rd2269;
	prmt.b32 	%r3129, %r3127, %r3128, %r1546;
	prmt.b32 	%r3130, %r3127, %r3128, %r1545;
	mov.b64	%rd2270, {%r3130, %r3129};
	add.s64 	%rd2271, %rd2265, %rd1384;
	add.s64 	%rd2272, %rd2271, %rd2270;
	xor.b64  	%rd2273, %rd2272, %rd2267;
	mov.b64	{%r3131, %r3132}, %rd2273;
	prmt.b32 	%r3133, %r3131, %r3132, %r1552;
	prmt.b32 	%r3134, %r3131, %r3132, %r1551;
	mov.b64	%rd2274, {%r3134, %r3133};
	add.s64 	%rd2275, %rd2274, %rd2268;
	xor.b64  	%rd2276, %rd2275, %rd2270;
	mov.b64	{%r1270, %r1271}, %rd2276;
	// inline asm
	shf.l.wrap.b32 %r1265, %r1271, %r1270, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1269, %r1270, %r1271, %r1536;
	// inline asm
	mov.b64	%rd2277, {%r1265, %r1269};
	add.s64 	%rd2278, %rd2193, %rd1380;
	add.s64 	%rd2279, %rd2278, %rd2230;
	xor.b64  	%rd2280, %rd2279, %rd2218;
	mov.b64	{%r3135, %r3136}, %rd2280;
	mov.b64	%rd2281, {%r3136, %r3135};
	add.s64 	%rd2282, %rd2281, %rd2205;
	xor.b64  	%rd2283, %rd2282, %rd2193;
	mov.b64	{%r3137, %r3138}, %rd2283;
	prmt.b32 	%r3139, %r3137, %r3138, %r1546;
	prmt.b32 	%r3140, %r3137, %r3138, %r1545;
	mov.b64	%rd2284, {%r3140, %r3139};
	add.s64 	%rd2285, %rd2279, %rd1388;
	add.s64 	%rd2286, %rd2285, %rd2284;
	xor.b64  	%rd2287, %rd2286, %rd2281;
	mov.b64	{%r3141, %r3142}, %rd2287;
	prmt.b32 	%r3143, %r3141, %r3142, %r1552;
	prmt.b32 	%r3144, %r3141, %r3142, %r1551;
	mov.b64	%rd2288, {%r3144, %r3143};
	add.s64 	%rd2289, %rd2288, %rd2282;
	xor.b64  	%rd2290, %rd2289, %rd2284;
	mov.b64	{%r1278, %r1279}, %rd2290;
	// inline asm
	shf.l.wrap.b32 %r1273, %r1279, %r1278, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1277, %r1278, %r1279, %r1536;
	// inline asm
	mov.b64	%rd2291, {%r1273, %r1277};
	add.s64 	%rd2292, %rd2244, %rd1384;
	add.s64 	%rd2293, %rd2292, %rd2291;
	xor.b64  	%rd2294, %rd2293, %rd2260;
	mov.b64	{%r3145, %r3146}, %rd2294;
	mov.b64	%rd2295, {%r3146, %r3145};
	add.s64 	%rd2296, %rd2295, %rd2275;
	xor.b64  	%rd2297, %rd2296, %rd2291;
	mov.b64	{%r3147, %r3148}, %rd2297;
	prmt.b32 	%r3149, %r3147, %r3148, %r1546;
	prmt.b32 	%r3150, %r3147, %r3148, %r1545;
	mov.b64	%rd2298, {%r3150, %r3149};
	add.s64 	%rd2299, %rd2293, %rd1393;
	add.s64 	%rd2300, %rd2299, %rd2298;
	xor.b64  	%rd2301, %rd2295, %rd2300;
	mov.b64	{%r3151, %r3152}, %rd2301;
	prmt.b32 	%r3153, %r3151, %r3152, %r1552;
	prmt.b32 	%r3154, %r3151, %r3152, %r1551;
	mov.b64	%rd2302, {%r3154, %r3153};
	add.s64 	%rd2303, %rd2296, %rd2302;
	xor.b64  	%rd2304, %rd2303, %rd2298;
	mov.b64	{%r1286, %r1287}, %rd2304;
	// inline asm
	shf.l.wrap.b32 %r1281, %r1287, %r1286, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1285, %r1286, %r1287, %r1536;
	// inline asm
	mov.b64	%rd2305, {%r1281, %r1285};
	add.s64 	%rd2306, %rd2249, %rd1392;
	add.s64 	%rd2307, %rd2306, %rd2258;
	xor.b64  	%rd2308, %rd2274, %rd2307;
	mov.b64	{%r3155, %r3156}, %rd2308;
	mov.b64	%rd2309, {%r3156, %r3155};
	add.s64 	%rd2310, %rd2289, %rd2309;
	xor.b64  	%rd2311, %rd2310, %rd2249;
	mov.b64	{%r3157, %r3158}, %rd2311;
	prmt.b32 	%r3159, %r3157, %r3158, %r1546;
	prmt.b32 	%r3160, %r3157, %r3158, %r1545;
	mov.b64	%rd2312, {%r3160, %r3159};
	add.s64 	%rd2313, %rd2307, %rd1387;
	add.s64 	%rd2314, %rd2313, %rd2312;
	xor.b64  	%rd2315, %rd2314, %rd2309;
	mov.b64	{%r3161, %r3162}, %rd2315;
	prmt.b32 	%r3163, %r3161, %r3162, %r1552;
	prmt.b32 	%r3164, %r3161, %r3162, %r1551;
	mov.b64	%rd2316, {%r3164, %r3163};
	add.s64 	%rd2317, %rd2316, %rd2310;
	xor.b64  	%rd2318, %rd2317, %rd2312;
	mov.b64	{%r1294, %r1295}, %rd2318;
	// inline asm
	shf.l.wrap.b32 %r1289, %r1295, %r1294, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1293, %r1294, %r1295, %r1536;
	// inline asm
	mov.b64	%rd2319, {%r1289, %r1293};
	add.s64 	%rd2320, %rd2263, %rd1389;
	add.s64 	%rd2321, %rd2320, %rd2272;
	xor.b64  	%rd2322, %rd2288, %rd2321;
	mov.b64	{%r3165, %r3166}, %rd2322;
	mov.b64	%rd2323, {%r3166, %r3165};
	add.s64 	%rd2324, %rd2323, %rd2247;
	xor.b64  	%rd2325, %rd2324, %rd2263;
	mov.b64	{%r3167, %r3168}, %rd2325;
	prmt.b32 	%r3169, %r3167, %r3168, %r1546;
	prmt.b32 	%r3170, %r3167, %r3168, %r1545;
	mov.b64	%rd2326, {%r3170, %r3169};
	add.s64 	%rd2327, %rd2321, %rd1381;
	add.s64 	%rd2328, %rd2327, %rd2326;
	xor.b64  	%rd2329, %rd2328, %rd2323;
	mov.b64	{%r3171, %r3172}, %rd2329;
	prmt.b32 	%r3173, %r3171, %r3172, %r1552;
	prmt.b32 	%r3174, %r3171, %r3172, %r1551;
	mov.b64	%rd2330, {%r3174, %r3173};
	add.s64 	%rd2331, %rd2330, %rd2324;
	xor.b64  	%rd2332, %rd2331, %rd2326;
	mov.b64	{%r1302, %r1303}, %rd2332;
	// inline asm
	shf.l.wrap.b32 %r1297, %r1303, %r1302, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1301, %r1302, %r1303, %r1536;
	// inline asm
	mov.b64	%rd2333, {%r1297, %r1301};
	add.s64 	%rd2334, %rd2277, %rd1394;
	add.s64 	%rd2335, %rd2334, %rd2286;
	xor.b64  	%rd2336, %rd2335, %rd2246;
	mov.b64	{%r3175, %r3176}, %rd2336;
	mov.b64	%rd2337, {%r3176, %r3175};
	add.s64 	%rd2338, %rd2337, %rd2261;
	xor.b64  	%rd2339, %rd2338, %rd2277;
	mov.b64	{%r3177, %r3178}, %rd2339;
	prmt.b32 	%r3179, %r3177, %r3178, %r1546;
	prmt.b32 	%r3180, %r3177, %r3178, %r1545;
	mov.b64	%rd2340, {%r3180, %r3179};
	add.s64 	%rd2341, %rd2335, %rd1386;
	add.s64 	%rd2342, %rd2341, %rd2340;
	xor.b64  	%rd2343, %rd2342, %rd2337;
	mov.b64	{%r3181, %r3182}, %rd2343;
	prmt.b32 	%r3183, %r3181, %r3182, %r1552;
	prmt.b32 	%r3184, %r3181, %r3182, %r1551;
	mov.b64	%rd2344, {%r3184, %r3183};
	add.s64 	%rd2345, %rd2344, %rd2338;
	xor.b64  	%rd2346, %rd2345, %rd2340;
	mov.b64	{%r1310, %r1311}, %rd2346;
	// inline asm
	shf.l.wrap.b32 %r1305, %r1311, %r1310, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1309, %r1310, %r1311, %r1536;
	// inline asm
	mov.b64	%rd2347, {%r1305, %r1309};
	add.s64 	%rd2348, %rd2300, %rd1390;
	add.s64 	%rd2349, %rd2348, %rd2319;
	xor.b64  	%rd2350, %rd2344, %rd2349;
	mov.b64	{%r3185, %r3186}, %rd2350;
	mov.b64	%rd2351, {%r3186, %r3185};
	add.s64 	%rd2352, %rd2351, %rd2331;
	xor.b64  	%rd2353, %rd2352, %rd2319;
	mov.b64	{%r3187, %r3188}, %rd2353;
	prmt.b32 	%r3189, %r3187, %r3188, %r1546;
	prmt.b32 	%r3190, %r3187, %r3188, %r1545;
	mov.b64	%rd2354, {%r3190, %r3189};
	add.s64 	%rd2355, %rd2349, %rd1380;
	add.s64 	%rd2356, %rd2355, %rd2354;
	xor.b64  	%rd2357, %rd2351, %rd2356;
	mov.b64	{%r3191, %r3192}, %rd2357;
	prmt.b32 	%r3193, %r3191, %r3192, %r1552;
	prmt.b32 	%r3194, %r3191, %r3192, %r1551;
	mov.b64	%rd2358, {%r3194, %r3193};
	add.s64 	%rd2359, %rd2358, %rd2352;
	xor.b64  	%rd2360, %rd2359, %rd2354;
	mov.b64	{%r1318, %r1319}, %rd2360;
	// inline asm
	shf.l.wrap.b32 %r1313, %r1319, %r1318, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1317, %r1318, %r1319, %r1536;
	// inline asm
	mov.b64	%rd2361, {%r1313, %r1317};
	add.s64 	%rd2362, %rd2314, %rd1391;
	add.s64 	%rd2363, %rd2362, %rd2333;
	xor.b64  	%rd2364, %rd2363, %rd2302;
	mov.b64	{%r3195, %r3196}, %rd2364;
	mov.b64	%rd2365, {%r3196, %r3195};
	add.s64 	%rd2366, %rd2365, %rd2345;
	xor.b64  	%rd2367, %rd2366, %rd2333;
	mov.b64	{%r3197, %r3198}, %rd2367;
	prmt.b32 	%r3199, %r3197, %r3198, %r1546;
	prmt.b32 	%r3200, %r3197, %r3198, %r1545;
	mov.b64	%rd2368, {%r3200, %r3199};
	add.s64 	%rd2369, %rd2363, %rd1385;
	add.s64 	%rd2370, %rd2369, %rd2368;
	xor.b64  	%rd2371, %rd2370, %rd2365;
	mov.b64	{%r3201, %r3202}, %rd2371;
	prmt.b32 	%r3203, %r3201, %r3202, %r1552;
	prmt.b32 	%r3204, %r3201, %r3202, %r1551;
	mov.b64	%rd2372, {%r3204, %r3203};
	add.s64 	%rd2373, %rd2372, %rd2366;
	xor.b64  	%rd2374, %rd2373, %rd2368;
	mov.b64	{%r1326, %r1327}, %rd2374;
	// inline asm
	shf.l.wrap.b32 %r1321, %r1327, %r1326, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1325, %r1326, %r1327, %r1536;
	// inline asm
	mov.b64	%rd2375, {%r1321, %r1325};
	add.s64 	%rd2376, %rd2328, %rd1402;
	add.s64 	%rd2377, %rd2376, %rd2347;
	xor.b64  	%rd2378, %rd2377, %rd2316;
	mov.b64	{%r3205, %r3206}, %rd2378;
	mov.b64	%rd2379, {%r3206, %r3205};
	add.s64 	%rd2380, %rd2379, %rd2303;
	xor.b64  	%rd2381, %rd2380, %rd2347;
	mov.b64	{%r3207, %r3208}, %rd2381;
	prmt.b32 	%r3209, %r3207, %r3208, %r1546;
	prmt.b32 	%r3210, %r3207, %r3208, %r1545;
	mov.b64	%rd2382, {%r3210, %r3209};
	add.s64 	%rd2383, %rd2377, %rd1382;
	add.s64 	%rd2384, %rd2383, %rd2382;
	xor.b64  	%rd2385, %rd2384, %rd2379;
	mov.b64	{%r3211, %r3212}, %rd2385;
	prmt.b32 	%r3213, %r3211, %r3212, %r1552;
	prmt.b32 	%r3214, %r3211, %r3212, %r1551;
	mov.b64	%rd2386, {%r3214, %r3213};
	add.s64 	%rd2387, %rd2386, %rd2380;
	xor.b64  	%rd2388, %rd2387, %rd2382;
	mov.b64	{%r1334, %r1335}, %rd2388;
	// inline asm
	shf.l.wrap.b32 %r1329, %r1335, %r1334, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1333, %r1334, %r1335, %r1536;
	// inline asm
	mov.b64	%rd2389, {%r1329, %r1333};
	add.s64 	%rd2390, %rd2305, %rd1388;
	add.s64 	%rd2391, %rd2390, %rd2342;
	xor.b64  	%rd2392, %rd2391, %rd2330;
	mov.b64	{%r3215, %r3216}, %rd2392;
	mov.b64	%rd2393, {%r3216, %r3215};
	add.s64 	%rd2394, %rd2393, %rd2317;
	xor.b64  	%rd2395, %rd2394, %rd2305;
	mov.b64	{%r3217, %r3218}, %rd2395;
	prmt.b32 	%r3219, %r3217, %r3218, %r1546;
	prmt.b32 	%r3220, %r3217, %r3218, %r1545;
	mov.b64	%rd2396, {%r3220, %r3219};
	add.s64 	%rd2397, %rd2391, %rd1383;
	add.s64 	%rd2398, %rd2397, %rd2396;
	xor.b64  	%rd2399, %rd2398, %rd2393;
	mov.b64	{%r3221, %r3222}, %rd2399;
	prmt.b32 	%r3223, %r3221, %r3222, %r1552;
	prmt.b32 	%r3224, %r3221, %r3222, %r1551;
	mov.b64	%rd2400, {%r3224, %r3223};
	add.s64 	%rd2401, %rd2400, %rd2394;
	xor.b64  	%rd2402, %rd2401, %rd2396;
	mov.b64	{%r1342, %r1343}, %rd2402;
	// inline asm
	shf.l.wrap.b32 %r1337, %r1343, %r1342, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1341, %r1342, %r1343, %r1536;
	// inline asm
	mov.b64	%rd2403, {%r1337, %r1341};
	add.s64 	%rd2404, %rd2356, %rd1388;
	add.s64 	%rd2405, %rd2404, %rd2403;
	xor.b64  	%rd2406, %rd2405, %rd2372;
	mov.b64	{%r3225, %r3226}, %rd2406;
	mov.b64	%rd2407, {%r3226, %r3225};
	add.s64 	%rd2408, %rd2407, %rd2387;
	xor.b64  	%rd2409, %rd2408, %rd2403;
	mov.b64	{%r3227, %r3228}, %rd2409;
	prmt.b32 	%r3229, %r3227, %r3228, %r1546;
	prmt.b32 	%r3230, %r3227, %r3228, %r1545;
	mov.b64	%rd2410, {%r3230, %r3229};
	add.s64 	%rd2411, %rd2405, %rd1380;
	add.s64 	%rd2412, %rd2411, %rd2410;
	xor.b64  	%rd2413, %rd2407, %rd2412;
	mov.b64	{%r3231, %r3232}, %rd2413;
	prmt.b32 	%r3233, %r3231, %r3232, %r1552;
	prmt.b32 	%r3234, %r3231, %r3232, %r1551;
	mov.b64	%rd2414, {%r3234, %r3233};
	add.s64 	%rd2415, %rd2408, %rd2414;
	xor.b64  	%rd2416, %rd2415, %rd2410;
	mov.b64	{%r1350, %r1351}, %rd2416;
	// inline asm
	shf.l.wrap.b32 %r1345, %r1351, %r1350, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1349, %r1350, %r1351, %r1536;
	// inline asm
	mov.b64	%rd2417, {%r1345, %r1349};
	add.s64 	%rd2418, %rd2361, %rd1386;
	add.s64 	%rd2419, %rd2418, %rd2370;
	xor.b64  	%rd2420, %rd2386, %rd2419;
	mov.b64	{%r3235, %r3236}, %rd2420;
	mov.b64	%rd2421, {%r3236, %r3235};
	add.s64 	%rd2422, %rd2401, %rd2421;
	xor.b64  	%rd2423, %rd2422, %rd2361;
	mov.b64	{%r3237, %r3238}, %rd2423;
	prmt.b32 	%r3239, %r3237, %r3238, %r1546;
	prmt.b32 	%r3240, %r3237, %r3238, %r1545;
	mov.b64	%rd2424, {%r3240, %r3239};
	add.s64 	%rd2425, %rd2419, %rd1382;
	add.s64 	%rd2426, %rd2425, %rd2424;
	xor.b64  	%rd2427, %rd2426, %rd2421;
	mov.b64	{%r3241, %r3242}, %rd2427;
	prmt.b32 	%r3243, %r3241, %r3242, %r1552;
	prmt.b32 	%r3244, %r3241, %r3242, %r1551;
	mov.b64	%rd2428, {%r3244, %r3243};
	add.s64 	%rd2429, %rd2428, %rd2422;
	xor.b64  	%rd2430, %rd2429, %rd2424;
	mov.b64	{%r1358, %r1359}, %rd2430;
	// inline asm
	shf.l.wrap.b32 %r1353, %r1359, %r1358, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1357, %r1358, %r1359, %r1536;
	// inline asm
	mov.b64	%rd2431, {%r1353, %r1357};
	add.s64 	%rd2432, %rd2375, %rd1385;
	add.s64 	%rd2433, %rd2432, %rd2384;
	xor.b64  	%rd2434, %rd2400, %rd2433;
	mov.b64	{%r3245, %r3246}, %rd2434;
	mov.b64	%rd2435, {%r3246, %r3245};
	add.s64 	%rd2436, %rd2435, %rd2359;
	xor.b64  	%rd2437, %rd2436, %rd2375;
	mov.b64	{%r3247, %r3248}, %rd2437;
	prmt.b32 	%r3249, %r3247, %r3248, %r1546;
	prmt.b32 	%r3250, %r3247, %r3248, %r1545;
	mov.b64	%rd2438, {%r3250, %r3249};
	add.s64 	%rd2439, %rd2433, %rd1384;
	add.s64 	%rd2440, %rd2439, %rd2438;
	xor.b64  	%rd2441, %rd2440, %rd2435;
	mov.b64	{%r3251, %r3252}, %rd2441;
	prmt.b32 	%r3253, %r3251, %r3252, %r1552;
	prmt.b32 	%r3254, %r3251, %r3252, %r1551;
	mov.b64	%rd2442, {%r3254, %r3253};
	add.s64 	%rd2443, %rd2442, %rd2436;
	xor.b64  	%rd2444, %rd2443, %rd2438;
	mov.b64	{%r1366, %r1367}, %rd2444;
	// inline asm
	shf.l.wrap.b32 %r1361, %r1367, %r1366, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1365, %r1366, %r1367, %r1536;
	// inline asm
	mov.b64	%rd2445, {%r1361, %r1365};
	add.s64 	%rd2446, %rd2389, %rd1402;
	add.s64 	%rd2447, %rd2446, %rd2398;
	xor.b64  	%rd2448, %rd2447, %rd2358;
	mov.b64	{%r3255, %r3256}, %rd2448;
	mov.b64	%rd2449, {%r3256, %r3255};
	add.s64 	%rd2450, %rd2449, %rd2373;
	xor.b64  	%rd2451, %rd2450, %rd2389;
	mov.b64	{%r3257, %r3258}, %rd2451;
	prmt.b32 	%r3259, %r3257, %r3258, %r1546;
	prmt.b32 	%r3260, %r3257, %r3258, %r1545;
	mov.b64	%rd2452, {%r3260, %r3259};
	add.s64 	%rd2453, %rd2447, %rd1383;
	add.s64 	%rd2454, %rd2453, %rd2452;
	xor.b64  	%rd2455, %rd2454, %rd2449;
	mov.b64	{%r3261, %r3262}, %rd2455;
	prmt.b32 	%r3263, %r3261, %r3262, %r1552;
	prmt.b32 	%r3264, %r3261, %r3262, %r1551;
	mov.b64	%rd2456, {%r3264, %r3263};
	add.s64 	%rd2457, %rd2456, %rd2450;
	xor.b64  	%rd2458, %rd2457, %rd2452;
	mov.b64	{%r1374, %r1375}, %rd2458;
	// inline asm
	shf.l.wrap.b32 %r1369, %r1375, %r1374, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1373, %r1374, %r1375, %r1536;
	// inline asm
	mov.b64	%rd2459, {%r1369, %r1373};
	add.s64 	%rd2460, %rd2412, %rd1393;
	add.s64 	%rd2461, %rd2460, %rd2431;
	xor.b64  	%rd2462, %rd2456, %rd2461;
	mov.b64	{%r3265, %r3266}, %rd2462;
	mov.b64	%rd2463, {%r3266, %r3265};
	add.s64 	%rd2464, %rd2463, %rd2443;
	xor.b64  	%rd2465, %rd2464, %rd2431;
	mov.b64	{%r3267, %r3268}, %rd2465;
	prmt.b32 	%r3269, %r3267, %r3268, %r1546;
	prmt.b32 	%r3270, %r3267, %r3268, %r1545;
	mov.b64	%rd2466, {%r3270, %r3269};
	add.s64 	%rd2467, %rd2461, %rd1389;
	add.s64 	%rd2468, %rd2467, %rd2466;
	xor.b64  	%rd2469, %rd2463, %rd2468;
	mov.b64	{%r3271, %r3272}, %rd2469;
	prmt.b32 	%r3273, %r3271, %r3272, %r1552;
	prmt.b32 	%r3274, %r3271, %r3272, %r1551;
	mov.b64	%rd2470, {%r3274, %r3273};
	add.s64 	%rd2471, %rd2470, %rd2464;
	xor.b64  	%rd2472, %rd2471, %rd2466;
	mov.b64	{%r1382, %r1383}, %rd2472;
	// inline asm
	shf.l.wrap.b32 %r1377, %r1383, %r1382, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1381, %r1382, %r1383, %r1536;
	// inline asm
	mov.b64	%rd2473, {%r1377, %r1381};
	add.s64 	%rd2474, %rd2426, %rd1387;
	add.s64 	%rd2475, %rd2474, %rd2445;
	xor.b64  	%rd2476, %rd2475, %rd2414;
	mov.b64	{%r3275, %r3276}, %rd2476;
	mov.b64	%rd2477, {%r3276, %r3275};
	add.s64 	%rd2478, %rd2477, %rd2457;
	xor.b64  	%rd2479, %rd2478, %rd2445;
	mov.b64	{%r3277, %r3278}, %rd2479;
	prmt.b32 	%r3279, %r3277, %r3278, %r1546;
	prmt.b32 	%r3280, %r3277, %r3278, %r1545;
	mov.b64	%rd2480, {%r3280, %r3279};
	add.s64 	%rd2481, %rd2475, %rd1392;
	add.s64 	%rd2482, %rd2481, %rd2480;
	xor.b64  	%rd2483, %rd2482, %rd2477;
	mov.b64	{%r3281, %r3282}, %rd2483;
	prmt.b32 	%r3283, %r3281, %r3282, %r1552;
	prmt.b32 	%r3284, %r3281, %r3282, %r1551;
	mov.b64	%rd2484, {%r3284, %r3283};
	add.s64 	%rd2485, %rd2484, %rd2478;
	xor.b64  	%rd2486, %rd2485, %rd2480;
	mov.b64	{%r1390, %r1391}, %rd2486;
	// inline asm
	shf.l.wrap.b32 %r1385, %r1391, %r1390, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1389, %r1390, %r1391, %r1536;
	// inline asm
	mov.b64	%rd2487, {%r1385, %r1389};
	add.s64 	%rd2488, %rd2440, %rd1381;
	add.s64 	%rd2489, %rd2488, %rd2459;
	xor.b64  	%rd2490, %rd2489, %rd2428;
	mov.b64	{%r3285, %r3286}, %rd2490;
	mov.b64	%rd2491, {%r3286, %r3285};
	add.s64 	%rd2492, %rd2491, %rd2415;
	xor.b64  	%rd2493, %rd2492, %rd2459;
	mov.b64	{%r3287, %r3288}, %rd2493;
	prmt.b32 	%r3289, %r3287, %r3288, %r1546;
	prmt.b32 	%r3290, %r3287, %r3288, %r1545;
	mov.b64	%rd2494, {%r3290, %r3289};
	add.s64 	%rd2495, %rd2489, %rd1390;
	add.s64 	%rd2496, %rd2495, %rd2494;
	xor.b64  	%rd2497, %rd2496, %rd2491;
	mov.b64	{%r3291, %r3292}, %rd2497;
	prmt.b32 	%r3293, %r3291, %r3292, %r1552;
	prmt.b32 	%r3294, %r3291, %r3292, %r1551;
	mov.b64	%rd2498, {%r3294, %r3293};
	add.s64 	%rd2499, %rd2498, %rd2492;
	xor.b64  	%rd2500, %rd2499, %rd2494;
	mov.b64	{%r1398, %r1399}, %rd2500;
	// inline asm
	shf.l.wrap.b32 %r1393, %r1399, %r1398, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1397, %r1398, %r1399, %r1536;
	// inline asm
	mov.b64	%rd2501, {%r1393, %r1397};
	add.s64 	%rd2502, %rd2417, %rd1391;
	add.s64 	%rd2503, %rd2502, %rd2454;
	xor.b64  	%rd2504, %rd2503, %rd2442;
	mov.b64	{%r3295, %r3296}, %rd2504;
	mov.b64	%rd2505, {%r3296, %r3295};
	add.s64 	%rd2506, %rd2505, %rd2429;
	xor.b64  	%rd2507, %rd2506, %rd2417;
	mov.b64	{%r3297, %r3298}, %rd2507;
	prmt.b32 	%r3299, %r3297, %r3298, %r1546;
	prmt.b32 	%r3300, %r3297, %r3298, %r1545;
	mov.b64	%rd2508, {%r3300, %r3299};
	add.s64 	%rd2509, %rd2503, %rd1394;
	add.s64 	%rd2510, %rd2509, %rd2508;
	xor.b64  	%rd2511, %rd2510, %rd2505;
	mov.b64	{%r3301, %r3302}, %rd2511;
	prmt.b32 	%r3303, %r3301, %r3302, %r1552;
	prmt.b32 	%r3304, %r3301, %r3302, %r1551;
	mov.b64	%rd2512, {%r3304, %r3303};
	add.s64 	%rd2513, %rd2512, %rd2506;
	xor.b64  	%rd2514, %rd2513, %rd2508;
	mov.b64	{%r1406, %r1407}, %rd2514;
	// inline asm
	shf.l.wrap.b32 %r1401, %r1407, %r1406, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1405, %r1406, %r1407, %r1536;
	// inline asm
	mov.b64	%rd2515, {%r1401, %r1405};
	add.s64 	%rd2516, %rd2468, %rd1394;
	add.s64 	%rd2517, %rd2516, %rd2515;
	xor.b64  	%rd2518, %rd2517, %rd2484;
	mov.b64	{%r3305, %r3306}, %rd2518;
	mov.b64	%rd2519, {%r3306, %r3305};
	add.s64 	%rd2520, %rd2519, %rd2499;
	xor.b64  	%rd2521, %rd2520, %rd2515;
	mov.b64	{%r3307, %r3308}, %rd2521;
	prmt.b32 	%r3309, %r3307, %r3308, %r1546;
	prmt.b32 	%r3310, %r3307, %r3308, %r1545;
	mov.b64	%rd2522, {%r3310, %r3309};
	add.s64 	%rd2523, %rd2517, %rd1402;
	add.s64 	%rd2524, %rd2523, %rd2522;
	xor.b64  	%rd2525, %rd2519, %rd2524;
	mov.b64	{%r3311, %r3312}, %rd2525;
	prmt.b32 	%r3313, %r3311, %r3312, %r1552;
	prmt.b32 	%r3314, %r3311, %r3312, %r1551;
	mov.b64	%rd2526, {%r3314, %r3313};
	add.s64 	%rd2527, %rd2520, %rd2526;
	xor.b64  	%rd2528, %rd2527, %rd2522;
	mov.b64	{%r1414, %r1415}, %rd2528;
	// inline asm
	shf.l.wrap.b32 %r1409, %r1415, %r1414, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1413, %r1414, %r1415, %r1536;
	// inline asm
	mov.b64	%rd2529, {%r1409, %r1413};
	add.s64 	%rd2530, %rd2473, %rd1380;
	add.s64 	%rd2531, %rd2530, %rd2482;
	xor.b64  	%rd2532, %rd2498, %rd2531;
	mov.b64	{%r3315, %r3316}, %rd2532;
	mov.b64	%rd2533, {%r3316, %r3315};
	add.s64 	%rd2534, %rd2513, %rd2533;
	xor.b64  	%rd2535, %rd2534, %rd2473;
	mov.b64	{%r3317, %r3318}, %rd2535;
	prmt.b32 	%r3319, %r3317, %r3318, %r1546;
	prmt.b32 	%r3320, %r3317, %r3318, %r1545;
	mov.b64	%rd2536, {%r3320, %r3319};
	add.s64 	%rd2537, %rd2531, %rd1381;
	add.s64 	%rd2538, %rd2537, %rd2536;
	xor.b64  	%rd2539, %rd2538, %rd2533;
	mov.b64	{%r3321, %r3322}, %rd2539;
	prmt.b32 	%r3323, %r3321, %r3322, %r1552;
	prmt.b32 	%r3324, %r3321, %r3322, %r1551;
	mov.b64	%rd2540, {%r3324, %r3323};
	add.s64 	%rd2541, %rd2540, %rd2534;
	xor.b64  	%rd2542, %rd2541, %rd2536;
	mov.b64	{%r1422, %r1423}, %rd2542;
	// inline asm
	shf.l.wrap.b32 %r1417, %r1423, %r1422, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1421, %r1422, %r1423, %r1536;
	// inline asm
	mov.b64	%rd2543, {%r1417, %r1421};
	add.s64 	%rd2544, %rd2487, %rd1382;
	add.s64 	%rd2545, %rd2544, %rd2496;
	xor.b64  	%rd2546, %rd2512, %rd2545;
	mov.b64	{%r3325, %r3326}, %rd2546;
	mov.b64	%rd2547, {%r3326, %r3325};
	add.s64 	%rd2548, %rd2547, %rd2471;
	xor.b64  	%rd2549, %rd2548, %rd2487;
	mov.b64	{%r3327, %r3328}, %rd2549;
	prmt.b32 	%r3329, %r3327, %r3328, %r1546;
	prmt.b32 	%r3330, %r3327, %r3328, %r1545;
	mov.b64	%rd2550, {%r3330, %r3329};
	add.s64 	%rd2551, %rd2545, %rd1383;
	add.s64 	%rd2552, %rd2551, %rd2550;
	xor.b64  	%rd2553, %rd2552, %rd2547;
	mov.b64	{%r3331, %r3332}, %rd2553;
	prmt.b32 	%r3333, %r3331, %r3332, %r1552;
	prmt.b32 	%r3334, %r3331, %r3332, %r1551;
	mov.b64	%rd2554, {%r3334, %r3333};
	add.s64 	%rd2555, %rd2554, %rd2548;
	xor.b64  	%rd2556, %rd2555, %rd2550;
	mov.b64	{%r1430, %r1431}, %rd2556;
	// inline asm
	shf.l.wrap.b32 %r1425, %r1431, %r1430, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1429, %r1430, %r1431, %r1536;
	// inline asm
	mov.b64	%rd2557, {%r1425, %r1429};
	add.s64 	%rd2558, %rd2501, %rd1384;
	add.s64 	%rd2559, %rd2558, %rd2510;
	xor.b64  	%rd2560, %rd2559, %rd2470;
	mov.b64	{%r3335, %r3336}, %rd2560;
	mov.b64	%rd2561, {%r3336, %r3335};
	add.s64 	%rd2562, %rd2561, %rd2485;
	xor.b64  	%rd2563, %rd2562, %rd2501;
	mov.b64	{%r3337, %r3338}, %rd2563;
	prmt.b32 	%r3339, %r3337, %r3338, %r1546;
	prmt.b32 	%r3340, %r3337, %r3338, %r1545;
	mov.b64	%rd2564, {%r3340, %r3339};
	add.s64 	%rd2565, %rd2559, %rd1385;
	add.s64 	%rd2566, %rd2565, %rd2564;
	xor.b64  	%rd2567, %rd2566, %rd2561;
	mov.b64	{%r3341, %r3342}, %rd2567;
	prmt.b32 	%r3343, %r3341, %r3342, %r1552;
	prmt.b32 	%r3344, %r3341, %r3342, %r1551;
	mov.b64	%rd2568, {%r3344, %r3343};
	add.s64 	%rd2569, %rd2568, %rd2562;
	xor.b64  	%rd2570, %rd2569, %rd2564;
	mov.b64	{%r1438, %r1439}, %rd2570;
	// inline asm
	shf.l.wrap.b32 %r1433, %r1439, %r1438, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1437, %r1438, %r1439, %r1536;
	// inline asm
	mov.b64	%rd2571, {%r1433, %r1437};
	add.s64 	%rd2572, %rd2524, %rd1386;
	add.s64 	%rd2573, %rd2572, %rd2543;
	xor.b64  	%rd2574, %rd2568, %rd2573;
	mov.b64	{%r3345, %r3346}, %rd2574;
	mov.b64	%rd2575, {%r3346, %r3345};
	add.s64 	%rd2576, %rd2575, %rd2555;
	xor.b64  	%rd2577, %rd2576, %rd2543;
	mov.b64	{%r3347, %r3348}, %rd2577;
	prmt.b32 	%r3349, %r3347, %r3348, %r1546;
	prmt.b32 	%r3350, %r3347, %r3348, %r1545;
	mov.b64	%rd2578, {%r3350, %r3349};
	add.s64 	%rd2579, %rd2573, %rd1387;
	add.s64 	%rd2580, %rd2579, %rd2578;
	xor.b64  	%rd2581, %rd2575, %rd2580;
	mov.b64	{%r3351, %r3352}, %rd2581;
	prmt.b32 	%r3353, %r3351, %r3352, %r1552;
	prmt.b32 	%r3354, %r3351, %r3352, %r1551;
	mov.b64	%rd2582, {%r3354, %r3353};
	add.s64 	%rd2583, %rd2582, %rd2576;
	xor.b64  	%rd2584, %rd2583, %rd2578;
	mov.b64	{%r1446, %r1447}, %rd2584;
	// inline asm
	shf.l.wrap.b32 %r1441, %r1447, %r1446, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1445, %r1446, %r1447, %r1536;
	// inline asm
	mov.b64	%rd2585, {%r1441, %r1445};
	add.s64 	%rd2586, %rd2538, %rd1388;
	add.s64 	%rd2587, %rd2586, %rd2557;
	xor.b64  	%rd2588, %rd2587, %rd2526;
	mov.b64	{%r3355, %r3356}, %rd2588;
	mov.b64	%rd2589, {%r3356, %r3355};
	add.s64 	%rd2590, %rd2589, %rd2569;
	xor.b64  	%rd2591, %rd2590, %rd2557;
	mov.b64	{%r3357, %r3358}, %rd2591;
	prmt.b32 	%r3359, %r3357, %r3358, %r1546;
	prmt.b32 	%r3360, %r3357, %r3358, %r1545;
	mov.b64	%rd2592, {%r3360, %r3359};
	add.s64 	%rd2593, %rd2587, %rd1389;
	add.s64 	%rd2594, %rd2593, %rd2592;
	xor.b64  	%rd2595, %rd2594, %rd2589;
	mov.b64	{%r3361, %r3362}, %rd2595;
	prmt.b32 	%r3363, %r3361, %r3362, %r1552;
	prmt.b32 	%r3364, %r3361, %r3362, %r1551;
	mov.b64	%rd2596, {%r3364, %r3363};
	add.s64 	%rd2597, %rd2596, %rd2590;
	xor.b64  	%rd2598, %rd2597, %rd2592;
	mov.b64	{%r1454, %r1455}, %rd2598;
	// inline asm
	shf.l.wrap.b32 %r1449, %r1455, %r1454, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1453, %r1454, %r1455, %r1536;
	// inline asm
	mov.b64	%rd2599, {%r1449, %r1453};
	add.s64 	%rd2600, %rd2552, %rd1390;
	add.s64 	%rd2601, %rd2600, %rd2571;
	xor.b64  	%rd2602, %rd2601, %rd2540;
	mov.b64	{%r3365, %r3366}, %rd2602;
	mov.b64	%rd2603, {%r3366, %r3365};
	add.s64 	%rd2604, %rd2603, %rd2527;
	xor.b64  	%rd2605, %rd2604, %rd2571;
	mov.b64	{%r3367, %r3368}, %rd2605;
	prmt.b32 	%r3369, %r3367, %r3368, %r1546;
	prmt.b32 	%r3370, %r3367, %r3368, %r1545;
	mov.b64	%rd2606, {%r3370, %r3369};
	add.s64 	%rd2607, %rd2601, %rd1391;
	add.s64 	%rd2608, %rd2607, %rd2606;
	xor.b64  	%rd2609, %rd2608, %rd2603;
	mov.b64	{%r3371, %r3372}, %rd2609;
	prmt.b32 	%r3373, %r3371, %r3372, %r1552;
	prmt.b32 	%r3374, %r3371, %r3372, %r1551;
	mov.b64	%rd2610, {%r3374, %r3373};
	add.s64 	%rd2611, %rd2610, %rd2604;
	xor.b64  	%rd2612, %rd2611, %rd2606;
	mov.b64	{%r1462, %r1463}, %rd2612;
	// inline asm
	shf.l.wrap.b32 %r1457, %r1463, %r1462, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1461, %r1462, %r1463, %r1536;
	// inline asm
	mov.b64	%rd2613, {%r1457, %r1461};
	add.s64 	%rd2614, %rd2529, %rd1392;
	add.s64 	%rd2615, %rd2614, %rd2566;
	xor.b64  	%rd2616, %rd2615, %rd2554;
	mov.b64	{%r3375, %r3376}, %rd2616;
	mov.b64	%rd2617, {%r3376, %r3375};
	add.s64 	%rd2618, %rd2617, %rd2541;
	xor.b64  	%rd2619, %rd2618, %rd2529;
	mov.b64	{%r3377, %r3378}, %rd2619;
	prmt.b32 	%r3379, %r3377, %r3378, %r1546;
	prmt.b32 	%r3380, %r3377, %r3378, %r1545;
	mov.b64	%rd2620, {%r3380, %r3379};
	add.s64 	%rd2621, %rd2615, %rd1393;
	add.s64 	%rd2622, %rd2621, %rd2620;
	xor.b64  	%rd2623, %rd2622, %rd2617;
	mov.b64	{%r3381, %r3382}, %rd2623;
	prmt.b32 	%r3383, %r3381, %r3382, %r1552;
	prmt.b32 	%r3384, %r3381, %r3382, %r1551;
	mov.b64	%rd2624, {%r3384, %r3383};
	add.s64 	%rd2625, %rd2624, %rd2618;
	xor.b64  	%rd2626, %rd2625, %rd2620;
	mov.b64	{%r1470, %r1471}, %rd2626;
	// inline asm
	shf.l.wrap.b32 %r1465, %r1471, %r1470, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1469, %r1470, %r1471, %r1536;
	// inline asm
	mov.b64	%rd2627, {%r1465, %r1469};
	add.s64 	%rd2628, %rd2580, %rd1392;
	add.s64 	%rd2629, %rd2628, %rd2627;
	xor.b64  	%rd2630, %rd2629, %rd2596;
	mov.b64	{%r3385, %r3386}, %rd2630;
	mov.b64	%rd2631, {%r3386, %r3385};
	add.s64 	%rd2632, %rd2631, %rd2611;
	xor.b64  	%rd2633, %rd2632, %rd2627;
	mov.b64	{%r3387, %r3388}, %rd2633;
	prmt.b32 	%r3389, %r3387, %r3388, %r1546;
	prmt.b32 	%r3390, %r3387, %r3388, %r1545;
	mov.b64	%rd2634, {%r3390, %r3389};
	add.s64 	%rd2635, %rd2629, %rd1388;
	add.s64 	%rd2636, %rd2635, %rd2634;
	xor.b64  	%rd2637, %rd2631, %rd2636;
	mov.b64	{%r3391, %r3392}, %rd2637;
	prmt.b32 	%r3393, %r3391, %r3392, %r1552;
	prmt.b32 	%r3394, %r3391, %r3392, %r1551;
	mov.b64	%rd2638, {%r3394, %r3393};
	add.s64 	%rd2639, %rd2632, %rd2638;
	xor.b64  	%rd2640, %rd2639, %rd2634;
	mov.b64	{%r1478, %r1479}, %rd2640;
	// inline asm
	shf.l.wrap.b32 %r1473, %r1479, %r1478, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1477, %r1478, %r1479, %r1536;
	// inline asm
	mov.b64	%rd2641, {%r1473, %r1477};
	add.s64 	%rd2642, %rd2585, %rd1382;
	add.s64 	%rd2643, %rd2642, %rd2594;
	xor.b64  	%rd2644, %rd2610, %rd2643;
	mov.b64	{%r3395, %r3396}, %rd2644;
	mov.b64	%rd2645, {%r3396, %r3395};
	add.s64 	%rd2646, %rd2625, %rd2645;
	xor.b64  	%rd2647, %rd2646, %rd2585;
	mov.b64	{%r3397, %r3398}, %rd2647;
	prmt.b32 	%r3399, %r3397, %r3398, %r1546;
	prmt.b32 	%r3400, %r3397, %r3398, %r1545;
	mov.b64	%rd2648, {%r3400, %r3399};
	add.s64 	%rd2649, %rd2643, %rd1386;
	add.s64 	%rd2650, %rd2649, %rd2648;
	xor.b64  	%rd2651, %rd2650, %rd2645;
	mov.b64	{%r3401, %r3402}, %rd2651;
	prmt.b32 	%r3403, %r3401, %r3402, %r1552;
	prmt.b32 	%r3404, %r3401, %r3402, %r1551;
	mov.b64	%rd2652, {%r3404, %r3403};
	add.s64 	%rd2653, %rd2652, %rd2646;
	xor.b64  	%rd2654, %rd2653, %rd2648;
	mov.b64	{%r1486, %r1487}, %rd2654;
	// inline asm
	shf.l.wrap.b32 %r1481, %r1487, %r1486, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1485, %r1486, %r1487, %r1536;
	// inline asm
	mov.b64	%rd2655, {%r1481, %r1485};
	add.s64 	%rd2656, %rd2599, %rd1387;
	add.s64 	%rd2657, %rd2656, %rd2608;
	xor.b64  	%rd2658, %rd2624, %rd2657;
	mov.b64	{%r3405, %r3406}, %rd2658;
	mov.b64	%rd2659, {%r3406, %r3405};
	add.s64 	%rd2660, %rd2659, %rd2583;
	xor.b64  	%rd2661, %rd2660, %rd2599;
	mov.b64	{%r3407, %r3408}, %rd2661;
	prmt.b32 	%r3409, %r3407, %r3408, %r1546;
	prmt.b32 	%r3410, %r3407, %r3408, %r1545;
	mov.b64	%rd2662, {%r3410, %r3409};
	add.s64 	%rd2663, %rd2657, %rd1393;
	add.s64 	%rd2664, %rd2663, %rd2662;
	xor.b64  	%rd2665, %rd2664, %rd2659;
	mov.b64	{%r3411, %r3412}, %rd2665;
	prmt.b32 	%r3413, %r3411, %r3412, %r1552;
	prmt.b32 	%r3414, %r3411, %r3412, %r1551;
	mov.b64	%rd2666, {%r3414, %r3413};
	add.s64 	%rd2667, %rd2666, %rd2660;
	xor.b64  	%rd2668, %rd2667, %rd2662;
	mov.b64	{%r1494, %r1495}, %rd2668;
	// inline asm
	shf.l.wrap.b32 %r1489, %r1495, %r1494, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1493, %r1494, %r1495, %r1536;
	// inline asm
	mov.b64	%rd2669, {%r1489, %r1493};
	add.s64 	%rd2670, %rd2613, %rd1391;
	add.s64 	%rd2671, %rd2670, %rd2622;
	xor.b64  	%rd2672, %rd2671, %rd2582;
	mov.b64	{%r3415, %r3416}, %rd2672;
	mov.b64	%rd2673, {%r3416, %r3415};
	add.s64 	%rd2674, %rd2673, %rd2597;
	xor.b64  	%rd2675, %rd2674, %rd2613;
	mov.b64	{%r3417, %r3418}, %rd2675;
	prmt.b32 	%r3419, %r3417, %r3418, %r1546;
	prmt.b32 	%r3420, %r3417, %r3418, %r1545;
	mov.b64	%rd2676, {%r3420, %r3419};
	add.s64 	%rd2677, %rd2671, %rd1384;
	add.s64 	%rd2678, %rd2677, %rd2676;
	xor.b64  	%rd2679, %rd2678, %rd2673;
	mov.b64	{%r3421, %r3422}, %rd2679;
	prmt.b32 	%r3423, %r3421, %r3422, %r1552;
	prmt.b32 	%r3424, %r3421, %r3422, %r1551;
	mov.b64	%rd2680, {%r3424, %r3423};
	add.s64 	%rd2681, %rd2680, %rd2674;
	xor.b64  	%rd2682, %rd2681, %rd2676;
	mov.b64	{%r1502, %r1503}, %rd2682;
	// inline asm
	shf.l.wrap.b32 %r1497, %r1503, %r1502, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1501, %r1502, %r1503, %r1536;
	// inline asm
	mov.b64	%rd2683, {%r1497, %r1501};
	add.s64 	%rd2684, %rd2636, %rd1402;
	add.s64 	%rd2685, %rd2684, %rd2655;
	xor.b64  	%rd2686, %rd2680, %rd2685;
	mov.b64	{%r3425, %r3426}, %rd2686;
	mov.b64	%rd2687, {%r3426, %r3425};
	add.s64 	%rd2688, %rd2687, %rd2667;
	xor.b64  	%rd2689, %rd2688, %rd2655;
	mov.b64	{%r3427, %r3428}, %rd2689;
	prmt.b32 	%r3429, %r3427, %r3428, %r1546;
	prmt.b32 	%r3430, %r3427, %r3428, %r1545;
	mov.b64	%rd2690, {%r3430, %r3429};
	add.s64 	%rd2691, %rd2685, %rd1390;
	add.s64 	%rd2692, %rd2691, %rd2690;
	xor.b64  	%rd2693, %rd2687, %rd2692;
	mov.b64	{%r3431, %r3432}, %rd2693;
	prmt.b32 	%r3433, %r3431, %r3432, %r1552;
	prmt.b32 	%r3434, %r3431, %r3432, %r1551;
	mov.b64	%rd2694, {%r3434, %r3433};
	add.s64 	%rd2695, %rd2694, %rd2688;
	xor.b64  	%rd2696, %rd2695, %rd2690;
	mov.b64	{%r1510, %r1511}, %rd2696;
	// inline asm
	shf.l.wrap.b32 %r1505, %r1511, %r1510, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1509, %r1510, %r1511, %r1536;
	// inline asm
	add.s64 	%rd2697, %rd2650, %rd1394;
	add.s64 	%rd2698, %rd2697, %rd2669;
	xor.b64  	%rd2699, %rd2698, %rd2638;
	mov.b64	{%r3435, %r3436}, %rd2699;
	mov.b64	%rd2700, {%r3436, %r3435};
	add.s64 	%rd2701, %rd2700, %rd2681;
	xor.b64  	%rd2702, %rd2701, %rd2669;
	mov.b64	{%r3437, %r3438}, %rd2702;
	prmt.b32 	%r3439, %r3437, %r3438, %r1546;
	prmt.b32 	%r3440, %r3437, %r3438, %r1545;
	mov.b64	%rd2703, {%r3440, %r3439};
	add.s64 	%rd2704, %rd2698, %rd1380;
	add.s64 	%rd2705, %rd2704, %rd2703;
	xor.b64  	%rd2706, %rd2705, %rd2700;
	mov.b64	{%r3441, %r3442}, %rd2706;
	prmt.b32 	%r3443, %r3441, %r3442, %r1552;
	prmt.b32 	%r3444, %r3441, %r3442, %r1551;
	mov.b64	%rd2707, {%r3444, %r3443};
	add.s64 	%rd2708, %rd2707, %rd2701;
	xor.b64  	%rd2709, %rd2708, %rd2703;
	mov.b64	{%r1518, %r1519}, %rd2709;
	// inline asm
	shf.l.wrap.b32 %r1513, %r1519, %r1518, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1517, %r1518, %r1519, %r1536;
	// inline asm
	add.s64 	%rd2710, %rd2664, %rd1389;
	add.s64 	%rd2711, %rd2710, %rd2683;
	xor.b64  	%rd2712, %rd2711, %rd2652;
	mov.b64	{%r3445, %r3446}, %rd2712;
	mov.b64	%rd2713, {%r3446, %r3445};
	add.s64 	%rd2714, %rd2713, %rd2639;
	xor.b64  	%rd2715, %rd2714, %rd2683;
	mov.b64	{%r3447, %r3448}, %rd2715;
	prmt.b32 	%r3449, %r3447, %r3448, %r1546;
	prmt.b32 	%r3450, %r3447, %r3448, %r1545;
	mov.b64	%rd2716, {%r3450, %r3449};
	add.s64 	%rd2717, %rd2711, %rd1385;
	add.s64 	%rd2718, %rd2717, %rd2716;
	xor.b64  	%rd2719, %rd2718, %rd2713;
	mov.b64	{%r3451, %r3452}, %rd2719;
	prmt.b32 	%r3453, %r3451, %r3452, %r1552;
	prmt.b32 	%r3454, %r3451, %r3452, %r1551;
	mov.b64	%rd2720, {%r3454, %r3453};
	add.s64 	%rd2721, %rd2720, %rd2714;
	xor.b64  	%rd2722, %rd2721, %rd2716;
	mov.b64	{%r1526, %r1527}, %rd2722;
	// inline asm
	shf.l.wrap.b32 %r1521, %r1527, %r1526, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1525, %r1526, %r1527, %r1536;
	// inline asm
	mov.b64	%rd2723, {%r1521, %r1525};
	add.s64 	%rd2724, %rd2641, %rd1383;
	add.s64 	%rd2725, %rd2724, %rd2678;
	xor.b64  	%rd2726, %rd2725, %rd2666;
	mov.b64	{%r3455, %r3456}, %rd2726;
	mov.b64	%rd2727, {%r3456, %r3455};
	add.s64 	%rd2728, %rd2727, %rd2653;
	xor.b64  	%rd2729, %rd2728, %rd2641;
	mov.b64	{%r3457, %r3458}, %rd2729;
	prmt.b32 	%r3459, %r3457, %r3458, %r1546;
	prmt.b32 	%r3460, %r3457, %r3458, %r1545;
	mov.b64	%rd2730, {%r3460, %r3459};
	add.s64 	%rd2731, %rd2725, %rd1381;
	add.s64 	%rd2732, %rd2731, %rd2730;
	xor.b64  	%rd2733, %rd2732, %rd2727;
	mov.b64	{%r3461, %r3462}, %rd2733;
	prmt.b32 	%r3463, %r3461, %r3462, %r1552;
	prmt.b32 	%r3464, %r3461, %r3462, %r1551;
	mov.b64	%rd2734, {%r3464, %r3463};
	add.s64 	%rd2735, %rd2734, %rd2728;
	xor.b64  	%rd2736, %rd2735, %rd2730;
	mov.b64	{%r1534, %r1535}, %rd2736;
	// inline asm
	shf.l.wrap.b32 %r1529, %r1535, %r1534, %r1536;
	// inline asm
	// inline asm
	shf.l.wrap.b32 %r1533, %r1534, %r1535, %r1536;
	// inline asm
	xor.b64  	%rd2737, %rd2694, %rd1379;
	xor.b64  	%rd2738, %rd2737, %rd2723;
	mov.b64	{%r3465, %r3466}, %rd2738;
	setp.ne.s32	%p1, %r3466, 0;
	@%p1 bra 	BB13_2;

	ld.param.u64 	%rd2740, [_Z30blake2b_512_double_block_benchILj256EEvPyPKvy_param_0];
	cvta.to.global.u64 	%rd2739, %rd2740;
	st.global.u64 	[%rd2739], %rd1;

BB13_2:
	ret;
}


